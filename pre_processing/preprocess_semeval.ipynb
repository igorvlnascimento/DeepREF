{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('open-nre': conda)"
  },
  "interpreter": {
   "hash": "66f85137aa5e2a5257f5c4f2ba28a89ccc01c866862ea6aef1209745d61fbdc8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.1.json: 139kB [00:00, 10.3MB/s]                    \n",
      "2021-06-17 19:51:22 INFO: Downloading default packages for language: en (English)...\n",
      "2021-06-17 19:51:22,607 - stanza - INFO - Downloading default packages for language: en (English)...\n",
      "2021-06-17 19:51:23 INFO: File exists: /home/igor/stanza_resources/en/default.zip.\n",
      "2021-06-17 19:51:23,302 - stanza - INFO - File exists: /home/igor/stanza_resources/en/default.zip.\n",
      "2021-06-17 19:51:27 INFO: Finished downloading models and saved to /home/igor/stanza_resources.\n",
      "2021-06-17 19:51:27,238 - stanza - INFO - Finished downloading models and saved to /home/igor/stanza_resources.\n",
      "[nltk_data] Downloading package stopwords to /home/igor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "#import sys\n",
    "#sys.path.append('../')\n",
    "import nltk\n",
    "import stanza\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "from ast import literal_eval # to convert the string tuple form to an actual tuple\n",
    "RESOURCE_PATH = \"/mnt/projects/OpenNRE/benchmark/raw_semeval\"\n",
    "OUTPUT_PATH = \"/mnt/projects/OpenNRE/benchmark/semeval2010\"\n",
    "outdir = 'original/'\n",
    "indir = 'original/'\n",
    "outdir1 = 'entity_blinding/'\n",
    "outdir2 = 'punct_stop_digit/'\n",
    "outdir3 = 'punct_digit/'\n",
    "outdir4 = 'ner_blinding/'\n",
    "def res(path): return os.path.join(RESOURCE_PATH, path)\n",
    "def out(path): return os.path.join(OUTPUT_PATH, path)\n",
    "from opennre.dataset.converters.converter_semeval2010 import get_dataset_dataframe, write_dataframe,\\\n",
    "read_dataframe, check_equality_of_written_and_read_df, write_into_txt, write_relations_json\n",
    "from opennre.dataset.preprocess import replace_with_concept, replace_digit_punctuation_stop_word,\\\n",
    "get_entity_positions_and_replacement_sentence, preprocess, replace_ner\n",
    "\n",
    "def makedir(outdir, out):\n",
    "    if not os.path.exists(out(outdir)):\n",
    "        os.makedirs(out(outdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-06-17 19:51:27 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-06-17 19:51:27,624 - stanza - INFO - Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2021-06-17 19:51:27 INFO: Use device: gpu\n",
      "2021-06-17 19:51:27,661 - stanza - INFO - Use device: gpu\n",
      "2021-06-17 19:51:27 INFO: Loading: tokenize\n",
      "2021-06-17 19:51:27,662 - stanza - INFO - Loading: tokenize\n",
      "2021-06-17 19:51:29 INFO: Done loading processors!\n",
      "2021-06-17 19:51:29,768 - stanza - INFO - Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors=\"tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 8000/8000 [00:55<00:00, 144.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = get_dataset_dataframe(nlp, res('TRAIN_FILE.TXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2717/2717 [00:18<00:00, 143.27it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = get_dataset_dataframe(nlp, res('TEST_FILE_FULL.TXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                      original_sentence         e1  \\\n",
       "0     The most common audits were about waste and re...     audits   \n",
       "1               The company fabricates plastic chairs .    company   \n",
       "2     The school master teaches the lesson with a st...     master   \n",
       "3     The suspect dumped the dead body into a local ...       body   \n",
       "4     Avian influenza is an infectious disease of bi...  influenza   \n",
       "...                                                 ...        ...   \n",
       "2712  After seating all the idols, which itself take...       king   \n",
       "2713  The minister attributed the slow production of...  materials   \n",
       "2714  The umbrella frame is provided with a movable ...   umbrella   \n",
       "2715  Manos: The Hands of Fate is a low-budget horro...       film   \n",
       "2716  A few days before the service, Tom Burris had ...     casket   \n",
       "\n",
       "              e2              relation_type  \\\n",
       "0          waste       Message-Topic(e1,e2)   \n",
       "1         chairs    Product-Producer(e2,e1)   \n",
       "2          stick   Instrument-Agency(e2,e1)   \n",
       "3      reservoir  Entity-Destination(e1,e2)   \n",
       "4          virus        Cause-Effect(e2,e1)   \n",
       "...          ...                        ...   \n",
       "2712       broom   Instrument-Agency(e2,e1)   \n",
       "2713  industries    Product-Producer(e1,e2)   \n",
       "2714       frame     Component-Whole(e2,e1)   \n",
       "2715    salesman    Product-Producer(e1,e2)   \n",
       "2716        ring  Entity-Destination(e2,e1)   \n",
       "\n",
       "                                               metadata  \\\n",
       "0     {'e1': {'word': 'audits', 'word_index': [(3, 3...   \n",
       "1     {'e1': {'word': 'company', 'word_index': [(1, ...   \n",
       "2     {'e1': {'word': 'master', 'word_index': [(2, 2...   \n",
       "3     {'e1': {'word': 'body', 'word_index': [(5, 5)]...   \n",
       "4     {'e1': {'word': 'influenza', 'word_index': [(1...   \n",
       "...                                                 ...   \n",
       "2712  {'e1': {'word': 'king', 'word_index': [(13, 13...   \n",
       "2713  {'e1': {'word': 'materials', 'word_index': [(8...   \n",
       "2714  {'e1': {'word': 'umbrella', 'word_index': [(1,...   \n",
       "2715  {'e1': {'word': 'film', 'word_index': [(12, 12...   \n",
       "2716  {'e1': {'word': 'casket', 'word_index': [(14, ...   \n",
       "\n",
       "                                     tokenized_sentence  \n",
       "0     The most common audits were about waste and re...  \n",
       "1               The company fabricates plastic chairs .  \n",
       "2     The school master teaches the lesson with a st...  \n",
       "3     The suspect dumped the dead body into a local ...  \n",
       "4     Avian influenza is an infectious disease of bi...  \n",
       "...                                                 ...  \n",
       "2712  After seating all the idols , which itself tak...  \n",
       "2713  The minister attributed the slow production of...  \n",
       "2714  The umbrella frame is provided with a movable ...  \n",
       "2715  Manos : The Hands of Fate is a low - budget ho...  \n",
       "2716  A few days before the service , Tom Burris had...  \n",
       "\n",
       "[2717 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_sentence</th>\n      <th>e1</th>\n      <th>e2</th>\n      <th>relation_type</th>\n      <th>metadata</th>\n      <th>tokenized_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The most common audits were about waste and re...</td>\n      <td>audits</td>\n      <td>waste</td>\n      <td>Message-Topic(e1,e2)</td>\n      <td>{'e1': {'word': 'audits', 'word_index': [(3, 3...</td>\n      <td>The most common audits were about waste and re...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The company fabricates plastic chairs .</td>\n      <td>company</td>\n      <td>chairs</td>\n      <td>Product-Producer(e2,e1)</td>\n      <td>{'e1': {'word': 'company', 'word_index': [(1, ...</td>\n      <td>The company fabricates plastic chairs .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The school master teaches the lesson with a st...</td>\n      <td>master</td>\n      <td>stick</td>\n      <td>Instrument-Agency(e2,e1)</td>\n      <td>{'e1': {'word': 'master', 'word_index': [(2, 2...</td>\n      <td>The school master teaches the lesson with a st...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The suspect dumped the dead body into a local ...</td>\n      <td>body</td>\n      <td>reservoir</td>\n      <td>Entity-Destination(e1,e2)</td>\n      <td>{'e1': {'word': 'body', 'word_index': [(5, 5)]...</td>\n      <td>The suspect dumped the dead body into a local ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Avian influenza is an infectious disease of bi...</td>\n      <td>influenza</td>\n      <td>virus</td>\n      <td>Cause-Effect(e2,e1)</td>\n      <td>{'e1': {'word': 'influenza', 'word_index': [(1...</td>\n      <td>Avian influenza is an infectious disease of bi...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2712</th>\n      <td>After seating all the idols, which itself take...</td>\n      <td>king</td>\n      <td>broom</td>\n      <td>Instrument-Agency(e2,e1)</td>\n      <td>{'e1': {'word': 'king', 'word_index': [(13, 13...</td>\n      <td>After seating all the idols , which itself tak...</td>\n    </tr>\n    <tr>\n      <th>2713</th>\n      <td>The minister attributed the slow production of...</td>\n      <td>materials</td>\n      <td>industries</td>\n      <td>Product-Producer(e1,e2)</td>\n      <td>{'e1': {'word': 'materials', 'word_index': [(8...</td>\n      <td>The minister attributed the slow production of...</td>\n    </tr>\n    <tr>\n      <th>2714</th>\n      <td>The umbrella frame is provided with a movable ...</td>\n      <td>umbrella</td>\n      <td>frame</td>\n      <td>Component-Whole(e2,e1)</td>\n      <td>{'e1': {'word': 'umbrella', 'word_index': [(1,...</td>\n      <td>The umbrella frame is provided with a movable ...</td>\n    </tr>\n    <tr>\n      <th>2715</th>\n      <td>Manos: The Hands of Fate is a low-budget horro...</td>\n      <td>film</td>\n      <td>salesman</td>\n      <td>Product-Producer(e1,e2)</td>\n      <td>{'e1': {'word': 'film', 'word_index': [(12, 12...</td>\n      <td>Manos : The Hands of Fate is a low - budget ho...</td>\n    </tr>\n    <tr>\n      <th>2716</th>\n      <td>A few days before the service, Tom Burris had ...</td>\n      <td>casket</td>\n      <td>ring</td>\n      <td>Entity-Destination(e2,e1)</td>\n      <td>{'e1': {'word': 'casket', 'word_index': [(14, ...</td>\n      <td>A few days before the service , Tom Burris had...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2717 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_entity_rows(df):\n",
    "    empty_entity_rows = []\n",
    "    def find_empty_entity_number(row):\n",
    "        metadata = row.metadata\n",
    "        e1 = metadata['e1']['word_index']\n",
    "        e2 = metadata['e2']['word_index']\n",
    "        if not e1 or not e2:\n",
    "            empty_entity_rows.append(row.row_num)\n",
    "    temp_df = df.copy()\n",
    "    temp_df.insert(0, 'row_num', range(0, len(temp_df)))\n",
    "    temp_df.apply(find_empty_entity_number, axis=1)\n",
    "    return empty_entity_rows\n",
    "\n",
    "def get_empty_rows_array(empty_entity_rows, df):\n",
    "    empty_rows_array = []\n",
    "    for index in empty_entity_rows:\n",
    "        e1 = df.iloc[index].e1\n",
    "        e2 = df.iloc[index].e2\n",
    "        original_sentence = df.iloc[index].original_sentence\n",
    "        tokenized_sentence = df.iloc[index].tokenized_sentence\n",
    "        metadata = df.iloc[index].metadata\n",
    "        empty_rows_array.append([index, original_sentence, e1, e2, metadata, tokenized_sentence])\n",
    "    new_df = pd.DataFrame(data=empty_rows_array,    # values\n",
    "             columns=['index_original', 'original_sentence' , 'e1', 'e2', 'metadata', 'tokenized_sentence'])\n",
    "    return empty_rows_array, new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_vals(df):\n",
    "    empty_entity_rows = get_empty_entity_rows(df)\n",
    "    empty_rows_array, new_df = get_empty_rows_array(empty_entity_rows, df)\n",
    "    return empty_rows_array, new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([],\n",
       " Empty DataFrame\n",
       " Columns: [index_original, original_sentence, e1, e2, metadata, tokenized_sentence]\n",
       " Index: [])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "get_empty_vals(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([],\n",
       " Empty DataFrame\n",
       " Columns: [index_original, original_sentence, e1, e2, metadata, tokenized_sentence]\n",
       " Index: [])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "get_empty_vals(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out(outdir)):\n",
    "    os.makedirs(out(outdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframe(df_train, out(outdir + 'semeval2010_train_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = read_dataframe(out(outdir + 'semeval2010_train_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# The first checks with the pd.equals method, and the other does a manual checking per column\n",
    "check_equality_of_written_and_read_df(df_train, df_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframe(df_test, out(outdir + 'semeval2010_val_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy = read_dataframe(out(outdir + 'semeval2010_val_original.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "check_equality_of_written_and_read_df(df_test, df_test_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 8613.69it/s]\n"
     ]
    }
   ],
   "source": [
    "write_into_txt(df_train, out(outdir + 'semeval2010_train_original.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2717/2717 [00:00<00:00, 8332.75it/s]\n"
     ]
    }
   ],
   "source": [
    "write_into_txt(df_test, out(outdir + 'semeval2010_val_original.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-06-17 19:52:48 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-06-17 19:52:48,587 - stanza - INFO - Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-06-17 19:52:48 INFO: Use device: gpu\n",
      "2021-06-17 19:52:48,589 - stanza - INFO - Use device: gpu\n",
      "2021-06-17 19:52:48 INFO: Loading: tokenize\n",
      "2021-06-17 19:52:48,590 - stanza - INFO - Loading: tokenize\n",
      "2021-06-17 19:52:48 INFO: Loading: ner\n",
      "2021-06-17 19:52:48,597 - stanza - INFO - Loading: ner\n",
      "2021-06-17 19:52:49 INFO: Done loading processors!\n",
      "2021-06-17 19:52:49,096 - stanza - INFO - Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors=\"tokenize,ner\", tokenize_no_ssplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " original_dataframe_names = ['semeval2010_train', 'semeval2010_val']\n",
    " makedir(outdir1, out)\n",
    " makedir(outdir2, out)\n",
    " makedir(outdir3, out)\n",
    " makedir(outdir4, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "place_by': 'O', 'insert': None}, '13:15': {'replace_by': 'O', 'insert': None}, '18:24': {'replace_by': 'O', 'insert': None}, '27:30': {'replace_by': 'O', 'insert': None}, '33:38': {'replace_by': 'O', 'insert': None}, '41:44': {'replace_by': 'O', 'insert': None}, '47:47': {'replace_by': 'O', 'insert': None}, '50:57': {'replace_by': 'O', 'insert': None}, '60:63': {'replace_by': 'O', 'insert': None}, '66:70': {'replace_by': 'O', 'insert': None}, '73:74': {'replace_by': 'S-CARDINAL', 'insert': None}, '77:78': {'replace_by': 'O', 'insert': None}, '81:89': {'replace_by': 'O', 'insert': None}, '92:92': {'replace_by': 'O', 'insert': None}, '95:99': {'replace_by': 'O', 'insert': None}, '102:104': {'replace_by': 'O', 'insert': None}, '107:109': {'replace_by': 'O', 'insert': None}, '112:115': {'replace_by': 'O', 'insert': None}, '118:118': {'replace_by': 'O', 'insert': None}, '121:129': {'replace_by': 'O', 'insert': None}, '132:134': {'replace_by': 'O', 'insert': None}, '137:139': {'replace_by': 'O', 'insert': None}, '142:143': {'replace_by': 'O', 'insert': None}, '146:148': {'replace_by': 'O', 'insert': None}, '151:158': {'replace_by': 'O', 'insert': None}, '161:165': {'replace_by': 'O', 'insert': None}, '168:168': {'replace_by': 'O', 'insert': None}, '0:0:ESTART': {'replace_by': None, 'insert': 'ESTART'}, '3:3:EEND': {'replace_by': None, 'insert': 'EEND'}, '5:5:EOTHERSTART': {'replace_by': None, 'insert': 'EOTHERSTART'}, '11:11:EOTHEREND': {'replace_by': None, 'insert': 'EOTHEREND'}}\n",
      "sorted_positions: ['0:0:ESTART', '0:2', '3:3:EEND', '5:5:EOTHERSTART', '5:10', '11:11:EOTHEREND', '13:15', '18:24', '27:30', '33:38', '41:44', '47:47', '50:57', '60:63', '66:70', '73:74', '77:78', '81:89', '92:92', '95:99', '102:104', '107:109', '112:115', '118:118', '121:129', '132:134', '137:139', '142:143', '146:148', '151:158', '161:165', '168:168']\n",
      "['Eventually', ',', 'the', 'player', 'descends', 'into', 'a', 'maze', 'of', 'catacombs', 'and', 'a', '\"', 'fake', 'Y2', '\"', '.']\n",
      "ner_repl_dict: {'0:9': {'replace_by': 'O', 'insert': None}, '12:12': {'replace_by': 'O', 'insert': None}, '15:17': {'replace_by': 'O', 'insert': None}, '20:25': {'replace_by': 'O', 'insert': None}, '28:35': {'replace_by': 'O', 'insert': None}, '38:41': {'replace_by': 'O', 'insert': None}, '44:44': {'replace_by': 'O', 'insert': None}, '47:50': {'replace_by': 'O', 'insert': None}, '53:54': {'replace_by': 'O', 'insert': None}, '57:65': {'replace_by': 'O', 'insert': None}, '68:70': {'replace_by': 'O', 'insert': None}, '73:73': {'replace_by': 'O', 'insert': None}, '76:76': {'replace_by': 'O', 'insert': None}, '79:82': {'replace_by': 'O', 'insert': None}, '85:86': {'replace_by': 'O', 'insert': None}, '89:89': {'replace_by': 'O', 'insert': None}, '92:92': {'replace_by': 'O', 'insert': None}, '0:0:EEITHERSTART': {'replace_by': None, 'insert': 'EEITHERSTART'}, '10:10:EEITHEREND': {'replace_by': None, 'insert': 'EEITHEREND'}}\n",
      "sorted_positions: ['0:0:EEITHERSTART', '0:9', '10:10:EEITHEREND', '12:12', '15:17', '20:25', '28:35', '38:41', '44:44', '47:50', '53:54', '57:65', '68:70', '73:73', '76:76', '79:82', '85:86', '89:89', '92:92']\n",
      "['More', 'weapons', 'are', 'being', 'delivered', 'to', 'the', 'French', 'navy', '.']\n",
      "ner_repl_dict: {'0:3': {'replace_by': 'O', 'insert': None}, '6:12': {'replace_by': 'O', 'insert': None}, '15:17': {'replace_by': 'O', 'insert': None}, '20:24': {'replace_by': 'O', 'insert': None}, '27:35': {'replace_by': 'O', 'insert': None}, '38:39': {'replace_by': 'O', 'insert': None}, '42:44': {'replace_by': 'O', 'insert': None}, '47:52': {'replace_by': 'S-NORP', 'insert': None}, '55:58': {'replace_by': 'O', 'insert': None}, '61:61': {'replace_by': 'O', 'insert': None}, '0:0:ESTART': {'replace_by': None, 'insert': 'ESTART'}, '4:4:EEND': {'replace_by': None, 'insert': 'EEND'}, '6:6:EOTHERSTART': {'replace_by': None, 'insert': 'EOTHERSTART'}, '13:13:EOTHEREND': {'replace_by': None, 'insert': 'EOTHEREND'}}\n",
      "sorted_positions: ['0:0:ESTART', '0:3', '4:4:EEND', '6:6:EOTHERSTART', '6:12', '13:13:EOTHEREND', '15:17', '20:24', '27:35', '38:39', '42:44', '47:52', '55:58', '61:61']\n",
      "['And', 'from', 'the', 'slope', 'of', 'the', 'line', 'we', 'compute', 'the', 'amount', 'of', 'time', 'which', 'has', 'passed', 'since', 'the', 'pool', 'of', 'matter', 'became', 'separated', 'into', 'individual', 'objects', '.']\n",
      "ner_repl_dict: {'0:2': {'replace_by': 'O', 'insert': None}, '5:8': {'replace_by': 'O', 'insert': None}, '11:13': {'replace_by': 'O', 'insert': None}, '16:20': {'replace_by': 'O', 'insert': None}, '23:24': {'replace_by': 'O', 'insert': None}, '27:29': {'replace_by': 'O', 'insert': None}, '32:35': {'replace_by': 'O', 'insert': None}, '38:39': {'replace_by': 'O', 'insert': None}, '42:48': {'replace_by': 'O', 'insert': None}, '51:53': {'replace_by': 'O', 'insert': None}, '56:61': {'replace_by': 'O', 'insert': None}, '64:65': {'replace_by': 'O', 'insert': None}, '68:71': {'replace_by': 'O', 'insert': None}, '74:78': {'replace_by': 'O', 'insert': None}, '81:83': {'replace_by': 'O', 'insert': None}, '86:91': {'replace_by': 'O', 'insert': None}, '94:98': {'replace_by': 'O', 'insert': None}, '101:103': {'replace_by': 'O', 'insert': None}, '106:109': {'replace_by': 'O', 'insert': None}, '112:113': {'replace_by': 'O', 'insert': None}, '116:121': {'replace_by': 'O', 'insert': None}, '124:129': {'replace_by': 'O', 'insert': None}, '132:140': {'replace_by': 'O', 'insert': None}, '143:146': {'replace_by': 'O', 'insert': None}, '149:158': {'replace_by': 'O', 'insert': None}, '161:167': {'replace_by': 'O', 'insert': None}, '170:170': {'replace_by': 'O', 'insert': None}, '3:3:ESTART': {'replace_by': None, 'insert': 'ESTART'}, '4:4:EEND': {'replace_by': None, 'insert': 'EEND'}, '5:5:EOTHERSTART': {'replace_by': None, 'insert': 'EOTHERSTART'}, '9:9:EOTHEREND': {'replace_by': None, 'insert': 'EOTHEREND'}}\n",
      "sorted_positions: ['0:2', '3:3:ESTART', '4:4:EEND', '5:5:EOTHERSTART', '5:8', '9:9:EOTHEREND', '11:13', '16:20', '23:24', '27:29', '32:35', '38:39', '42:48', '51:53', '56:61', '64:65', '68:71', '74:78', '81:83', '86:91', '94:98', '101:103', '106:109', '112:113', '116:121', '124:129', '132:140', '143:146', '149:158', '161:167', '170:170']\n",
      "['Then', 'there', \"'s\", 'a', 'bloat', 'of', 'hippopotamuses', ',', 'which', 'is', 'really', 'quite', 'insulting', 'to', 'full', '-', 'figured', 'hippopotami', ',', 'who', 'can', 'look', 'quite', 'nice', 'if', 'they', 'put', 'flowers', 'in', 'their', 'ears', 'and', 'become', '(', 'old', 'joke', 'alert', ')', 'hippypotamuses', '.']\n",
      "ner_repl_dict: {'0:3': {'replace_by': 'O', 'insert': None}, '6:10': {'replace_by': 'O', 'insert': None}, '13:14': {'replace_by': 'O', 'insert': None}, '17:17': {'replace_by': 'O', 'insert': None}, '20:24': {'replace_by': 'O', 'insert': None}, '27:28': {'replace_by': 'O', 'insert': None}, '31:44': {'replace_by': 'O', 'insert': None}, '47:47': {'replace_by': 'O', 'insert': None}, '50:54': {'replace_by': 'O', 'insert': None}, '57:58': {'replace_by': 'O', 'insert': None}, '61:66': {'replace_by': 'O', 'insert': None}, '69:73': {'replace_by': 'O', 'insert': None}, '76:84': {'replace_by': 'O', 'insert': None}, '87:88': {'replace_by': 'O', 'insert': None}, '91:94': {'replace_by': 'O', 'insert': None}, '97:97': {'replace_by': 'O', 'insert': None}, '100:106': {'replace_by': 'O', 'insert': None}, '109:119': {'replace_by': 'O', 'insert': None}, '122:122': {'replace_by': 'O', 'insert': None}, '125:127': {'replace_by': 'O', 'insert': None}, '130:132': {'replace_by': 'O', 'insert': None}, '135:138': {'replace_by': 'O', 'insert': None}, '141:145': {'replace_by': 'O', 'insert': None}, '148:151': {'replace_by': 'O', 'insert': None}, '154:155': {'replace_by': 'O', 'insert': None}, '158:161': {'replace_by': 'O', 'insert': None}, '164:166': {'replace_by': 'O', 'insert': None}, '169:175': {'replace_by': 'O', 'insert': None}, '178:179': {'replace_by': 'O', 'insert': None}, '182:186': {'replace_by': 'O', 'insert': None}, '189:192': {'replace_by': 'O', 'insert': None}, '195:197': {'replace_by': 'O', 'insert': None}, '200:205': {'replace_by': 'O', 'insert': None}, '208:208': {'replace_by': 'O', 'insert': None}, '211:213': {'replace_by': 'O', 'insert': None}, '216:219': {'replace_by': 'O', 'insert': None}, '222:226': {'replace_by': 'O', 'insert': None}, '229:229': {'replace_by': 'O', 'insert': None}, '232:245': {'replace_by': 'O', 'insert': None}, '248:248': {'replace_by': 'O', 'insert': None}, '4:4:ESTART': {'replace_by': None, 'insert': 'ESTART'}, '5:5:EEND': {'replace_by': None, 'insert': 'EEND'}, '6:6:EOTHERSTART': {'replace_by': None, 'insert': 'EOTHERSTART'}, '11:11:EOTHEREND': {'replace_by': None, 'insert': 'EOTHEREND'}}\n",
      "sorted_positions: ['0:3', '4:4:ESTART', '5:5:EEND', '6:6:EOTHERSTART', '6:10', '11:11:EOTHEREND', '13:14', '17:17', '20:24', '27:28', '31:44', '47:47', '50:54', '57:58', '61:66', '69:73', '76:84', '87:88', '91:94', '97:97', '100:106', '109:119', '122:122', '125:127', '130:132', '135:138', '141:145', '148:151', '154:155', '158:161', '164:166', '169:175', '178:179', '182:186', '189:192', '195:197', '200:205', '208:208', '211:213', '216:219', '222:226', '229:229', '232:245', '248:248']\n",
      "['A', 'squirrel', 'popped', 'out', 'of', 'a', 'woman', \"'s\", 'shirt', 'when', 'she', 'was', 'interviewed', '.']\n",
      "ner_repl_dict: {'0:0': {'replace_by': 'O', 'insert': None}, '3:10': {'replace_by': 'O', 'insert': None}, '13:18': {'replace_by': 'O', 'insert': None}, '21:23': {'replace_by': 'O', 'insert': None}, '26:27': {'replace_by': 'O', 'insert': None}, '30:30': {'replace_by': 'O', 'insert': None}, '33:37': {'replace_by': 'O', 'insert': None}, '40:41': {'replace_by': 'O', 'insert': None}, '44:48': {'replace_by': 'O', 'insert': None}, '51:54': {'replace_by': 'O', 'insert': None}, '57:59': {'replace_by': 'O', 'insert': None}, '62:64': {'replace_by': 'O', 'insert': None}, '67:77': {'replace_by': 'O', 'insert': None}, '80:80': {'replace_by': 'O', 'insert': None}, '1:1:ESTART': {'replace_by': None, 'insert': 'ESTART'}, '2:2:EEND': {'replace_by': None, 'insert': 'EEND'}, '3:3:EOTHERSTART': {'replace_by': None, 'insert': 'EOTHERSTART'}, '11:11:EOTHEREND': {'replace_by': None, 'insert': 'EOTHEREND'}}\n",
      "sorted_positions: ['0:0', '1:1:ESTART', '2:2:EEND', '3:3:EOTHERSTART', '3:10', '11:11:EOTHEREND', '13:18', '21:23', '26:27', '30:30', '33:37', '40:41', '44:48', '51:54', '57:59', '62:64', '67:77', '80:80']\n",
      "['It', 'was', 'windy', 'and', 'cold', 'yesterday', 'and', 'there', 'is', 'still', 'some', 'remnants', 'of', 'snow', 'left', 'from', 'early', 'yesterdays', 'storm', '.']\n",
      "ner_repl_dict: {'0:1': {'replace_by': 'O', 'insert': None}, '4:6': {'replace_by': 'O', 'insert': None}, '9:13': {'replace_by': 'O', 'insert': None}, '16:18': {'replace_by': 'O', 'insert': None}, '21:24': {'replace_by': 'O', 'insert': None}, '27:35': {'replace_by': 'S-DATE', 'insert': None}, '38:40': {'replace_by': 'O', 'insert': None}, '43:47': {'replace_by': 'O', 'insert': None}, '50:51': {'replace_by': 'O', 'insert': None}, '54:58': {'replace_by': 'O', 'insert': None}, '61:64': {'replace_by': 'O', 'insert': None}, '67:74': {'replace_by': 'O', 'insert': None}, '77:78': {'replace_by': 'O', 'insert': None}, '81:84': {'replace_by': 'O', 'insert': None}, '87:90': {'replace_by': 'O', 'insert': None}, '93:96': {'replace_by': 'O', 'insert': None}, '99:103': {'replace_by': 'O', 'insert': None}, '106:115': {'replace_by': 'S-DATE', 'insert': None}, '118:122': {'replace_by': 'O', 'insert': None}, '125:125': {'replace_by': 'O', 'insert': None}, '9:9:ESTART': {'replace_by': None, 'insert': 'ESTART'}, '14:14:EEND': {'replace_by': None, 'insert': 'EEND'}, '16:16:EOTHERSTART': {'replace_by': None, 'insert': 'EOTHERSTART'}, '19:19:EOTHEREND': {'replace_by': None, 'insert': 'EOTHEREND'}}\n",
      "sorted_positions: ['0:1', '4:6', '9:9:ESTART', '9:13', '14:14:EEND', '16:16:EOTHERSTART', '16:18', '19:19:EOTHEREND', '21:24', '27:35', '38:40', '43:47', '50:51', '54:58', '61:64', '67:74', '77:78', '81:84', '87:90', '93:96', '99:103', '106:115', '118:122', '125:125']\n",
      "['After', 'seating', 'all', 'the', 'idols', ',', 'which', 'itself', 'takes', 'hours', ',', 'the', 'traditional', 'king', 'sweeps', 'the', 'chariot', 'with', 'a', 'golden', 'broom', '.']\n",
      "ner_repl_dict: {'0:4': {'replace_by': 'O', 'insert': None}, '7:13': {'replace_by': 'O', 'insert': None}, '16:18': {'replace_by': 'O', 'insert': None}, '21:23': {'replace_by': 'O', 'insert': None}, '26:30': {'replace_by': 'O', 'insert': None}, '33:33': {'replace_by': 'O', 'insert': None}, '36:40': {'replace_by': 'O', 'insert': None}, '43:48': {'replace_by': 'O', 'insert': None}, '51:55': {'replace_by': 'O', 'insert': None}, '58:62': {'replace_by': 'E-TIME', 'insert': None}, '65:65': {'replace_by': 'O', 'insert': None}, '68:70': {'replace_by': 'O', 'insert': None}, '73:83': {'replace_by': 'O', 'insert': None}, '86:89': {'replace_by': 'O', 'insert': None}, '92:97': {'replace_by': 'O', 'insert': None}, '100:102': {'replace_by': 'O', 'insert': None}, '105:111': {'replace_by': 'O', 'insert': None}, '114:117': {'replace_by': 'O', 'insert': None}, '120:120': {'replace_by': 'O', 'insert': None}, '123:128': {'replace_by': 'O', 'insert': None}, '131:135': {'replace_by': 'O', 'insert': None}, '138:138': {'replace_by': 'O', 'insert': None}, '7:7:ESTART': {'replace_by': None, 'insert': 'ESTART'}, '14:14:EEND': {'replace_by': None, 'insert': 'EEND'}, '20:20:EOTHERSTART': {'replace_by': None, 'insert': 'EOTHERSTART'}, '21:21:EOTHEREND': {'replace_by': None, 'insert': 'EOTHEREND'}}\n",
      "sorted_positions: ['0:4', '7:7:ESTART', '7:13', '14:14:EEND', '16:18', '20:20:EOTHERSTART', '21:21:EOTHEREND', '21:23', '26:30', '33:33', '36:40', '43:48', '51:55', '58:62', '65:65', '68:70', '73:83', '86:89', '92:97', '100:102', '105:111', '114:117', '120:120', '123:128', '131:135', '138:138']\n",
      "['The', 'minister', 'attributed', 'the', 'slow', 'production', 'of', 'the', 'materials', 'by', 'the', 'local', 'industries', 'to', 'their', 'limited', 'capacities', '.']\n",
      "ner_repl_dict: {'0:2': {'replace_by': 'O', 'insert': None}, '5:12': {'replace_by': 'O', 'insert': None}, '15:24': {'replace_by': 'O', 'insert': None}, '27:29': {'replace_by': 'O', 'insert': None}, '32:35': {'replace_by': 'O', 'insert': None}, '38:47': {'replace_by': 'O', 'insert': None}, '50:51': {'replace_by': 'O', 'insert': None}, '54:56': {'replace_by': 'O', 'insert': None}, '59:67': {'replace_by': 'O', 'insert': None}, '70:71': {'replace_by': 'O', 'insert': None}, '74:76': {'replace_by': 'O', 'insert': None}, '79:83': {'replace_by': 'O', 'insert': None}, '86:95': {'replace_by': 'O', 'insert': None}, '98:99': {'replace_by': 'O', 'insert': None}, '102:106': {'replace_by': 'O', 'insert': None}, '109:115': {'replace_by': 'O', 'insert': None}, '118:127': {'replace_by': 'O', 'insert': None}, '130:130': {'replace_by': 'O', 'insert': None}, '5:5:EEITHERSTART': {'replace_by': None, 'insert': 'EEITHERSTART'}, '13:13:EEITHEREND': {'replace_by': None, 'insert': 'EEITHEREND'}}\n",
      "sorted_positions: ['0:2', '5:5:EEITHERSTART', '5:12', '13:13:EEITHEREND', '15:24', '27:29', '32:35', '38:47', '50:51', '54:56', '59:67', '70:71', '74:76', '79:83', '86:95', '98:99', '102:106', '109:115', '118:127', '130:130']\n",
      "['The', 'umbrella', 'frame', 'is', 'provided', 'with', 'a', 'movable', 'yoke', 'at', 'the', 'upper', 'end', 'and', 'a', 'stationary', 'yoke', 'at', 'an', 'intermediate', 'point', '.']\n",
      "ner_repl_dict: {'0:2': {'replace_by': 'O', 'insert': None}, '5:12': {'replace_by': 'O', 'insert': None}, '15:19': {'replace_by': 'O', 'insert': None}, '22:23': {'replace_by': 'O', 'insert': None}, '26:33': {'replace_by': 'O', 'insert': None}, '36:39': {'replace_by': 'O', 'insert': None}, '42:42': {'replace_by': 'O', 'insert': None}, '45:51': {'replace_by': 'O', 'insert': None}, '54:57': {'replace_by': 'O', 'insert': None}, '60:61': {'replace_by': 'O', 'insert': None}, '64:66': {'replace_by': 'O', 'insert': None}, '69:73': {'replace_by': 'O', 'insert': None}, '76:78': {'replace_by': 'O', 'insert': None}, '81:83': {'replace_by': 'O', 'insert': None}, '86:86': {'replace_by': 'O', 'insert': None}, '89:98': {'replace_by': 'O', 'insert': None}, '101:104': {'replace_by': 'O', 'insert': None}, '107:108': {'replace_by': 'O', 'insert': None}, '111:112': {'replace_by': 'O', 'insert': None}, '115:126': {'replace_by': 'O', 'insert': None}, '129:133': {'replace_by': 'O', 'insert': None}, '136:136': {'replace_by': 'O', 'insert': None}, '0:0:EEITHERSTART': {'replace_by': None, 'insert': 'EEITHERSTART'}, '3:3:EEITHEREND': {'replace_by': None, 'insert': 'EEITHEREND'}}\n",
      "sorted_positions: ['0:0:EEITHERSTART', '0:2', '3:3:EEITHEREND', '5:12', '15:19', '22:23', '26:33', '36:39', '42:42', '45:51', '54:57', '60:61', '64:66', '69:73', '76:78', '81:83', '86:86', '89:98', '101:104', '107:108', '111:112', '115:126', '129:133', '136:136']\n",
      "['Manos', ':', 'The', 'Hands', 'of', 'Fate', 'is', 'a', 'low', '-', 'budget', 'horror', 'film', 'made', 'by', 'El', 'Paso', 'fertilizer', 'salesman', 'Hal', 'P.', 'Warren', '.']\n",
      "ner_repl_dict: {'0:4': {'replace_by': 'S-PERSON', 'insert': None}, '7:7': {'replace_by': 'O', 'insert': None}, '10:12': {'replace_by': 'O', 'insert': None}, '15:19': {'replace_by': 'O', 'insert': None}, '22:23': {'replace_by': 'O', 'insert': None}, '26:29': {'replace_by': 'O', 'insert': None}, '32:33': {'replace_by': 'O', 'insert': None}, '36:36': {'replace_by': 'O', 'insert': None}, '39:41': {'replace_by': 'O', 'insert': None}, '44:44': {'replace_by': 'O', 'insert': None}, '47:52': {'replace_by': 'O', 'insert': None}, '55:60': {'replace_by': 'O', 'insert': None}, '63:66': {'replace_by': 'O', 'insert': None}, '69:72': {'replace_by': 'O', 'insert': None}, '75:76': {'replace_by': 'O', 'insert': None}, '79:80': {'replace_by': 'O', 'insert': None}, '83:86': {'replace_by': 'O', 'insert': None}, '89:98': {'replace_by': 'O', 'insert': None}, '101:108': {'replace_by': 'O', 'insert': None}, '111:113': {'replace_by': 'S-PERSON', 'insert': None}, '116:117': {'replace_by': 'O', 'insert': None}, '120:125': {'replace_by': 'S-PERSON', 'insert': None}, '128:128': {'replace_by': 'O', 'insert': None}, '10:10:ESTART': {'replace_by': None, 'insert': 'ESTART'}, '13:13:EEND': {'replace_by': None, 'insert': 'EEND'}, '15:15:EOTHERSTART': {'replace_by': None, 'insert': 'EOTHERSTART'}, '20:20:EOTHEREND': {'replace_by': None, 'insert': 'EOTHEREND'}}\n",
      "sorted_positions: ['0:4', '7:7', '10:10:ESTART', '10:12', '13:13:EEND', '15:15:EOTHERSTART', '15:19', '20:20:EOTHEREND', '22:23', '26:29', '32:33', '36:36', '39:41', '44:44', '47:52', '55:60', '63:66', '69:72', '75:76', '79:80', '83:86', '89:98', '101:108', '111:113', '116:117', '120:125', '128:128']\n",
      "['A', 'few', 'days', 'before', 'the', 'service', ',', 'Tom', 'Burris', 'had', 'thrown', 'into', 'Karen', \"'s\", 'casket', 'his', 'wedding', 'ring', '.']\n",
      "ner_repl_dict: {'0:0': {'replace_by': 'O', 'insert': None}, '3:5': {'replace_by': 'O', 'insert': None}, '8:11': {'replace_by': 'E-DATE', 'insert': None}, '14:19': {'replace_by': 'O', 'insert': None}, '22:24': {'replace_by': 'O', 'insert': None}, '27:33': {'replace_by': 'O', 'insert': None}, '36:36': {'replace_by': 'O', 'insert': None}, '39:41': {'replace_by': 'S-PERSON', 'insert': None}, '44:49': {'replace_by': 'S-PERSON', 'insert': None}, '52:54': {'replace_by': 'O', 'insert': None}, '57:62': {'replace_by': 'O', 'insert': None}, '65:68': {'replace_by': 'O', 'insert': None}, '71:75': {'replace_by': 'S-PERSON', 'insert': None}, '78:79': {'replace_by': 'O', 'insert': None}, '82:87': {'replace_by': 'O', 'insert': None}, '90:92': {'replace_by': 'O', 'insert': None}, '95:101': {'replace_by': 'O', 'insert': None}, '104:107': {'replace_by': 'O', 'insert': None}, '110:110': {'replace_by': 'O', 'insert': None}, '14:14:EEITHERSTART': {'replace_by': None, 'insert': 'EEITHERSTART'}, '20:20:EEITHEREND': {'replace_by': None, 'insert': 'EEITHEREND'}}\n",
      "sorted_positions: ['0:0', '3:5', '8:11', '14:14:EEITHERSTART', '14:19', '20:20:EEITHEREND', '22:24', '27:33', '36:36', '39:41', '44:49', '52:54', '57:62', '65:68', '71:75', '78:79', '82:87', '90:92', '95:101', '104:107', '110:110']\n"
     ]
    }
   ],
   "source": [
    "for original_df_name in original_dataframe_names:\n",
    "    type1 = preprocess(read_dataframe, out(indir + original_df_name + '_original.csv'), nlp)\n",
    "    type2 = preprocess(read_dataframe, out(indir + original_df_name + '_original.csv'), nlp, 2)\n",
    "    type3 = preprocess(read_dataframe, out(indir + original_df_name + '_original.csv'), nlp, 3)\n",
    "    type4 = preprocess(read_dataframe, out(indir + original_df_name + '_original.csv'), nlp, 4)\n",
    "    write_dataframe(type1, out(outdir1 + original_df_name + '_entity_blinding.csv'))\n",
    "    write_dataframe(type2, out(outdir2 + original_df_name + '_punct_stop_digit.csv'))\n",
    "    write_dataframe(type3, out(outdir3 + original_df_name + '_punct_digit.csv'))\n",
    "    write_dataframe(type4, out(outdir4 + original_df_name + '_ner_blinding.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 8608.49it/s]\n",
      "100%|██████████| 8000/8000 [00:00<00:00, 8782.84it/s]\n",
      "100%|██████████| 8000/8000 [00:00<00:00, 8612.40it/s]\n",
      "100%|██████████| 8000/8000 [00:00<00:00, 9034.36it/s]\n",
      "100%|██████████| 2717/2717 [00:00<00:00, 8788.70it/s]\n",
      "100%|██████████| 2717/2717 [00:00<00:00, 8344.91it/s]\n",
      "100%|██████████| 2717/2717 [00:00<00:00, 5795.81it/s]\n",
      "100%|██████████| 2717/2717 [00:00<00:00, 5736.40it/s]\n"
     ]
    }
   ],
   "source": [
    " for original_df_name in original_dataframe_names:\n",
    "     type1 = read_dataframe(out(outdir1 + original_df_name + '_entity_blinding.csv'))\n",
    "     type2 = read_dataframe(out(outdir2 + original_df_name + '_punct_stop_digit.csv'))\n",
    "     type3 = read_dataframe(out(outdir3 + original_df_name + '_punct_digit.csv'))\n",
    "     type4 = read_dataframe(out(outdir4 + original_df_name + '_ner_blinding.csv'))\n",
    "     write_into_txt(type1, out(outdir1 + original_df_name + '_entity_blinding.txt'))\n",
    "     write_into_txt(type2, out(outdir2 + original_df_name + '_punct_stop_digit.txt'))\n",
    "     write_into_txt(type3, out(outdir3 + original_df_name + '_punct_digit.txt'))\n",
    "     write_into_txt(type4, out(outdir4 + original_df_name + '_ner_blinding.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_relations_json(out(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_file_length(out, filename):\n",
    "    return len(open(out(filename)).readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8000\n8000\n8000\n8000\n7916\n"
     ]
    }
   ],
   "source": [
    "print(output_file_length(out, indir + 'semeval2010_train_original.txt'))\n",
    "print(output_file_length(out, outdir1 + 'semeval2010_train_entity_blinding.txt'))\n",
    "print(output_file_length(out, outdir2 + 'semeval2010_train_punct_stop_digit.txt'))\n",
    "print(output_file_length(out, outdir3 + 'semeval2010_train_punct_digit.txt'))\n",
    "print(output_file_length(out, outdir4 + 'semeval2010_train_ner_blinding.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2717\n2717\n2717\n2717\n2688\n"
     ]
    }
   ],
   "source": [
    "print(output_file_length(out, indir + 'semeval2010_val_original.txt'))\n",
    "print(output_file_length(out, outdir1 + 'semeval2010_val_entity_blinding.txt'))\n",
    "print(output_file_length(out, outdir2 + 'semeval2010_val_punct_stop_digit.txt'))\n",
    "print(output_file_length(out, outdir3 + 'semeval2010_val_punct_digit.txt'))\n",
    "print(output_file_length(out, outdir4 + 'semeval2010_val_ner_blinding.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}