Trained with model cnn, pretrain glove, dataset semeval2010 and preprocessing original:
Confusion matrix:
[[102  10   0   0   2   0   6   0   5   1   0   3   0   0   0   0   0   0
   21]
 [ 10  84   0   0   2   0   1  10   0   2   0   0   0   0   0   1   0   0
   24]
 [  0   0   7   0   0   0   0   2   2   1   0   1   1   0   0   0   0   0
   18]
 [  0   0   0 174   0   0   0   0   0   4   0   0   0   5   0   0   0   0
   11]
 [  0   0   0   0 254  12   1   0   0   0   0   3   0   0   0   0   0   0
   21]
 [  1   0   0   0   7 134   0   0   0   1   0   3   0   0   0   0   0   0
    7]
 [  2   0   0   0   1   0 172   0   1   0   0   1   0   0   0   0   0   0
   33]
 [  5  13   0   0   2   0   1  62   1   0   0   0   0   0   0   1   0   0
   38]
 [  1   0   0   0   0   0   1   2 174   1   0   2   0   0   0   0   0   0
   20]
 [  0   1   0   6   3   1   1   0   0 170   0   0   0   1   0   0   0   0
   28]
 [  0   0   0   3   0   0   1   0   0   0 115   0   0   0   0   0   0   0
   15]
 [  8   1   1   0   0   2   0   0   4   0   0 124   0   0   0   1   0   0
   21]
 [  1   0   0   0   0   0   9   0   0   1   0   0  22   0   0   0   0   0
   18]
 [  0   0   0   4   0   0   1   2   1   5   0   1   0  77   0   0   0   0
   17]
 [  1   0   0   0   1   0   0   0   0   0   0   0   0   0  37   0   0   0
    8]
 [  5   0   0   0   0   0   0   0   0   0   0   0   0   0   1  26   0   0
    7]
 [  1   0   0   0   0   0   0   2   0   0   0   0   0   1   0   0   0   0
   18]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1]
 [ 25  14   5   8  26  19  26  12  28  22   7   9   5  12   0   3   0   0
  233]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.63      0.68      0.65       150
                    Other       0.68      0.63      0.65       134
 Instrument-Agency(e2,e1)       0.54      0.22      0.31        32
 Member-Collection(e1,e2)       0.89      0.90      0.89       194
      Cause-Effect(e2,e1)       0.85      0.87      0.86       291
Entity-Destination(e1,e2)       0.80      0.88      0.83       153
 Content-Container(e1,e2)       0.78      0.82      0.80       210
     Message-Topic(e1,e2)       0.67      0.50      0.58       123
  Product-Producer(e2,e1)       0.81      0.87      0.83       201
 Member-Collection(e2,e1)       0.82      0.81      0.81       211
     Entity-Origin(e1,e2)       0.94      0.86      0.90       134
      Cause-Effect(e1,e2)       0.84      0.77      0.80       162
   Component-Whole(e1,e2)       0.79      0.43      0.56        51
     Message-Topic(e2,e1)       0.80      0.71      0.75       108
  Product-Producer(e1,e2)       0.97      0.79      0.87        47
     Entity-Origin(e2,e1)       0.81      0.67      0.73        39
 Content-Container(e2,e1)       0.00      0.00      0.00        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.42      0.51      0.46       454

                 accuracy                           0.72      2717
                macro avg       0.69      0.63      0.65      2717
             weighted avg       0.73      0.72      0.72      2717

Micro precision: 0.8035217794253939
Micro recall: 0.7662395050817499
Micro F1: 0.7844379099751189

Trained with model cnn, pretrain glove, dataset semeval2010 and preprocessing punct_digit:
Confusion matrix:
[[103  10   0   0   2   0   5   1   5   1   0   2   0   0   0   0   0   0
   21]
 [ 10  83   0   0   2   0   2   8   0   1   0   0   0   0   0   0   0   0
   28]
 [  0   0   6   0   0   0   0   1   2   1   0   1   1   0   0   0   0   0
   20]
 [  0   0   0 175   0   0   0   0   0   5   0   0   0   3   0   0   0   0
   11]
 [  0   0   0   0 259  11   1   0   0   0   0   1   0   0   0   0   0   0
   19]
 [  0   0   0   0   7 136   0   0   0   1   0   4   0   0   0   0   0   0
    5]
 [  2   0   0   0   1   0 173   0   1   2   0   1   0   0   0   0   0   0
   30]
 [  8  15   0   0   1   0   1  65   1   2   0   0   0   0   0   0   0   0
   30]
 [  0   0   0   0   0   0   0   3 176   1   0   2   0   0   0   0   0   0
   19]
 [  0   0   0   7   2   2   2   0   0 175   0   0   0   1   0   0   0   0
   22]
 [  0   0   0   4   0   0   2   0   0   0 114   0   0   0   0   0   0   0
   14]
 [  8   1   0   0   0   2   0   0   6   0   0 126   0   0   0   1   0   0
   18]
 [  1   0   0   0   1   0   6   0   0   0   0   0  23   0   0   0   0   0
   20]
 [  0   0   0   5   0   0   1   1   0   7   0   1   0  80   0   0   0   0
   13]
 [  0   2   0   0   0   0   0   0   0   0   0   0   0   0  38   0   0   0
    7]
 [  5   0   0   0   0   0   0   0   0   0   0   0   0   0   1  26   0   0
    7]
 [  2   1   0   0   0   0   0   0   0   0   1   0   0   3   0   0   5   0
   10]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0
    0]
 [ 28  18   1   8  26  18  27  13  29  26   6  11   5  12   1   1   0   0
  224]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.62      0.69      0.65       150
                    Other       0.64      0.62      0.63       134
 Instrument-Agency(e2,e1)       0.86      0.19      0.31        32
 Member-Collection(e1,e2)       0.88      0.90      0.89       194
      Cause-Effect(e2,e1)       0.86      0.89      0.88       291
Entity-Destination(e1,e2)       0.80      0.89      0.84       153
 Content-Container(e1,e2)       0.79      0.82      0.80       210
     Message-Topic(e1,e2)       0.71      0.53      0.60       123
  Product-Producer(e2,e1)       0.80      0.88      0.84       201
 Member-Collection(e2,e1)       0.79      0.83      0.81       211
     Entity-Origin(e1,e2)       0.94      0.85      0.89       134
      Cause-Effect(e1,e2)       0.85      0.78      0.81       162
   Component-Whole(e1,e2)       0.79      0.45      0.58        51
     Message-Topic(e2,e1)       0.81      0.74      0.77       108
  Product-Producer(e1,e2)       0.95      0.81      0.87        47
     Entity-Origin(e2,e1)       0.90      0.67      0.76        39
 Content-Container(e2,e1)       1.00      0.23      0.37        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.43      0.49      0.46       454

                 accuracy                           0.73      2717
                macro avg       0.76      0.64      0.67      2717
             weighted avg       0.74      0.73      0.73      2717

Micro precision: 0.8017280582082765
Micro recall: 0.7790543526292532
Micro F1: 0.7902285970416854

Trained with model cnn, pretrain glove, dataset semeval2010 and preprocessing punct_stop_digit:
Confusion matrix:
[[ 83   8   0   0   2   2   5   3   5   0   0  16   1   0   0   1   0   0
   24]
 [  5  68   0   0   3   3   1  15   0   3   1   0   0   0   0   0   0   0
   35]
 [  0   0   8   0   0   0   0   1   4   1   0   0   1   1   0   0   0   0
   16]
 [  1   0   0 161   0   0   1   0   0   3  11   1   0   1   0   0   0   0
   15]
 [  1   2   0   0 227   6   1   1   0   7   0   4   0   1   0   0   0   0
   41]
 [  3   1   0   0  17 112   0   0   0   1   0   4   0   0   0   2   0   0
   13]
 [  3   0   0   0   1   0 149   0   1   1   0   0   2   4   0   0   0   0
   49]
 [  4  10   0   0   1   0   1  56   1   1   0   0   0   0   0   0   0   0
   49]
 [  2   0   1   0   1   1   3   1 149   0   0   2   0   0   0   0   1   0
   40]
 [  2   1   0   3  20   2   3   1   1 121   0   2   0   7   0   0   0   0
   48]
 [  0   0   0  27   0   0   2   0   0   0  93   0   0   0   0   0   0   0
   12]
 [ 11   0   1   0   1   3   1   2   7   0   0 110   0   0   0   0   0   0
   26]
 [  0   0   0   0   0   0   9   0   0   0   0   0  14   0   0   0   0   0
   28]
 [  1   0   0   4   2   0   5   3   1   5   0   1   0  65   0   0   1   0
   20]
 [  0   1   0   0   0   0   0   1   0   1   0   0   0   0  39   0   0   0
    5]
 [  2   0   0   0   1   4   0   0   1   0   0   0   0   0   1  25   0   0
    5]
 [  2   1   0   0   0   0   0   0   0   0   0   0   0   1   0   0   6   0
   12]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1]
 [ 18  12   2  10  22  11  27  16  22  23   7  15   1   8   1   5   2   0
  252]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.60      0.55      0.58       150
                    Other       0.65      0.51      0.57       134
 Instrument-Agency(e2,e1)       0.67      0.25      0.36        32
 Member-Collection(e1,e2)       0.79      0.83      0.81       194
      Cause-Effect(e2,e1)       0.76      0.78      0.77       291
Entity-Destination(e1,e2)       0.78      0.73      0.75       153
 Content-Container(e1,e2)       0.72      0.71      0.71       210
     Message-Topic(e1,e2)       0.56      0.46      0.50       123
  Product-Producer(e2,e1)       0.78      0.74      0.76       201
 Member-Collection(e2,e1)       0.72      0.57      0.64       211
     Entity-Origin(e1,e2)       0.83      0.69      0.76       134
      Cause-Effect(e1,e2)       0.71      0.68      0.69       162
   Component-Whole(e1,e2)       0.74      0.27      0.40        51
     Message-Topic(e2,e1)       0.74      0.60      0.66       108
  Product-Producer(e1,e2)       0.95      0.83      0.89        47
     Entity-Origin(e2,e1)       0.76      0.64      0.69        39
 Content-Container(e2,e1)       0.60      0.27      0.37        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.36      0.56      0.44       454

                 accuracy                           0.64      2717
                macro avg       0.67      0.56      0.60      2717
             weighted avg       0.67      0.64      0.65      2717

Micro precision: 0.7334649555774926
Micro recall: 0.6566504639858595
Micro F1: 0.6929354161809279

Trained with model cnn, pretrain glove, dataset semeval2010 and preprocessing entity_blinding:
Confusion matrix:
[[ 72  19   0   0   0   0   3  10   9   1   0   1   0   0   5   3   0   0
   27]
 [ 12  80   0   0   2   0   2   4   0   0   0   0   0   4   0   2   0   0
   28]
 [  2   0   0   0   2   1   0   0   0   1   0   8   1   0   0   0   0   0
   17]
 [  0   1   0 167   0   0   0   0   0  11   0   0   0   5   1   0   0   0
    9]
 [  0   0   0   0 259   8   0   0   0   0   0   1   0   0   0   0   0   0
   23]
 [  0   0   0   0  10 128   0   0   0   0   0   3   0   0   0   0   0   0
   12]
 [  0   1   0   0   1   0 149   3   1   0   0   0   0   0   0   0   0   0
   55]
 [ 11  12   0   0   0   0   5  42   3   0   0   2   0   0   0   0   0   0
   48]
 [  4   0   0   0   0   0   0   2 176   1   0   3   0   0   1   0   0   0
   14]
 [  0   0   0  13   2   0   1   0   0 165   1   1   0   0   0   0   0   0
   28]
 [  3   2   0   2   0   0   2   0   0   0 109   0   0   0   1   0   0   0
   15]
 [  2   1   0   0   1   2   0   0   5   0   0 118   0   0   2   1   0   0
   30]
 [  0   0   0   0   0   0   6   0   0   1   0   0  20   0   0   0   0   0
   24]
 [  4   0   0   7   0   0   1   1   0   6   0   2   0  53   2   0   0   0
   32]
 [  5   2   0   0   1   0   0   0   0   0   0   1   0   1  26   0   0   0
   11]
 [  6   1   0   0   0   0   0   0   1   0   0   0   0   0   1  25   0   0
    5]
 [  1   3   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0
   17]
 [  0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0
    0]
 [ 26  24   0   7  38  20  25  15  38  22   3  29   2  13   9   2   0   0
  181]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.49      0.48      0.48       150
                    Other       0.55      0.60      0.57       134
 Instrument-Agency(e2,e1)       0.00      0.00      0.00        32
 Member-Collection(e1,e2)       0.85      0.86      0.86       194
      Cause-Effect(e2,e1)       0.82      0.89      0.85       291
Entity-Destination(e1,e2)       0.81      0.84      0.82       153
 Content-Container(e1,e2)       0.77      0.71      0.74       210
     Message-Topic(e1,e2)       0.54      0.34      0.42       123
  Product-Producer(e2,e1)       0.76      0.88      0.81       201
 Member-Collection(e2,e1)       0.79      0.78      0.79       211
     Entity-Origin(e1,e2)       0.96      0.81      0.88       134
      Cause-Effect(e1,e2)       0.70      0.73      0.71       162
   Component-Whole(e1,e2)       0.87      0.39      0.54        51
     Message-Topic(e2,e1)       0.69      0.49      0.57       108
  Product-Producer(e1,e2)       0.54      0.55      0.55        47
     Entity-Origin(e2,e1)       0.76      0.64      0.69        39
 Content-Container(e2,e1)       0.00      0.00      0.00        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.31      0.40      0.35       454

                 accuracy                           0.65      2717
                macro avg       0.59      0.55      0.56      2717
             weighted avg       0.65      0.65      0.65      2717

Micro precision: 0.742176553012611
Micro recall: 0.7021652673442333
Micro F1: 0.7216167120799273

Trained with model pcnn, pretrain glove, dataset semeval2010 and preprocessing original:
Confusion matrix:
[[106   7   0   0   2   0   5   1   7   1   0   1   0   0   0   0   0   0
   20]
 [  7  85   0   0   2   0   3  11   0   1   1   0   0   0   0   0   1   0
   23]
 [  0   0  14   0   0   0   0   0   4   1   0   0   1   0   0   0   0   0
   12]
 [  0   0   0 176   0   0   0   0   0   4   0   0   0   3   0   0   0   0
   11]
 [  0   0   0   0 249  11   1   1   0   1   0   2   0   0   0   0   0   0
   26]
 [  1   0   0   0   8 130   0   0   0   1   0   3   0   1   0   1   0   0
    8]
 [  3   1   0   0   0   0 178   0   1   2   0   1   1   0   0   0   0   0
   23]
 [  3  17   0   0   2   1   1  71   1   2   0   0   0   0   0   1   0   0
   24]
 [  1   0   0   0   0   0   1   2 172   0   0   2   0   1   0   0   0   0
   22]
 [  0   0   0   7   4   1   1   0   0 178   0   0   0   1   0   0   0   0
   19]
 [  1   0   0   3   0   0   0   0   0   0 118   0   0   0   0   0   0   0
   12]
 [  8   1   3   0   0   2   0   0   6   0   0 129   0   0   0   1   1   0
   11]
 [  1   0   0   0   0   0   6   1   0   0   0   0  31   0   0   0   0   0
   12]
 [  0   0   0   4   0   0   1   1   1   5   0   1   0  83   0   0   2   0
   10]
 [  0   0   0   0   1   1   0   1   0   1   0   0   0   0  37   0   0   0
    6]
 [  3   0   0   0   0   0   0   0   0   0   0   0   0   0   2  28   0   0
    6]
 [  0   1   0   0   0   0   0   0   0   0   1   0   0   6   0   0   5   0
    9]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1]
 [ 22  15   5   9  26  18  27  11  29  22   8  10   4  12   1   1   0   0
  234]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.68      0.71      0.69       150
                    Other       0.67      0.63      0.65       134
 Instrument-Agency(e2,e1)       0.64      0.44      0.52        32
 Member-Collection(e1,e2)       0.88      0.91      0.90       194
      Cause-Effect(e2,e1)       0.85      0.86      0.85       291
Entity-Destination(e1,e2)       0.79      0.85      0.82       153
 Content-Container(e1,e2)       0.79      0.85      0.82       210
     Message-Topic(e1,e2)       0.71      0.58      0.64       123
  Product-Producer(e2,e1)       0.78      0.86      0.82       201
 Member-Collection(e2,e1)       0.81      0.84      0.83       211
     Entity-Origin(e1,e2)       0.92      0.88      0.90       134
      Cause-Effect(e1,e2)       0.87      0.80      0.83       162
   Component-Whole(e1,e2)       0.84      0.61      0.70        51
     Message-Topic(e2,e1)       0.78      0.77      0.77       108
  Product-Producer(e1,e2)       0.93      0.79      0.85        47
     Entity-Origin(e2,e1)       0.88      0.72      0.79        39
 Content-Container(e2,e1)       0.56      0.23      0.32        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.48      0.52      0.50       454

                 accuracy                           0.74      2717
                macro avg       0.73      0.67      0.69      2717
             weighted avg       0.75      0.74      0.74      2717

Micro precision: 0.803411131059246
Micro recall: 0.7909854175872735
Micro F1: 0.7971498552660877

Trained with model pcnn, pretrain glove, dataset semeval2010 and preprocessing punct_digit:
Confusion matrix:
[[103   8   0   0   2   0   6   0   5   1   0   2   0   0   0   1   0   0
   22]
 [  9  84   0   0   2   0   1   9   0   1   1   0   0   0   0   0   0   0
   27]
 [  0   0  16   0   0   0   0   0   1   1   0   0   1   0   0   0   0   0
   13]
 [  0   0   0 177   0   0   0   0   0   4   0   0   0   2   0   0   0   0
   11]
 [  0   0   0   0 258   9   1   0   0   0   0   2   0   0   0   0   0   0
   21]
 [  1   0   0   0   6 133   0   0   0   2   0   3   0   0   0   1   0   0
    7]
 [  4   0   0   0   1   0 170   1   1   2   0   1   0   0   0   0   0   0
   30]
 [  7  15   0   0   2   0   1  71   2   2   0   0   0   0   0   0   0   0
   23]
 [  1   0   0   0   0   0   2   2 173   1   0   3   0   0   0   0   0   0
   19]
 [  0   0   0   5   4   1   1   0   0 179   0   0   0   1   0   0   0   0
   20]
 [  1   0   0   2   0   0   2   0   0   0 116   0   0   0   0   0   0   0
   13]
 [  9   0   2   0   0   2   0   0   5   0   0 127   0   0   0   1   0   0
   16]
 [  1   0   0   0   1   0   6   1   0   0   0   0  33   0   0   0   0   0
    9]
 [  0   0   0   4   0   0   1   1   0   6   0   0   0  83   0   0   1   0
   12]
 [  1   0   0   0   1   0   0   1   0   0   0   0   0   0  38   0   0   0
    6]
 [  2   0   0   0   0   1   0   0   1   0   0   0   0   0   2  28   0   0
    5]
 [  1   1   0   0   0   0   0   0   0   0   0   0   0   4   0   0   6   0
   10]
 [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0]
 [ 22  18   6   7  24  17  22   7  29  24   8   8   3  14   2   2   0   0
  241]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.63      0.69      0.66       150
                    Other       0.67      0.63      0.65       134
 Instrument-Agency(e2,e1)       0.67      0.50      0.57        32
 Member-Collection(e1,e2)       0.91      0.91      0.91       194
      Cause-Effect(e2,e1)       0.86      0.89      0.87       291
Entity-Destination(e1,e2)       0.82      0.87      0.84       153
 Content-Container(e1,e2)       0.80      0.81      0.80       210
     Message-Topic(e1,e2)       0.76      0.58      0.66       123
  Product-Producer(e2,e1)       0.80      0.86      0.83       201
 Member-Collection(e2,e1)       0.80      0.85      0.82       211
     Entity-Origin(e1,e2)       0.93      0.87      0.90       134
      Cause-Effect(e1,e2)       0.87      0.78      0.82       162
   Component-Whole(e1,e2)       0.89      0.65      0.75        51
     Message-Topic(e2,e1)       0.80      0.77      0.78       108
  Product-Producer(e1,e2)       0.90      0.81      0.85        47
     Entity-Origin(e2,e1)       0.85      0.72      0.78        39
 Content-Container(e2,e1)       0.86      0.27      0.41        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.48      0.53      0.50       454

                 accuracy                           0.75      2717
                macro avg       0.75      0.68      0.71      2717
             weighted avg       0.76      0.75      0.75      2717

Micro precision: 0.8114828209764918
Micro recall: 0.793194874060981
Micro F1: 0.8022346368715084

Trained with model pcnn, pretrain glove, dataset semeval2010 and preprocessing punct_stop_digit:
Confusion matrix:
[[ 92   5   0   0   3   2   3   1   3   0   1  15   2   0   1   0   0   0
   22]
 [  6  81   0   0   2   1   1   7   0   2   2   0   0   1   0   0   1   0
   30]
 [  0   0  17   0   0   0   0   1   3   1   0   0   1   0   0   0   0   0
    9]
 [  1   0   0 164   0   0   0   0   0   1  13   1   0   2   0   0   0   0
   12]
 [  1   1   0   0 218  11   3   2   2   3   0   4   0   2   0   0   0   0
   44]
 [  3   1   0   0  15 111   0   0   1   0   0   7   0   0   0   0   0   0
   15]
 [  6   1   0   0   2   0 154   1   2   3   0   1   2   2   0   0   0   0
   36]
 [  3   9   0   0   1   0   1  70   1   2   0   0   1   2   0   0   0   0
   33]
 [  6   0   2   0   0   1   2   2 153   0   0   2   0   2   0   0   2   0
   29]
 [  2   1   0   4  26   3   1   4   1 121   0   3   1   5   2   0   0   0
   37]
 [  0   0   0  20   0   0   1   0   0   0 102   1   0   0   0   0   0   0
   10]
 [ 12   2   3   0   0   4   1   1   7   1   1 108   0   0   0   0   0   0
   22]
 [  3   0   0   0   0   0   8   2   0   0   0   0  25   0   0   0   0   0
   13]
 [  0   0   0   3   2   0   4   3   1   6   0   2   0  70   0   0   1   0
   16]
 [  0   1   0   1   0   1   0   0   0   1   0   0   0   0  38   0   0   0
    5]
 [  2   0   0   0   2   5   0   0   0   0   0   0   0   0   1  26   0   0
    3]
 [  2   1   0   0   0   0   0   0   1   0   0   0   0   1   0   0  10   0
    7]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1]
 [ 25  15   6  15  23  11  24  10  20  22   7  21   7  11   2   4   3   0
  228]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.56      0.61      0.59       150
                    Other       0.69      0.60      0.64       134
 Instrument-Agency(e2,e1)       0.61      0.53      0.57        32
 Member-Collection(e1,e2)       0.79      0.85      0.82       194
      Cause-Effect(e2,e1)       0.74      0.75      0.75       291
Entity-Destination(e1,e2)       0.74      0.73      0.73       153
 Content-Container(e1,e2)       0.76      0.73      0.75       210
     Message-Topic(e1,e2)       0.67      0.57      0.62       123
  Product-Producer(e2,e1)       0.78      0.76      0.77       201
 Member-Collection(e2,e1)       0.74      0.57      0.65       211
     Entity-Origin(e1,e2)       0.81      0.76      0.78       134
      Cause-Effect(e1,e2)       0.65      0.67      0.66       162
   Component-Whole(e1,e2)       0.64      0.49      0.56        51
     Message-Topic(e2,e1)       0.71      0.65      0.68       108
  Product-Producer(e1,e2)       0.86      0.81      0.84        47
     Entity-Origin(e2,e1)       0.87      0.67      0.75        39
 Content-Container(e2,e1)       0.59      0.45      0.51        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.40      0.50      0.44       454

                 accuracy                           0.66      2717
                macro avg       0.66      0.62      0.64      2717
             weighted avg       0.67      0.66      0.66      2717

Micro precision: 0.7272727272727273
Micro recall: 0.68935041979673
Micro F1: 0.7078039927404718

Trained with model pcnn, pretrain glove, dataset semeval2010 and preprocessing entity_blinding:
Confusion matrix:
[[ 73  14   0   0   0   0   5   8   9   0   0   0   0   0   6   3   0   0
   32]
 [  9  84   0   0   2   0   3   7   0   0   1   0   0   2   0   3   0   0
   23]
 [  2   0   1   0   0   1   0   0   1   1   0  10   1   0   0   0   0   0
   15]
 [  0   1   0 167   0   0   0   0   0  13   0   0   0   5   1   0   0   0
    7]
 [  0   0   0   0 261   7   0   0   0   0   0   1   0   0   0   0   0   0
   22]
 [  0   0   0   0   8 132   0   0   0   1   0   4   0   0   0   0   0   0
    8]
 [  1   0   0   0   1   0 154   2   1   1   0   0   0   0   0   0   0   0
   50]
 [ 10  12   0   0   0   0   8  49   4   0   0   3   0   0   1   0   0   0
   36]
 [  4   0   0   0   0   0   0   2 185   1   0   4   0   0   0   0   0   0
    5]
 [  0   0   0  13   2   0   1   0   0 170   0   1   0   0   0   0   0   0
   24]
 [  1   1   0   3   0   0   2   0   0   0 113   0   0   0   2   0   0   0
   12]
 [  2   1   1   0   0   3   1   1   7   0   0 121   0   1   2   1   0   0
   21]
 [  0   0   0   0   0   0   3   2   0   1   0   1  23   0   0   0   0   0
   21]
 [  1   0   0   7   0   0   1   3   0   5   0   1   0  53   2   0   0   0
   35]
 [  2   0   0   0   1   0   0   2   0   0   0   0   0   1  28   0   1   0
   12]
 [  4   1   0   0   0   0   0   0   1   0   0   0   0   0   1  25   0   0
    7]
 [  1   2   0   0   0   0   1   0   0   0   0   0   0   2   0   0   2   0
   14]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1]
 [ 21  25   0   9  39  22  28  13  42  22   4  28   3  16   9   2   0   0
  171]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.56      0.49      0.52       150
                    Other       0.60      0.63      0.61       134
 Instrument-Agency(e2,e1)       0.50      0.03      0.06        32
 Member-Collection(e1,e2)       0.84      0.86      0.85       194
      Cause-Effect(e2,e1)       0.83      0.90      0.86       291
Entity-Destination(e1,e2)       0.80      0.86      0.83       153
 Content-Container(e1,e2)       0.74      0.73      0.74       210
     Message-Topic(e1,e2)       0.55      0.40      0.46       123
  Product-Producer(e2,e1)       0.74      0.92      0.82       201
 Member-Collection(e2,e1)       0.79      0.81      0.80       211
     Entity-Origin(e1,e2)       0.96      0.84      0.90       134
      Cause-Effect(e1,e2)       0.70      0.75      0.72       162
   Component-Whole(e1,e2)       0.85      0.45      0.59        51
     Message-Topic(e2,e1)       0.66      0.49      0.56       108
  Product-Producer(e1,e2)       0.54      0.60      0.57        47
     Entity-Origin(e2,e1)       0.74      0.64      0.68        39
 Content-Container(e2,e1)       0.67      0.09      0.16        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.33      0.38      0.35       454

                 accuracy                           0.67      2717
                macro avg       0.65      0.57      0.58      2717
             weighted avg       0.67      0.67      0.66      2717

Micro precision: 0.7455701953657429
Micro recall: 0.725143614670791
Micro F1: 0.735215053763441

Trained with model transformer, pretrain bert-base-uncased, dataset semeval2010 and preprocessing original:
Confusion matrix:
[[124   3   0   0   0   0   2   0   3   0   0   1   0   0   0   4   0   0
   13]
 [  4 102   0   0   0   0   1   5   0   0   0   0   0   0   0   0   0   0
   22]
 [  0   0  23   0   1   0   0   0   0   1   0   1   1   1   0   0   0   0
    4]
 [  0   0   0 182   0   0   0   0   0   3   2   0   0   2   0   0   0   0
    5]
 [  0   0   0   0 271   5   0   0   0   0   0   2   0   0   0   0   0   0
   13]
 [  0   0   0   0   6 135   0   0   0   0   0   5   1   0   0   0   0   0
    6]
 [  1   0   0   0   0   0 199   0   2   0   0   0   0   0   0   0   0   0
    8]
 [  1   0   0   0   0   0   0 108   0   0   1   0   1   0   0   0   0   0
   12]
 [  2   0   0   0   0   0   0   0 184   0   0   1   0   0   0   0   0   0
   14]
 [  1   0   0   4   1   1   0   0   0 189   0   1   0   2   0   0   0   0
   12]
 [  0   0   0   1   0   0   1   1   0   0 129   0   0   0   0   0   0   0
    2]
 [  4   0   1   0   1   1   0   0   1   0   0 144   0   0   0   0   1   0
    9]
 [  0   0   0   0   0   0   0   0   0   0   0   0  50   0   0   0   0   0
    1]
 [  0   0   0   4   0   0   0   1   0   4   0   0   0  92   0   0   1   0
    6]
 [  0   1   0   0   0   0   0   1   0   0   0   0   1   0  41   0   0   0
    3]
 [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   0  37   0   0
    0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0  15   0
    5]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1]
 [ 11   9  11   9  16  11  17  12  28  20  10  17   8   9   2   2   0   0
  262]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.83      0.83      0.83       150
                    Other       0.89      0.76      0.82       134
 Instrument-Agency(e2,e1)       0.66      0.72      0.69        32
 Member-Collection(e1,e2)       0.91      0.94      0.92       194
      Cause-Effect(e2,e1)       0.92      0.93      0.92       291
Entity-Destination(e1,e2)       0.88      0.88      0.88       153
 Content-Container(e1,e2)       0.90      0.95      0.93       210
     Message-Topic(e1,e2)       0.84      0.88      0.86       123
  Product-Producer(e2,e1)       0.84      0.92      0.88       201
 Member-Collection(e2,e1)       0.87      0.90      0.88       211
     Entity-Origin(e1,e2)       0.91      0.96      0.93       134
      Cause-Effect(e1,e2)       0.84      0.89      0.86       162
   Component-Whole(e1,e2)       0.81      0.98      0.88        51
     Message-Topic(e2,e1)       0.85      0.85      0.85       108
  Product-Producer(e1,e2)       0.95      0.87      0.91        47
     Entity-Origin(e2,e1)       0.86      0.95      0.90        39
 Content-Container(e2,e1)       0.88      0.68      0.77        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.66      0.58      0.62       454

                 accuracy                           0.84      2717
                macro avg       0.81      0.81      0.81      2717
             weighted avg       0.84      0.84      0.84      2717

Micro precision: 0.8732212160413971
Micro recall: 0.8948298718515245
Micro F1: 0.8838934962898297

Trained with model transformer, pretrain bert-base-uncased, dataset semeval2010 and preprocessing punct_digit:
Confusion matrix:
[[129   6   0   0   0   0   2   1   3   1   0   1   0   0   0   2   0   0
    5]
 [  3 105   0   0   0   0   0   5   0   0   0   0   0   0   0   0   0   0
   21]
 [  0   0  23   0   1   0   0   0   0   1   0   2   0   0   0   0   0   0
    5]
 [  0   0   0 183   0   0   0   0   0   1   1   0   0   1   0   0   0   0
    8]
 [  0   0   0   0 275   2   0   0   0   0   0   1   0   0   0   0   0   0
   13]
 [  0   0   0   0   7 137   0   0   0   0   0   3   0   0   0   0   0   0
    6]
 [  2   0   0   0   0   0 198   0   1   0   0   0   0   0   0   0   0   0
    9]
 [  0   1   0   0   0   0   0 114   0   0   0   0   0   0   0   0   0   0
    8]
 [  2   0   0   0   0   0   0   0 190   0   0   0   0   0   0   0   0   0
    9]
 [  0   0   1   5   1   1   0   0   0 188   0   0   0   3   0   0   0   0
   12]
 [  0   0   0   1   0   0   2   1   0   0 129   0   0   0   0   0   0   0
    1]
 [  4   0   5   0   0   1   0   0   2   0   0 133   0   0   0   0   2   0
   15]
 [  0   0   0   0   0   0   0   0   0   0   0   0  47   0   0   0   0   0
    4]
 [  0   0   0   4   0   0   0   1   0   5   0   0   0  94   0   0   1   0
    3]
 [  0   0   0   0   0   0   0   3   0   0   2   0   0   0  40   0   0   0
    2]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  39   0   0
    0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  19   0
    2]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1]
 [ 15  10   8   7  14  13  18  16  25  18  10   5   6  12   2   4   0   0
  271]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.83      0.86      0.85       150
                    Other       0.86      0.78      0.82       134
 Instrument-Agency(e2,e1)       0.62      0.72      0.67        32
 Member-Collection(e1,e2)       0.92      0.94      0.93       194
      Cause-Effect(e2,e1)       0.92      0.95      0.93       291
Entity-Destination(e1,e2)       0.89      0.90      0.89       153
 Content-Container(e1,e2)       0.90      0.94      0.92       210
     Message-Topic(e1,e2)       0.81      0.93      0.86       123
  Product-Producer(e2,e1)       0.86      0.95      0.90       201
 Member-Collection(e2,e1)       0.88      0.89      0.88       211
     Entity-Origin(e1,e2)       0.91      0.96      0.93       134
      Cause-Effect(e1,e2)       0.92      0.82      0.87       162
   Component-Whole(e1,e2)       0.89      0.92      0.90        51
     Message-Topic(e2,e1)       0.85      0.87      0.86       108
  Product-Producer(e1,e2)       0.95      0.85      0.90        47
     Entity-Origin(e2,e1)       0.87      1.00      0.93        39
 Content-Container(e2,e1)       0.86      0.86      0.86        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.69      0.60      0.64       454

                 accuracy                           0.85      2717
                macro avg       0.81      0.83      0.82      2717
             weighted avg       0.85      0.85      0.85      2717

Micro precision: 0.8798449612403101
Micro recall: 0.9027839151568714
Micro F1: 0.8911668484187569

Trained with model transformer, pretrain bert-base-uncased, dataset semeval2010 and preprocessing punct_stop_digit:
Confusion matrix:
[[120   5   0   0   1   0   2   2   1   0   0   8   0   0   0   0   0   0
   11]
 [  7  97   0   0   1   0   1   8   0   1   0   0   0   0   0   0   0   0
   19]
 [  0   0  23   0   1   0   0   0   0   1   0   1   1   0   0   0   0   0
    5]
 [  0   0   0 172   0   0   0   0   0   1   8   0   1   3   0   0   0   0
    9]
 [  0   3   0   0 256   4   0   0   0   7   0   2   0   0   0   0   0   0
   19]
 [  2   0   0   0  15 123   0   0   0   2   0   2   0   1   1   0   0   0
    7]
 [  1   0   0   0   0   0 190   0   2   3   0   1   1   0   0   0   0   0
   12]
 [  0   1   0   0   2   0   0 106   0   3   0   0   0   0   0   0   0   0
   11]
 [  5   0   0   1   0   0   0   0 176   0   0   0   0   1   0   0   1   0
   17]
 [  1   1   0   3  16   2   1   1   0 157   1   3   3   7   0   0   0   0
   15]
 [  0   0   0  10   0   0   1   0   0   0 117   0   0   0   0   0   0   0
    6]
 [  6   0   2   0   1   0   0   0   1   0   0 139   0   0   0   0   0   0
   13]
 [  0   0   0   1   0   0   2   0   0   2   0   0  42   0   0   0   0   0
    4]
 [  0   0   0   4   1   0   1   2   1   4   0   0   0  83   0   0   0   0
   12]
 [  0   1   0   0   1   0   0   0   0   0   0   0   0   0  41   0   0   0
    4]
 [  2   0   0   0   2   3   0   0   1   0   0   0   0   0   0  30   0   0
    1]
 [  1   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13   0
    7]
 [  0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
    0]
 [ 19  15  10   9  24  10  26  10  24  16   7  11   4  12   3   2   0   0
  252]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.73      0.80      0.76       150
                    Other       0.79      0.72      0.75       134
 Instrument-Agency(e2,e1)       0.66      0.72      0.69        32
 Member-Collection(e1,e2)       0.86      0.89      0.87       194
      Cause-Effect(e2,e1)       0.80      0.88      0.84       291
Entity-Destination(e1,e2)       0.87      0.80      0.83       153
 Content-Container(e1,e2)       0.85      0.90      0.88       210
     Message-Topic(e1,e2)       0.82      0.86      0.84       123
  Product-Producer(e2,e1)       0.85      0.88      0.86       201
 Member-Collection(e2,e1)       0.80      0.74      0.77       211
     Entity-Origin(e1,e2)       0.88      0.87      0.88       134
      Cause-Effect(e1,e2)       0.83      0.86      0.84       162
   Component-Whole(e1,e2)       0.81      0.82      0.82        51
     Message-Topic(e2,e1)       0.77      0.77      0.77       108
  Product-Producer(e1,e2)       0.91      0.87      0.89        47
     Entity-Origin(e2,e1)       0.94      0.77      0.85        39
 Content-Container(e2,e1)       0.93      0.59      0.72        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.59      0.56      0.57       454

                 accuracy                           0.79      2717
                macro avg       0.77      0.75      0.76      2717
             weighted avg       0.78      0.79      0.78      2717

Micro precision: 0.822067160924553
Micro recall: 0.8329650905877154
Micro F1: 0.8274802458296752

Trained with model transformer, pretrain bert-base-uncased, dataset semeval2010 and preprocessing entity_blinding:
Confusion matrix:
[[110   9   0   0   0   0   3   4   5   0   0   2   0   0   0   4   0   0
   13]
 [  8 101   0   0   2   0   2   4   0   0   0   0   0   0   0   0   0   0
   17]
 [  0   1  13   0   1   0   0   0   0   1   0   4   2   0   0   0   0   0
   10]
 [  0   0   0 177   0   0   0   0   0   6   0   0   0   3   0   0   0   0
    8]
 [  0   0   0   0 277   4   0   0   0   0   0   0   0   0   0   0   0   0
   10]
 [  0   0   0   0   5 141   0   0   0   0   0   4   0   0   0   0   0   0
    3]
 [  1   0   0   0   0   0 189   0   1   0   0   0   0   0   0   0   0   0
   19]
 [  0   4   3   0   0   0   2  91   4   0   4   1   1   0   0   0   0   0
   13]
 [  3   0   0   0   0   0   1   0 173   0   0   4   0   0   0   0   0   0
   20]
 [  0   0   0   7   1   0   2   0   0 189   0   1   0   1   0   0   0   0
   10]
 [  1   0   0   1   0   0   1   2   0   0 123   0   0   0   0   0   0   0
    6]
 [  5   1   1   0   1   1   0   0   0   0   0 136   1   0   2   0   1   0
   13]
 [  0   0   0   0   0   0   0   0   0   0   0   2  44   0   0   0   0   0
    5]
 [  0   0   0   5   0   0   1   2   0   6   0   0   0  79   0   0   1   0
   14]
 [  0   0   0   0   0   0   0   2   0   0   2   0   0   0  34   1   0   0
    8]
 [  3   0   0   0   0   0   0   0   1   0   0   0   0   0   1  31   0   0
    3]
 [  0   0   0   0   0   0   0   0   0   0   1   1   0   1   0   0  12   0
    7]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1]
 [ 18  19   8   9  26  24  26  15  31  20   7  28   8  12   3   4   4   0
  192]]
Test set results:
                           precision    recall  f1-score   support

   Component-Whole(e2,e1)       0.74      0.73      0.74       150
                    Other       0.75      0.75      0.75       134
 Instrument-Agency(e2,e1)       0.52      0.41      0.46        32
 Member-Collection(e1,e2)       0.89      0.91      0.90       194
      Cause-Effect(e2,e1)       0.88      0.95      0.92       291
Entity-Destination(e1,e2)       0.83      0.92      0.87       153
 Content-Container(e1,e2)       0.83      0.90      0.86       210
     Message-Topic(e1,e2)       0.76      0.74      0.75       123
  Product-Producer(e2,e1)       0.80      0.86      0.83       201
 Member-Collection(e2,e1)       0.85      0.90      0.87       211
     Entity-Origin(e1,e2)       0.90      0.92      0.91       134
      Cause-Effect(e1,e2)       0.74      0.84      0.79       162
   Component-Whole(e1,e2)       0.79      0.86      0.82        51
     Message-Topic(e2,e1)       0.82      0.73      0.77       108
  Product-Producer(e1,e2)       0.85      0.72      0.78        47
     Entity-Origin(e2,e1)       0.78      0.79      0.78        39
 Content-Container(e2,e1)       0.67      0.55      0.60        22
 Instrument-Agency(e1,e2)       0.00      0.00      0.00         1
Entity-Destination(e2,e1)       0.52      0.42      0.46       454

                 accuracy                           0.78      2717
                macro avg       0.73      0.73      0.73      2717
             weighted avg       0.77      0.78      0.77      2717

Micro precision: 0.8187633262260128
Micro recall: 0.8484312859036677
Micro F1: 0.8333333333333333

Trained with model cnn, pretrain glove, dataset ddi and preprocessing original:
Confusion matrix:
[[  86    5    1    0  129]
 [   1  165    3    0  191]
 [   0    3  130    0  169]
 [   0   41    1   21   33]
 [  24   75   30    2 4606]]
Test set results:
              precision    recall  f1-score   support

      advise       0.77      0.39      0.52       221
      effect       0.57      0.46      0.51       360
   mechanism       0.79      0.43      0.56       302
         int       0.91      0.22      0.35        96
        none       0.90      0.97      0.93      4737

    accuracy                           0.88      5716
   macro avg       0.79      0.49      0.57      5716
weighted avg       0.87      0.88      0.86      5716

Micro precision: 0.8761371588523443
Micro recall: 0.8761371588523443
Micro F1: 0.8761371588523443

Trained with model cnn, pretrain glove, dataset ddi and preprocessing punct_digit:
Confusion matrix:
[[ 103    2    1    0  115]
 [   1  163    2    0  194]
 [   1    3  109    0  189]
 [   0   39    0   19   38]
 [  25   78   43    1 4590]]
Test set results:
              precision    recall  f1-score   support

      advise       0.79      0.47      0.59       221
      effect       0.57      0.45      0.51       360
   mechanism       0.70      0.36      0.48       302
         int       0.95      0.20      0.33        96
        none       0.90      0.97      0.93      4737

    accuracy                           0.87      5716
   macro avg       0.78      0.49      0.57      5716
weighted avg       0.86      0.87      0.86      5716

Micro precision: 0.8719384184744576
Micro recall: 0.8719384184744576
Micro F1: 0.8719384184744576

Trained with model cnn, pretrain glove, dataset ddi and preprocessing punct_stop_digit:
Confusion matrix:
[[  61    4    1    0  155]
 [   1  125    0    0  234]
 [   0    3   67    0  232]
 [   0   37    0    1   58]
 [  26   43   29    0 4639]]
Test set results:
              precision    recall  f1-score   support

      advise       0.69      0.28      0.39       221
      effect       0.59      0.35      0.44       360
   mechanism       0.69      0.22      0.34       302
         int       1.00      0.01      0.02        96
        none       0.87      0.98      0.92      4737

    accuracy                           0.86      5716
   macro avg       0.77      0.37      0.42      5716
weighted avg       0.84      0.86      0.83      5716

Micro precision: 0.8560181945416375
Micro recall: 0.8560181945416375
Micro F1: 0.8560181945416375

Trained with model cnn, pretrain glove, dataset ddi and preprocessing entity_blinding:
Confusion matrix:
[[ 107    4    2    0  108]
 [   0  187    5    0  168]
 [   0    5  118    0  179]
 [   0   37    0   27   32]
 [  23   71   33    1 4609]]
Test set results:
              precision    recall  f1-score   support

      advise       0.82      0.48      0.61       221
      effect       0.62      0.52      0.56       360
   mechanism       0.75      0.39      0.51       302
         int       0.96      0.28      0.44        96
        none       0.90      0.97      0.94      4737

    accuracy                           0.88      5716
   macro avg       0.81      0.53      0.61      5716
weighted avg       0.88      0.88      0.87      5716

Micro precision: 0.8831350594821553
Micro recall: 0.8831350594821553
Micro F1: 0.8831350594821552

Trained with model pcnn, pretrain glove, dataset ddi and preprocessing original:
Confusion matrix:
[[  96    2    2    0  121]
 [   0  188    5    0  167]
 [   0    5  120    0  177]
 [   0   38    0   29   29]
 [  35   61   33    1 4607]]
Test set results:
              precision    recall  f1-score   support

      advise       0.73      0.43      0.55       221
      effect       0.64      0.52      0.57       360
   mechanism       0.75      0.40      0.52       302
         int       0.97      0.30      0.46        96
        none       0.90      0.97      0.94      4737

    accuracy                           0.88      5716
   macro avg       0.80      0.53      0.61      5716
weighted avg       0.87      0.88      0.87      5716

Micro precision: 0.8817354793561931
Micro recall: 0.8817354793561931
Micro F1: 0.8817354793561931

Trained with model pcnn, pretrain glove, dataset ddi and preprocessing punct_digit:
Confusion matrix:
[[ 107    1    2    1  110]
 [   1  190    1    0  168]
 [   2    7  107    0  186]
 [   0   38    0   27   31]
 [  31   67   40    1 4598]]
Test set results:
              precision    recall  f1-score   support

      advise       0.76      0.48      0.59       221
      effect       0.63      0.53      0.57       360
   mechanism       0.71      0.35      0.47       302
         int       0.93      0.28      0.43        96
        none       0.90      0.97      0.94      4737

    accuracy                           0.88      5716
   macro avg       0.79      0.52      0.60      5716
weighted avg       0.87      0.88      0.87      5716

Micro precision: 0.8798110566829951
Micro recall: 0.8798110566829951
Micro F1: 0.8798110566829951

Trained with model pcnn, pretrain glove, dataset ddi and preprocessing punct_stop_digit:
Confusion matrix:
[[  86    5    0    1  129]
 [   3  179    3    2  173]
 [   1    4   90    0  207]
 [   0   39    0   23   34]
 [  46   76   45    3 4567]]
Test set results:
              precision    recall  f1-score   support

      advise       0.63      0.39      0.48       221
      effect       0.59      0.50      0.54       360
   mechanism       0.65      0.30      0.41       302
         int       0.79      0.24      0.37        96
        none       0.89      0.96      0.93      4737

    accuracy                           0.87      5716
   macro avg       0.71      0.48      0.55      5716
weighted avg       0.85      0.87      0.85      5716

Micro precision: 0.8651154653603919
Micro recall: 0.8651154653603919
Micro F1: 0.8651154653603919

Trained with model pcnn, pretrain glove, dataset ddi and preprocessing entity_blinding:
Confusion matrix:
[[ 102    2    1    0  116]
 [   0  191    5    0  164]
 [   0    5  112    0  185]
 [   0   38    0   38   20]
 [  18   59   32    8 4620]]
Test set results:
              precision    recall  f1-score   support

      advise       0.85      0.46      0.60       221
      effect       0.65      0.53      0.58       360
   mechanism       0.75      0.37      0.50       302
         int       0.83      0.40      0.54        96
        none       0.90      0.98      0.94      4737

    accuracy                           0.89      5716
   macro avg       0.80      0.55      0.63      5716
weighted avg       0.88      0.89      0.87      5716

Micro precision: 0.8857592722183345
Micro recall: 0.8857592722183345
Micro F1: 0.8857592722183345

Trained with model transformer, pretrain bert-base-uncased, dataset ddi and preprocessing original:
Confusion matrix:
[[ 183    0    0    2   36]
 [   2  314    7    1   36]
 [   4    6  220   16   56]
 [   0   38    3   43   12]
 [   7   39   32   37 4622]]
Test set results:
              precision    recall  f1-score   support

      advise       0.93      0.83      0.88       221
      effect       0.79      0.87      0.83       360
   mechanism       0.84      0.73      0.78       302
         int       0.43      0.45      0.44        96
        none       0.97      0.98      0.97      4737

    accuracy                           0.94      5716
   macro avg       0.79      0.77      0.78      5716
weighted avg       0.94      0.94      0.94      5716

Micro precision: 0.9415675297410777
Micro recall: 0.9415675297410777
Micro F1: 0.9415675297410777

Trained with model transformer, pretrain bert-base-uncased, dataset ddi and preprocessing punct_digit:
Confusion matrix:
[[ 169    2    1    2   47]
 [   3  287    5    3   62]
 [   7    6  219   17   53]
 [   0   38    2   43   13]
 [  14   37   32   28 4626]]
Test set results:
              precision    recall  f1-score   support

      advise       0.88      0.76      0.82       221
      effect       0.78      0.80      0.79       360
   mechanism       0.85      0.73      0.78       302
         int       0.46      0.45      0.46        96
        none       0.96      0.98      0.97      4737

    accuracy                           0.93      5716
   macro avg       0.78      0.74      0.76      5716
weighted avg       0.93      0.93      0.93      5716

Micro precision: 0.9349195241427571
Micro recall: 0.9349195241427571
Micro F1: 0.9349195241427571

Trained with model transformer, pretrain bert-base-uncased, dataset ddi and preprocessing punct_stop_digit:
Confusion matrix:
[[ 155   13    3    2   48]
 [   6  254    6    1   93]
 [   7    9  212    0   74]
 [   0   41    3   35   17]
 [  63   53   59    7 4555]]
Test set results:
              precision    recall  f1-score   support

      advise       0.67      0.70      0.69       221
      effect       0.69      0.71      0.70       360
   mechanism       0.75      0.70      0.72       302
         int       0.78      0.36      0.50        96
        none       0.95      0.96      0.96      4737

    accuracy                           0.91      5716
   macro avg       0.77      0.69      0.71      5716
weighted avg       0.91      0.91      0.91      5716

Micro precision: 0.9116515045486354
Micro recall: 0.9116515045486354
Micro F1: 0.9116515045486354

Trained with model transformer, pretrain bert-base-uncased, dataset ddi and preprocessing entity_blinding:
Confusion matrix:
[[ 189    2    0    2   28]
 [   1  302    4    0   53]
 [   7    6  240    0   49]
 [   0   39    2   43   12]
 [  18   51   35   16 4617]]
Test set results:
              precision    recall  f1-score   support

      advise       0.88      0.86      0.87       221
      effect       0.76      0.84      0.79       360
   mechanism       0.85      0.79      0.82       302
         int       0.70      0.45      0.55        96
        none       0.97      0.97      0.97      4737

    accuracy                           0.94      5716
   macro avg       0.83      0.78      0.80      5716
weighted avg       0.94      0.94      0.94      5716

Micro precision: 0.9431420573827851
Micro recall: 0.9431420573827851
Micro F1: 0.9431420573827851

Trained with model transformer, pretrain dmis-lab-biobert-v11, dataset ddi and preprocessing original:
Confusion matrix:
[[ 168    0    0    1   52]
 [   1  303    4    0   52]
 [   4    2  252    0   44]
 [   0   41    4   39   12]
 [  14   28   29    8 4658]]
Test set results:
              precision    recall  f1-score   support

      advise       0.90      0.76      0.82       221
      effect       0.81      0.84      0.83       360
   mechanism       0.87      0.83      0.85       302
         int       0.81      0.41      0.54        96
        none       0.97      0.98      0.97      4737

    accuracy                           0.95      5716
   macro avg       0.87      0.77      0.80      5716
weighted avg       0.95      0.95      0.95      5716

Micro precision: 0.9482155353393982
Micro recall: 0.9482155353393982
Micro F1: 0.9482155353393982

Trained with model transformer, pretrain dmis-lab-biobert-v11, dataset ddi and preprocessing punct_digit:
Confusion matrix:
[[ 179    4    1    2   35]
 [   4  292    5    0   59]
 [   7    3  254    0   38]
 [   0   40    3   39   14]
 [  24   31   26   35 4621]]
Test set results:
              precision    recall  f1-score   support

      advise       0.84      0.81      0.82       221
      effect       0.79      0.81      0.80       360
   mechanism       0.88      0.84      0.86       302
         int       0.51      0.41      0.45        96
        none       0.97      0.98      0.97      4737

    accuracy                           0.94      5716
   macro avg       0.80      0.77      0.78      5716
weighted avg       0.94      0.94      0.94      5716

Micro precision: 0.9420923722883136
Micro recall: 0.9420923722883136
Micro F1: 0.9420923722883136

Trained with model transformer, pretrain dmis-lab-biobert-v11, dataset ddi and preprocessing punct_stop_digit:
Confusion matrix:
[[ 140   11    4    2   64]
 [   5  268    2    6   79]
 [   6    6  212    0   78]
 [   0   41    3   39   13]
 [  37   52   55   11 4582]]
Test set results:
              precision    recall  f1-score   support

      advise       0.74      0.63      0.68       221
      effect       0.71      0.74      0.73       360
   mechanism       0.77      0.70      0.73       302
         int       0.67      0.41      0.51        96
        none       0.95      0.97      0.96      4737

    accuracy                           0.92      5716
   macro avg       0.77      0.69      0.72      5716
weighted avg       0.91      0.92      0.91      5716

Micro precision: 0.9168999300209937
Micro recall: 0.9168999300209937
Micro F1: 0.9168999300209937

Trained with model transformer, pretrain dmis-lab-biobert-v11, dataset ddi and preprocessing entity_blinding:
Confusion matrix:
[[ 185    0    0    2   34]
 [   1  295    6    1   57]
 [   7    3  248    0   44]
 [   0   37    2   42   15]
 [  45   38   44   10 4600]]
Test set results:
              precision    recall  f1-score   support

      advise       0.78      0.84      0.81       221
      effect       0.79      0.82      0.80       360
   mechanism       0.83      0.82      0.82       302
         int       0.76      0.44      0.56        96
        none       0.97      0.97      0.97      4737

    accuracy                           0.94      5716
   macro avg       0.83      0.78      0.79      5716
weighted avg       0.94      0.94      0.94      5716

Micro precision: 0.9394681595521344
Micro recall: 0.9394681595521344
Micro F1: 0.9394681595521344

Trained with model transformer, pretrain scibert, dataset ddi and preprocessing original:

Test set results:
              precision    recall  f1-score   support

      advise       0.88      0.83      0.86       221
      effect       0.78      0.80      0.79       360
   mechanism       0.85      0.83      0.84       302
         int       0.79      0.35      0.49        96
        none       0.97      0.98      0.98      4737

    accuracy                           0.95      5716
   macro avg       0.86      0.76      0.79      5716
weighted avg       0.95      0.95      0.95      5716

Micro precision: 0.9478656403079077
Micro recall: 0.9478656403079077
Micro F1: 0.9478656403079077


Trained with model transformer, pretrain scibert, dataset ddi and preprocessing punct_digit:

Test set results:
              precision    recall  f1-score   support

      advise       0.86      0.82      0.84       221
      effect       0.76      0.76      0.76       360
   mechanism       0.87      0.80      0.84       302
         int       0.63      0.39      0.48        96
        none       0.97      0.98      0.97      4737

    accuracy                           0.94      5716
   macro avg       0.82      0.75      0.78      5716
weighted avg       0.94      0.94      0.94      5716

Micro precision: 0.940517844646606
Micro recall: 0.940517844646606
Micro F1: 0.940517844646606

Trained with model transformer, pretrain scibert, dataset ddi and preprocessing punct_stop_digit:

Test set results:
              precision    recall  f1-score   support

      advise       0.77      0.79      0.78       221
      effect       0.66      0.75      0.70       360
   mechanism       0.76      0.73      0.74       302
         int       0.69      0.39      0.49        96
        none       0.96      0.96      0.96      4737

    accuracy                           0.92      5716
   macro avg       0.77      0.72      0.73      5716
weighted avg       0.92      0.92      0.92      5716

Micro precision: 0.9174247725682295
Micro recall: 0.9174247725682295
Micro F1: 0.9174247725682295

Trained with model transformer, pretrain scibert, dataset ddi and preprocessing entity_blinding:

Test set results:
              precision    recall  f1-score   support

      advise       0.87      0.90      0.88       221
      effect       0.79      0.84      0.81       360
   mechanism       0.88      0.84      0.86       302
         int       0.72      0.46      0.56        96
        none       0.98      0.98      0.98      4737

    accuracy                           0.95      5716
   macro avg       0.85      0.80      0.82      5716
weighted avg       0.95      0.95      0.95      5716

Micro precision: 0.9515395381385584
Micro recall: 0.9515395381385584
Micro F1: 0.9515395381385584


