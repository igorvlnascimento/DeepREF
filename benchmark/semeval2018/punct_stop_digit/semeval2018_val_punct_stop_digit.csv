original_sentence	e1	e2	relation_type	metadata	preprocessed_sentence
In Semantic Role Labeling (SRL), arguments are usually limited in a syntax subtree.	arguments	syntax subtree	part_whole	{'e1': {'word': 'arguments', 'word_index': [(7, 7)], 'id': 'C08-1105.2'}, 'e2': {'word': 'syntax subtree', 'word_index': [(10, 11)], 'id': 'C08-1105.3'}}	Semantic Role Labeling ( SRL ) , arguments usually limited syntax subtree .
It is reasonable to label arguments locally in such a sub-tree rather than a whole tree.	arguments	sub-tree	part_whole	{'e1': {'word': 'arguments', 'word_index': [(2, 2)], 'id': 'C08-1105.4'}, 'e2': {'word': 'sub-tree', 'word_index': [(4, 6)], 'id': 'C08-1105.5'}}	reasonable label arguments locally sub - tree rather whole tree .
The anchor group approach achieves an accuracy of 87.75% and the single anchor approach achieves 83.63%.	anchor group approach	accuracy	result	{'e1': {'word': 'anchor group approach', 'word_index': [(0, 2)], 'id': 'C08-1105.15'}, 'e2': {'word': 'accuracy', 'word_index': [(4, 4)], 'id': 'C08-1105.16'}}	anchor group approach achieves accuracy 87.75 % single anchor approach achieves 83.63 %.
Experimental results also indicate that the prediction of MP improves semantic role labeling.	prediction of MP	semantic role labeling	result	{'e1': {'word': 'prediction of MP', 'word_index': [(4, 5)], 'id': 'C08-1105.18'}, 'e2': {'word': 'semantic role labeling', 'word_index': [(7, 9)], 'id': 'C08-1105.19'}}	Experimental results also indicate prediction MP improves semantic role labeling .
Recently the LATL has undertaken the development of a multilingual translation system based on a symbolic parsing technology and on a transfer-based translation model.	symbolic parsing technology	multilingual translation system	usage	{'e1': {'word': 'symbolic parsing technology', 'word_index': [(8, 10)], 'id': 'L08-1579.2'}, 'e2': {'word': 'multilingual translation system', 'word_index': [(4, 6)], 'id': 'L08-1579.1'}}	Recently LATL undertaken development multilingual translation system based symbolic parsing technology transfer - based translation model .
This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning.	term weight learning	verbal word senses	usage	{'e1': {'word': 'term weight learning', 'word_index': [(10, 12)], 'id': 'E99-1028.2'}, 'e2': {'word': 'verbal word senses', 'word_index': [(6, 8)], 'id': 'E99-1028.1'}}	paper describes unsupervised learning algorithm disambiguating verbal word senses using term weight learning .
This is quite different from current approaches to semantic parsing or chunking that depend on full statistical syntactic parsers that require tree bank style annotation.	statistical syntactic parsers	chunking	usage	{'e1': {'word': 'statistical syntactic parsers', 'word_index': [(9, 11)], 'id': 'N04-4037.10'}, 'e2': {'word': 'chunking', 'word_index': [(6, 6)], 'id': 'N04-4037.9'}}	quite different current approaches semantic parsing chunking depend full statistical syntactic parsers require tree bank style annotation .
We compare it with a recently proposed word-by-word semantic chunker and present results that show that the phrase-by-phrase approach performs better than its word-by-word counterpart.	phrase-by-phrase approach	word-by-word counterpart	compare	{'e1': {'word': 'phrase-by-phrase approach', 'word_index': [(12, 15)], 'id': 'N04-4037.13'}, 'e2': {'word': 'word-by-word counterpart', 'word_index': [(20, 23)], 'id': 'N04-4037.14'}}	compare recently proposed word - word semantic chunker and results that that phrase - by- approach performs better than its - by - counterpart .
The primary objective of this basic research is to develop improved methods and models for acoustic recognition of continuous speech.	acoustic recognition	continuous speech	usage	{'e1': {'word': 'acoustic recognition', 'word_index': [(8, 9)], 'id': 'H91-1080.1'}, 'e2': {'word': 'continuous speech', 'word_index': [(10, 11)], 'id': 'H91-1080.2'}}	primary objective basic research develop improved methods models acoustic recognition continuous speech .
The work has focussed on developing accurate and detailed mathematical models of phonemes and their coarticulation for the purpose of large-vocabulary continuous speech recognition.	phonemes	large-vocabulary continuous speech recognition	usage	{'e1': {'word': 'phonemes', 'word_index': [(7, 7)], 'id': 'H91-1080.3'}, 'e2': {'word': 'large-vocabulary continuous speech recognition', 'word_index': [(10, 15)], 'id': 'H91-1080.4'}}	work focussed developing accurate detailed mathematical models phonemes coarticulation purpose large - vocabulary continuous speech recognition .
KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus.	word list	corpus	part_whole	{'e1': {'word': 'word list', 'word_index': [(3, 4)], 'id': 'W00-0902.5'}, 'e2': {'word': 'corpus', 'word_index': [(9, 9)], 'id': 'W00-0902.6'}}	KeyWords compares a word list extracted from ' the corpus ' ( the which is in ) with list made from corpus .
KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus.	word list	reference corpus	part_whole	{'e1': {'word': 'word list', 'word_index': [(17, 18)], 'id': 'W00-0902.8'}, 'e2': {'word': 'reference corpus', 'word_index': [(21, 22)], 'id': 'W00-0902.9'}}	KeyWords compares a list extracted from ' the corpus ' ( the which is in ) with word list made from reference corpus .
Five English corpora were compared to reference corpora of various sizes (varying from two to 100 times larger than the study corpus).	English corpora	reference corpora	compare	{'e1': {'word': 'English corpora', 'word_index': [(1, 2)], 'id': 'W00-0902.15'}, 'e2': {'word': 'reference corpora', 'word_index': [(4, 5)], 'id': 'W00-0902.16'}}	Five English corpora compared reference corpora various sizes ( varying two 100 times larger study corpus ) .
The results indicate that a reference corpus that is five times as large as the study corpus yielded a larger number of keywords than a smaller reference corpus.	keywords	corpus	part_whole	{'e1': {'word': 'keywords', 'word_index': [(12, 12)], 'id': 'W00-0902.20'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'W00-0902.19'}}	results indicate reference corpus five times large study corpus yielded larger number keywords smaller reference corpus .
The implication is that a larger reference corpus is not always better than a smaller one, for WordSmith Tools Keywords analysis, while a reference corpus that is less than five times the size of the study corpus may not be reliable.	reference corpus	study corpus	compare	{'e1': {'word': 'reference corpus', 'word_index': [(14, 15)], 'id': 'W00-0902.28'}, 'e2': {'word': 'study corpus', 'word_index': [(20, 21)], 'id': 'W00-0902.29'}}	implication larger reference corpus always better smaller one , WordSmith Tools Keywords analysis , reference corpus less five times size study corpus may reliable .
In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a non-lexicalised and a lexicalised mode.	dependency analyser	Italian	usage	{'e1': {'word': 'dependency analyser', 'word_index': [(5, 6)], 'id': 'W02-1501.1'}, 'e2': {'word': 'Italian', 'word_index': [(7, 7)], 'id': 'W02-1501.2'}}	paper report qualitative evaluation performance dependency analyser Italian runs non-lexicalised lexicalised mode .
Results shed light on the contribution of types of lexical information to parsing.	lexical information	parsing	usage	{'e1': {'word': 'lexical information', 'word_index': [(5, 6)], 'id': 'W02-1501.5'}, 'e2': {'word': 'parsing', 'word_index': [(7, 7)], 'id': 'W02-1501.6'}}	Results shed light contribution types lexical information parsing .
In international phonology and speech synthesis research, it has been suggested that the relative informativeness of a word can be used to predict pitch prominence.	relative informativeness	word	model-feature	{'e1': {'word': 'relative informativeness', 'word_index': [(7, 8)], 'id': 'W99-0619.3'}, 'e2': {'word': 'word', 'word_index': [(9, 9)], 'id': 'W99-0619.4'}}	international phonology speech synthesis research , suggested relative informativeness word used predict pitch prominence .
In this paper, we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted measures of informativeness.	measures	informativeness	model-feature	{'e1': {'word': 'measures', 'word_index': [(12, 12)], 'id': 'W99-0619.7'}, 'e2': {'word': 'informativeness', 'word_index': [(13, 13)], 'id': 'W99-0619.8'}}	paper , provide empirical evidence support existence correlation employing two widely accepted measures informativeness .
Our experiments show that there is a positive correlation between the informativeness of a word and its pitch accent assignment.	informativeness	word	model-feature	{'e1': {'word': 'informativeness', 'word_index': [(4, 4)], 'id': 'W99-0619.9'}, 'e2': {'word': 'word', 'word_index': [(5, 5)], 'id': 'W99-0619.10'}}	experiments show positive correlation informativeness word pitch accent assignment .
The computation of word informativeness is inexpensive and can be incorporated into speech synthesis systems easily.	word informativeness	speech synthesis systems	usage	{'e1': {'word': 'word informativeness', 'word_index': [(1, 2)], 'id': 'W99-0619.14'}, 'e2': {'word': 'speech synthesis systems', 'word_index': [(5, 7)], 'id': 'W99-0619.15'}}	computation word informativeness inexpensive incorporated speech synthesis systems easily .
"""An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed."	parsing algorithm	augmented context-free grammars	usage	{'e1': {'word': 'parsing algorithm', 'word_index': [(2, 3)], 'id': 'J87-1004.1'}, 'e2': {'word': 'augmented context-free grammars', 'word_index': [(4, 8)], 'id': 'J87-1004.2'}}	""" efficient parsing algorithm augmented context - free grammars introduced , application line natural language interfaces discussed ."
The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar.	generalized LR parsing algorithm	LR shift-reduce parsing table	usage	{'e1': {'word': 'generalized LR parsing algorithm', 'word_index': [(1, 4)], 'id': 'J87-1004.4'}, 'e2': {'word': 'LR shift-reduce parsing table', 'word_index': [(7, 12)], 'id': 'J87-1004.5'}}	algorithm generalized LR parsing algorithm , precomputes LR shift - reduce parsing table ( possibly multiple entries ) given augmented context - free grammar .
We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables.	LR parsing tables	chart parsing algorithm	usage	{'e1': {'word': 'LR parsing tables', 'word_index': [(10, 12)], 'id': 'J87-1004.14'}, 'e2': {'word': 'chart parsing algorithm', 'word_index': [(5, 7)], 'id': 'J87-1004.13'}}	also view parsing algorithm extended chart parsing algorithm efficiently guided LR parsing tables .
"Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, based on the technique developed at CMU."""	on-line parser	Japanese language	usage	{'e1': {'word': 'on-line parser', 'word_index': [(3, 5)], 'id': 'J87-1004.21'}, 'e2': {'word': 'Japanese language', 'word_index': [(6, 7)], 'id': 'J87-1004.22'}}	"Also , commercial - line parser Japanese language built Intelligent Technology Incorporation , based technique developed CMU . """
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).	hybrid statistical and structural semantic model	multi-stage spoken language understanding (SLU)	usage	{'e1': {'word': 'hybrid statistical and structural semantic model', 'word_index': [(2, 6)], 'id': 'W04-3002.1'}, 'e2': {'word': 'multi-stage spoken language understanding (SLU)', 'word_index': [(7, 13)], 'id': 'W04-3002.2'}}	article proposes hybrid statistical structural semantic model multi-stage spoken language understanding ( SLU ) .
The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted.	weighted finite-state transducer (WFST)-based parser	SLU	usage	{'e1': {'word': 'weighted finite-state transducer (WFST)-based parser', 'word_index': [(4, 14)], 'id': 'W04-3002.4'}, 'e2': {'word': 'SLU', 'word_index': [(2, 2)], 'id': 'W04-3002.3'}}	first stage SLU utilizes weighted finite - state transducer ( WFST ) - based parser , encodes regular grammar concepts extracted .
The proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger.	n-gram semantic tagger	regular grammar	usage	{'e1': {'word': 'n-gram semantic tagger', 'word_index': [(10, 12)], 'id': 'W04-3002.7'}, 'e2': {'word': 'regular grammar', 'word_index': [(3, 4)], 'id': 'W04-3002.6'}}	proposed method improves regular grammar model incorporating well - known n-gram semantic tagger .
This paper presents a corpus-based account of structural priming in human sentence processing, focusing on the role that syntactic representations play in such an account.	structural priming	sentence processing	model-feature	{'e1': {'word': 'structural priming', 'word_index': [(6, 7)], 'id': 'W06-1637.2'}, 'e2': {'word': 'sentence processing', 'word_index': [(9, 10)], 'id': 'W06-1637.3'}}	paper presents corpus - based account structural priming human sentence processing , focusing role syntactic representations play account .
We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue, annotated syntactically with Combinatory Categorial Grammar (CCG) derivations.	spontaneous spoken dialogue	corpus	part_whole	{'e1': {'word': 'spontaneous spoken dialogue', 'word_index': [(6, 8)], 'id': 'W06-1637.7'}, 'e2': {'word': 'corpus', 'word_index': [(5, 5)], 'id': 'W06-1637.6'}}	estimate strength structural priming effects corpus spontaneous spoken dialogue , annotated syntactically Combinatory Categorial Grammar ( CCG ) derivations .
In particular, we present evidence for priming between lexical and syntactic categories encoding partially satisfied sub-categorization frames, and we show that priming effects exist both for incremental and normal-form CCG derivations.	priming effects	incremental and normal-form CCG derivations	model-feature	{'e1': {'word': 'priming effects', 'word_index': [(15, 16)], 'id': 'W06-1637.13'}, 'e2': {'word': 'incremental and normal-form CCG derivations', 'word_index': [(18, 23)], 'id': 'W06-1637.14'}}	particular , present evidence priming lexical syntactic categories encoding partially satisfied sub-categorization frames , show priming effects exist incremental normal - form CCG derivations .
This paper describes the application of an ensemble of indexing and classification systems, which have been shown to be successful in information retrieval and classification of medical literature, to a new task of assigning ICD-9-CM codes to the clinical history and impression sections of radiology reports.	information retrieval	medical literature	usage	{'e1': {'word': 'information retrieval', 'word_index': [(10, 11)], 'id': 'W07-1014.1'}, 'e2': {'word': 'medical literature', 'word_index': [(13, 14)], 'id': 'W07-1014.2'}}	paper describes application ensemble indexing classification systems , shown successful information retrieval classification medical literature , new task assigning ICD-9 - CM codes to the and impression of radiology .
This type of summary could serve as an effective navigation tool for accessing information in long texts, such as books.	summary	texts	model-feature	{'e1': {'word': 'summary', 'word_index': [(1, 1)], 'id': 'P07-1069.2'}, 'e2': {'word': 'texts', 'word_index': [(10, 10)], 'id': 'P07-1069.4'}}	type summary could serve effective navigation tool accessing information long texts , books .
To generate a coherent table-of-contents, we need to capture both global dependencies across different titles in the table and local constraints within sections.	titles	table-of-contents	usage	{'e1': {'word': 'titles', 'word_index': [(13, 13)], 'id': 'P07-1069.8'}, 'e2': {'word': 'table-of-contents', 'word_index': [(2, 5)], 'id': 'P07-1069.6'}}	generate coherent table - - contents , need capture global dependencies across different titles table local constraints within sections .
First, we compare cruiser with a baseline system-initiative DS, and show that users prefer cruiser.	cruiser	system-initiative DS	compare	{'e1': {'word': 'cruiser', 'word_index': [(3, 3)], 'id': 'P08-1055.7'}, 'e2': {'word': 'system-initiative DS', 'word_index': [(5, 8)], 'id': 'P08-1055.8'}}	First , compare cruiser baseline system - initiative DS , show users prefer cruiser .
To improve the Mandarin large vocabulary continuous speech recognition (LVCSR), a unified framework based approach is introduced to exploit multi-level linguistic knowledge.	multi-level linguistic knowledge	large vocabulary continuous speech recognition (LVCSR)	usage	{'e1': {'word': 'multi-level linguistic knowledge', 'word_index': [(17, 19)], 'id': 'D08-1086.3'}, 'e2': {'word': 'large vocabulary continuous speech recognition (LVCSR)', 'word_index': [(2, 9)], 'id': 'D08-1086.2'}}	improve Mandarin large vocabulary continuous speech recognition ( LVCSR ) , unified framework based approach introduced exploit multi-level linguistic knowledge .
In this framework, each knowledge source is represented by a Weighted Finite State Transducer (WFST), and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources.	Weighted Finite State Transducer (WFST)	knowledge source	model-feature	{'e1': {'word': 'Weighted Finite State Transducer (WFST)', 'word_index': [(5, 11)], 'id': 'D08-1086.5'}, 'e2': {'word': 'knowledge source', 'word_index': [(2, 3)], 'id': 'D08-1086.4'}}	framework , knowledge source represented Weighted Finite State Transducer ( WFST ) , combined obtain so-called analyzer integrating multi-level knowledge sources .
Due to the uniform transducer representation, any knowledge source can be easily integrated into the analyzer, as long as it can be encoded into WFSTs.	knowledge source	WFSTs	usage	{'e1': {'word': 'knowledge source', 'word_index': [(5, 6)], 'id': 'D08-1086.8'}, 'e2': {'word': 'WFSTs', 'word_index': [(13, 13)], 'id': 'D08-1086.9'}}	Due uniform transducer representation , knowledge source easily integrated analyzer , long encoded WFSTs .
In this paper we discuss algorithms for clustering words into classes from unlabeled text using unsupervised algorithms, based on distributional and morphological information.	words	unlabeled text	part_whole	{'e1': {'word': 'words', 'word_index': [(4, 4)], 'id': 'E03-1009.1'}, 'e2': {'word': 'unlabeled text', 'word_index': [(6, 7)], 'id': 'E03-1009.2'}}	paper discuss algorithms clustering words classes unlabeled text using unsupervised algorithms , based distributional morphological information .
"""This paper presents an unsupervised relation extraction algorithm, which induces relations between entity pairs by grouping them into a ""natural"" number of clusters based on the similarity of their contexts."	unsupervised relation extraction algorithm	entity pairs	usage	{'e1': {'word': 'unsupervised relation extraction algorithm', 'word_index': [(3, 6)], 'id': 'I05-2045.2'}, 'e2': {'word': 'entity pairs', 'word_index': [(10, 11)], 'id': 'I05-2045.4'}}	""" paper presents unsupervised relation extraction algorithm , induces relations entity pairs grouping "" natural "" number clusters based similarity contexts ."
This paper presents a method that measures the similarity between compound nouns in different languages to locate translation equivalents from corpora.	translation equivalents	corpora	part_whole	{'e1': {'word': 'translation equivalents', 'word_index': [(10, 11)], 'id': 'C02-1065.5'}, 'e2': {'word': 'corpora', 'word_index': [(12, 12)], 'id': 'C02-1065.6'}}	paper presents method measures similarity compound nouns different languages locate translation equivalents corpora .
The method uses information from unrelated corpora in different languages that do not have to be parallel.	corpora	languages	model-feature	{'e1': {'word': 'corpora', 'word_index': [(4, 4)], 'id': 'C02-1065.7'}, 'e2': {'word': 'languages', 'word_index': [(6, 6)], 'id': 'C02-1065.8'}}	method uses information unrelated corpora different languages parallel .
The method compares the contexts of target compound nouns and translation candidates in the word or semantic attribute level.	contexts	translation candidates	compare	{'e1': {'word': 'contexts', 'word_index': [(2, 2)], 'id': 'C02-1065.10'}, 'e2': {'word': 'translation candidates', 'word_index': [(6, 7)], 'id': 'C02-1065.12'}}	method compares contexts target compound nouns translation candidates word semantic attribute level .
In this paper, we show how this measuring method can be applied to select the best English translation candidate for Japanese compound nouns in more than 70% of the cases.	English translation	Japanese compound nouns	usage	{'e1': {'word': 'English translation', 'word_index': [(8, 9)], 'id': 'C02-1065.15'}, 'e2': {'word': 'Japanese compound nouns', 'word_index': [(11, 13)], 'id': 'C02-1065.16'}}	paper , show measuring method applied select best English translation candidate Japanese compound nouns 70 % cases .
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text	text mining	statistical Arabic NLP	part_whole	{'e1': {'word': 'text mining', 'word_index': [(6, 7)], 'id': 'L08-1611.2'}, 'e2': {'word': 'statistical Arabic NLP', 'word_index': [(1, 3)], 'id': 'L08-1611.1'}}	Applications statistical Arabic NLP general , text mining specific , along tools underneath perform much better statistical processing operates deeper language factorization ( ) raw text
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text	statistical processing	language factorization(s)	usage	{'e1': {'word': 'statistical processing', 'word_index': [(16, 17)], 'id': 'L08-1611.3'}, 'e2': {'word': 'language factorization(s)', 'word_index': [(20, 23)], 'id': 'L08-1611.4'}}	Applications statistical Arabic NLP general , text mining specific , along tools underneath perform much better statistical processing operates deeper language factorization ( ) raw text
While building this LR, we had to go beyond the conventional exclusive collection of words from dictionaries and thesauri that cannot alone produce a satisfactory coverage of this highly inflective and derivative language.	words	dictionaries	part_whole	{'e1': {'word': 'words', 'word_index': [(8, 8)], 'id': 'L08-1611.9'}, 'e2': {'word': 'dictionaries', 'word_index': [(9, 9)], 'id': 'L08-1611.10'}}	building LR , go beyond conventional exclusive collection words dictionaries thesauri cannot alone produce satisfactory coverage highly inflective derivative language .
With the aid of the same large-scale Arabic morphological analyzer and PoS tagger in the runtime, the possible senses of virtually any given Arabic word are retrievable.	PoS tagger	Arabic word	usage	{'e1': {'word': 'PoS tagger', 'word_index': [(7, 8)], 'id': 'L08-1611.21'}, 'e2': {'word': 'Arabic word', 'word_index': [(15, 16)], 'id': 'L08-1611.22'}}	aid large - scale Arabic morphological analyzer PoS tagger runtime , possible senses virtually given Arabic word retrievable .
Similarly to the well-established ROVER approach of ( Fiscus, 1997 ) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network.	confusion network	consensus translation	usage	{'e1': {'word': 'confusion network', 'word_index': [(20, 21)], 'id': 'E06-1005.7'}, 'e2': {'word': 'consensus translation', 'word_index': [(16, 17)], 'id': 'E06-1005.6'}}	Similarly well - established ROVER approach ( Fiscus , 1997 ) combining speech recognition hypotheses , consensus translation computed voting confusion network .
To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.	statistical alignment algorithm	word alignments	usage	{'e1': {'word': 'statistical alignment algorithm', 'word_index': [(13, 15)], 'id': 'E06-1005.11'}, 'e2': {'word': 'word alignments', 'word_index': [(6, 7)], 'id': 'E06-1005.9'}}	create confusion network , produce pairwise word alignments original machine translation hypotheses enhanced statistical alignment algorithm explicitly models word reordering .
The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.	document	alignment	usage	{'e1': {'word': 'document', 'word_index': [(2, 2)], 'id': 'E06-1005.13'}, 'e2': {'word': 'alignment', 'word_index': [(10, 10)], 'id': 'E06-1005.16'}}	context whole document translations rather single sentence taken account produce alignment .
The method was also tested in the framework of multi-source and speech translation.	method	multi-source and speech translation	usage	{'e1': {'word': 'method', 'word_index': [(0, 0)], 'id': 'E06-1005.20'}, 'e2': {'word': 'multi-source and speech translation', 'word_index': [(4, 6)], 'id': 'E06-1005.21'}}	method also tested framework multi-source speech translation .
We present a two stage parser that recovers Penn Treebank style syntactic analyses of new sentences including skeletal syntactic structure, and, for the first time, both function tags and empty categories.	syntactic analyses	sentences	usage	{'e1': {'word': 'syntactic analyses', 'word_index': [(8, 9)], 'id': 'N06-1024.3'}, 'e2': {'word': 'sentences', 'word_index': [(11, 11)], 'id': 'N06-1024.4'}}	present two stage parser recovers Penn Treebank style syntactic analyses new sentences including skeletal syntactic structure , , first time , function tags empty categories .
The accuracy of the first-stage parser on the standard Parseval metric matches that of the ( Collins, 2003 ) parser on which it is based, despite the data fragmentation caused by the greatly enriched space of possible node labels.	parser	parser	compare	{'e1': {'word': 'parser', 'word_index': [(4, 4)], 'id': 'N06-1024.9'}, 'e2': {'word': 'parser', 'word_index': [(14, 14)], 'id': 'N06-1024.11'}}	accuracy first - stage parser standard Parseval metric matches ( Collins , 2003 ) parser based , despite data fragmentation caused greatly enriched space possible node labels .
Both anaphora resolution and prepositional phrase (PP) attachment are the most frequent ambiguities in natural language processing.	ambiguities	natural language processing	model-feature	{'e1': {'word': 'ambiguities', 'word_index': [(9, 9)], 'id': 'E95-1041.3'}, 'e2': {'word': 'natural language processing', 'word_index': [(10, 12)], 'id': 'E95-1041.4'}}	anaphora resolution prepositional phrase ( PP ) attachment frequent ambiguities natural language processing .
Just as with speaker adaptation in speaker-independent system, two vocabulary adaptation algorithms [5] are implemented in order to tailor the VI subword models to the target vocabulary.	vocabulary adaptation algorithms	VI subword models	usage	{'e1': {'word': 'vocabulary adaptation algorithms', 'word_index': [(8, 10)], 'id': 'H92-1033.4'}, 'e2': {'word': 'VI subword models', 'word_index': [(17, 19)], 'id': 'H92-1033.5'}}	speaker adaptation speaker - independent system , two vocabulary adaptation algorithms [ 5 ] implemented order tailor VI subword models target vocabulary .
Over the past 9 years, the Applied Science and Engineering Laboratories (ASEL) at the University of Delaware and the duPont Hospital for Children, has been involved with applying natural language processing (NLP) technologies to the field of AAC.	natural language processing (NLP) technologies	AAC	usage	{'e1': {'word': 'natural language processing (NLP) technologies', 'word_index': [(19, 25)], 'id': 'W97-0503.2'}, 'e2': {'word': 'AAC', 'word_index': [(27, 27)], 'id': 'W97-0503.3'}}	past 9 years , Applied Science Engineering Laboratories ( ASEL ) University Delaware duPont Hospital Children , involved applying natural language processing ( NLP ) technologies field AAC .
One of the major projects at ASEL (The COMPAN-SION project) has been concerned with the application of primarily lexical semantics and sentence generation technology to expand telegraphic input into full sentences.	sentence generation technology	telegraphic input	usage	{'e1': {'word': 'sentence generation technology', 'word_index': [(15, 17)], 'id': 'W97-0503.5'}, 'e2': {'word': 'telegraphic input', 'word_index': [(19, 20)], 'id': 'W97-0503.6'}}	One major projects ASEL ( COMPAN - SION project ) concerned application primarily lexical semantics sentence generation technology expand telegraphic input full sentences .
We view the entire problem as series of classification problems and employ memory-based learning (MBL) to resolve them.	memory-based learning (MBL)	classification problems	usage	{'e1': {'word': 'memory-based learning (MBL)', 'word_index': [(7, 13)], 'id': 'W00-1210.3'}, 'e2': {'word': 'classification problems', 'word_index': [(4, 5)], 'id': 'W00-1210.2'}}	view entire problem series classification problems employ memory - based learning ( MBL ) resolve .
The problem of word segmentation affects all aspects of Chinese language processing, including the development of text-to-speech synthesis systems.	word segmentation	Chinese language processing	part_whole	{'e1': {'word': 'word segmentation', 'word_index': [(1, 2)], 'id': 'W02-1813.1'}, 'e2': {'word': 'Chinese language processing', 'word_index': [(5, 7)], 'id': 'W02-1813.2'}}	problem word segmentation affects aspects Chinese language processing , including development text - -speech synthesis systems .
This paper treats the classification of the semantic functions performed by adnominal constituents in Japanese, where many parts of speech act as adnominal constituents.	adnominal constituents	Japanese	part_whole	{'e1': {'word': 'adnominal constituents', 'word_index': [(6, 7)], 'id': 'W00-0110.3'}, 'e2': {'word': 'Japanese', 'word_index': [(8, 8)], 'id': 'W00-0110.4'}}	paper treats classification semantic functions performed adnominal constituents Japanese , many parts speech act adnominal constituents .
"adjectives and ""noun + NO"" (in English ""of + noun"") structures, which have a broad range of semantic functions, are discussed."	semantic functions	"""noun + NO"" (in English ""of + noun"") structures"	model-feature	"{'e1': {'word': 'semantic functions', 'word_index': [(16, 17)], 'id': 'W00-0110.11'}, 'e2': {'word': '""noun + NO"" (in English ""of + noun"") structures', 'word_index': [(1, 12)], 'id': 'W00-0110.10'}}"	"adjectives "" noun + "" ( English "" + noun "" ) structures , broad range semantic functions , discussed ."
The feasibility of this was verified with a self-organizing semantic map based on a neural network model.	neural network model	self-organizing semantic map	usage	{'e1': {'word': 'neural network model', 'word_index': [(8, 10)], 'id': 'W00-0110.15'}, 'e2': {'word': 'self-organizing semantic map', 'word_index': [(2, 6)], 'id': 'W00-0110.14'}}	feasibility verified self - organizing semantic map based neural network model .
Recent corpus-based work on word sense disambiguation explores the application of statistical pattern recognition procedures to lexical co-occurrence data from very large text databases.	statistical pattern recognition procedures	lexical co-occurrence data	usage	{'e1': {'word': 'statistical pattern recognition procedures', 'word_index': [(10, 13)], 'id': 'J95-1001.2'}, 'e2': {'word': 'lexical co-occurrence data', 'word_index': [(14, 16)], 'id': 'J95-1001.3'}}	Recent corpus - based work word sense disambiguation explores application statistical pattern recognition procedures lexical co-occurrence data large text databases .
Statistical methods play a definite role in this work, helping to organize and analyze data, but the disambiguation method itself does not employ statistical data or decision criteria.	statistical data	disambiguation method	usage	{'e1': {'word': 'statistical data', 'word_index': [(15, 16)], 'id': 'J95-1001.12'}, 'e2': {'word': 'disambiguation method', 'word_index': [(12, 13)], 'id': 'J95-1001.11'}}	Statistical methods play definite role work , helping organize analyze data , disambiguation method employ statistical data decision criteria .
The approach is illustrated by an experiment discriminating among the senses of adjectives, which have been relatively neglected in work on sense disambiguation.	senses	adjectives	model-feature	{'e1': {'word': 'senses', 'word_index': [(5, 5)], 'id': 'J95-1001.17'}, 'e2': {'word': 'adjectives', 'word_index': [(6, 6)], 'id': 'J95-1001.18'}}	approach illustrated experiment discriminating among senses adjectives , relatively neglected work sense disambiguation .
In particular, the paper assesses the potential of nouns for discriminating among the senses of adjectives that modify them.	senses	adjectives	model-feature	{'e1': {'word': 'senses', 'word_index': [(8, 8)], 'id': 'J95-1001.21'}, 'e2': {'word': 'adjectives', 'word_index': [(9, 9)], 'id': 'J95-1001.22'}}	particular , paper assesses potential nouns discriminating among senses adjectives modify .
This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: and About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur.	ambiguous adjectives	English	part_whole	{'e1': {'word': 'ambiguous adjectives', 'word_index': [(6, 7)], 'id': 'J95-1001.23'}, 'e2': {'word': 'English', 'word_index': [(8, 8)], 'id': 'J95-1001.24'}}	assessment based empirical study five frequent ambiguous adjectives English : three - quarters instances adjectives disambiguated almost errorlessly nouns modify syntactic constructions occur .
This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: and About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur.	nouns	syntactic constructions	model-feature	{'e1': {'word': 'nouns', 'word_index': [(18, 18)], 'id': 'J95-1001.26'}, 'e2': {'word': 'syntactic constructions', 'word_index': [(20, 21)], 'id': 'J95-1001.27'}}	assessment based empirical study five frequent ambiguous adjectives English : three - quarters instances adjectives disambiguated almost errorlessly nouns modify syntactic constructions occur .
Furthermore, a small number of semantic attributes supply a compact means of representing the noun clues in a very few rules.	semantic attributes	noun	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(4, 5)], 'id': 'J95-1001.29'}, 'e2': {'word': 'noun', 'word_index': [(10, 10)], 'id': 'J95-1001.30'}}	Furthermore , small number semantic attributes supply compact means representing noun clues rules .
The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined syntactic relations to the ambiguous adjective.	sense	ambiguous modified noun	model-feature	{'e1': {'word': 'sense', 'word_index': [(0, 0)], 'id': 'J95-1001.34'}, 'e2': {'word': 'ambiguous modified noun', 'word_index': [(1, 3)], 'id': 'J95-1001.35'}}	sense ambiguous modified noun may needed determine relevant semantic attribute disambiguation target adjective ; adjectives , verbs , grammatical constructions show evidence high reliability , sometimes high applicability , stand specific , well - defined syntactic relations ambiguous adjective .
The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined syntactic relations to the ambiguous adjective.	semantic attribute	target adjective	model-feature	{'e1': {'word': 'semantic attribute', 'word_index': [(8, 9)], 'id': 'J95-1001.36'}, 'e2': {'word': 'target adjective', 'word_index': [(11, 12)], 'id': 'J95-1001.38'}}	sense ambiguous modified noun may needed determine relevant semantic attribute disambiguation target adjective ; adjectives , verbs , grammatical constructions show evidence high reliability , sometimes high applicability , stand specific , well - defined syntactic relations ambiguous adjective .
This paper presents our method of incorporating character clustering based on mutual information into Decision-Tree Dictionary-less morphological analysis.	mutual information	character clustering	usage	{'e1': {'word': 'mutual information', 'word_index': [(7, 8)], 'id': 'P98-1108.7'}, 'e2': {'word': 'character clustering', 'word_index': [(4, 5)], 'id': 'P98-1108.6'}}	paper presents method incorporating character clustering based mutual information Decision - Tree Dictionary - less morphological analysis .
By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text.	tagging	text	usage	{'e1': {'word': 'tagging', 'word_index': [(10, 10)], 'id': 'P98-1108.12'}, 'e2': {'word': 'text', 'word_index': [(12, 12)], 'id': 'P98-1108.14'}}	using natural classes , confirmed morphological analyzer significantly improved tokenizing tagging Japanese text .
As a case study in one direction, we discuss the recent development of an automatic method for evaluating definition questions based on n-gram overlap, a commonly-used technique in summarization evaluation.	n-gram overlap	automatic method for evaluating definition questions	usage	{'e1': {'word': 'n-gram overlap', 'word_index': [(14, 15)], 'id': 'W05-0906.4'}, 'e2': {'word': 'automatic method for evaluating definition questions', 'word_index': [(8, 12)], 'id': 'W05-0906.3'}}	case study one direction , discuss recent development automatic method evaluating definition questions based n-gram overlap , commonly - used technique summarization evaluation .
SYSTRAN'S Chinese word segmentation is one important component of its Chinese-English machine translation system.	Chinese word segmentation	Chinese-English machine translation system	part_whole	{'e1': {'word': 'Chinese word segmentation', 'word_index': [(1, 3)], 'id': 'W03-1729.1'}, 'e2': {'word': 'Chinese-English machine translation system', 'word_index': [(7, 12)], 'id': 'W03-1729.2'}}	SYSTRAN'S Chinese word segmentation one important component Chinese - English machine translation system .
The Chinese word segmentation module uses a rule-based approach, based on a large dictionary and fine-grained linguistic rules.	rule-based approach	Chinese word segmentation	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(5, 8)], 'id': 'W03-1729.4'}, 'e2': {'word': 'Chinese word segmentation', 'word_index': [(0, 2)], 'id': 'W03-1729.3'}}	Chinese word segmentation module uses rule - based approach , based large dictionary fine - grained linguistic rules .
It works on general-purpose texts from different Chinese-speaking regions, with comparable performance.	Chinese-speaking regions	general-purpose texts	model-feature	{'e1': {'word': 'Chinese-speaking regions', 'word_index': [(6, 8)], 'id': 'W03-1729.8'}, 'e2': {'word': 'general-purpose texts', 'word_index': [(1, 4)], 'id': 'W03-1729.7'}}	works general - purpose texts different Chinese -speaking regions , with performance .
ParaMor, our minimally supervised morphology induction algorithm, retrusses the word forms of raw text corpora back onto their paradigmatic skeletons; performing on par with state-of-the-art minimally supervised morphology induction algorithms at morphological analysis of English and German.	morphological analysis	English	usage	{'e1': {'word': 'morphological analysis', 'word_index': [(31, 32)], 'id': 'W07-1315.6'}, 'e2': {'word': 'English', 'word_index': [(33, 33)], 'id': 'W07-1315.7'}}	ParaMor , minimally supervised morphology induction algorithm , retrusses word forms raw text corpora back onto paradigmatic skeletons ; performing par state - - - art minimally supervised morphology induction algorithms morphological analysis English German .
And with these structures in hand, ParaMor then annotates word forms with morpheme boundaries.	morpheme boundaries	word forms	model-feature	{'e1': {'word': 'morpheme boundaries', 'word_index': [(8, 9)], 'id': 'W07-1315.12'}, 'e2': {'word': 'word forms', 'word_index': [(6, 7)], 'id': 'W07-1315.11'}}	structures hand , Para Mor annotates word forms morpheme boundaries .
To set ParaMor 's few free parameters we analyze a training corpus of Spanish.	Spanish	training corpus	part_whole	{'e1': {'word': 'Spanish', 'word_index': [(8, 8)], 'id': 'W07-1315.14'}, 'e2': {'word': 'training corpus', 'word_index': [(6, 7)], 'id': 'W07-1315.13'}}	set ParaMor 's free parameters analyze training corpus Spanish .
Without adjusting parameters, we induce the morphological structure of English and German.	morphological structure	English	model-feature	{'e1': {'word': 'morphological structure', 'word_index': [(5, 6)], 'id': 'W07-1315.15'}, 'e2': {'word': 'English', 'word_index': [(7, 7)], 'id': 'W07-1315.16'}}	Without adjusting parameters , induce morphological structure English German .
The other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores.	distance features	multi-level confidence scores	usage	{'e1': {'word': 'distance features', 'word_index': [(5, 6)], 'id': 'P08-2055.8'}, 'e2': {'word': 'multi-level confidence scores', 'word_index': [(10, 12)], 'id': 'P08-2055.9'}}	contribution work incorporated novel long distance features address challenges computing multi-level confidence scores .
Using Conditional Maximum Entropy (CME) classifier with all the selected features, we reached an annotation error rate of 26.0% in the SWBD corpus, compared with a subtree error rate of 41.91%, a closely related benchmark with the Charniak parser from ( Kahn et al., 2005 ).	Conditional Maximum Entropy (CME) classifier	annotation error rate	result	{'e1': {'word': 'Conditional Maximum Entropy (CME) classifier', 'word_index': [(1, 7)], 'id': 'P08-2055.10'}, 'e2': {'word': 'annotation error rate', 'word_index': [(12, 14)], 'id': 'P08-2055.11'}}	Using Conditional Maximum Entropy ( CME ) classifier selected features , reached annotation error rate 26.0 % SWBD corpus , compared subtree error rate 41.91 % , closely related benchmark Charniak parser ( Kahn et al. , 2005 ) .
Coreference resolution systems usually attempt to find a suitable antecedent for (almost) every noun phrase	Coreference resolution systems	noun phrase	usage	{'e1': {'word': 'Coreference resolution systems', 'word_index': [(0, 2)], 'id': 'P03-2012.1'}, 'e2': {'word': 'noun phrase', 'word_index': [(12, 13)], 'id': 'P03-2012.2'}}	Coreference resolution systems usually attempt find suitable antecedent ( almost ) every noun phrase
We use a small training corpus (MUC-7), but also acquire some data from the Internet.	data	Internet	part_whole	{'e1': {'word': 'data', 'word_index': [(12, 12)], 'id': 'P03-2012.8'}, 'e2': {'word': 'Internet', 'word_index': [(13, 13)], 'id': 'P03-2012.9'}}	use small training corpus ( MUC - 7 ) , also acquire data Internet .
Combining our classifiers sequentially, we achieve 88.9% precision and 84.6% recall for discourse new entities.	classifiers	precision	result	{'e1': {'word': 'classifiers', 'word_index': [(1, 1)], 'id': 'P03-2012.10'}, 'e2': {'word': 'precision', 'word_index': [(7, 7)], 'id': 'P03-2012.11'}}	Combining classifiers sequentially , achieve 88.9 % precision 84.6 % recall discourse new entities .
We expect our classifiers to provide a good prefiltering for coreference resolution systems, improving both their speed and performance.	classifiers	coreference resolution systems	usage	{'e1': {'word': 'classifiers', 'word_index': [(1, 1)], 'id': 'P03-2012.14'}, 'e2': {'word': 'coreference resolution systems', 'word_index': [(5, 7)], 'id': 'P03-2012.15'}}	expect classifiers provide good prefiltering coreference resolution systems , improving speed performance .
MBDP-1 is a knowledge-free segmentation algorithm that bootstraps its own lexicon, which starts out empty.	knowledge-free segmentation algorithm	lexicon	usage	{'e1': {'word': 'knowledge-free segmentation algorithm', 'word_index': [(2, 6)], 'id': 'P01-1013.3'}, 'e2': {'word': 'lexicon', 'word_index': [(8, 8)], 'id': 'P01-1013.4'}}	MBDP -1 knowledge - free segmentation algorithm bootstraps lexicon , starts empty .
In this paper, we present methods that allow the users of a natural language processor (NLP) to define, inspect, and modify any case frame information associated with the words and phrases known to the system.	case frame information	words	model-feature	{'e1': {'word': 'case frame information', 'word_index': [(17, 19)], 'id': 'C86-1108.2'}, 'e2': {'word': 'words', 'word_index': [(21, 21)], 'id': 'C86-1108.3'}}	paper , present methods allow users natural language processor ( NLP ) define , inspect , modify case frame information associated words phrases known system .
The number and sizes of parallel corpora keep growing, which makes it necessary to have automatic methods of processing them: combining, checking and improving corpora quality, etc.	corpora quality	parallel corpora	model-feature	{'e1': {'word': 'corpora quality', 'word_index': [(17, 18)], 'id': 'L08-1114.2'}, 'e2': {'word': 'parallel corpora', 'word_index': [(2, 3)], 'id': 'L08-1114.1'}}	number sizes parallel corpora keep growing , makes necessary automatic methods processing : combining , checking improving corpora quality , etc.
The method takes into consideration slight differences in the source documents, different levels of segmentation of the input corpora, encoding differences and other aspects of the task.	segmentation	input corpora	usage	{'e1': {'word': 'segmentation', 'word_index': [(10, 10)], 'id': 'L08-1114.10'}, 'e2': {'word': 'input corpora', 'word_index': [(11, 12)], 'id': 'L08-1114.11'}}	method takes consideration slight differences source documents , different levels segmentation input corpora , encoding differences aspects task .
In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts.	Estonian-English	JRC-Acquis corpus	part_whole	{'e1': {'word': 'Estonian-English', 'word_index': [(3, 5)], 'id': 'L08-1114.12'}, 'e2': {'word': 'JRC-Acquis corpus', 'word_index': [(7, 10)], 'id': 'L08-1114.13'}}	first experiment , Estonian - English part JRC - Acquis corpus combined another corpus legislation texts .
In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts.	legislation texts	corpus	part_whole	{'e1': {'word': 'legislation texts', 'word_index': [(14, 15)], 'id': 'L08-1114.15'}, 'e2': {'word': 'corpus', 'word_index': [(13, 13)], 'id': 'L08-1114.14'}}	first experiment , Estonian - English part JRC - Acquis corpus combined another corpus legislation texts .
The generation module supports the seamless integration of full grammar rules, templates and canned text.	grammar rules	generation module	usage	{'e1': {'word': 'grammar rules', 'word_index': [(6, 7)], 'id': 'E03-1019.6'}, 'e2': {'word': 'generation module', 'word_index': [(0, 1)], 'id': 'E03-1019.5'}}	generation module supports seamless integration full grammar rules , templates canned text .
Ambiguity is the fundamental property of natural language	Ambiguity	natural language	model-feature	{'e1': {'word': 'Ambiguity', 'word_index': [(0, 0)], 'id': 'C02-1079.1'}, 'e2': {'word': 'natural language', 'word_index': [(3, 4)], 'id': 'C02-1079.2'}}	Ambiguity fundamental property natural language
Perhaps, the most burdensome case of ambiguity manifests itself on the syntactic level of analysis.	ambiguity	syntactic level of analysis	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(4, 4)], 'id': 'C02-1079.3'}, 'e2': {'word': 'syntactic level of analysis', 'word_index': [(6, 8)], 'id': 'C02-1079.4'}}	Perhaps , burdensome case ambiguity manifests syntactic level analysis .
The presented methods are based on language specific features of synthetical languages and they improve the results of simple stochastic approaches.	language specific features	synthetical languages	model-feature	{'e1': {'word': 'language specific features', 'word_index': [(3, 5)], 'id': 'C02-1079.7'}, 'e2': {'word': 'synthetical languages', 'word_index': [(6, 7)], 'id': 'C02-1079.8'}}	presented methods based language specific features synthetical languages improve results simple stochastic approaches .
The texts are in English and Czech.	texts	English	model-feature	{'e1': {'word': 'texts', 'word_index': [(0, 0)], 'id': 'L08-1197.3'}, 'e2': {'word': 'English', 'word_index': [(1, 1)], 'id': 'L08-1197.4'}}	texts English Czech .
This paper presents techniques for multimedia annotation and their application to video summarization and translation.	multimedia annotation	video summarization and translation	usage	{'e1': {'word': 'multimedia annotation', 'word_index': [(3, 4)], 'id': 'C02-1098.2'}, 'e2': {'word': 'video summarization and translation', 'word_index': [(6, 8)], 'id': 'C02-1098.3'}}	paper presents techniques multimedia annotation application video summarization translation .
A video scene description consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes.	semi-automatically detected keyframes	video scene description	part_whole	{'e1': {'word': 'semi-automatically detected keyframes', 'word_index': [(4, 6)], 'id': 'C02-1098.12'}, 'e2': {'word': 'video scene description', 'word_index': [(0, 2)], 'id': 'C02-1098.11'}}	video scene description consists semi-automatically detected keyframes scene video clip time codes scenes .
The text data in the multimedia annotation are syntactically and semantically structured using linguistic annotation.	text data	syntactically and semantically structured	model-feature	{'e1': {'word': 'text data', 'word_index': [(0, 1)], 'id': 'C02-1098.14'}, 'e2': {'word': 'syntactically and semantically structured', 'word_index': [(4, 6)], 'id': 'C02-1098.16'}}	text data multimedia annotation syntactically semantically structured using linguistic annotation .
The proposed multimedia summarization works upon a multimodal document that consists of a video, keyframes of scenes, and transcripts of the scenes.	 multimedia summarization	multimodal document	usage	{'e1': {'word': ' multimedia summarization', 'word_index': [(1, 2)], 'id': 'C02-1098.18'}, 'e2': {'word': 'multimodal document', 'word_index': [(5, 6)], 'id': 'C02-1098.19'}}	proposed multimedia summarization works upon multimodal document consists video , keyframes scenes , transcripts scenes .
The multimedia translation automatically generates several versions of multimedia content in different languages.	languages	multimedia content	model-feature	{'e1': {'word': 'languages', 'word_index': [(9, 9)], 'id': 'C02-1098.26'}, 'e2': {'word': 'multimedia content', 'word_index': [(6, 7)], 'id': 'C02-1098.25'}}	multimedia translation automatically generates several versions multimedia content different languages .
When machine translation (MT) knowledge is automatically constructed from bilingual corpora, redundant rules are acquired due to translation variety.	bilingual corpora	machine translation (MT) knowledge	usage	{'e1': {'word': 'bilingual corpora', 'word_index': [(8, 9)], 'id': 'E03-1029.2'}, 'e2': {'word': 'machine translation (MT) knowledge', 'word_index': [(0, 5)], 'id': 'E03-1029.1'}}	machine translation ( MT ) knowledge automatically constructed bilingual corpora , redundant rules acquired due translation variety .
These rules increase ambiguity or cause incorrect MT results.	rules	ambiguity	result	{'e1': {'word': 'rules', 'word_index': [(0, 0)], 'id': 'E03-1029.4'}, 'e2': {'word': 'ambiguity', 'word_index': [(2, 2)], 'id': 'E03-1029.5'}}	rules increase ambiguity cause incorrect MT results .
"To overcome this problem, we constrain the sentences used for knowledge extraction to ""the appropriate bilingual sentences for the MT""."	sentences	knowledge extraction	usage	{'e1': {'word': 'sentences', 'word_index': [(4, 4)], 'id': 'E03-1029.7'}, 'e2': {'word': 'knowledge extraction', 'word_index': [(6, 7)], 'id': 'E03-1029.8'}}	"overcome problem , constrain sentences used knowledge extraction "" appropriate bilingual sentences MT "" ."
"To overcome this problem, we constrain the sentences used for knowledge extraction to ""the appropriate bilingual sentences for the MT""."	bilingual sentences	MT	usage	{'e1': {'word': 'bilingual sentences', 'word_index': [(10, 11)], 'id': 'E03-1029.9'}, 'e2': {'word': 'MT', 'word_index': [(12, 12)], 'id': 'E03-1029.10'}}	"overcome problem , constrain sentences used knowledge extraction "" appropriate bilingual sentences MT "" ."
For the creation of these scholarly digital editions the AAC edition philosophy and edition principles have been applied whereby new corpus research methods have been made use of for questions of computational philology and textual studies in a digital environment.	AAC edition philosophy and edition principles	scholarly digital editions	usage	{'e1': {'word': 'AAC edition philosophy and edition principles', 'word_index': [(4, 8)], 'id': 'L08-1405.9'}, 'e2': {'word': 'scholarly digital editions', 'word_index': [(1, 3)], 'id': 'L08-1405.8'}}	creation scholarly digital editions AAC edition philosophy edition principles applied whereby new corpus research methods made use questions computational philology textual studies digital environment .
The evaluation results show our system can achieve an F measure of 0.9400.967 for different testing corpora.	system	F measure	result	{'e1': {'word': 'system', 'word_index': [(3, 3)], 'id': 'I05-3026.5'}, 'e2': {'word': 'F measure', 'word_index': [(5, 6)], 'id': 'I05-3026.6'}}	evaluation results show system achieve F measure 0.9400.967 different testing corpora .
In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others.	machine learning algorithm	shallow semantic parsing	usage	{'e1': {'word': 'machine learning algorithm', 'word_index': [(3, 5)], 'id': 'N04-1030.1'}, 'e2': {'word': 'shallow semantic parsing', 'word_index': [(6, 8)], 'id': 'N04-1030.2'}}	paper , propose machine learning algorithm shallow semantic parsing , extending work Gildea Jurafsky ( 2002 ) , Surdeanu et al. ( 2003 ) others .
Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers.	Support Vector Machines	algorithm	usage	{'e1': {'word': 'Support Vector Machines', 'word_index': [(2, 4)], 'id': 'N04-1030.4'}, 'e2': {'word': 'algorithm', 'word_index': [(0, 0)], 'id': 'N04-1030.3'}}	algorithm based Support Vector Machines show give improvement performance earlier classifiers .
We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.	test set	AQUAINT corpus	part_whole	{'e1': {'word': 'test set', 'word_index': [(10, 11)], 'id': 'N04-1030.7'}, 'e2': {'word': 'AQUAINT corpus', 'word_index': [(13, 14)], 'id': 'N04-1030.8'}}	show performance improvements number new features measure ability generalize new test set drawn AQUAINT corpus .
Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences.	language model	speech understanding	usage	{'e1': {'word': 'language model', 'word_index': [(2, 3)], 'id': 'A94-1010.1'}, 'e2': {'word': 'speech understanding', 'word_index': [(5, 6)], 'id': 'A94-1010.2'}}	Many kinds language model used speech understanding suffer imperfect modeling intra-sentential contextual influences .
I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into sub corpora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster.	sentences	training corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(4, 4)], 'id': 'A94-1010.5'}, 'e2': {'word': 'training corpus', 'word_index': [(5, 6)], 'id': 'A94-1010.6'}}	argue problem addressed clustering sentences training corpus automatically sub corpora criterion entropy reduction , calculating separate language model parameters cluster .
This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model.	clustering	contextual effects	model-feature	{'e1': {'word': 'clustering', 'word_index': [(1, 1)], 'id': 'A94-1010.11'}, 'e2': {'word': 'contextual effects', 'word_index': [(6, 7)], 'id': 'A94-1010.12'}}	kind clustering offers way represent important contextual effects therefore significantly improve performance model .
It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclustered model.	clustering	model	result	{'e1': {'word': 'clustering', 'word_index': [(26, 26)], 'id': 'A94-1010.16'}, 'e2': {'word': 'model', 'word_index': [(29, 29)], 'id': 'A94-1010.17'}}	also offers reasonably automatic means gather evidence whether complex , context - sensitive model using general kind linguistic information likely reward effort would required develop : clustering improves performance model , proves existence context dependencies , exploited unclustered model .
As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain.	clustering	models	result	{'e1': {'word': 'clustering', 'word_index': [(6, 6)], 'id': 'A94-1010.20'}, 'e2': {'word': 'models', 'word_index': [(8, 8)], 'id': 'A94-1010.21'}}	evidence claims , present results showing clustering improves models others ATIS domain .
This paper presents a parsing system for the detection of syntactic errors.	parsing system	detection of syntactic errors	usage	{'e1': {'word': 'parsing system', 'word_index': [(2, 3)], 'id': 'A00-3005.1'}, 'e2': {'word': 'detection of syntactic errors', 'word_index': [(4, 6)], 'id': 'A00-3005.2'}}	paper presents parsing system detection syntactic errors .
It combines a robust partial parser which obtains the main sentence components and a finite-state parser used for the description of syntactic error patterns.	finite-state parser	syntactic error patterns	usage	{'e1': {'word': 'finite-state parser', 'word_index': [(8, 11)], 'id': 'A00-3005.5'}, 'e2': {'word': 'syntactic error patterns', 'word_index': [(14, 16)], 'id': 'A00-3005.6'}}	combines robust partial parser obtains main sentence components finite - state parser used description syntactic error patterns .
The system has been tested on a corpus of real texts, containing both correct and incorrect sentences, with promising results.	texts	corpus	part_whole	{'e1': {'word': 'texts', 'word_index': [(4, 4)], 'id': 'A00-3005.9'}, 'e2': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'A00-3005.8'}}	system tested corpus real texts , containing correct incorrect sentences , promising results .
The objectives of this project are to advance our understanding of the merits of current text analysis techniques, as applied to the performance of realistic text analysis tasks, and to achieve this understanding by means of a sound performance evaluation methodology.	text analysis techniques	realistic text analysis tasks	usage	{'e1': {'word': 'text analysis techniques', 'word_index': [(6, 8)], 'id': 'H92-1111.1'}, 'e2': {'word': 'realistic text analysis tasks', 'word_index': [(12, 15)], 'id': 'H92-1111.3'}}	objectives project advance understanding merits current text analysis techniques , applied performance realistic text analysis tasks , achieve understanding means sound performance evaluation methodology .
Talk'n'Travel is a fully conversational, mixed-initiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English.	general spoken English	questions	model-feature	{'e1': {'word': 'general spoken English', 'word_index': [(25, 27)], 'id': 'A00-1010.8'}, 'e2': {'word': 'questions', 'word_index': [(21, 21)], 'id': 'A00-1010.7'}}	Talk 'n ' Travel fully conversational , mixed - initiative system allows user specify constraints travel plan arbitrary order , ask questions , etc. , general spoken English .
The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met.	plan-based agenda mechanism	system	usage	{'e1': {'word': 'plan-based agenda mechanism', 'word_index': [(3, 7)], 'id': 'A00-1010.10'}, 'e2': {'word': 'system', 'word_index': [(0, 0)], 'id': 'A00-1010.9'}}	system operates according plan - based agenda mechanism , rather finite state network , attempts negotiate user constraints met .
As mentioned in Mitkov (1996), solving the anaphora and extracting the antecedent are key issues in a correct translation.	anaphora	translation	part_whole	{'e1': {'word': 'anaphora', 'word_index': [(7, 7)], 'id': 'W99-0210.3'}, 'e2': {'word': 'translation', 'word_index': [(13, 13)], 'id': 'W99-0210.5'}}	mentioned Mitkov ( 1996 ) , solving anaphora extracting antecedent key issues correct translation .
The SS stores the lexical, syntactic, morphologic and semantic information of every constituent of the grammar.	lexical, syntactic, morphologic and semantic information	constituent	model-feature	{'e1': {'word': 'lexical, syntactic, morphologic and semantic information', 'word_index': [(2, 8)], 'id': 'W99-0210.10'}, 'e2': {'word': 'constituent', 'word_index': [(10, 10)], 'id': 'W99-0210.11'}}	SS stores lexical , syntactic , morphologic semantic information every constituent grammar .
This mechanism could be added to a MT system such as an additional module to solve anaphora generation problem.	mechanism	MT system	usage	{'e1': {'word': 'mechanism', 'word_index': [(0, 0)], 'id': 'W99-0210.23'}, 'e2': {'word': 'MT system', 'word_index': [(3, 4)], 'id': 'W99-0210.24'}}	mechanism could added MT system additional module solve anaphora generation problem .
This paper addresses language engineering infrastructure issues by considering whether standard V&amp;V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&amp;V in the context of language processing systems.	standard V&amp;V methods	evaluation practices	compare	{'e1': {'word': 'standard V&amp;V methods', 'word_index': [(8, 12)], 'id': 'W01-0906.6'}, 'e2': {'word': 'evaluation practices', 'word_index': [(15, 16)], 'id': 'W01-0906.7'}}	paper addresses language engineering infrastructure issues considering whether standard V&amp ; V methods fundamentally different evaluation practices commonly used NLP systems , proposes practical approaches applying V&amp ; V context language processing systems .
This paper addresses language engineering infrastructure issues by considering whether standard V&amp;V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&amp;V in the context of language processing systems.	V&amp;V	language processing systems	usage	{'e1': {'word': 'V&amp;V', 'word_index': [(26, 28)], 'id': 'W01-0906.9'}, 'e2': {'word': 'language processing systems', 'word_index': [(30, 32)], 'id': 'W01-0906.10'}}	paper addresses language engineering infrastructure issues considering whether standard V&amp ; V methods fundamentally different evaluation practices commonly used NLP systems , proposes practical approaches applying V&amp ; V context language processing systems .
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text.	paragraphs	document	part_whole	{'e1': {'word': 'paragraphs', 'word_index': [(7, 7)], 'id': 'W03-1102.1'}, 'e2': {'word': 'document', 'word_index': [(9, 9)], 'id': 'W03-1102.2'}}	paper , propose practical approach extracting relevant paragraphs original document form summary Thai text .
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text.	summary	Thai text	model-feature	{'e1': {'word': 'summary', 'word_index': [(11, 11)], 'id': 'W03-1102.3'}, 'e2': {'word': 'Thai text', 'word_index': [(12, 13)], 'id': 'W03-1102.4'}}	paper , propose practical approach extracting relevant paragraphs original document form summary Thai text .
The idea of our approach is to exploit both the local and global properties of paragraphs.	local and global properties	paragraphs	model-feature	{'e1': {'word': 'local and global properties', 'word_index': [(3, 5)], 'id': 'W03-1102.5'}, 'e2': {'word': 'paragraphs', 'word_index': [(6, 6)], 'id': 'W03-1102.6'}}	idea approach exploit local global properties paragraphs .
The local property can be considered as clusters of significant words within each paragraph, while the global property can be thought of as relations of all paragraphs in a document.	clusters	significant words	model-feature	{'e1': {'word': 'clusters', 'word_index': [(3, 3)], 'id': 'W03-1102.8'}, 'e2': {'word': 'significant words', 'word_index': [(4, 5)], 'id': 'W03-1102.9'}}	local property considered clusters significant words within paragraph , global property thought relations paragraphs document .
Syntax-based Machine Translation systems have recently become a focus of research with much hope that they will outperform traditional Phrase- Based Statistical Machine Translation (PBSMT)	Syntax-based Machine Translation systems	traditional Phrase- Based Statistical Machine Translation (PBSMT)	compare	{'e1': {'word': 'Syntax-based Machine Translation systems', 'word_index': [(0, 5)], 'id': 'W08-0410.1'}, 'e2': {'word': 'traditional Phrase- Based Statistical Machine Translation (PBSMT)', 'word_index': [(13, 22)], 'id': 'W08-0410.2'}}	Syntax - based Machine Translation systems recently become focus research much hope outperform traditional Phrase - Based Statistical Machine Translation ( PBSMT )
Toward this goal, we present a method for analyzing the morphosyntactic content of language from an Elicitation Corpus such as the one included in the LDC's upcoming LCTL language packs.	morphosyntactic content	Elicitation Corpus	part_whole	{'e1': {'word': 'morphosyntactic content', 'word_index': [(6, 7)], 'id': 'W08-0410.3'}, 'e2': {'word': 'Elicitation Corpus', 'word_index': [(9, 10)], 'id': 'W08-0410.5'}}	Toward goal , present method analyzing morphosyntactic content language Elicitation Corpus one included LDC 's upcoming LCTL language packs .
By providing this tool that can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved.	rich features	structure-based MT models	usage	{'e1': {'word': 'rich features', 'word_index': [(8, 9)], 'id': 'W08-0410.12'}, 'e2': {'word': 'structure-based MT models', 'word_index': [(3, 7)], 'id': 'W08-0410.11'}}	providing tool augment structure - based MT models rich features , believe discriminative power current models improved .
This article outlines a quantitative method for segmenting texts into thematically coherent units.	quantitative method	texts	usage	{'e1': {'word': 'quantitative method', 'word_index': [(2, 3)], 'id': 'P98-2243.1'}, 'e2': {'word': 'texts', 'word_index': [(5, 5)], 'id': 'P98-2243.2'}}	article outlines quantitative method segmenting texts thematically coherent units .
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.	network of lexical collocations	method	usage	{'e1': {'word': 'network of lexical collocations', 'word_index': [(2, 4)], 'id': 'P98-2243.5'}, 'e2': {'word': 'method', 'word_index': [(0, 0)], 'id': 'P98-2243.4'}}	method relies network lexical collocations compute thematic coherence different parts text lexical cohesiveness words .
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.	lexical cohesiveness	words	model-feature	{'e1': {'word': 'lexical cohesiveness', 'word_index': [(11, 12)], 'id': 'P98-2243.8'}, 'e2': {'word': 'words', 'word_index': [(13, 13)], 'id': 'P98-2243.9'}}	method relies network lexical collocations compute thematic coherence different parts text lexical cohesiveness words .
We also present the results of an experiment about locating boundaries between a series of concatened texts.	boundaries	texts	part_whole	{'e1': {'word': 'boundaries', 'word_index': [(5, 5)], 'id': 'P98-2243.10'}, 'e2': {'word': 'texts', 'word_index': [(8, 8)], 'id': 'P98-2243.11'}}	also present results experiment locating boundaries series concatened texts .
In this work, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves.	model	sense assignment	usage	{'e1': {'word': 'model', 'word_index': [(3, 3)], 'id': 'W04-1908.1'}, 'e2': {'word': 'sense assignment', 'word_index': [(4, 5)], 'id': 'W04-1908.2'}}	work , introduce model sense assignment relies assigning senses contexts within words appear , rather words .
In this work, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves.	words	contexts	model-feature	{'e1': {'word': 'words', 'word_index': [(11, 11)], 'id': 'W04-1908.5'}, 'e2': {'word': 'contexts', 'word_index': [(9, 9)], 'id': 'W04-1908.4'}}	work , introduce model sense assignment relies assigning senses contexts within words appear , rather words .
In this paper we describe a morphological analysis method based on a maximum entropy model.	maximum entropy model	morphological analysis method	usage	{'e1': {'word': 'maximum entropy model', 'word_index': [(6, 8)], 'id': 'W01-0512.2'}, 'e2': {'word': 'morphological analysis method', 'word_index': [(2, 4)], 'id': 'W01-0512.1'}}	paper describe morphological analysis method based maximum entropy model .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	model	method	usage	{'e1': {'word': 'model', 'word_index': [(2, 2)], 'id': 'W01-0512.4'}, 'e2': {'word': 'method', 'word_index': [(0, 0)], 'id': 'W01-0512.3'}}	method uses model consult dictionary large amount lexical information also identify unknown words learning certain characteristics .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	lexical information	dictionary	part_whole	{'e1': {'word': 'lexical information', 'word_index': [(7, 8)], 'id': 'W01-0512.6'}, 'e2': {'word': 'dictionary', 'word_index': [(4, 4)], 'id': 'W01-0512.5'}}	method uses model consult dictionary large amount lexical information also identify unknown words learning certain characteristics .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	characteristics	unknown words	model-feature	{'e1': {'word': 'characteristics', 'word_index': [(15, 15)], 'id': 'W01-0512.8'}, 'e2': {'word': 'unknown words', 'word_index': [(11, 12)], 'id': 'W01-0512.7'}}	method uses model consult dictionary large amount lexical information also identify unknown words learning certain characteristics .
Finally, we present Corporator, an Open Source software which was designed for collecting corpus from RSS feeds.	corpus	RSS feeds	part_whole	{'e1': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'W06-1707.10'}, 'e2': {'word': 'RSS feeds', 'word_index': [(11, 12)], 'id': 'W06-1707.11'}}	Finally , present Corporator , Open Source software designed collecting corpus RSS feeds .
Several SVMs are trained using information from pyramids of summary content units.	summary content units	SVMs	usage	{'e1': {'word': 'summary content units', 'word_index': [(6, 8)], 'id': 'P07-2015.4'}, 'e2': {'word': 'SVMs', 'word_index': [(1, 1)], 'id': 'P07-2015.3'}}	Several SVMs trained using information pyramids summary content units .
Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.	performance	DUC-2005	compare	{'e1': {'word': 'performance', 'word_index': [(0, 0)], 'id': 'P07-2015.5'}, 'e2': {'word': 'DUC-2005', 'word_index': [(5, 5)], 'id': 'P07-2015.6'}}	performance compared best performing systems DUC-2005 , using both and Pan , an scoring method for evaluation .
Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.	automatic scoring method	pyramid evaluation	usage	{'e1': {'word': 'automatic scoring method', 'word_index': [(13, 14)], 'id': 'P07-2015.9'}, 'e2': {'word': 'pyramid evaluation', 'word_index': [(16, 16)], 'id': 'P07-2015.10'}}	performance compared best performing systems DUC-2005 , using both ROUGE auto , an scoring method for evaluation .
We present a novel unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees.	unsupervised method	sentence compression	usage	{'e1': {'word': 'unsupervised method', 'word_index': [(2, 3)], 'id': 'W08-1105.1'}, 'e2': {'word': 'sentence compression', 'word_index': [(4, 5)], 'id': 'W08-1105.2'}}	present novel unsupervised method sentence compression relies dependency tree representation shortens sentences removing subtrees .
We demonstrate that the choice of the parser affects the performance of the system.	parser	performance	result	{'e1': {'word': 'parser', 'word_index': [(2, 2)], 'id': 'W08-1105.8'}, 'e2': {'word': 'performance', 'word_index': [(4, 4)], 'id': 'W08-1105.9'}}	demonstrate choice parser affects performance system .
We also apply the method to German and report the results of an evaluation with humans.	method	German	usage	{'e1': {'word': 'method', 'word_index': [(2, 2)], 'id': 'W08-1105.11'}, 'e2': {'word': 'German', 'word_index': [(3, 3)], 'id': 'W08-1105.12'}}	also apply method German report results evaluation humans .
Our approach even outperformed the hand coded system on NER in Spanish, and it achieved high accuracies in Portuguese.	NER	Spanish	usage	{'e1': {'word': 'NER', 'word_index': [(6, 6)], 'id': 'P05-2005.5'}, 'e2': {'word': 'Spanish', 'word_index': [(7, 7)], 'id': 'P05-2005.6'}}	approach even outperformed hand coded system NER Spanish , achieved high accuracies Portuguese .
A karaka based approach to parsing of Indian languages is described.	parsing	Indian languages	usage	{'e1': {'word': 'parsing', 'word_index': [(3, 3)], 'id': 'C90-3005.2'}, 'e2': {'word': 'Indian languages', 'word_index': [(4, 5)], 'id': 'C90-3005.3'}}	karaka based approach parsing Indian languages described .
It has been used for building a parser of Hindi for a prototype Machine Translation system.	parser	Hindi	usage	{'e1': {'word': 'parser', 'word_index': [(2, 2)], 'id': 'C90-3005.4'}, 'e2': {'word': 'Hindi', 'word_index': [(3, 3)], 'id': 'C90-3005.5'}}	used building parser Hindi prototype Machine Translation system .
This paper presents our work on the detection of temporal information in web pages.	detection	web pages	usage	{'e1': {'word': 'detection', 'word_index': [(3, 3)], 'id': 'L08-1559.1'}, 'e2': {'word': 'web pages', 'word_index': [(6, 7)], 'id': 'L08-1559.3'}}	paper presents work detection temporal information web pages .
The pages examined within the scope of this study were taken from the tourism sector and the temporal information in question is thus particular to this area.	temporal information	pages	part_whole	{'e1': {'word': 'temporal information', 'word_index': [(8, 9)], 'id': 'L08-1559.5'}, 'e2': {'word': 'pages', 'word_index': [(0, 0)], 'id': 'L08-1559.4'}}	pages examined within scope study taken tourism sector temporal information question thus particular area .
The differences that exist between extraction from plain textual data and extraction from the web are brought to light.	extraction	plain textual data	usage	{'e1': {'word': 'extraction', 'word_index': [(2, 2)], 'id': 'L08-1559.6'}, 'e2': {'word': 'plain textual data', 'word_index': [(3, 5)], 'id': 'L08-1559.7'}}	differences exist extraction plain textual data extraction web brought light .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	patterns	symbolic approach	usage	{'e1': {'word': 'patterns', 'word_index': [(4, 4)], 'id': 'L08-1559.16'}, 'e2': {'word': 'symbolic approach', 'word_index': [(1, 2)], 'id': 'L08-1559.15'}}	adopt symbolic approach relying patterns rules detection , extraction annotation temporal expressions ; method based use transducers .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	rules	detection	usage	{'e1': {'word': 'rules', 'word_index': [(5, 5)], 'id': 'L08-1559.17'}, 'e2': {'word': 'detection', 'word_index': [(6, 6)], 'id': 'L08-1559.18'}}	adopt symbolic approach relying patterns rules detection , extraction annotation temporal expressions ; method based use transducers .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	annotation	temporal expressions	usage	{'e1': {'word': 'annotation', 'word_index': [(9, 9)], 'id': 'L08-1559.20'}, 'e2': {'word': 'temporal expressions', 'word_index': [(10, 11)], 'id': 'L08-1559.21'}}	adopt symbolic approach relying patterns rules detection , extraction annotation temporal expressions ; method based use transducers .
The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German.	read and spontaneous speech	German Ph@ttSessionz speech database	part_whole	{'e1': {'word': 'read and spontaneous speech', 'word_index': [(7, 9)], 'id': 'L08-1196.2'}, 'e2': {'word': 'German Ph@ttSessionz speech database', 'word_index': [(2, 5)], 'id': 'L08-1196.1'}}	first release German Ph@ttSessionz speech database contains read spontaneous speech 864 adolescent speakers largest database kind German .
In this paper, we present a cross-sectional study of f0 measurements on this database.	cross-sectional study	f0 measurements	topic	{'e1': {'word': 'cross-sectional study', 'word_index': [(3, 5)], 'id': 'L08-1196.7'}, 'e2': {'word': 'f0 measurements', 'word_index': [(7, 8)], 'id': 'L08-1196.8'}}	paper , present cross -sectional study of f0 measurements on .
Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers.	relative f0 variability	male and female speakers	model-feature	{'e1': {'word': 'relative f0 variability', 'word_index': [(8, 10)], 'id': 'L08-1196.11'}, 'e2': {'word': 'male and female speakers', 'word_index': [(11, 13)], 'id': 'L08-1196.12'}}	Furthermore , shows perceptive mel-scale , little difference relative f0 variability male female speakers .
The study provides statistically reliable voice parameters of adolescent speakers for German.	voice parameters	adolescent speakers	model-feature	{'e1': {'word': 'voice parameters', 'word_index': [(4, 5)], 'id': 'L08-1196.17'}, 'e2': {'word': 'adolescent speakers', 'word_index': [(6, 7)], 'id': 'L08-1196.18'}}	study provides statistically reliable voice parameters adolescent speakers German .
The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability.	utterances	user input	part_whole	{'e1': {'word': 'utterances', 'word_index': [(11, 11)], 'id': 'L08-1196.22'}, 'e2': {'word': 'user input', 'word_index': [(9, 10)], 'id': 'L08-1196.21'}}	results may contribute making spoken dialog systems robust restricting user input utterances low f0 variability .
The platform will support researchers and engineers with well-developed and standardized resources and application tools thereby avoiding duplicate activities from scratch and amplifying overall effort on the domain.	standardized resources	platform	usage	{'e1': {'word': 'standardized resources', 'word_index': [(7, 8)], 'id': 'C96-2185.9'}, 'e2': {'word': 'platform', 'word_index': [(0, 0)], 'id': 'C96-2185.8'}}	platform support researchers engineers well - developed standardized resources application tools thereby avoiding duplicate activities scratch amplifying overall effort domain .
We present in this article, as a part of aspectual operation system, a generation system of iterative expressions using a set of operators called iterative operators.	generation system	aspectual operation system	part_whole	{'e1': {'word': 'generation system', 'word_index': [(8, 9)], 'id': 'E83-1003.2'}, 'e2': {'word': 'aspectual operation system', 'word_index': [(4, 6)], 'id': 'E83-1003.1'}}	present article , part aspectual operation system , generation system iterative expressions using set operators called iterative operators .
The classification has been carried out especially in consideration of the durative / non-durative character of the denoted events and also in consideration of existence / non-existence of a culmination point (or a boundary) in the events.	durative / non-durative character	events	model-feature	{'e1': {'word': 'durative / non-durative character', 'word_index': [(4, 7)], 'id': 'E83-1003.11'}, 'e2': {'word': 'events', 'word_index': [(9, 9)], 'id': 'E83-1003.12'}}	classification carried especially consideration durative / non-durative character denoted events also consideration existence / non-existence culmination point ( boundary ) events .
In this paper, we show that time-synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.	cross-word triphones	time-synchronous one-pass decoding	usage	{'e1': {'word': 'cross-word triphones', 'word_index': [(11, 14)], 'id': 'H94-1080.16'}, 'e2': {'word': 'time-synchronous one-pass decoding', 'word_index': [(3, 9)], 'id': 'H94-1080.15'}}	paper , show time - synchronous one - pass decoding using cross - word triphones trigram language model implemented using dynamically built tree - structured network .
In this paper, we show that time-synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.	tree-structured network	trigram language model	usage	{'e1': {'word': 'tree-structured network', 'word_index': [(22, 25)], 'id': 'H94-1080.18'}, 'e2': {'word': 'trigram language model', 'word_index': [(15, 17)], 'id': 'H94-1080.17'}}	paper , show time - synchronous one - pass decoding using cross - word triphones trigram language model implemented using dynamically built tree - structured network .
It was included in the HTK large vocabulary speech recognition system used for the 1993 ARPA WSJ evaluation and experimental results are presented for that task.	HTK large vocabulary speech recognition system	1993 ARPA WSJ evaluation	usage	{'e1': {'word': 'HTK large vocabulary speech recognition system', 'word_index': [(1, 6)], 'id': 'H94-1080.20'}, 'e2': {'word': '1993 ARPA WSJ evaluation', 'word_index': [(8, 11)], 'id': 'H94-1080.21'}}	included HTK large vocabulary speech recognition system used 1993 ARPA WSJ evaluation experimental results presented task .
Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model.	parallel bi-lingual data	statistical machine translation systems	usage	{'e1': {'word': 'parallel bi-lingual data', 'word_index': [(7, 9)], 'id': 'D08-1090.2'}, 'e2': {'word': 'statistical machine translation systems', 'word_index': [(2, 5)], 'id': 'D08-1090.1'}}	Traditionally , statistical machine translation systems relied parallel bi-lingual data train translation model .
While bi-lingual parallel data are expensive to generate, monolingual data are relatively common.	bi-lingual parallel data	monolingual data	compare	{'e1': {'word': 'bi-lingual parallel data', 'word_index': [(0, 2)], 'id': 'D08-1090.4'}, 'e2': {'word': 'monolingual data', 'word_index': [(6, 7)], 'id': 'D08-1090.5'}}	bi-lingual parallel data expensive generate , monolingual data relatively common .
Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language.	monolingual data	language model	usage	{'e1': {'word': 'monolingual data', 'word_index': [(1, 2)], 'id': 'D08-1090.6'}, 'e2': {'word': 'language model', 'word_index': [(8, 9)], 'id': 'D08-1090.7'}}	Yet monolingual data under-utilized , used primarily training language model target language .
This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories.	monolingual target data	statistical machine translation system	usage	{'e1': {'word': 'monolingual target data', 'word_index': [(5, 7)], 'id': 'D08-1090.9'}, 'e2': {'word': 'statistical machine translation system', 'word_index': [(10, 13)], 'id': 'D08-1090.10'}}	paper describes novel method utilizing monolingual target data improve performance statistical machine translation system news stories .
For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to thesource documents.	documents	source documents	compare	{'e1': {'word': 'documents', 'word_index': [(12, 12)], 'id': 'D08-1090.18'}, 'e2': {'word': 'source documents', 'word_index': [(15, 16)], 'id': 'D08-1090.19'}}	every source document translated , large monolingual data set target language searched documents might comparable source documents .
These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document.	documents	MT system	usage	{'e1': {'word': 'documents', 'word_index': [(0, 0)], 'id': 'D08-1090.20'}, 'e2': {'word': 'MT system', 'word_index': [(3, 4)], 'id': 'D08-1090.21'}}	documents used adapt MT system increase probability generating texts resemble comparable document .
Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system.	language and translation models	baseline system	compare	{'e1': {'word': 'language and translation models', 'word_index': [(4, 6)], 'id': 'D08-1090.23'}, 'e2': {'word': 'baseline system', 'word_index': [(10, 11)], 'id': 'D08-1090.24'}}	Experimental results obtained adapting language translation models show substantial gains baseline system .
This paper describes an unsupervised knowledge-lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus.	ambiguous word	corpus	part_whole	{'e1': {'word': 'ambiguous word', 'word_index': [(11, 12)], 'id': 'E06-2007.3'}, 'e2': {'word': 'corpus', 'word_index': [(15, 15)], 'id': 'E06-2007.4'}}	paper describes unsupervised knowledge - lean methodology automatically determining number senses ambiguous word used large corpus .
This paper describes the Unisys MUC-3 text understanding system, a system based upon a three-tiered approach to text processing in which a powerful knowledge-based form of information retrieval plays a central role.	three-tiered approach	system	usage	{'e1': {'word': 'three-tiered approach', 'word_index': [(13, 16)], 'id': 'M91-1032.3'}, 'e2': {'word': 'system', 'word_index': [(10, 10)], 'id': 'M91-1032.2'}}	paper describes Unisys MUC - 3 text understanding system , system based upon three - tiered approach text processing powerful knowledge - based form information retrieval plays central role .
A decision was made to focus on the development of a knowledge-based information retrieval component, and this precluded the integration of Pundit into the prototype.	Pundit	prototype	part_whole	{'e1': {'word': 'Pundit', 'word_index': [(13, 13)], 'id': 'M91-1032.23'}, 'e2': {'word': 'prototype', 'word_index': [(14, 14)], 'id': 'M91-1032.24'}}	decision made focus development knowledge - based information retrieval component , precluded integration Pundit prototype .
ARBITER is a Prolog program that extracts assertions about macromolecular binding relationships from biomedical text.	macromolecular binding relationships	biomedical text	part_whole	{'e1': {'word': 'macromolecular binding relationships', 'word_index': [(5, 7)], 'id': 'A00-1026.3'}, 'e2': {'word': 'biomedical text', 'word_index': [(8, 9)], 'id': 'A00-1026.4'}}	ARBITER Prolog program extracts assertions macromolecular binding relationships biomedical text .
After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted.	ARBITER	MEDLINE abstracts	usage	{'e1': {'word': 'ARBITER', 'word_index': [(3, 3)], 'id': 'A00-1026.8'}, 'e2': {'word': 'MEDLINE abstracts', 'word_index': [(8, 9)], 'id': 'A00-1026.9'}}	discussing formal evaluation ARBITER , report application 491,000 MEDLINE abstracts , almost 25,000 binding relationships suitable entry database macro-molecular function extracted .
After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted.	macro-molecular function	database	part_whole	{'e1': {'word': 'macro-molecular function', 'word_index': [(18, 19)], 'id': 'A00-1026.12'}, 'e2': {'word': 'database', 'word_index': [(17, 17)], 'id': 'A00-1026.11'}}	discussing formal evaluation ARBITER , report application 491,000 MEDLINE abstracts , almost 25,000 binding relationships suitable entry database macro-molecular function extracted .
The resolution of lexical ambiguity is important for most natural language processing tasks, and a range of computational techniques have been proposed for its solution.	lexical ambiguity	natural language processing tasks	part_whole	{'e1': {'word': 'lexical ambiguity', 'word_index': [(1, 2)], 'id': 'H92-1046.2'}, 'e2': {'word': 'natural language processing tasks', 'word_index': [(4, 7)], 'id': 'H92-1046.3'}}	resolution lexical ambiguity important natural language processing tasks , range computational techniques proposed solution .
In this paper, we describe a method for lexical disambiguation of text using the definitions in a machine-readable dictionary together with the technique of simulated annealing.	definitions	lexical disambiguation	usage	{'e1': {'word': 'definitions', 'word_index': [(8, 8)], 'id': 'H92-1046.7'}, 'e2': {'word': 'lexical disambiguation', 'word_index': [(4, 5)], 'id': 'H92-1046.5'}}	paper , describe method lexical disambiguation text using definitions machine - readable dictionary together technique simulated annealing .
Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.	hand coding	lexical entries	usage	{'e1': {'word': 'hand coding', 'word_index': [(13, 14)], 'id': 'H92-1046.23'}, 'e2': {'word': 'lexical entries', 'word_index': [(15, 16)], 'id': 'H92-1046.24'}}	initial results sample set 50 sentences comparable researchers , fully automatic method requires hand coding lexical entries , hand tagging text .
Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.	hand tagging	text	usage	{'e1': {'word': 'hand tagging', 'word_index': [(18, 19)], 'id': 'H92-1046.25'}, 'e2': {'word': 'text', 'word_index': [(20, 20)], 'id': 'H92-1046.26'}}	initial results sample set 50 sentences comparable researchers , fully automatic method requires hand coding lexical entries , hand tagging text .
To date, this array of formal and natural language processing technologies has been used to perform mass changes to legacy textual databases and to facilitate user interfacing to relational databases and software applications.	formal and natural language processing technologies	legacy textual databases	usage	{'e1': {'word': 'formal and natural language processing technologies', 'word_index': [(3, 7)], 'id': 'W97-0909.3'}, 'e2': {'word': 'legacy textual databases', 'word_index': [(12, 14)], 'id': 'W97-0909.4'}}	date , array formal natural language processing technologies used perform mass changes legacy textual databases facilitate user interfacing relational databases software applications .
We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations.	Rhetorical Structure trees	parallel corpus	part_whole	{'e1': {'word': 'Rhetorical Structure trees', 'word_index': [(8, 10)], 'id': 'W00-1403.3'}, 'e2': {'word': 'parallel corpus', 'word_index': [(6, 7)], 'id': 'W00-1403.2'}}	present discourse annotation work aimed constructing parallel corpus Rhetorical Structure trees collection Japanese texts corresponding English translations .
The approach to knowledge representation taken in a multi-modal multi-domain dialogue system - SmartKom - is presented.	knowledge representation	multi-modal multi-domain dialogue system - SmartKom -	usage	{'e1': {'word': 'knowledge representation', 'word_index': [(1, 2)], 'id': 'W03-0903.1'}, 'e2': {'word': 'multi-modal multi-domain dialogue system - SmartKom -', 'word_index': [(4, 10)], 'id': 'W03-0903.2'}}	approach knowledge representation taken multi-modal multi-domain dialogue system - SmartKom - presented .
This paper presents intial work on a system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admission Test (LSAT) and the analytic section of the Graduate Record Exam (GRE).	logic puzzles	Law School Admission Test (LSAT)	part_whole	{'e1': {'word': 'logic puzzles', 'word_index': [(21, 22)], 'id': 'W04-0902.5'}, 'e2': {'word': 'Law School Admission Test (LSAT)', 'word_index': [(25, 31)], 'id': 'W04-0902.6'}}	paper presents intial work system bridges robust , broad - coverage natural language processing precise semantics automated reasoning , focusing solving logic puzzles drawn sources Law School Admission Test ( LSAT ) analytic section Graduate Record Exam ( GRE ) .
We highlight key challenges, and discuss the representations and performance of the prototype system.	performance	prototype system	model-feature	{'e1': {'word': 'performance', 'word_index': [(6, 6)], 'id': 'W04-0902.9'}, 'e2': {'word': 'prototype system', 'word_index': [(7, 8)], 'id': 'W04-0902.10'}}	highlight key challenges , discuss representations performance prototype system .
In this paper, a new parsing model is proposed to formulate the complete chunking problem as a series of boundary detection subtasks.	chunking problem	parsing model	model-feature	{'e1': {'word': 'chunking problem', 'word_index': [(8, 9)], 'id': 'W06-0113.11'}, 'e2': {'word': 'parsing model', 'word_index': [(3, 4)], 'id': 'W06-0113.10'}}	paper , new parsing model proposed formulate complete chunking problem series boundary detection subtasks .
By applying SVM algorithm to these subtasks, we have achieved the best F-Score of 76.56% and 82.26% respectively.	SVM algorithm	F-Score	result	{'e1': {'word': 'SVM algorithm', 'word_index': [(1, 2)], 'id': 'W06-0113.17'}, 'e2': {'word': 'F-Score', 'word_index': [(7, 8)], 'id': 'W06-0113.18'}}	applying SVM algorithm subtasks , achieved best F- Score of % and % respectively .
In this paper, we introduce a WordNet-based measure of semantic relatedness by combining the structure and content of WordNet with co-occurrence information derived from raw text.	co-occurrence information	raw text	part_whole	{'e1': {'word': 'co-occurrence information', 'word_index': [(13, 14)], 'id': 'W06-2501.4'}, 'e2': {'word': 'raw text', 'word_index': [(16, 17)], 'id': 'W06-2501.5'}}	paper , introduce WordNet - based measure semantic relatedness combining structure content WordNet co-occurrence information derived raw text .
We use the co-occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet.	gloss vectors	concept	model-feature	{'e1': {'word': 'gloss vectors', 'word_index': [(8, 9)], 'id': 'W06-2501.8'}, 'e2': {'word': 'concept', 'word_index': [(11, 11)], 'id': 'W06-2501.9'}}	use co-occurrence information along Word Net definitions build gloss vectors corresponding concept Word Net .
Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors.	Numeric scores of relatedness	concepts	model-feature	{'e1': {'word': 'Numeric scores of relatedness', 'word_index': [(0, 2)], 'id': 'W06-2501.11'}, 'e2': {'word': 'concepts', 'word_index': [(5, 5)], 'id': 'W06-2501.12'}}	Numeric scores relatedness assigned pair concepts measuring cosine angle respective gloss vectors .
We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness.	semantic relatedness	word sense disambiguation algorithm	usage	{'e1': {'word': 'semantic relatedness', 'word_index': [(19, 20)], 'id': 'W06-2501.18'}, 'e2': {'word': 'word sense disambiguation algorithm', 'word_index': [(14, 17)], 'id': 'W06-2501.17'}}	show measure compares favorably measures respect human judgments semantic relatedness , performs well used word sense disambiguation algorithm relies semantic relatedness .
In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the cooccurrence information.	cooccurrence information	plain text corpus	part_whole	{'e1': {'word': 'cooccurrence information', 'word_index': [(12, 13)], 'id': 'W06-2501.23'}, 'e2': {'word': 'plain text corpus', 'word_index': [(7, 9)], 'id': 'W06-2501.22'}}	addition , adapted different domains , since plain text corpus used derive cooccurrence information .
This paper describes our system as used in the RTE3 task.	system	RTE3 task	usage	{'e1': {'word': 'system', 'word_index': [(2, 2)], 'id': 'W07-1403.1'}, 'e2': {'word': 'RTE3 task', 'word_index': [(4, 5)], 'id': 'W07-1403.2'}}	paper describes system used RTE3 task .
The system maps premise and hypothesis pairs into an abstract knowledge representation (AKR) and then performs entailment and contradiction detection (ecd) on the resulting AKRs.	abstract knowledge representation (AKR)	premise and hypothesis pairs	model-feature	{'e1': {'word': 'abstract knowledge representation (AKR)', 'word_index': [(5, 10)], 'id': 'W07-1403.5'}, 'e2': {'word': 'premise and hypothesis pairs', 'word_index': [(2, 4)], 'id': 'W07-1403.4'}}	system maps premise hypothesis pairs abstract knowledge representation ( AKR ) performs entailment contradiction detection ( ecd ) resulting AKRs .
Two versions of ECD were used in RTE3, one with strict ECD and one with looser ECD.	ECD	RTE3	usage	{'e1': {'word': 'ECD', 'word_index': [(2, 2)], 'id': 'W07-1403.8'}, 'e2': {'word': 'RTE3', 'word_index': [(4, 4)], 'id': 'W07-1403.9'}}	Two versions ECD used RTE3 , one strict ECD one looser ECD .
We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax.	natural language strings	semantics	model-feature	{'e1': {'word': 'natural language strings', 'word_index': [(10, 12)], 'id': 'P07-1105.3'}, 'e2': {'word': 'semantics', 'word_index': [(14, 14)], 'id': 'P07-1105.4'}}	propose new language learning model learns syntactic-semantic grammar small number natural language strings annotated semantics , along basic assumptions natural language syntax .
In this paper, we propose a novel graph based sentence ranking algorithm, namely PNR2, for update summarization.	graph based sentence ranking algorithm	update summarization	usage	{'e1': {'word': 'graph based sentence ranking algorithm', 'word_index': [(4, 8)], 'id': 'C08-1062.7'}, 'e2': {'word': 'update summarization', 'word_index': [(13, 14)], 'id': 'C08-1062.9'}}	paper , propose novel graph based sentence ranking algorithm , namely PNR2 , update summarization .
This paper discusses the application of the Expectation-Maximization (EM) clustering algorithm to the task of Chinese verb sense discrimination.	Expectation-Maximization (EM) clustering algorithm	Chinese verb sense discrimination	usage	{'e1': {'word': 'Expectation-Maximization (EM) clustering algorithm', 'word_index': [(3, 10)], 'id': 'P04-1038.1'}, 'e2': {'word': 'Chinese verb sense discrimination', 'word_index': [(12, 15)], 'id': 'P04-1038.2'}}	paper discusses application Expectation - Maximization ( EM ) clustering algorithm task Chinese verb sense discrimination .
The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.	rich linguistic features	model	usage	{'e1': {'word': 'rich linguistic features', 'word_index': [(2, 4)], 'id': 'P04-1038.4'}, 'e2': {'word': 'model', 'word_index': [(0, 0)], 'id': 'P04-1038.3'}}	model utilized rich linguistic features capture predicate - argument structure information target verbs .
The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.	predicate-argument structure information	target verbs	model-feature	{'e1': {'word': 'predicate-argument structure information', 'word_index': [(6, 10)], 'id': 'P04-1038.5'}, 'e2': {'word': 'target verbs', 'word_index': [(11, 12)], 'id': 'P04-1038.6'}}	model utilized rich linguistic features capture predicate - argument structure information target verbs .
A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.	semantic taxonomy	Chinese nouns	model-feature	{'e1': {'word': 'semantic taxonomy', 'word_index': [(0, 1)], 'id': 'P04-1038.7'}, 'e2': {'word': 'Chinese nouns', 'word_index': [(2, 3)], 'id': 'P04-1038.8'}}	semantic taxonomy Chinese nouns , built semi-automatically based two electronic Chinese semantic dictionaries , used provide semantic features model .
A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.	semantic features	model	usage	{'e1': {'word': 'semantic features', 'word_index': [(16, 17)], 'id': 'P04-1038.10'}, 'e2': {'word': 'model', 'word_index': [(18, 18)], 'id': 'P04-1038.11'}}	semantic taxonomy Chinese nouns , built semi-automatically based two electronic Chinese semantic dictionaries , used provide semantic features model .
We further enhanced the model with certain fine-grained semantic categories called lexical sets.	fine-grained semantic categories	model	usage	{'e1': {'word': 'fine-grained semantic categories', 'word_index': [(3, 7)], 'id': 'P04-1038.20'}, 'e2': {'word': 'model', 'word_index': [(1, 1)], 'id': 'P04-1038.19'}}	enhanced model certain fine - grained semantic categories called lexical sets .
Our results indicate that these lexical sets improve the model's performance for the three most challenging verbs chosen from the first set of experiments.	lexical sets	model	result	{'e1': {'word': 'lexical sets', 'word_index': [(2, 3)], 'id': 'P04-1038.22'}, 'e2': {'word': 'model', 'word_index': [(5, 5)], 'id': 'P04-1038.23'}}	results indicate lexical sets improve model 's performance three challenging verbs chosen first set experiments .
This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar.	dependency structure grammar	speech recognition and understanding	usage	{'e1': {'word': 'dependency structure grammar', 'word_index': [(10, 12)], 'id': 'C88-1082.3'}, 'e2': {'word': 'speech recognition and understanding', 'word_index': [(6, 8)], 'id': 'C88-1082.2'}}	paper proposes efficient linguistic processing strategy speech recognition understanding using dependency structure grammar .
After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships.	phoneme recognition	phrase recognition	usage	{'e1': {'word': 'phoneme recognition', 'word_index': [(5, 6)], 'id': 'C88-1082.9'}, 'e2': {'word': 'phrase recognition', 'word_index': [(2, 3)], 'id': 'C88-1082.8'}}	speech processing phrase recognition based phoneme recognition , parser extracts sentence best likelihood taking account phonetic likelihood phrase candidates linguistic likelihood semantic inter-phrase dependency relationships .
A fast parsing algorithm using breadth-first search is also proposed.	breadth-first search	parsing algorithm	usage	{'e1': {'word': 'breadth-first search', 'word_index': [(4, 6)], 'id': 'C88-1082.18'}, 'e2': {'word': 'parsing algorithm', 'word_index': [(1, 2)], 'id': 'C88-1082.17'}}	fast parsing algorithm using breadth- first search is proposed .
The predictor pre-selects the phrase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing.	transition rules	predictor	usage	{'e1': {'word': 'transition rules', 'word_index': [(5, 6)], 'id': 'C88-1082.21'}, 'e2': {'word': 'predictor', 'word_index': [(0, 0)], 'id': 'C88-1082.19'}}	predictor pre-selects phrase candidates using transition rules combined dependency structure reduce amount phonetic processing .
The experimental results show that it greatly increases the accuracy of speech recognitions, and the breadth-first parsing algorithm and predictor increase processing speed.	predictor	processing speed	result	{'e1': {'word': 'predictor', 'word_index': [(14, 14)], 'id': 'C88-1082.29'}, 'e2': {'word': 'processing speed', 'word_index': [(16, 17)], 'id': 'C88-1082.30'}}	experimental results show greatly increases accuracy speech recognitions , breadth- first parsing algorithm and predictor increase processing speed .
In the EU-funded project, QALL-ME, a domain-specific ontology was developed and applied for question answering in the domain of tourism, along with the assistance of two upper ontologies for concept expansion and reasoning.	domain-specific ontology	question answering	usage	{'e1': {'word': 'domain-specific ontology', 'word_index': [(8, 9)], 'id': 'L08-1178.6'}, 'e2': {'word': 'question answering', 'word_index': [(13, 13)], 'id': 'L08-1178.7'}}	EU - funded project , QALL - , domain-specific ontology was developed applied answering in the tourism along with the two ontologies for concept and reasoning
The design of the ontology is presented in the paper, and a semi-automatic alignment procedure is described with some alignment results given as well.	semi-automatic alignment procedure	alignment results	result	{'e1': {'word': 'semi-automatic alignment procedure', 'word_index': [(5, 7)], 'id': 'L08-1178.14'}, 'e2': {'word': 'alignment results', 'word_index': [(9, 10)], 'id': 'L08-1178.15'}}	design ontology presented paper , semi-automatic alignment procedure described alignment results given well .
Furthermore, the aligned ontology was used to semantically annotate original data obtained from the tourism web sites and natural language questions.	natural language questions	data	part_whole	{'e1': {'word': 'natural language questions', 'word_index': [(13, 15)], 'id': 'L08-1178.18'}, 'e2': {'word': 'data', 'word_index': [(8, 8)], 'id': 'L08-1178.17'}}	Furthermore , aligned ontology used semantically annotate original data obtained tourism web sites natural language questions .
The storage schema of the annotated data and the data access method for retrieving answers from the annotated data are also reported in the paper.	data access method	annotated data	usage	{'e1': {'word': 'data access method', 'word_index': [(4, 6)], 'id': 'L08-1178.21'}, 'e2': {'word': 'annotated data', 'word_index': [(9, 10)], 'id': 'L08-1178.22'}}	storage schema annotated data data access method retrieving answers annotated data also reported paper .
In particular we discuss a natural language generation system that is composed of SPoT, a trainable sentence planner, and FERGUS, a stochastic surface realizer.	SPoT	natural language generation system	part_whole	{'e1': {'word': 'SPoT', 'word_index': [(7, 7)], 'id': 'C02-1138.9'}, 'e2': {'word': 'natural language generation system', 'word_index': [(2, 5)], 'id': 'C02-1138.8'}}	particular discuss natural language generation system composed SPoT , trainable sentence planner , FERGUS , stochastic surface realizer .
We show how these stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system.	NLG components	real-time dialog system	part_whole	{'e1': {'word': 'NLG components', 'word_index': [(14, 15)], 'id': 'C02-1138.13'}, 'e2': {'word': 'real-time dialog system', 'word_index': [(17, 21)], 'id': 'C02-1138.14'}}	show stochastic NLG components made work together , ported new domains apparent ease , NLG components integrated real - time dialog system .
In the current work, we produce a manual segmentation of laughter in a large corpus of interactive multi-party seminars, which promises to be a valuable resource for acoustic modeling purposes.	interactive multi-party seminars	corpus	part_whole	{'e1': {'word': 'interactive multi-party seminars', 'word_index': [(9, 11)], 'id': 'L08-1016.10'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'L08-1016.9'}}	current work , produce manual segmentation laughter large corpus interactive multi-party seminars , promises valuable resource acoustic modeling purposes .
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further.	grammar PCFG*	extended GLR parsing algorithm	usage	{'e1': {'word': 'grammar PCFG*', 'word_index': [(6, 8)], 'id': 'C02-2028.2'}, 'e2': {'word': 'extended GLR parsing algorithm', 'word_index': [(2, 5)], 'id': 'C02-2028.1'}}	paper presents extended GLR parsing algorithm grammar PCFG * based Tomita 's GLR parsing algorithm extends .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	PCFG	grammar PCFG*	usage	{'e1': {'word': 'PCFG', 'word_index': [(7, 7)], 'id': 'C02-2028.5'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(3, 5)], 'id': 'C02-2028.4'}}	also define new grammar PCFG * based PCFG assigns probability also frequency associated rule .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	frequency	rule	model-feature	{'e1': {'word': 'frequency', 'word_index': [(11, 11)], 'id': 'C02-2028.7'}, 'e2': {'word': 'rule', 'word_index': [(13, 13)], 'id': 'C02-2028.8'}}	also define new grammar PCFG * based PCFG assigns probability also frequency associated rule .
So our syntactic parsing system is implemented based on rule-based approach and statistics approach.	rule-based approach	syntactic parsing system	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(5, 8)], 'id': 'C02-2028.10'}, 'e2': {'word': 'syntactic parsing system', 'word_index': [(0, 2)], 'id': 'C02-2028.9'}}	syntactic parsing system implemented based rule - based approach statistics approach .
In this paper, we discuss lemma identification in Japanese morphological analysis, which is crucial for a proper formulation of morphological analysis that benefits not only NLP researchers but also corpus linguists.	lemma identification	Japanese morphological analysis	usage	{'e1': {'word': 'lemma identification', 'word_index': [(3, 4)], 'id': 'L08-1535.1'}, 'e2': {'word': 'Japanese morphological analysis', 'word_index': [(5, 7)], 'id': 'L08-1535.2'}}	paper , discuss lemma identification Japanese morphological analysis , crucial proper formulation morphological analysis benefits NLP researchers also corpus linguists .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	words	vocabulary	part_whole	{'e1': {'word': 'words', 'word_index': [(9, 9)], 'id': 'L08-1535.8'}, 'e2': {'word': 'vocabulary', 'word_index': [(6, 6)], 'id': 'L08-1535.6'}}	Since Japanese words often variation orthography vocabulary Japanese consists words several different origins , sometimes happens one writing form corresponds lemma single writing form corresponds two lemmas different readings / meanings .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	writing form	lemma	model-feature	{'e1': {'word': 'writing form', 'word_index': [(17, 18)], 'id': 'L08-1535.9'}, 'e2': {'word': 'lemma', 'word_index': [(20, 20)], 'id': 'L08-1535.10'}}	Since Japanese words often variation orthography vocabulary Japanese consists words several different origins , sometimes happens one writing form corresponds lemma single writing form corresponds two lemmas different readings / meanings .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	writing form	lemmas	model-feature	{'e1': {'word': 'writing form', 'word_index': [(22, 23)], 'id': 'L08-1535.11'}, 'e2': {'word': 'lemmas', 'word_index': [(26, 26)], 'id': 'L08-1535.12'}}	Since Japanese words often variation orthography vocabulary Japanese consists words several different origins , sometimes happens one writing form corresponds lemma single writing form corresponds two lemmas different readings / meanings .
The mapping from a writing form onto a lemma is important in linguistic analysis of corpora.	linguistic analysis	corpora	topic	{'e1': {'word': 'linguistic analysis', 'word_index': [(6, 7)], 'id': 'L08-1535.17'}, 'e2': {'word': 'corpora', 'word_index': [(8, 8)], 'id': 'L08-1535.18'}}	mapping writing form onto lemma important linguistic analysis corpora .
The current study focuses on disambiguation of heteronyms, words with the same writing form but with different word forms.	disambiguation	heteronyms	usage	{'e1': {'word': 'disambiguation', 'word_index': [(3, 3)], 'id': 'L08-1535.19'}, 'e2': {'word': 'heteronyms', 'word_index': [(4, 4)], 'id': 'L08-1535.20'}}	current study focuses disambiguation heteronyms , words writing form different word forms .
To resolve heteronym ambiguity, we make use of goshu information, the classification of words based on their origin.	origin	words	model-feature	{'e1': {'word': 'origin', 'word_index': [(12, 12)], 'id': 'L08-1535.28'}, 'e2': {'word': 'words', 'word_index': [(10, 10)], 'id': 'L08-1535.27'}}	resolve heteronym ambiguity , make use goshu information , classification words based origin .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	words	goshu classes	part_whole	{'e1': {'word': 'words', 'word_index': [(2, 2)], 'id': 'L08-1535.29'}, 'e2': {'word': 'goshu classes', 'word_index': [(3, 4)], 'id': 'L08-1535.30'}}	Founded fact words goshu classes likely combine compound words words classes , employ statistical model based CRFs using goshu information .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	words	classes	part_whole	{'e1': {'word': 'words', 'word_index': [(9, 9)], 'id': 'L08-1535.32'}, 'e2': {'word': 'classes', 'word_index': [(10, 10)], 'id': 'L08-1535.33'}}	Founded fact words goshu classes likely combine compound words words classes , employ statistical model based CRFs using goshu information .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	CRFs	statistical model	usage	{'e1': {'word': 'CRFs', 'word_index': [(16, 16)], 'id': 'L08-1535.35'}, 'e2': {'word': 'statistical model', 'word_index': [(13, 14)], 'id': 'L08-1535.34'}}	Founded fact words goshu classes likely combine compound words words classes , employ statistical model based CRFs using goshu information .
Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.	goshu information	performance	result	{'e1': {'word': 'goshu information', 'word_index': [(4, 5)], 'id': 'L08-1535.38'}, 'e2': {'word': 'performance', 'word_index': [(8, 8)], 'id': 'L08-1535.39'}}	Experimental results show use goshu information considerably improves performance heteronym disambiguation lemma identification , suggesting goshu information solves lemma identification task effectively .
Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.	information	lemma identification task	usage	{'e1': {'word': 'information', 'word_index': [(16, 16)], 'id': 'L08-1535.42'}, 'e2': {'word': 'lemma identification task', 'word_index': [(18, 20)], 'id': 'L08-1535.43'}}	Experimental results show use goshu information considerably improves performance heteronym disambiguation lemma identification , suggesting goshu information solves lemma identification task effectively .
An event detection algorithm identifies the collocations that may cause an event in a specific timestamp.	event detection algorithm	collocations	usage	{'e1': {'word': 'event detection algorithm', 'word_index': [(0, 2)], 'id': 'L08-1003.6'}, 'e2': {'word': 'collocations', 'word_index': [(4, 4)], 'id': 'L08-1003.7'}}	event detection algorithm identifies collocations may cause event specific timestamp .
An event summarization algorithm retrieves a set of collocations which describe an event.	event summarization algorithm	collocations	usage	{'e1': {'word': 'event summarization algorithm', 'word_index': [(0, 2)], 'id': 'L08-1003.8'}, 'e2': {'word': 'collocations', 'word_index': [(5, 5)], 'id': 'L08-1003.9'}}	event summarization algorithm retrieves set collocations describe event .
We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis (LSA) to predict team performance.	Latent Semantic Analysis (LSA)	tagging	usage	{'e1': {'word': 'Latent Semantic Analysis (LSA)', 'word_index': [(8, 13)], 'id': 'N04-4025.2'}, 'e2': {'word': 'tagging', 'word_index': [(4, 4)], 'id': 'N04-4025.1'}}	describe two approaches analyzing tagging team discourse using Latent Semantic Analysis ( LSA ) predict team performance .
A huge amount of translation work needs to be done when creating and updating technical documentation.	translation work	technical documentation	usage	{'e1': {'word': 'translation work', 'word_index': [(2, 3)], 'id': 'A94-1044.3'}, 'e2': {'word': 'technical documentation', 'word_index': [(8, 9)], 'id': 'A94-1044.4'}}	huge amount translation work needs done creating updating technical documentation .
The objective of this project is a pilot study of several new ideas for the automatic adaptation and improvement of natural language processing (NLP) systems.	pilot study	automatic adaptation	topic	{'e1': {'word': 'pilot study', 'word_index': [(2, 3)], 'id': 'H91-1079.1'}, 'e2': {'word': 'automatic adaptation', 'word_index': [(7, 8)], 'id': 'H91-1079.2'}}	objective project pilot study several new ideas automatic adaptation improvement natural language processing ( NLP ) systems .
The effort focuses particularly on automatically inferring the meaning of new words in context and on developing partial interpretations of language that is either fragmentary or beyond the capability of the NLP system to understand.	meaning	words	model-feature	{'e1': {'word': 'meaning', 'word_index': [(5, 5)], 'id': 'H91-1079.5'}, 'e2': {'word': 'words', 'word_index': [(7, 7)], 'id': 'H91-1079.6'}}	effort focuses particularly automatically inferring meaning new words context developing partial interpretations language either fragmentary beyond capability NLP system understand .
The NLP system uses large annotated corpora, such as those being developed under the DARPA-funded TREE-BANK project at the University of Pennsylvania, to adapt by acquiring syntactic and semantic information from the annotated examples.	large annotated corpora	NLP system	usage	{'e1': {'word': 'large annotated corpora', 'word_index': [(3, 5)], 'id': 'H91-1079.13'}, 'e2': {'word': 'NLP system', 'word_index': [(0, 1)], 'id': 'H91-1079.12'}}	NLP system uses large annotated corpora , developed DARPA - funded TREE-BANK project at the Pennsylvania to adapt acquiring and semantic from the .
The NLP system uses large annotated corpora, such as those being developed under the DARPA-funded TREE-BANK project at the University of Pennsylvania, to adapt by acquiring syntactic and semantic information from the annotated examples.	syntactic and semantic information	annotated examples	part_whole	{'e1': {'word': 'syntactic and semantic information', 'word_index': [(19, 21)], 'id': 'H91-1079.15'}, 'e2': {'word': 'annotated examples', 'word_index': [(24, 24)], 'id': 'H91-1079.16'}}	NLP system uses large annotated corpora , developed DARPA - funded TREE-BANK project at the Pennsylvania to adapt acquiring syntactic and semantic from the examples .
Statistical language modeling, based on probability estimates derived from the large corpora, will provide a means of ranking alternative interpretations of fragments.	probability estimates	Statistical language modeling	usage	{'e1': {'word': 'probability estimates', 'word_index': [(5, 6)], 'id': 'H91-1079.18'}, 'e2': {'word': 'Statistical language modeling', 'word_index': [(0, 2)], 'id': 'H91-1079.17'}}	Statistical language modeling , based probability estimates derived large corpora , provide means ranking alternative interpretations fragments .
Lexicalized concepts are organized by semantic relations (synonymy, antonymy, hyponymy, meronymy, etc.)	semantic relations	Lexicalized concepts	model-feature	{'e1': {'word': 'semantic relations', 'word_index': [(3, 4)], 'id': 'H92-1116.6'}, 'e2': {'word': 'Lexicalized concepts', 'word_index': [(0, 1)], 'id': 'H92-1116.5'}}	Lexicalized concepts organized semantic relations ( synonymy , antonymy , hyponymy , meronymy , etc. )
Work under this grant is intended to extend and upgrade WordNet, to make it generally available, and to develop it as a tool for use in practical applications.	WordNet	applications	usage	{'e1': {'word': 'WordNet', 'word_index': [(5, 5)], 'id': 'H92-1116.14'}, 'e2': {'word': 'applications', 'word_index': [(15, 15)], 'id': 'H92-1116.15'}}	Work grant intended extend upgrade WordNet , make generally available , develop tool use practical applications .
In order to make it available for information retrieval and machine translation, a system is being developed English text as input and automatically gives as output the same text augmented by syntactic and semantic anotations that disambiguate all of the substantive words.	syntactic and semantic anotations	substantive words	model-feature	{'e1': {'word': 'syntactic and semantic anotations', 'word_index': [(18, 20)], 'id': 'H92-1116.20'}, 'e2': {'word': 'substantive words', 'word_index': [(22, 23)], 'id': 'H92-1116.21'}}	order make available information retrieval machine translation , system developed English text input automatically gives output text augmented syntactic semantic anotations disambiguate substantive words .
Initially, the semantic tagging is being done manually so that we can (1) obtain extensive experience with the tagging process and (2) create a database of correctly tagged text for use in testing proposals for automatic sense disambiguation.	text	database	part_whole	{'e1': {'word': 'text', 'word_index': [(21, 21)], 'id': 'H92-1116.25'}, 'e2': {'word': 'database', 'word_index': [(18, 18)], 'id': 'H92-1116.24'}}	Initially , semantic tagging done manually ( 1 ) obtain extensive experience tagging process ( 2 ) create database correctly tagged text use testing proposals automatic sense disambiguation .
Review previous designs involving TIPSTER technology, to support you design process.Determine if your application can benefit from upgrading to advanced TIPSTER technology that has been developed since your application was implemented.	TIPSTER technology	application	usage	{'e1': {'word': 'TIPSTER technology', 'word_index': [(16, 17)], 'id': 'X96-1060.8'}, 'e2': {'word': 'application', 'word_index': [(12, 12)], 'id': 'X96-1060.7'}}	Review previous designs involving TIPSTER technology , support design process . Determine application benefit upgrading advanced TIPSTER technology developed since application implemented .
Accurate lemmatization of German nouns mandates the use of a lexicon.	lemmatization	German nouns	usage	{'e1': {'word': 'lemmatization', 'word_index': [(1, 1)], 'id': 'H05-1080.1'}, 'e2': {'word': 'German nouns', 'word_index': [(2, 3)], 'id': 'H05-1080.2'}}	Accurate lemmatization German nouns mandates use lexicon .
We present a self-learning lemmatizer capable of automatically creating a full-form lexicon by processing German documents.	self-learning lemmatizer	German documents	usage	{'e1': {'word': 'self-learning lemmatizer', 'word_index': [(1, 4)], 'id': 'H05-1080.5'}, 'e2': {'word': 'German documents', 'word_index': [(13, 14)], 'id': 'H05-1080.7'}}	present self - learning lemmatizer capable automatically creating full - form lexicon processing German documents .
In this paper we describe how Why-Atlas creates and utilizes a proof-based representation of student essays.	proof-based representation	Why-Atlas	usage	{'e1': {'word': 'proof-based representation', 'word_index': [(6, 9)], 'id': 'W02-0211.6'}, 'e2': {'word': 'Why-Atlas', 'word_index': [(2, 3)], 'id': 'W02-0211.5'}}	paper describe - Atlas creates utilizes proof - based representation student essays .
We describe how it creates the proof given the output of sentence-level understanding, how it uses the proofs to give students feedback, some preliminary runtime measures, and the work we are currently doing to derive additional benefits from a proof-based approach for tutoring applications.	proof-based approach	tutoring applications	usage	{'e1': {'word': 'proof-based approach', 'word_index': [(25, 28)], 'id': 'W02-0211.10'}, 'e2': {'word': 'tutoring applications', 'word_index': [(29, 30)], 'id': 'W02-0211.11'}}	describe creates proof given output sentence - level understanding , uses proofs give students feedback , preliminary runtime measures , work currently derive additional benefits proof - based approach tutoring applications .
Compared with syntactic knowledge, semantic knowledge is more difficult to annotate, for ambiguity problem is more serious.	syntactic knowledge	semantic knowledge	compare	{'e1': {'word': 'syntactic knowledge', 'word_index': [(1, 2)], 'id': 'W03-1712.13'}, 'e2': {'word': 'semantic knowledge', 'word_index': [(4, 5)], 'id': 'W03-1712.14'}}	Compared syntactic knowledge , semantic knowledge difficult annotate , ambiguity problem serious .
Finally, we will compare our corpus with other well-known corpora.	corpus	corpora	compare	{'e1': {'word': 'corpus', 'word_index': [(3, 3)], 'id': 'W03-1712.17'}, 'e2': {'word': 'corpora', 'word_index': [(7, 7)], 'id': 'W03-1712.18'}}	Finally , compare corpus well - known corpora .
These techniques, developed within the Augmented Transition Network (ATN) model, are shown to be adequate to handle many of these cases.	techniques	Augmented Transition Network (ATN) model	usage	{'e1': {'word': 'techniques', 'word_index': [(0, 0)], 'id': 'J81-2002.7'}, 'e2': {'word': 'Augmented Transition Network (ATN) model', 'word_index': [(4, 10)], 'id': 'J81-2002.8'}}	techniques , developed within Augmented Transition Network ( ATN ) model , shown adequate handle many cases .
Once identified, reference chains can be extended across segment boundaries, thus enabling the application of CT over the entire discourse.	CT	discourse	usage	{'e1': {'word': 'CT', 'word_index': [(12, 12)], 'id': 'P98-1044.13'}, 'e2': {'word': 'discourse', 'word_index': [(14, 14)], 'id': 'P98-1044.14'}}	identified , reference chains extended across segment boundaries , thus enabling application CT entire discourse .
We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains.	CT	global discourse	usage	{'e1': {'word': 'CT', 'word_index': [(7, 7)], 'id': 'P98-1044.17'}, 'e2': {'word': 'global discourse', 'word_index': [(9, 10)], 'id': 'P98-1044.18'}}	describe processes veins defined discourse structure trees CT applied global discourse using chains .
We also define a discourse smoothness index which can be used to compare different discourse structures and interpretations, and show how VT can be used to abstract a span of text in the context of the whole discourse.	discourse smoothness index	discourse structures and interpretations	usage	{'e1': {'word': 'discourse smoothness index', 'word_index': [(2, 4)], 'id': 'P98-1044.20'}, 'e2': {'word': 'discourse structures and interpretations', 'word_index': [(8, 10)], 'id': 'P98-1044.21'}}	also define discourse smoothness index used compare different discourse structures interpretations , show VT used abstract span text context whole discourse .
HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports in order to satisfy a given scenario.	natural language	exploratory questions	model-feature	{'e1': {'word': 'natural language', 'word_index': [(13, 14)], 'id': 'W04-2507.7'}, 'e2': {'word': 'exploratory questions', 'word_index': [(11, 12)], 'id': 'W04-2507.6'}}	HITIQA interactive open-domain question answering technology designed allow analysts pose complex exploratory questions natural language obtain relevant information units prepare briefing reports order satisfy given scenario .
The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space.	data-driven semantics	system	usage	{'e1': {'word': 'data-driven semantics', 'word_index': [(3, 4)], 'id': 'W04-2507.12'}, 'e2': {'word': 'system', 'word_index': [(0, 0)], 'id': 'W04-2507.11'}}	system uses novel data-driven semantics conduct clarification dialogue user explores scope context desired answer space .
One is that it resulted in the first freely distributable corpus of fully anonymized clinical text.	fully anonymized clinical text	corpus	part_whole	{'e1': {'word': 'fully anonymized clinical text', 'word_index': [(6, 9)], 'id': 'W07-1013.4'}, 'e2': {'word': 'corpus', 'word_index': [(5, 5)], 'id': 'W07-1013.3'}}	One resulted first freely distributable corpus fully anonymized clinical text .
The other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels.	set of labels	categorization	usage	{'e1': {'word': 'set of labels', 'word_index': [(9, 10)], 'id': 'W07-1013.7'}, 'e2': {'word': 'categorization', 'word_index': [(4, 4)], 'id': 'W07-1013.6'}}	key feature task required categorization respect large commercially significant set labels .
Many systems performed at levels approaching the inter-coder agreement, suggesting that human-like performance on this task is within the reach of currently available technologies.	human-like performance	currently available technologies	compare	{'e1': {'word': 'human-like performance', 'word_index': [(11, 14)], 'id': 'W07-1013.12'}, 'e2': {'word': 'currently available technologies', 'word_index': [(18, 20)], 'id': 'W07-1013.13'}}	Many systems performed levels approaching inter - coder agreement , suggesting human - like performance task within reach currently available technologies .
In this paper we describe automatic information nuggetization and its application to text comparison.	automatic information nuggetization	text comparison	usage	{'e1': {'word': 'automatic information nuggetization', 'word_index': [(2, 4)], 'id': 'N07-2055.1'}, 'e2': {'word': 'text comparison', 'word_index': [(6, 7)], 'id': 'N07-2055.2'}}	paper describe automatic information nuggetization application text comparison .
More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material.	machine-generated nuggets	evaluation material	usage	{'e1': {'word': 'machine-generated nuggets', 'word_index': [(5, 8)], 'id': 'N07-2055.3'}, 'e2': {'word': 'evaluation material', 'word_index': [(11, 12)], 'id': 'N07-2055.4'}}	specifically , take close look machine - generated nuggets used create evaluation material .
A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.	semiautomatic annotation scheme	gold-standard data	usage	{'e1': {'word': 'semiautomatic annotation scheme', 'word_index': [(0, 2)], 'id': 'N07-2055.5'}, 'e2': {'word': 'gold-standard data', 'word_index': [(5, 8)], 'id': 'N07-2055.6'}}	semiautomatic annotation scheme designed produce gold - standard data exceptionally high inter-human agreement .
This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts.	features	retrieval	usage	{'e1': {'word': 'features', 'word_index': [(7, 7)], 'id': 'P07-3007.1'}, 'e2': {'word': 'retrieval', 'word_index': [(8, 8)], 'id': 'P07-3007.2'}}	paper presents results experiments tested different kinds features retrieval Chinese opinionated texts .
We assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR, but with some distinct features.	retrieval	IR	part_whole	{'e1': {'word': 'retrieval', 'word_index': [(2, 2)], 'id': 'P07-3007.4'}, 'e2': {'word': 'IR', 'word_index': [(11, 11)], 'id': 'P07-3007.6'}}	assume task retrieval opinionated texts ( OIR ) regarded subtask general IR , distinct features .
This paper discusses the supervised learning of morphology using stochastic transducers, trained using the Expectation-Maximization (EM) algorithm.	stochastic transducers	supervised learning	usage	{'e1': {'word': 'stochastic transducers', 'word_index': [(6, 7)], 'id': 'P02-1065.3'}, 'e2': {'word': 'supervised learning', 'word_index': [(2, 3)], 'id': 'P02-1065.1'}}	paper discusses supervised learning morphology using stochastic transducers , trained using Expectation - Maximization ( EM ) algorithm .
These are evaluated and compared ondata sets from English, German, Slovene and Arabic.	data sets	English	part_whole	{'e1': {'word': 'data sets', 'word_index': [(2, 3)], 'id': 'P02-1065.9'}, 'e2': {'word': 'English', 'word_index': [(4, 4)], 'id': 'P02-1065.10'}}	evaluated compared data sets English , German , Slovene Arabic .
Speech recognition problems are a reality in current spoken dialogue systems	Speech recognition problems	spoken dialogue systems	part_whole	{'e1': {'word': 'Speech recognition problems', 'word_index': [(0, 2)], 'id': 'P06-1025.1'}, 'e2': {'word': 'spoken dialogue systems', 'word_index': [(5, 7)], 'id': 'P06-1025.2'}}	Speech recognition problems reality current spoken dialogue systems
We apply Chi Square (%2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns.	Chi Square (%2) analysis	corpus of speech-based computer tutoring dialogues	usage	{'e1': {'word': 'Chi Square (%2) analysis', 'word_index': [(1, 7)], 'id': 'P06-1025.5'}, 'e2': {'word': 'corpus of speech-based computer tutoring dialogues', 'word_index': [(8, 14)], 'id': 'P06-1025.6'}}	apply Chi Square ( % 2 ) analysis corpus speech - based computer tutoring dialogues discover dependencies within across turns .
In an interlingual knowledge-based machine translation system, ambiguity arises when the source language analyzer produces more than one interlingua expression for a source sentence.	interlingua expression	source sentence	model-feature	{'e1': {'word': 'interlingua expression', 'word_index': [(15, 16)], 'id': 'C94-1012.4'}, 'e2': {'word': 'source sentence', 'word_index': [(17, 18)], 'id': 'C94-1012.5'}}	interlingual knowledge - based machine translation system , ambiguity arises source language analyzer produces one interlingua expression source sentence .
We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence.	test sentences	corpus	part_whole	{'e1': {'word': 'test sentences', 'word_index': [(5, 6)], 'id': 'C94-1012.12'}, 'e2': {'word': 'corpus', 'word_index': [(4, 4)], 'id': 'C94-1012.11'}}	also test methods large corpus test sentences , order illustrate different disambiguation methods reduce average number parses per sentence .
We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence.	disambiguation methods	parses	result	{'e1': {'word': 'disambiguation methods', 'word_index': [(11, 12)], 'id': 'C94-1012.13'}, 'e2': {'word': 'parses', 'word_index': [(16, 16)], 'id': 'C94-1012.14'}}	also test methods large corpus test sentences , order illustrate different disambiguation methods reduce average number parses per sentence .
The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding.	dynamic programs	machine translation decoding	usage	{'e1': {'word': 'dynamic programs', 'word_index': [(12, 13)], 'id': 'D08-1012.3'}, 'e2': {'word': 'machine translation decoding', 'word_index': [(14, 16)], 'id': 'D08-1012.4'}}	intersection tree transducer - based translation models n-gram language models results huge dynamic programs machine translation decoding .
In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language.	clustered encoding	encoding-based methods	usage	{'e1': {'word': 'clustered encoding', 'word_index': [(19, 20)], 'id': 'D08-1012.10'}, 'e2': {'word': 'encoding-based methods', 'word_index': [(12, 15)], 'id': 'D08-1012.9'}}	contrast previous order - based bigram- to trigram approaches , we on encoding - based methods , which a clustered encoding of language .
Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.	decoding cascade for trigram language models	bigram-to-trigram decoder	compare	{'e1': {'word': 'decoding cascade for trigram language models', 'word_index': [(3, 7)], 'id': 'D08-1012.16'}, 'e2': {'word': 'bigram-to-trigram decoder', 'word_index': [(13, 15)], 'id': 'D08-1012.18'}}	Moreover , entire decoding cascade trigram language models faster corresponding bigram pass alone bigram- to decoder .
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain.	pronoun resolution	biomedical texts	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(4, 5)], 'id': 'L08-1071.1'}, 'e2': {'word': 'biomedical texts', 'word_index': [(6, 7)], 'id': 'L08-1071.2'}}	paper presents findings feasibility pronoun resolution biomedical texts , comparison conducting pronoun resolution newswire domain .
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain.	pronoun resolution	newswire domain	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(11, 12)], 'id': 'L08-1071.3'}, 'e2': {'word': 'newswire domain', 'word_index': [(13, 14)], 'id': 'L08-1071.4'}}	paper presents findings feasibility pronoun resolution biomedical texts , comparison conducting pronoun resolution newswire domain .
Sharing portions of grammars across languages greatly reduces the costs of multilingual grammar engineering.	grammars	multilingual grammar engineering	result	{'e1': {'word': 'grammars', 'word_index': [(2, 2)], 'id': 'C00-1005.1'}, 'e2': {'word': 'multilingual grammar engineering', 'word_index': [(8, 10)], 'id': 'C00-1005.3'}}	Sharing portions grammars across languages greatly reduces costs multilingual grammar engineering .
Taking grammatical relatedness seriously, we are particularly interested in designing linguistically motivated grammatical resources for Slavic languages to be used in applied and theoretical computational linguistics.	linguistically motivated grammatical resources	applied and theoretical computational linguistics	usage	{'e1': {'word': 'linguistically motivated grammatical resources', 'word_index': [(8, 11)], 'id': 'C00-1005.8'}, 'e2': {'word': 'applied and theoretical computational linguistics', 'word_index': [(15, 18)], 'id': 'C00-1005.10'}}	Taking grammatical relatedness seriously , particularly interested designing linguistically motivated grammatical resources Slavic languages used applied theoretical computational linguistics .
"On the basis of Slavic data, we show how a domain ontology conceptualising morpho-syntactic ""building blocks"" can serve as a basis of a shared grammar of Slavic."	domain ontology	shared grammar of Slavic	usage	{'e1': {'word': 'domain ontology', 'word_index': [(5, 6)], 'id': 'C00-1005.18'}, 'e2': {'word': 'shared grammar of Slavic', 'word_index': [(15, 17)], 'id': 'C00-1005.20'}}	"basis Slavic data , show domain ontology conceptualising morpho-syntactic "" building blocks "" serve basis shared grammar Slavic ."
We are currently developing MiniSTEx, a spatiotemporal annotation system to handle temporal and/or geospatial information directly and indirectly expressed in texts.	temporal and/or geospatial information	texts	part_whole	{'e1': {'word': 'temporal and/or geospatial information', 'word_index': [(8, 11)], 'id': 'L08-1561.6'}, 'e2': {'word': 'texts', 'word_index': [(15, 15)], 'id': 'L08-1561.7'}}	currently developing MiniSTEx , spatiotemporal annotation system handle temporal / geospatial information directly indirectly expressed texts .
A first version of MiniSTEx was originally developed for Dutch, keeping in mind that it should also be useful for other European languages, and for multilingual applications.	MiniSTEx	Dutch	usage	{'e1': {'word': 'MiniSTEx', 'word_index': [(2, 2)], 'id': 'L08-1561.11'}, 'e2': {'word': 'Dutch', 'word_index': [(5, 5)], 'id': 'L08-1561.12'}}	first version MiniSTEx originally developed Dutch , keeping mind also useful European languages , multilingual applications .
In this paper we describe a technique for improving the performance of an information extraction system for speech data by explicitly modeling the errors in the recognizer output.	information extraction system	speech data	usage	{'e1': {'word': 'information extraction system', 'word_index': [(5, 7)], 'id': 'H01-1034.4'}, 'e2': {'word': 'speech data', 'word_index': [(8, 9)], 'id': 'H01-1034.5'}}	paper describe technique improving performance information extraction system speech data explicitly modeling errors recognizer output .
In this paper, we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list.	QA system	type questions	usage	{'e1': {'word': 'QA system', 'word_index': [(3, 4)], 'id': 'I05-2006.1'}, 'e2': {'word': 'type questions', 'word_index': [(6, 7)], 'id': 'I05-2006.2'}}	paper , report QA system answer type questions based confirmed knowledge base developed using mails posted mailing list .
In this paper, we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list.	confirmed knowledge base	mails	part_whole	{'e1': {'word': 'confirmed knowledge base', 'word_index': [(9, 11)], 'id': 'I05-2006.3'}, 'e2': {'word': 'mails', 'word_index': [(14, 14)], 'id': 'I05-2006.4'}}	paper , report QA system answer type questions based confirmed knowledge base developed using mails posted mailing list .
We first discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents.	knowledge base	natural language documents	part_whole	{'e1': {'word': 'knowledge base', 'word_index': [(4, 5)], 'id': 'I05-2006.6'}, 'e2': {'word': 'natural language documents', 'word_index': [(7, 9)], 'id': 'I05-2006.7'}}	first discuss problem developing knowledge base using natural language documents : wrong information natural language documents .
We first discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents.	wrong information	natural language documents	part_whole	{'e1': {'word': 'wrong information', 'word_index': [(11, 12)], 'id': 'I05-2006.8'}, 'e2': {'word': 'natural language documents', 'word_index': [(13, 15)], 'id': 'I05-2006.9'}}	first discuss problem developing knowledge base using natural language documents : wrong information natural language documents .
Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails.	wrong information	mails	part_whole	{'e1': {'word': 'wrong information', 'word_index': [(4, 5)], 'id': 'I05-2006.10'}, 'e2': {'word': 'mails', 'word_index': [(6, 6)], 'id': 'I05-2006.11'}}	, describe method detecting wrong information mails posted mailing list developing knowledge base using mails .
Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails.	knowledge base	mails	part_whole	{'e1': {'word': 'knowledge base', 'word_index': [(11, 12)], 'id': 'I05-2006.13'}, 'e2': {'word': 'mails', 'word_index': [(14, 14)], 'id': 'I05-2006.14'}}	, describe method detecting wrong information mails posted mailing list developing knowledge base using mails .
Finally, we show that question and answer mails posted to a mailing list can be used as a knowledge base for a QA system.	knowledge base	QA system	usage	{'e1': {'word': 'knowledge base', 'word_index': [(10, 11)], 'id': 'I05-2006.17'}, 'e2': {'word': 'QA system', 'word_index': [(12, 13)], 'id': 'I05-2006.18'}}	Finally , show question answer mails posted mailing list used knowledge base QA system .
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.	reading difficulty	text passage	model-feature	{'e1': {'word': 'reading difficulty', 'word_index': [(6, 7)], 'id': 'N04-1025.1'}, 'e2': {'word': 'text passage', 'word_index': [(8, 9)], 'id': 'N04-1025.2'}}	demonstrate new research approach problem predicting reading difficulty text passage , recasting readability terms statistical language modeling .
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.	statistical language modeling	readability	model-feature	{'e1': {'word': 'statistical language modeling', 'word_index': [(14, 16)], 'id': 'N04-1025.4'}, 'e2': {'word': 'readability', 'word_index': [(12, 12)], 'id': 'N04-1025.3'}}	demonstrate new research approach problem predicting reading difficulty text passage , recasting readability terms statistical language modeling .
We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.	language models	multinomial naive Bayes classification	usage	{'e1': {'word': 'language models', 'word_index': [(10, 11)], 'id': 'N04-1025.6'}, 'e2': {'word': 'multinomial naive Bayes classification', 'word_index': [(4, 7)], 'id': 'N04-1025.5'}}	derive measure based extension multinomial naive Bayes classification combines multiple language models estimate likely grade level given passage .
We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.	semantic variables	readability measures	part_whole	{'e1': {'word': 'semantic variables', 'word_index': [(11, 12)], 'id': 'N04-1025.10'}, 'e2': {'word': 'readability measures', 'word_index': [(14, 15)], 'id': 'N04-1025.11'}}	perform predictions individual Web pages English compare performance widely - used semantic variables traditional readability measures .
Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).	language modeling approach	Web documents	usage	{'e1': {'word': 'language modeling approach', 'word_index': [(15, 17)], 'id': 'N04-1025.17'}, 'e2': {'word': 'Web documents', 'word_index': [(22, 23)], 'id': 'N04-1025.18'}}	traditional semantic variables type- token ratio gave the performance on calibrated test passages , while language modeling approach gave better accuracy for Web documents and passages ( less than words ) .
Syntactic and semantic information are both represented in the grammar in a uniform manner, similar to HPSG ( Pollard and Sag, 1987 ).	grammar	Syntactic and semantic information	model-feature	{'e1': {'word': 'grammar', 'word_index': [(4, 4)], 'id': 'M93-1024.7'}, 'e2': {'word': 'Syntactic and semantic information', 'word_index': [(0, 2)], 'id': 'M93-1024.6'}}	Syntactic semantic information represented grammar uniform manner , similar HPSG ( Pollard Sag , 1987 ) .
LINK has been used in several information extraction applications.	LINK	information extraction applications	usage	{'e1': {'word': 'LINK', 'word_index': [(0, 0)], 'id': 'M93-1024.9'}, 'e2': {'word': 'information extraction applications', 'word_index': [(3, 5)], 'id': 'M93-1024.10'}}	LINK used several information extraction applications .
In a project with General Motors, LINK was used to process terse free-form descriptions of symptoms displayed by malfunctioning automobiles, and the repairs which fixed them.	LINK	free-form descriptions	usage	{'e1': {'word': 'LINK', 'word_index': [(4, 4)], 'id': 'M93-1024.11'}, 'e2': {'word': 'free-form descriptions', 'word_index': [(8, 11)], 'id': 'M93-1024.12'}}	project General Motors , LINK used process terse free - form descriptions symptoms displayed malfunctioning automobiles , repairs fixed .
Reduplication, a central instance of prosodic morphology, is particularly challenging for state-of-the-art computational morphology, since it involves copying of some part of a phonological string	Reduplication	prosodic morphology	part_whole	{'e1': {'word': 'Reduplication', 'word_index': [(0, 0)], 'id': 'A00-2039.1'}, 'e2': {'word': 'prosodic morphology', 'word_index': [(4, 5)], 'id': 'A00-2039.2'}}	Reduplication , central instance prosodic morphology , particularly challenging state - - - art computational morphology , since involves copying part phonological string
In this paper I advocate a finite-state method that combines enriched lexical representations via intersection to implement the copying.	enriched lexical representations	finite-state method	usage	{'e1': {'word': 'enriched lexical representations', 'word_index': [(7, 9)], 'id': 'A00-2039.6'}, 'e2': {'word': 'finite-state method', 'word_index': [(2, 5)], 'id': 'A00-2039.5'}}	paper advocate finite - state method combines enriched lexical representations via intersection implement copying .
The proposal includes a resource-conscious variant of automata and can benefit from the existence of lazy algorithms.	lazy algorithms	resource-conscious variant of automata	usage	{'e1': {'word': 'lazy algorithms', 'word_index': [(9, 10)], 'id': 'A00-2039.8'}, 'e2': {'word': 'resource-conscious variant of automata', 'word_index': [(2, 6)], 'id': 'A00-2039.7'}}	proposal includes resource - conscious variant automata benefit existence lazy algorithms .
"These quick relevancy judgements require two steps: (1) recognizing an expression that is highly relevant to the given domain, e.g. ""were killed"" in the domain of terrorism, and (2) verifying that the context surrounding the expression is consistent with the relevancy guidelines for the domain, e.g. ""5 soldiers were killed by guerrillas"" is not consistent with the terrorism domain since victims of terrorist acts must be civilians."	expression	given domain	model-feature	{'e1': {'word': 'expression', 'word_index': [(11, 11)], 'id': 'H92-1094.3'}, 'e2': {'word': 'given domain', 'word_index': [(14, 15)], 'id': 'H92-1094.4'}}	"quick relevancy judgements require two steps : ( 1 ) recognizing expression highly relevant given domain , e.g. "" killed "" domain terrorism , ( 2 ) verifying context surrounding expression consistent relevancy guidelines domain , e.g. "" 5 soldiers killed guerrillas "" consistent terrorism domain since victims terrorist acts must civilians ."
The Relevancy Signatures Algorithm attempts to simulate the first step in this process by deriving reliable relevancy cues from a corpus of training texts and using these cues to quickly identify new texts that are highly likely to be relevant.	reliable relevancy cues	corpus of training texts	part_whole	{'e1': {'word': 'reliable relevancy cues', 'word_index': [(9, 11)], 'id': 'H92-1094.12'}, 'e2': {'word': 'corpus of training texts', 'word_index': [(12, 14)], 'id': 'H92-1094.13'}}	Relevancy Signatures Algorithm attempts simulate first step process deriving reliable relevancy cues corpus training texts using cues quickly identify new texts highly likely relevant .
Our system accepts an entire English news story (text) as query, and retrieves relevant Chinese broadcast news stories (audio) from the document collection.	relevant Chinese broadcast news stories (audio)	document collection	part_whole	{'e1': {'word': 'relevant Chinese broadcast news stories (audio)', 'word_index': [(12, 19)], 'id': 'H01-1050.6'}, 'e2': {'word': 'document collection', 'word_index': [(20, 21)], 'id': 'H01-1050.7'}}	system accepts entire English news story ( text ) query , retrieves relevant Chinese broadcast news stories ( audio ) document collection .
The English queries are translated into Chinese by means of a dictionary-based approach, where we have integrated phrase-based translation with word-by-word translation.	phrase-based translation	dictionary-based approach	usage	{'e1': {'word': 'phrase-based translation', 'word_index': [(11, 14)], 'id': 'H01-1050.16'}, 'e2': {'word': 'dictionary-based approach', 'word_index': [(5, 8)], 'id': 'H01-1050.15'}}	English queries translated Chinese means dictionary - based approach , integrated phrase - based translation word - - word translation .
Untranslatable named entities are transliterated by a novel subword translation technique.	novel subword translation technique	Untranslatable named entities	usage	{'e1': {'word': 'novel subword translation technique', 'word_index': [(4, 7)], 'id': 'H01-1050.19'}, 'e2': {'word': 'Untranslatable named entities', 'word_index': [(0, 2)], 'id': 'H01-1050.18'}}	Untranslatable named entities transliterated novel subword translation technique .
Experimental results demonstrate that the use of phrase-based translation and subword translation gave performance gains, and multi-scale retrieval outperforms word-based retrieval.	multi-scale retrieval	word-based retrieval	compare	{'e1': {'word': 'multi-scale retrieval', 'word_index': [(14, 15)], 'id': 'H01-1050.26'}, 'e2': {'word': 'word-based retrieval', 'word_index': [(17, 20)], 'id': 'H01-1050.27'}}	Experimental results demonstrate use phrase - based translation subword translation gave performance gains , multi-scale retrieval outperforms word - based retrieval .
But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English.	features	English past tense	model-feature	{'e1': {'word': 'features', 'word_index': [(4, 4)], 'id': 'W98-1240.10'}, 'e2': {'word': 'English past tense', 'word_index': [(0, 2)], 'id': 'W98-1240.9'}}	English past tense interesting features combination regular rules semi-productive strong verb patterns , many respects trivial morphological system - reflecting generally vestigal nature inflectional morphology within modern English .
But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English.	vestigal nature	inflectional morphology	model-feature	{'e1': {'word': 'vestigal nature', 'word_index': [(21, 22)], 'id': 'W98-1240.14'}, 'e2': {'word': 'inflectional morphology', 'word_index': [(23, 24)], 'id': 'W98-1240.15'}}	English past tense interesting features combination regular rules semi-productive strong verb patterns , many respects trivial morphological system - reflecting generally vestigal nature inflectional morphology within modern English .
We define, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages.	shallow syntactic analysis	statistical machine translation	usage	{'e1': {'word': 'shallow syntactic analysis', 'word_index': [(11, 13)], 'id': 'W03-1002.2'}, 'e2': {'word': 'statistical machine translation', 'word_index': [(6, 8)], 'id': 'W03-1002.1'}}	define , implement evaluate novel model statistical machine translation , based shallow syntactic analysis ( part -of speech tagging and chunking ) in and languages .
This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.	machine translation systems	language pairs	usage	{'e1': {'word': 'machine translation systems', 'word_index': [(4, 6)], 'id': 'W08-0309.2'}, 'e2': {'word': 'language pairs', 'word_index': [(8, 9)], 'id': 'W08-0309.3'}}	paper analyzes translation quality machine translation systems 10 language pairs translating Czech , English , French , German , Hungarian , Spanish .
We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information.	intra- and inter-annotator agreement	manual evaluation methodology	usage	{'e1': {'word': 'intra- and inter-annotator agreement', 'word_index': [(5, 8)], 'id': 'W08-0309.18'}, 'e2': {'word': 'manual evaluation methodology', 'word_index': [(1, 3)], 'id': 'W08-0309.17'}}	validate manual evaluation methodology measuring intra - inter-annotator agreement , collecting timing information .
In this paper, we describe issues in the translation of proper names from English to Chinese which we have faced in constructing a system for multilingual text generation supporting both languages.	translation	proper names	usage	{'e1': {'word': 'translation', 'word_index': [(4, 4)], 'id': 'P98-2220.1'}, 'e2': {'word': 'proper names', 'word_index': [(5, 6)], 'id': 'P98-2220.2'}}	paper , describe issues translation proper names English Chinese faced constructing system multilingual text generation supporting languages .
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.	backward maximum matching	CWS	usage	{'e1': {'word': 'backward maximum matching', 'word_index': [(2, 4)], 'id': 'W06-0119.6'}, 'e2': {'word': 'CWS', 'word_index': [(0, 0)], 'id': 'W06-0119.5'}}	CWS based backward maximum matching word support model ( WSM ) contextual - based Chinese unknown word identification .
The Arabic language has far richer systems of inflection and derivation than English which has very little morphology.	systems of inflection and derivation	Arabic language	model-feature	{'e1': {'word': 'systems of inflection and derivation', 'word_index': [(4, 6)], 'id': 'W06-3103.2'}, 'e2': {'word': 'Arabic language', 'word_index': [(0, 1)], 'id': 'W06-3103.1'}}	Arabic language far richer systems inflection derivation English little morphology .
Segmentation of inflected Arabic words is a way to smooth its highly morphological nature.	Segmentation	inflected Arabic words	usage	{'e1': {'word': 'Segmentation', 'word_index': [(0, 0)], 'id': 'W06-3103.8'}, 'e2': {'word': 'inflected Arabic words', 'word_index': [(1, 3)], 'id': 'W06-3103.9'}}	Segmentation inflected Arabic words way smooth highly morphological nature .
In this paper, we describe some statistically and linguistically motivated methods for Arabic word segmentation.	statistically and linguistically motivated methods	Arabic word segmentation	usage	{'e1': {'word': 'statistically and linguistically motivated methods', 'word_index': [(3, 6)], 'id': 'W06-3103.11'}, 'e2': {'word': 'Arabic word segmentation', 'word_index': [(7, 9)], 'id': 'W06-3103.12'}}	paper , describe statistically linguistically motivated methods Arabic word segmentation .
Then, we show the efficiency of proposed methods on the Arabic-English BTEC and NIST tasks.	proposed methods	Arabic-English BTEC and NIST tasks	usage	{'e1': {'word': 'proposed methods', 'word_index': [(3, 4)], 'id': 'W06-3103.13'}, 'e2': {'word': 'Arabic-English BTEC and NIST tasks', 'word_index': [(5, 8)], 'id': 'W06-3103.14'}}	, show efficiency proposed methods Arabic-English BTEC and NIST .
Developing a high-quality lexicon is often the first step towards building a POS tagger, which is in turn the front-end to many NLP systems.	high-quality lexicon	POS tagger	usage	{'e1': {'word': 'high-quality lexicon', 'word_index': [(1, 4)], 'id': 'W06-1647.4'}, 'e2': {'word': 'POS tagger', 'word_index': [(10, 11)], 'id': 'W06-1647.5'}}	Developing high - quality lexicon often first step towards building POS tagger , turn front - end many NLP systems .
We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Transductive SVMs, Spectral Graph Transducers, and a novel Transductive Clustering method.	Transductive SVMs	Spectral Graph Transducers	compare	{'e1': {'word': 'Transductive SVMs', 'word_index': [(14, 15)], 'id': 'W06-1647.10'}, 'e2': {'word': 'Spectral Graph Transducers', 'word_index': [(17, 19)], 'id': 'W06-1647.11'}}	frame lexicon acquisition problem transductive learning problem , perform comparisons three transductive algorithms : Transductive SVMs , Spectral Graph Transducers , novel Transductive Clustering method .
We present an API for computing the semantic relatedness of words in Wikipedia.	API	semantic relatedness	usage	{'e1': {'word': 'API', 'word_index': [(1, 1)], 'id': 'P07-2013.1'}, 'e2': {'word': 'semantic relatedness', 'word_index': [(3, 4)], 'id': 'P07-2013.2'}}	present API computing semantic relatedness words Wikipedia .
This paper presents a general platform, namely synchronous tree sequence substitution grammar (STSSG), for the grammar comparison study in Translational Equivalence Modeling (TEM) and Statistical Machine Translation (SMT).	synchronous tree sequence substitution grammar (STSSG)	grammar comparison study	usage	{'e1': {'word': 'synchronous tree sequence substitution grammar (STSSG)', 'word_index': [(6, 13)], 'id': 'C08-1138.1'}, 'e2': {'word': 'grammar comparison study', 'word_index': [(15, 17)], 'id': 'C08-1138.2'}}	paper presents general platform , namely synchronous tree sequence substitution grammar ( STSSG ) , grammar comparison study Translational Equivalence Modeling ( TEM ) Statistical Machine Translation ( SMT ) .
Experimental results show that the STSSG is able to better explain the data in parallel corpora than other grammars.	STSSG	other grammars	compare	{'e1': {'word': 'STSSG', 'word_index': [(3, 3)], 'id': 'C08-1138.11'}, 'e2': {'word': 'other grammars', 'word_index': [(10, 10)], 'id': 'C08-1138.13'}}	Experimental results show STSSG able better explain data parallel corpora grammars .
Our study further finds that the complexity of structure divergence is much higher than suggested in literature, which imposes a big challenge to syntactic transformation-based SMT.	structure divergence	syntactic transformation-based SMT	result	{'e1': {'word': 'structure divergence', 'word_index': [(3, 4)], 'id': 'C08-1138.14'}, 'e2': {'word': 'syntactic transformation-based SMT', 'word_index': [(13, 17)], 'id': 'C08-1138.15'}}	study finds complexity structure divergence much higher suggested literature , imposes big challenge syntactic transformation - based SMT .
This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms.	unsupervised method	automatic sentence extraction	usage	{'e1': {'word': 'unsupervised method', 'word_index': [(3, 4)], 'id': 'P04-3020.1'}, 'e2': {'word': 'automatic sentence extraction', 'word_index': [(5, 7)], 'id': 'P04-3020.2'}}	paper presents innovative unsupervised method automatic sentence extraction using graph - based ranking algorithms .
We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.	method	text summarization task	usage	{'e1': {'word': 'method', 'word_index': [(1, 1)], 'id': 'P04-3020.4'}, 'e2': {'word': 'text summarization task', 'word_index': [(3, 5)], 'id': 'P04-3020.5'}}	evaluate method context text summarization task , show results obtained compare favorably previously published results established benchmarks .
Preferred antecedents are a subset of the possible antecedents, selected by the application of extralinguistic knowledge.	Preferred antecedents	possible antecedents	part_whole	{'e1': {'word': 'Preferred antecedents', 'word_index': [(0, 1)], 'id': 'C90-2017.7'}, 'e2': {'word': 'possible antecedents', 'word_index': [(3, 4)], 'id': 'C90-2017.8'}}	Preferred antecedents subset possible antecedents , selected application extralinguistic knowledge .
This paper presents a syntactic description of a fragment of German that has been worked out within the machine translation project Eurotra.	syntactic description	German	model-feature	{'e1': {'word': 'syntactic description', 'word_index': [(2, 3)], 'id': 'C88-2123.1'}, 'e2': {'word': 'German', 'word_index': [(5, 5)], 'id': 'C88-2123.2'}}	paper presents syntactic description fragment German worked within machine translation project Eurotra .
It represents the syntactic part of the German module of this multilingual translation system.	syntactic part	German module	part_whole	{'e1': {'word': 'syntactic part', 'word_index': [(1, 2)], 'id': 'C88-2123.4'}, 'e2': {'word': 'German module', 'word_index': [(3, 4)], 'id': 'C88-2123.5'}}	represents syntactic part German module multilingual translation system .
We present an approach for querying collections of heterogeneous linguistic corpora that are annotated on multiple layers using arbitrary XML-based markup languages.	multiple layers	heterogeneous linguistic corpora	model-feature	{'e1': {'word': 'multiple layers', 'word_index': [(8, 9)], 'id': 'L08-1190.2'}, 'e2': {'word': 'heterogeneous linguistic corpora', 'word_index': [(4, 6)], 'id': 'L08-1190.1'}}	present approach querying collections heterogeneous linguistic corpora annotated multiple layers using arbitrary XML - based markup languages .
An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion.	OWL ontology	markup languages	usage	{'e1': {'word': 'OWL ontology', 'word_index': [(0, 1)], 'id': 'L08-1190.4'}, 'e2': {'word': 'markup languages', 'word_index': [(7, 8)], 'id': 'L08-1190.5'}}	OWL ontology provides homogenising view conceptually different markup languages common querying framework established using method ontology - based query expansion .
An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion.	ontology-based query expansion	querying framework	usage	{'e1': {'word': 'ontology-based query expansion', 'word_index': [(15, 19)], 'id': 'L08-1190.7'}, 'e2': {'word': 'querying framework', 'word_index': [(10, 11)], 'id': 'L08-1190.6'}}	OWL ontology provides homogenising view conceptually different markup languages common querying framework established using method ontology - based query expansion .
This interface can also be used for ontology-based querying of multiple corpora simultaneously.	ontology-based querying	corpora	usage	{'e1': {'word': 'ontology-based querying', 'word_index': [(3, 6)], 'id': 'L08-1190.12'}, 'e2': {'word': 'corpora', 'word_index': [(8, 8)], 'id': 'L08-1190.13'}}	interface also used ontology - based querying multiple corpora simultaneously .
We also evaluate our model on the related role-labelling task, and compare it with a standard role labeller.	model	standard role labeller	compare	{'e1': {'word': 'model', 'word_index': [(2, 2)], 'id': 'E06-1044.5'}, 'e2': {'word': 'standard role labeller', 'word_index': [(10, 12)], 'id': 'E06-1044.7'}}	also evaluate model related role - labelling task , compare standard role labeller .
For both tasks, our model benefits from class-based smoothing, which allows it to make correct argument-specific predictions despite a severe sparse data problem.	class-based smoothing	model	result	{'e1': {'word': 'class-based smoothing', 'word_index': [(4, 7)], 'id': 'E06-1044.10'}, 'e2': {'word': 'model', 'word_index': [(2, 2)], 'id': 'E06-1044.9'}}	tasks , model benefits class - based smoothing , allows make correct argument -specific predictions despite a sparse data problem .
The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.	sparse data	standard labeller	result	{'e1': {'word': 'sparse data', 'word_index': [(3, 4)], 'id': 'E06-1044.14'}, 'e2': {'word': 'standard labeller', 'word_index': [(0, 1)], 'id': 'E06-1044.13'}}	standard labeller suffers sparse data strong reliance syntactic cues , especially prediction task .
The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.	syntactic cues	prediction task	usage	{'e1': {'word': 'syntactic cues', 'word_index': [(7, 8)], 'id': 'E06-1044.15'}, 'e2': {'word': 'prediction task', 'word_index': [(11, 12)], 'id': 'E06-1044.16'}}	standard labeller suffers sparse data strong reliance syntactic cues , especially prediction task .
In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step.	'Pendulum Algorithm'	word sets	usage	"{'e1': {'word': ""'Pendulum Algorithm'"", 'word_index': [(5, 8)], 'id': 'C04-1178.6'}, 'e2': {'word': 'word sets', 'word_index': [(10, 11)], 'id': 'C04-1178.7'}}"	bootstrapping fashion , - called ' Pendulum Algorithm ' operates word sets obtained co-occurrence statistics large un- annotated corpus and error propagation low by step .
In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step.	co-occurrence statistics	large un-annotated corpus	part_whole	{'e1': {'word': 'co-occurrence statistics', 'word_index': [(13, 14)], 'id': 'C04-1178.8'}, 'e2': {'word': 'large un-annotated corpus', 'word_index': [(15, 18)], 'id': 'C04-1178.9'}}	bootstrapping fashion , - called ' Pendulum Algorithm ' operates word sets obtained co-occurrence statistics large un- annotated corpus and error propagation low by step .
The first algorithm, phone-dependent cepstral compensation, is similar in concept to the previously-described MFCDCN method, except that cepstral compensation vectors are selected according to the current phonetic hypothesis, rather than on the basis of SNR or VQ codeword identity.	phone-dependent cepstral compensation	MFCDCN method	compare	{'e1': {'word': 'phone-dependent cepstral compensation', 'word_index': [(3, 7)], 'id': 'H94-1066.4'}, 'e2': {'word': 'MFCDCN method', 'word_index': [(14, 15)], 'id': 'H94-1066.5'}}	first algorithm , phone - dependent cepstral compensation , similar concept previously - described MFCDCN method , except cepstral compensation vectors selected according current phonetic hypothesis , rather basis SNR VQ codeword identity .
Use of the various compensation algorithms in consort produces a reduction of error rates for SPHINX-II by as much as 40 percent relative to the rate achieved with cepstral mean normalization alone, in both development test sets and in the context of the 1993 ARPA CSR evaluations.	compensation algorithms	reduction of error rates	result	{'e1': {'word': 'compensation algorithms', 'word_index': [(2, 3)], 'id': 'H94-1066.12'}, 'e2': {'word': 'reduction of error rates', 'word_index': [(6, 8)], 'id': 'H94-1066.13'}}	Use various compensation algorithms consort produces reduction error rates SPHINX - II much 40 percent relative rate achieved cepstral mean normalization alone , development test sets context 1993 ARPA CSR evaluations .
One of the critical components of the CLT is a speech recognition system which is used to track the child's progress during oral reading and to provide sufficient information to detect reading miscues.	reading miscues	oral reading	part_whole	{'e1': {'word': 'reading miscues', 'word_index': [(18, 19)], 'id': 'C04-1182.7'}, 'e2': {'word': 'oral reading', 'word_index': [(12, 13)], 'id': 'C04-1182.6'}}	One critical components CLT speech recognition system used track child 's progress oral reading provide sufficient information detect reading miscues .
In this paper, we extend on prior work by examining a novel labeling of children's oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors.	labeling	oral reading audio data	model-feature	{'e1': {'word': 'labeling', 'word_index': [(7, 7)], 'id': 'C04-1182.8'}, 'e2': {'word': 'oral reading audio data', 'word_index': [(10, 13)], 'id': 'C04-1182.9'}}	paper , extend prior work examining novel labeling children 's oral reading audio data order better understand factors contribute significantly speech recognition errors .
Next, we consider the problem of detecting miscues during oral reading.	miscues	oral reading	part_whole	{'e1': {'word': 'miscues', 'word_index': [(5, 5)], 'id': 'C04-1182.14'}, 'e2': {'word': 'oral reading', 'word_index': [(6, 7)], 'id': 'C04-1182.15'}}	Next , consider problem detecting miscues oral reading .
