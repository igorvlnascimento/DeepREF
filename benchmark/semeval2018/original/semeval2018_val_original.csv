original_sentence	e1	e2	relation_type	metadata	tokenized_sentence
In Semantic Role Labeling (SRL), arguments are usually limited in a syntax subtree.	arguments	syntax subtree	part_whole	{'e1': {'word': 'arguments', 'word_index': [(8, 8)], 'id': 'C08-1105.2'}, 'e2': {'word': 'syntax subtree', 'word_index': [(14, 15)], 'id': 'C08-1105.3'}, 'entity_replacement': {'1:6': 'ENTITYUNRELATED', '8:8': 'ENTITY', '14:15': 'ENTITYOTHER'}}	In Semantic Role Labeling ( SRL ) , arguments are usually limited in a syntax subtree .
It is reasonable to label arguments locally in such a sub-tree rather than a whole tree.	arguments	sub-tree	part_whole	{'e1': {'word': 'arguments', 'word_index': [(5, 5)], 'id': 'C08-1105.4'}, 'e2': {'word': 'sub-tree', 'word_index': [(10, 12)], 'id': 'C08-1105.5'}, 'entity_replacement': {'5:5': 'ENTITY', '10:12': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED'}}	It is reasonable to label arguments locally in such a sub - tree rather than a whole tree .
The anchor group approach achieves an accuracy of 87.75% and the single anchor approach achieves 83.63%.	anchor group approach	accuracy	result	{'e1': {'word': 'anchor group approach', 'word_index': [(1, 3)], 'id': 'C08-1105.15'}, 'e2': {'word': 'accuracy', 'word_index': [(6, 6)], 'id': 'C08-1105.16'}, 'entity_replacement': {'1:3': 'ENTITY', '6:6': 'ENTITYOTHER', '12:14': 'ENTITYUNRELATED'}}	The anchor group approach achieves an accuracy of 87.75 % and the single anchor approach achieves 83.63 %.
Experimental results also indicate that the prediction of MP improves semantic role labeling.	prediction of MP	semantic role labeling	result	{'e1': {'word': 'prediction of MP', 'word_index': [(6, 8)], 'id': 'C08-1105.18'}, 'e2': {'word': 'semantic role labeling', 'word_index': [(10, 12)], 'id': 'C08-1105.19'}, 'entity_replacement': {'6:8': 'ENTITY', '10:12': 'ENTITYOTHER'}}	Experimental results also indicate that the prediction of MP improves semantic role labeling .
Recently the LATL has undertaken the development of a multilingual translation system based on a symbolic parsing technology and on a transfer-based translation model.	symbolic parsing technology	multilingual translation system	usage	{'e1': {'word': 'symbolic parsing technology', 'word_index': [(15, 17)], 'id': 'L08-1579.2'}, 'e2': {'word': 'multilingual translation system', 'word_index': [(9, 11)], 'id': 'L08-1579.1'}, 'entity_replacement': {'9:11': 'ENTITYOTHER', '15:17': 'ENTITY', '21:25': 'ENTITYUNRELATED'}}	Recently the LATL has undertaken the development of a multilingual translation system based on a symbolic parsing technology and on a transfer - based translation model .
This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning.	term weight learning	verbal word senses	usage	{'e1': {'word': 'term weight learning', 'word_index': [(12, 14)], 'id': 'E99-1028.2'}, 'e2': {'word': 'verbal word senses', 'word_index': [(8, 10)], 'id': 'E99-1028.1'}, 'entity_replacement': {'8:10': 'ENTITYOTHER', '12:14': 'ENTITY'}}	This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning .
This is quite different from current approaches to semantic parsing or chunking that depend on full statistical syntactic parsers that require tree bank style annotation.	statistical syntactic parsers	chunking	usage	{'e1': {'word': 'statistical syntactic parsers', 'word_index': [(16, 18)], 'id': 'N04-4037.10'}, 'e2': {'word': 'chunking', 'word_index': [(11, 11)], 'id': 'N04-4037.9'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '11:11': 'ENTITYOTHER', '16:18': 'ENTITY', '21:22': 'ENTITYUNRELATED'}}	This is quite different from current approaches to semantic parsing or chunking that depend on full statistical syntactic parsers that require tree bank style annotation .
We compare it with a recently proposed word-by-word semantic chunker and present results that show that the phrase-by-phrase approach performs better than its word-by-word counterpart.	phrase-by-phrase approach	word-by-word counterpart	compare	{'e1': {'word': 'phrase-by-phrase approach', 'word_index': [(20, 24)], 'id': 'N04-4037.13'}, 'e2': {'word': 'word-by-word counterpart', 'word_index': [(29, 34)], 'id': 'N04-4037.14'}, 'entity_replacement': {'7:12': 'ENTITYUNRELATED', '20:24': 'ENTITY', '29:34': 'ENTITYOTHER'}}	We compare it with a recently proposed word - by- word semantic chunker and present results that show that the phrase - by- phrase approach performs better than its word - by - word counterpart .
The primary objective of this basic research is to develop improved methods and models for acoustic recognition of continuous speech.	acoustic recognition	continuous speech	usage	{'e1': {'word': 'acoustic recognition', 'word_index': [(15, 16)], 'id': 'H91-1080.1'}, 'e2': {'word': 'continuous speech', 'word_index': [(18, 19)], 'id': 'H91-1080.2'}, 'entity_replacement': {'15:16': 'ENTITY', '18:19': 'ENTITYOTHER'}}	The primary objective of this basic research is to develop improved methods and models for acoustic recognition of continuous speech .
The work has focussed on developing accurate and detailed mathematical models of phonemes and their coarticulation for the purpose of large-vocabulary continuous speech recognition.	phonemes	large-vocabulary continuous speech recognition	usage	{'e1': {'word': 'phonemes', 'word_index': [(12, 12)], 'id': 'H91-1080.3'}, 'e2': {'word': 'large-vocabulary continuous speech recognition', 'word_index': [(20, 25)], 'id': 'H91-1080.4'}, 'entity_replacement': {'12:12': 'ENTITY', '20:25': 'ENTITYOTHER'}}	The work has focussed on developing accurate and detailed mathematical models of phonemes and their coarticulation for the purpose of large - vocabulary continuous speech recognition .
KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus.	word list	corpus	part_whole	{'e1': {'word': 'word list', 'word_index': [(3, 4)], 'id': 'W00-0902.5'}, 'e2': {'word': 'corpus', 'word_index': [(14, 14)], 'id': 'W00-0902.6'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '3:4': 'ENTITY', '14:14': 'ENTITYOTHER', '18:18': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED'}}	KeyWords compares a word list extracted from what has been called ' the study corpus ' ( the corpus which the researcher is interested in describing ) with a word list made from a reference corpus .
KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus.	word list	reference corpus	part_whole	{'e1': {'word': 'word list', 'word_index': [(29, 30)], 'id': 'W00-0902.8'}, 'e2': {'word': 'reference corpus', 'word_index': [(34, 35)], 'id': 'W00-0902.9'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '29:30': 'ENTITY', '34:35': 'ENTITYOTHER'}}	KeyWords compares a word list extracted from what has been called ' the study corpus ' ( the corpus which the researcher is interested in describing ) with a word list made from a reference corpus .
Five English corpora were compared to reference corpora of various sizes (varying from two to 100 times larger than the study corpus).	English corpora	reference corpora	compare	{'e1': {'word': 'English corpora', 'word_index': [(1, 2)], 'id': 'W00-0902.15'}, 'e2': {'word': 'reference corpora', 'word_index': [(6, 7)], 'id': 'W00-0902.16'}, 'entity_replacement': {'1:2': 'ENTITY', '6:7': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED'}}	Five English corpora were compared to reference corpora of various sizes ( varying from two to 100 times larger than the study corpus ) .
The results indicate that a reference corpus that is five times as large as the study corpus yielded a larger number of keywords than a smaller reference corpus.	keywords	corpus	part_whole	{'e1': {'word': 'keywords', 'word_index': [(22, 22)], 'id': 'W00-0902.20'}, 'e2': {'word': 'corpus', 'word_index': [(16, 16)], 'id': 'W00-0902.19'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '16:16': 'ENTITYOTHER', '22:22': 'ENTITY', '26:27': 'ENTITYUNRELATED'}}	The results indicate that a reference corpus that is five times as large as the study corpus yielded a larger number of keywords than a smaller reference corpus .
The implication is that a larger reference corpus is not always better than a smaller one, for WordSmith Tools Keywords analysis, while a reference corpus that is less than five times the size of the study corpus may not be reliable.	reference corpus	study corpus	compare	{'e1': {'word': 'reference corpus', 'word_index': [(25, 26)], 'id': 'W00-0902.28'}, 'e2': {'word': 'study corpus', 'word_index': [(37, 38)], 'id': 'W00-0902.29'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '25:26': 'ENTITY', '37:38': 'ENTITYOTHER'}}	The implication is that a larger reference corpus is not always better than a smaller one , for WordSmith Tools Keywords analysis , while a reference corpus that is less than five times the size of the study corpus may not be reliable .
In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a non-lexicalised and a lexicalised mode.	dependency analyser	Italian	usage	{'e1': {'word': 'dependency analyser', 'word_index': [(13, 14)], 'id': 'W02-1501.1'}, 'e2': {'word': 'Italian', 'word_index': [(16, 16)], 'id': 'W02-1501.2'}, 'entity_replacement': {'13:14': 'ENTITY', '16:16': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a non-lexicalised and a lexicalised mode .
Results shed light on the contribution of types of lexical information to parsing.	lexical information	parsing	usage	{'e1': {'word': 'lexical information', 'word_index': [(9, 10)], 'id': 'W02-1501.5'}, 'e2': {'word': 'parsing', 'word_index': [(12, 12)], 'id': 'W02-1501.6'}, 'entity_replacement': {'9:10': 'ENTITY', '12:12': 'ENTITYOTHER'}}	Results shed light on the contribution of types of lexical information to parsing .
In international phonology and speech synthesis research, it has been suggested that the relative informativeness of a word can be used to predict pitch prominence.	relative informativeness	word	model-feature	{'e1': {'word': 'relative informativeness', 'word_index': [(14, 15)], 'id': 'W99-0619.3'}, 'e2': {'word': 'word', 'word_index': [(18, 18)], 'id': 'W99-0619.4'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:5': 'ENTITYUNRELATED', '14:15': 'ENTITY', '18:18': 'ENTITYOTHER', '24:25': 'ENTITYUNRELATED'}}	In international phonology and speech synthesis research , it has been suggested that the relative informativeness of a word can be used to predict pitch prominence .
In this paper, we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted measures of informativeness.	measures	informativeness	model-feature	{'e1': {'word': 'measures', 'word_index': [(22, 22)], 'id': 'W99-0619.7'}, 'e2': {'word': 'informativeness', 'word_index': [(24, 24)], 'id': 'W99-0619.8'}, 'entity_replacement': {'22:22': 'ENTITY', '24:24': 'ENTITYOTHER'}}	In this paper , we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted measures of informativeness .
Our experiments show that there is a positive correlation between the informativeness of a word and its pitch accent assignment.	informativeness	word	model-feature	{'e1': {'word': 'informativeness', 'word_index': [(11, 11)], 'id': 'W99-0619.9'}, 'e2': {'word': 'word', 'word_index': [(14, 14)], 'id': 'W99-0619.10'}, 'entity_replacement': {'11:11': 'ENTITY', '14:14': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED'}}	Our experiments show that there is a positive correlation between the informativeness of a word and its pitch accent assignment .
The computation of word informativeness is inexpensive and can be incorporated into speech synthesis systems easily.	word informativeness	speech synthesis systems	usage	{'e1': {'word': 'word informativeness', 'word_index': [(3, 4)], 'id': 'W99-0619.14'}, 'e2': {'word': 'speech synthesis systems', 'word_index': [(12, 14)], 'id': 'W99-0619.15'}, 'entity_replacement': {'3:4': 'ENTITY', '12:14': 'ENTITYOTHER'}}	The computation of word informativeness is inexpensive and can be incorporated into speech synthesis systems easily .
"""An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed."	parsing algorithm	augmented context-free grammars	usage	{'e1': {'word': 'parsing algorithm', 'word_index': [(3, 4)], 'id': 'J87-1004.1'}, 'e2': {'word': 'augmented context-free grammars', 'word_index': [(6, 10)], 'id': 'J87-1004.2'}, 'entity_replacement': {'3:4': 'ENTITY', '6:10': 'ENTITYOTHER', '18:22': 'ENTITYUNRELATED'}}	""" An efficient parsing algorithm for augmented context - free grammars is introduced , and its application to on- line natural language interfaces discussed ."
The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar.	generalized LR parsing algorithm	LR shift-reduce parsing table	usage	{'e1': {'word': 'generalized LR parsing algorithm', 'word_index': [(4, 7)], 'id': 'J87-1004.4'}, 'e2': {'word': 'LR shift-reduce parsing table', 'word_index': [(12, 17)], 'id': 'J87-1004.5'}, 'entity_replacement': {'4:7': 'ENTITY', '12:17': 'ENTITYOTHER', '27:31': 'ENTITYUNRELATED'}}	The algorithm is a generalized LR parsing algorithm , which precomputes an LR shift - reduce parsing table ( possibly with multiple entries ) from a given augmented context - free grammar .
We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables.	LR parsing tables	chart parsing algorithm	usage	{'e1': {'word': 'LR parsing tables', 'word_index': [(16, 18)], 'id': 'J87-1004.14'}, 'e2': {'word': 'chart parsing algorithm', 'word_index': [(10, 12)], 'id': 'J87-1004.13'}, 'entity_replacement': {'10:12': 'ENTITYOTHER', '16:18': 'ENTITY'}}	We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables .
"Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, based on the technique developed at CMU."""	on-line parser	Japanese language	usage	{'e1': {'word': 'on-line parser', 'word_index': [(4, 7)], 'id': 'J87-1004.21'}, 'e2': {'word': 'Japanese language', 'word_index': [(9, 10)], 'id': 'J87-1004.22'}, 'entity_replacement': {'4:7': 'ENTITY', '9:10': 'ENTITYOTHER'}}	"Also , a commercial on - line parser for Japanese language is being built by Intelligent Technology Incorporation , based on the technique developed at CMU . """
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).	hybrid statistical and structural semantic model	multi-stage spoken language understanding (SLU)	usage	{'e1': {'word': 'hybrid statistical and structural semantic model', 'word_index': [(4, 9)], 'id': 'W04-3002.1'}, 'e2': {'word': 'multi-stage spoken language understanding (SLU)', 'word_index': [(11, 17)], 'id': 'W04-3002.2'}, 'entity_replacement': {'4:9': 'ENTITY', '11:17': 'ENTITYOTHER'}}	This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding ( SLU ) .
The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted.	weighted finite-state transducer (WFST)-based parser	SLU	usage	{'e1': {'word': 'weighted finite-state transducer (WFST)-based parser', 'word_index': [(8, 18)], 'id': 'W04-3002.4'}, 'e2': {'word': 'SLU', 'word_index': [(5, 5)], 'id': 'W04-3002.3'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '8:18': 'ENTITY', '23:24': 'ENTITYUNRELATED'}}	The first stage of this SLU utilizes a weighted finite - state transducer ( WFST ) - based parser , which encodes the regular grammar of concepts to be extracted .
The proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger.	n-gram semantic tagger	regular grammar	usage	{'e1': {'word': 'n-gram semantic tagger', 'word_index': [(14, 16)], 'id': 'W04-3002.7'}, 'e2': {'word': 'regular grammar', 'word_index': [(5, 6)], 'id': 'W04-3002.6'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '14:16': 'ENTITY'}}	The proposed method improves the regular grammar model by incorporating a well - known n-gram semantic tagger .
This paper presents a corpus-based account of structural priming in human sentence processing, focusing on the role that syntactic representations play in such an account.	structural priming	sentence processing	model-feature	{'e1': {'word': 'structural priming', 'word_index': [(9, 10)], 'id': 'W06-1637.2'}, 'e2': {'word': 'sentence processing', 'word_index': [(13, 14)], 'id': 'W06-1637.3'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '9:10': 'ENTITY', '13:14': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED'}}	This paper presents a corpus - based account of structural priming in human sentence processing , focusing on the role that syntactic representations play in such an account .
We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue, annotated syntactically with Combinatory Categorial Grammar (CCG) derivations.	spontaneous spoken dialogue	corpus	part_whole	{'e1': {'word': 'spontaneous spoken dialogue', 'word_index': [(12, 14)], 'id': 'W06-1637.7'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'W06-1637.6'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '10:10': 'ENTITYOTHER', '12:14': 'ENTITY', '19:24': 'ENTITYUNRELATED'}}	We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue , annotated syntactically with Combinatory Categorial Grammar ( CCG ) derivations .
In particular, we present evidence for priming between lexical and syntactic categories encoding partially satisfied sub-categorization frames, and we show that priming effects exist both for incremental and normal-form CCG derivations.	priming effects	incremental and normal-form CCG derivations	model-feature	{'e1': {'word': 'priming effects', 'word_index': [(23, 24)], 'id': 'W06-1637.13'}, 'e2': {'word': 'incremental and normal-form CCG derivations', 'word_index': [(28, 34)], 'id': 'W06-1637.14'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '9:12': 'ENTITYUNRELATED', '23:24': 'ENTITY', '28:34': 'ENTITYOTHER'}}	In particular , we present evidence for priming between lexical and syntactic categories encoding partially satisfied sub-categorization frames , and we show that priming effects exist both for incremental and normal - form CCG derivations .
This paper describes the application of an ensemble of indexing and classification systems, which have been shown to be successful in information retrieval and classification of medical literature, to a new task of assigning ICD-9-CM codes to the clinical history and impression sections of radiology reports.	information retrieval	medical literature	usage	{'e1': {'word': 'information retrieval', 'word_index': [(22, 23)], 'id': 'W07-1014.1'}, 'e2': {'word': 'medical literature', 'word_index': [(27, 28)], 'id': 'W07-1014.2'}, 'entity_replacement': {'22:23': 'ENTITY', '27:28': 'ENTITYOTHER', '36:39': 'ENTITYUNRELATED', '42:43': 'ENTITYUNRELATED', '49:49': 'ENTITYUNRELATED'}}	This paper describes the application of an ensemble of indexing and classification systems , which have been shown to be successful in information retrieval and classification of medical literature , to a new task of assigning ICD-9 - CM codes to the clinical history and impression sections of radiology reports .
This type of summary could serve as an effective navigation tool for accessing information in long texts, such as books.	summary	texts	model-feature	{'e1': {'word': 'summary', 'word_index': [(3, 3)], 'id': 'P07-1069.2'}, 'e2': {'word': 'texts', 'word_index': [(16, 16)], 'id': 'P07-1069.4'}, 'entity_replacement': {'3:3': 'ENTITY', '13:13': 'ENTITYUNRELATED', '16:16': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED'}}	This type of summary could serve as an effective navigation tool for accessing information in long texts , such as books .
To generate a coherent table-of-contents, we need to capture both global dependencies across different titles in the table and local constraints within sections.	titles	table-of-contents	usage	{'e1': {'word': 'titles', 'word_index': [(19, 19)], 'id': 'P07-1069.8'}, 'e2': {'word': 'table-of-contents', 'word_index': [(4, 8)], 'id': 'P07-1069.6'}, 'entity_replacement': {'4:8': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '19:19': 'ENTITY'}}	To generate a coherent table - of - contents , we need to capture both global dependencies across different titles in the table and local constraints within sections .
First, we compare cruiser with a baseline system-initiative DS, and show that users prefer cruiser.	cruiser	system-initiative DS	compare	{'e1': {'word': 'cruiser', 'word_index': [(4, 4)], 'id': 'P08-1055.7'}, 'e2': {'word': 'system-initiative DS', 'word_index': [(8, 11)], 'id': 'P08-1055.8'}, 'entity_replacement': {'4:4': 'ENTITY', '8:11': 'ENTITYOTHER', '18:18': 'ENTITYUNRELATED'}}	First , we compare cruiser with a baseline system - initiative DS , and show that users prefer cruiser .
To improve the Mandarin large vocabulary continuous speech recognition (LVCSR), a unified framework based approach is introduced to exploit multi-level linguistic knowledge.	multi-level linguistic knowledge	large vocabulary continuous speech recognition (LVCSR)	usage	{'e1': {'word': 'multi-level linguistic knowledge', 'word_index': [(22, 24)], 'id': 'D08-1086.3'}, 'e2': {'word': 'large vocabulary continuous speech recognition (LVCSR)', 'word_index': [(4, 11)], 'id': 'D08-1086.2'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '4:11': 'ENTITYOTHER', '22:24': 'ENTITY'}}	To improve the Mandarin large vocabulary continuous speech recognition ( LVCSR ) , a unified framework based approach is introduced to exploit multi-level linguistic knowledge .
In this framework, each knowledge source is represented by a Weighted Finite State Transducer (WFST), and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources.	Weighted Finite State Transducer (WFST)	knowledge source	model-feature	{'e1': {'word': 'Weighted Finite State Transducer (WFST)', 'word_index': [(11, 17)], 'id': 'D08-1086.5'}, 'e2': {'word': 'knowledge source', 'word_index': [(5, 6)], 'id': 'D08-1086.4'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '11:17': 'ENTITY', '31:33': 'ENTITYUNRELATED'}}	In this framework , each knowledge source is represented by a Weighted Finite State Transducer ( WFST ) , and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources .
Due to the uniform transducer representation, any knowledge source can be easily integrated into the analyzer, as long as it can be encoded into WFSTs.	knowledge source	WFSTs	usage	{'e1': {'word': 'knowledge source', 'word_index': [(8, 9)], 'id': 'D08-1086.8'}, 'e2': {'word': 'WFSTs', 'word_index': [(26, 26)], 'id': 'D08-1086.9'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '8:9': 'ENTITY', '26:26': 'ENTITYOTHER'}}	Due to the uniform transducer representation , any knowledge source can be easily integrated into the analyzer , as long as it can be encoded into WFSTs .
In this paper we discuss algorithms for clustering words into classes from unlabeled text using unsupervised algorithms, based on distributional and morphological information.	words	unlabeled text	part_whole	{'e1': {'word': 'words', 'word_index': [(8, 8)], 'id': 'E03-1009.1'}, 'e2': {'word': 'unlabeled text', 'word_index': [(12, 13)], 'id': 'E03-1009.2'}, 'entity_replacement': {'8:8': 'ENTITY', '12:13': 'ENTITYOTHER', '20:23': 'ENTITYUNRELATED'}}	In this paper we discuss algorithms for clustering words into classes from unlabeled text using unsupervised algorithms , based on distributional and morphological information .
"""This paper presents an unsupervised relation extraction algorithm, which induces relations between entity pairs by grouping them into a ""natural"" number of clusters based on the similarity of their contexts."	unsupervised relation extraction algorithm	entity pairs	usage	{'e1': {'word': 'unsupervised relation extraction algorithm', 'word_index': [(5, 8)], 'id': 'I05-2045.2'}, 'e2': {'word': 'entity pairs', 'word_index': [(14, 15)], 'id': 'I05-2045.4'}, 'entity_replacement': {'5:8': 'ENTITY', '12:12': 'ENTITYUNRELATED', '14:15': 'ENTITYOTHER', '33:33': 'ENTITYUNRELATED'}}	""" This paper presents an unsupervised relation extraction algorithm , which induces relations between entity pairs by grouping them into a "" natural "" number of clusters based on the similarity of their contexts ."
This paper presents a method that measures the similarity between compound nouns in different languages to locate translation equivalents from corpora.	translation equivalents	corpora	part_whole	{'e1': {'word': 'translation equivalents', 'word_index': [(17, 18)], 'id': 'C02-1065.5'}, 'e2': {'word': 'corpora', 'word_index': [(20, 20)], 'id': 'C02-1065.6'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '17:18': 'ENTITY', '20:20': 'ENTITYOTHER'}}	This paper presents a method that measures the similarity between compound nouns in different languages to locate translation equivalents from corpora .
The method uses information from unrelated corpora in different languages that do not have to be parallel.	corpora	languages	model-feature	{'e1': {'word': 'corpora', 'word_index': [(6, 6)], 'id': 'C02-1065.7'}, 'e2': {'word': 'languages', 'word_index': [(9, 9)], 'id': 'C02-1065.8'}, 'entity_replacement': {'6:6': 'ENTITY', '9:9': 'ENTITYOTHER'}}	The method uses information from unrelated corpora in different languages that do not have to be parallel .
The method compares the contexts of target compound nouns and translation candidates in the word or semantic attribute level.	contexts	translation candidates	compare	{'e1': {'word': 'contexts', 'word_index': [(4, 4)], 'id': 'C02-1065.10'}, 'e2': {'word': 'translation candidates', 'word_index': [(10, 11)], 'id': 'C02-1065.12'}, 'entity_replacement': {'4:4': 'ENTITY', '7:8': 'ENTITYUNRELATED', '10:11': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED'}}	The method compares the contexts of target compound nouns and translation candidates in the word or semantic attribute level .
In this paper, we show how this measuring method can be applied to select the best English translation candidate for Japanese compound nouns in more than 70% of the cases.	English translation	Japanese compound nouns	usage	{'e1': {'word': 'English translation', 'word_index': [(17, 18)], 'id': 'C02-1065.15'}, 'e2': {'word': 'Japanese compound nouns', 'word_index': [(21, 23)], 'id': 'C02-1065.16'}, 'entity_replacement': {'17:18': 'ENTITY', '21:23': 'ENTITYOTHER'}}	In this paper , we show how this measuring method can be applied to select the best English translation candidate for Japanese compound nouns in more than 70 % of the cases .
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text	text mining	statistical Arabic NLP	part_whole	{'e1': {'word': 'text mining', 'word_index': [(9, 10)], 'id': 'L08-1611.2'}, 'e2': {'word': 'statistical Arabic NLP', 'word_index': [(2, 4)], 'id': 'L08-1611.1'}, 'entity_replacement': {'2:4': 'ENTITYOTHER', '9:10': 'ENTITY', '24:25': 'ENTITYUNRELATED', '29:33': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED'}}	Applications of statistical Arabic NLP in general , and text mining in specific , along with the tools underneath perform much better as the statistical processing operates on deeper language factorization ( s ) than on raw text
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text	statistical processing	language factorization(s)	usage	{'e1': {'word': 'statistical processing', 'word_index': [(24, 25)], 'id': 'L08-1611.3'}, 'e2': {'word': 'language factorization(s)', 'word_index': [(29, 33)], 'id': 'L08-1611.4'}, 'entity_replacement': {'2:4': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '24:25': 'ENTITY', '29:33': 'ENTITYOTHER', '36:37': 'ENTITYUNRELATED'}}	Applications of statistical Arabic NLP in general , and text mining in specific , along with the tools underneath perform much better as the statistical processing operates on deeper language factorization ( s ) than on raw text
While building this LR, we had to go beyond the conventional exclusive collection of words from dictionaries and thesauri that cannot alone produce a satisfactory coverage of this highly inflective and derivative language.	words	dictionaries	part_whole	{'e1': {'word': 'words', 'word_index': [(15, 15)], 'id': 'L08-1611.9'}, 'e2': {'word': 'dictionaries', 'word_index': [(17, 17)], 'id': 'L08-1611.10'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '15:15': 'ENTITY', '17:17': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED', '30:33': 'ENTITYUNRELATED'}}	While building this LR , we had to go beyond the conventional exclusive collection of words from dictionaries and thesauri that cannot alone produce a satisfactory coverage of this highly inflective and derivative language .
With the aid of the same large-scale Arabic morphological analyzer and PoS tagger in the runtime, the possible senses of virtually any given Arabic word are retrievable.	PoS tagger	Arabic word	usage	{'e1': {'word': 'PoS tagger', 'word_index': [(13, 14)], 'id': 'L08-1611.21'}, 'e2': {'word': 'Arabic word', 'word_index': [(26, 27)], 'id': 'L08-1611.22'}, 'entity_replacement': {'6:11': 'ENTITYUNRELATED', '13:14': 'ENTITY', '26:27': 'ENTITYOTHER'}}	With the aid of the same large - scale Arabic morphological analyzer and PoS tagger in the runtime , the possible senses of virtually any given Arabic word are retrievable .
Similarly to the well-established ROVER approach of ( Fiscus, 1997 ) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network.	confusion network	consensus translation	usage	{'e1': {'word': 'confusion network', 'word_index': [(29, 30)], 'id': 'E06-1005.7'}, 'e2': {'word': 'consensus translation', 'word_index': [(21, 22)], 'id': 'E06-1005.6'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '16:18': 'ENTITYUNRELATED', '21:22': 'ENTITYOTHER', '29:30': 'ENTITY'}}	Similarly to the well - established ROVER approach of ( Fiscus , 1997 ) for combining speech recognition hypotheses , the consensus translation is computed by voting on a confusion network .
To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.	statistical alignment algorithm	word alignments	usage	{'e1': {'word': 'statistical alignment algorithm', 'word_index': [(20, 22)], 'id': 'E06-1005.11'}, 'e2': {'word': 'word alignments', 'word_index': [(9, 10)], 'id': 'E06-1005.9'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '9:10': 'ENTITYOTHER', '14:16': 'ENTITYUNRELATED', '20:22': 'ENTITY', '26:27': 'ENTITYUNRELATED'}}	To create the confusion network , we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering .
The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.	document	alignment	usage	{'e1': {'word': 'document', 'word_index': [(5, 5)], 'id': 'E06-1005.13'}, 'e2': {'word': 'alignment', 'word_index': [(20, 20)], 'id': 'E06-1005.16'}, 'entity_replacement': {'5:5': 'ENTITY', '7:7': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '20:20': 'ENTITYOTHER'}}	The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment .
The method was also tested in the framework of multi-source and speech translation.	method	multi-source and speech translation	usage	{'e1': {'word': 'method', 'word_index': [(1, 1)], 'id': 'E06-1005.20'}, 'e2': {'word': 'multi-source and speech translation', 'word_index': [(9, 12)], 'id': 'E06-1005.21'}, 'entity_replacement': {'1:1': 'ENTITY', '9:12': 'ENTITYOTHER'}}	The method was also tested in the framework of multi-source and speech translation .
We present a two stage parser that recovers Penn Treebank style syntactic analyses of new sentences including skeletal syntactic structure, and, for the first time, both function tags and empty categories.	syntactic analyses	sentences	usage	{'e1': {'word': 'syntactic analyses', 'word_index': [(11, 12)], 'id': 'N06-1024.3'}, 'e2': {'word': 'sentences', 'word_index': [(15, 15)], 'id': 'N06-1024.4'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '11:12': 'ENTITY', '15:15': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	We present a two stage parser that recovers Penn Treebank style syntactic analyses of new sentences including skeletal syntactic structure , and , for the first time , both function tags and empty categories .
The accuracy of the first-stage parser on the standard Parseval metric matches that of the ( Collins, 2003 ) parser on which it is based, despite the data fragmentation caused by the greatly enriched space of possible node labels.	parser	parser	compare	{'e1': {'word': 'parser', 'word_index': [(7, 7)], 'id': 'N06-1024.9'}, 'e2': {'word': 'parser', 'word_index': [(22, 22)], 'id': 'N06-1024.11'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:7': 'ENTITY', '11:12': 'ENTITYUNRELATED', '22:22': 'ENTITYOTHER', '31:32': 'ENTITYUNRELATED'}}	The accuracy of the first - stage parser on the standard Parseval metric matches that of the ( Collins , 2003 ) parser on which it is based , despite the data fragmentation caused by the greatly enriched space of possible node labels .
Both anaphora resolution and prepositional phrase (PP) attachment are the most frequent ambiguities in natural language processing.	ambiguities	natural language processing	model-feature	{'e1': {'word': 'ambiguities', 'word_index': [(14, 14)], 'id': 'E95-1041.3'}, 'e2': {'word': 'natural language processing', 'word_index': [(16, 18)], 'id': 'E95-1041.4'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:9': 'ENTITYUNRELATED', '14:14': 'ENTITY', '16:18': 'ENTITYOTHER'}}	Both anaphora resolution and prepositional phrase ( PP ) attachment are the most frequent ambiguities in natural language processing .
Just as with speaker adaptation in speaker-independent system, two vocabulary adaptation algorithms [5] are implemented in order to tailor the VI subword models to the target vocabulary.	vocabulary adaptation algorithms	VI subword models	usage	{'e1': {'word': 'vocabulary adaptation algorithms', 'word_index': [(12, 14)], 'id': 'H92-1033.4'}, 'e2': {'word': 'VI subword models', 'word_index': [(25, 27)], 'id': 'H92-1033.5'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '6:9': 'ENTITYUNRELATED', '12:14': 'ENTITY', '25:27': 'ENTITYOTHER', '30:31': 'ENTITYUNRELATED'}}	Just as with speaker adaptation in speaker - independent system , two vocabulary adaptation algorithms [ 5 ] are implemented in order to tailor the VI subword models to the target vocabulary .
Over the past 9 years, the Applied Science and Engineering Laboratories (ASEL) at the University of Delaware and the duPont Hospital for Children, has been involved with applying natural language processing (NLP) technologies to the field of AAC.	natural language processing (NLP) technologies	AAC	usage	{'e1': {'word': 'natural language processing (NLP) technologies', 'word_index': [(32, 38)], 'id': 'W97-0503.2'}, 'e2': {'word': 'AAC', 'word_index': [(43, 43)], 'id': 'W97-0503.3'}, 'entity_replacement': {'32:38': 'ENTITY', '43:43': 'ENTITYOTHER'}}	Over the past 9 years , the Applied Science and Engineering Laboratories ( ASEL ) at the University of Delaware and the duPont Hospital for Children , has been involved with applying natural language processing ( NLP ) technologies to the field of AAC .
One of the major projects at ASEL (The COMPAN-SION project) has been concerned with the application of primarily lexical semantics and sentence generation technology to expand telegraphic input into full sentences.	sentence generation technology	telegraphic input	usage	{'e1': {'word': 'sentence generation technology', 'word_index': [(25, 27)], 'id': 'W97-0503.5'}, 'e2': {'word': 'telegraphic input', 'word_index': [(30, 31)], 'id': 'W97-0503.6'}, 'entity_replacement': {'22:23': 'ENTITYUNRELATED', '25:27': 'ENTITY', '30:31': 'ENTITYOTHER', '34:34': 'ENTITYUNRELATED'}}	One of the major projects at ASEL ( The COMPAN - SION project ) has been concerned with the application of primarily lexical semantics and sentence generation technology to expand telegraphic input into full sentences .
We view the entire problem as series of classification problems and employ memory-based learning (MBL) to resolve them.	memory-based learning (MBL)	classification problems	usage	{'e1': {'word': 'memory-based learning (MBL)', 'word_index': [(12, 18)], 'id': 'W00-1210.3'}, 'e2': {'word': 'classification problems', 'word_index': [(8, 9)], 'id': 'W00-1210.2'}, 'entity_replacement': {'8:9': 'ENTITYOTHER', '12:18': 'ENTITY'}}	We view the entire problem as series of classification problems and employ memory - based learning ( MBL ) to resolve them .
The problem of word segmentation affects all aspects of Chinese language processing, including the development of text-to-speech synthesis systems.	word segmentation	Chinese language processing	part_whole	{'e1': {'word': 'word segmentation', 'word_index': [(3, 4)], 'id': 'W02-1813.1'}, 'e2': {'word': 'Chinese language processing', 'word_index': [(9, 11)], 'id': 'W02-1813.2'}, 'entity_replacement': {'3:4': 'ENTITY', '9:11': 'ENTITYOTHER', '17:22': 'ENTITYUNRELATED'}}	The problem of word segmentation affects all aspects of Chinese language processing , including the development of text - to -speech synthesis systems .
This paper treats the classification of the semantic functions performed by adnominal constituents in Japanese, where many parts of speech act as adnominal constituents.	adnominal constituents	Japanese	part_whole	{'e1': {'word': 'adnominal constituents', 'word_index': [(11, 12)], 'id': 'W00-0110.3'}, 'e2': {'word': 'Japanese', 'word_index': [(14, 14)], 'id': 'W00-0110.4'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '11:12': 'ENTITY', '14:14': 'ENTITYOTHER', '18:20': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED'}}	This paper treats the classification of the semantic functions performed by adnominal constituents in Japanese , where many parts of speech act as adnominal constituents .
"adjectives and ""noun + NO"" (in English ""of + noun"") structures, which have a broad range of semantic functions, are discussed."	semantic functions	"""noun + NO"" (in English ""of + noun"") structures"	model-feature	"{'e1': {'word': 'semantic functions', 'word_index': [(24, 25)], 'id': 'W00-0110.11'}, 'e2': {'word': '""noun + NO"" (in English ""of + noun"") structures', 'word_index': [(2, 16)], 'id': 'W00-0110.10'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '2:16': 'ENTITYOTHER', '24:25': 'ENTITY'}}"	"adjectives and "" noun + NO "" ( in English "" of + noun "" ) structures , which have a broad range of semantic functions , are discussed ."
The feasibility of this was verified with a self-organizing semantic map based on a neural network model.	neural network model	self-organizing semantic map	usage	{'e1': {'word': 'neural network model', 'word_index': [(16, 18)], 'id': 'W00-0110.15'}, 'e2': {'word': 'self-organizing semantic map', 'word_index': [(8, 12)], 'id': 'W00-0110.14'}, 'entity_replacement': {'8:12': 'ENTITYOTHER', '16:18': 'ENTITY'}}	The feasibility of this was verified with a self - organizing semantic map based on a neural network model .
Recent corpus-based work on word sense disambiguation explores the application of statistical pattern recognition procedures to lexical co-occurrence data from very large text databases.	statistical pattern recognition procedures	lexical co-occurrence data	usage	{'e1': {'word': 'statistical pattern recognition procedures', 'word_index': [(13, 16)], 'id': 'J95-1001.2'}, 'e2': {'word': 'lexical co-occurrence data', 'word_index': [(18, 20)], 'id': 'J95-1001.3'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '13:16': 'ENTITY', '18:20': 'ENTITYOTHER', '22:25': 'ENTITYUNRELATED'}}	Recent corpus - based work on word sense disambiguation explores the application of statistical pattern recognition procedures to lexical co-occurrence data from very large text databases .
Statistical methods play a definite role in this work, helping to organize and analyze data, but the disambiguation method itself does not employ statistical data or decision criteria.	statistical data	disambiguation method	usage	{'e1': {'word': 'statistical data', 'word_index': [(25, 26)], 'id': 'J95-1001.12'}, 'e2': {'word': 'disambiguation method', 'word_index': [(19, 20)], 'id': 'J95-1001.11'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '19:20': 'ENTITYOTHER', '25:26': 'ENTITY', '28:29': 'ENTITYUNRELATED'}}	Statistical methods play a definite role in this work , helping to organize and analyze data , but the disambiguation method itself does not employ statistical data or decision criteria .
The approach is illustrated by an experiment discriminating among the senses of adjectives, which have been relatively neglected in work on sense disambiguation.	senses	adjectives	model-feature	{'e1': {'word': 'senses', 'word_index': [(10, 10)], 'id': 'J95-1001.17'}, 'e2': {'word': 'adjectives', 'word_index': [(12, 12)], 'id': 'J95-1001.18'}, 'entity_replacement': {'10:10': 'ENTITY', '12:12': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED'}}	The approach is illustrated by an experiment discriminating among the senses of adjectives , which have been relatively neglected in work on sense disambiguation .
In particular, the paper assesses the potential of nouns for discriminating among the senses of adjectives that modify them.	senses	adjectives	model-feature	{'e1': {'word': 'senses', 'word_index': [(14, 14)], 'id': 'J95-1001.21'}, 'e2': {'word': 'adjectives', 'word_index': [(16, 16)], 'id': 'J95-1001.22'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '14:14': 'ENTITY', '16:16': 'ENTITYOTHER'}}	In particular , the paper assesses the potential of nouns for discriminating among the senses of adjectives that modify them .
This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: and About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur.	ambiguous adjectives	English	part_whole	{'e1': {'word': 'ambiguous adjectives', 'word_index': [(14, 15)], 'id': 'J95-1001.23'}, 'e2': {'word': 'English', 'word_index': [(17, 17)], 'id': 'J95-1001.24'}, 'entity_replacement': {'14:15': 'ENTITY', '17:17': 'ENTITYOTHER', '29:29': 'ENTITYUNRELATED', '37:37': 'ENTITYUNRELATED', '43:44': 'ENTITYUNRELATED'}}	This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English : and About three - quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur .
This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: and About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur.	nouns	syntactic constructions	model-feature	{'e1': {'word': 'nouns', 'word_index': [(37, 37)], 'id': 'J95-1001.26'}, 'e2': {'word': 'syntactic constructions', 'word_index': [(43, 44)], 'id': 'J95-1001.27'}, 'entity_replacement': {'14:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '37:37': 'ENTITY', '43:44': 'ENTITYOTHER'}}	This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English : and About three - quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur .
Furthermore, a small number of semantic attributes supply a compact means of representing the noun clues in a very few rules.	semantic attributes	noun	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(6, 7)], 'id': 'J95-1001.29'}, 'e2': {'word': 'noun', 'word_index': [(15, 15)], 'id': 'J95-1001.30'}, 'entity_replacement': {'6:7': 'ENTITY', '15:15': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED'}}	Furthermore , a small number of semantic attributes supply a compact means of representing the noun clues in a very few rules .
The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined syntactic relations to the ambiguous adjective.	sense	ambiguous modified noun	model-feature	{'e1': {'word': 'sense', 'word_index': [(1, 1)], 'id': 'J95-1001.34'}, 'e2': {'word': 'ambiguous modified noun', 'word_index': [(4, 6)], 'id': 'J95-1001.35'}, 'entity_replacement': {'1:1': 'ENTITY', '4:6': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '54:55': 'ENTITYUNRELATED', '58:59': 'ENTITYUNRELATED'}}	The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective ; and other adjectives , verbs , and grammatical constructions all show evidence of high reliability , and sometimes of high applicability , when they stand in specific , well - defined syntactic relations to the ambiguous adjective .
The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined syntactic relations to the ambiguous adjective.	semantic attribute	target adjective	model-feature	{'e1': {'word': 'semantic attribute', 'word_index': [(14, 15)], 'id': 'J95-1001.36'}, 'e2': {'word': 'target adjective', 'word_index': [(20, 21)], 'id': 'J95-1001.38'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:6': 'ENTITYUNRELATED', '14:15': 'ENTITY', '17:17': 'ENTITYUNRELATED', '20:21': 'ENTITYOTHER', '25:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '54:55': 'ENTITYUNRELATED', '58:59': 'ENTITYUNRELATED'}}	The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective ; and other adjectives , verbs , and grammatical constructions all show evidence of high reliability , and sometimes of high applicability , when they stand in specific , well - defined syntactic relations to the ambiguous adjective .
This paper presents our method of incorporating character clustering based on mutual information into Decision-Tree Dictionary-less morphological analysis.	mutual information	character clustering	usage	{'e1': {'word': 'mutual information', 'word_index': [(11, 12)], 'id': 'P98-1108.7'}, 'e2': {'word': 'character clustering', 'word_index': [(7, 8)], 'id': 'P98-1108.6'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '11:12': 'ENTITY', '14:21': 'ENTITYUNRELATED'}}	This paper presents our method of incorporating character clustering based on mutual information into Decision - Tree Dictionary - less morphological analysis .
By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text.	tagging	text	usage	{'e1': {'word': 'tagging', 'word_index': [(20, 20)], 'id': 'P98-1108.12'}, 'e2': {'word': 'text', 'word_index': [(22, 22)], 'id': 'P98-1108.14'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '20:20': 'ENTITY', '21:21': 'ENTITYUNRELATED', '22:22': 'ENTITYOTHER'}}	By using natural classes , we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text .
As a case study in one direction, we discuss the recent development of an automatic method for evaluating definition questions based on n-gram overlap, a commonly-used technique in summarization evaluation.	n-gram overlap	automatic method for evaluating definition questions	usage	{'e1': {'word': 'n-gram overlap', 'word_index': [(23, 24)], 'id': 'W05-0906.4'}, 'e2': {'word': 'automatic method for evaluating definition questions', 'word_index': [(15, 20)], 'id': 'W05-0906.3'}, 'entity_replacement': {'15:20': 'ENTITYOTHER', '23:24': 'ENTITY', '32:33': 'ENTITYUNRELATED'}}	As a case study in one direction , we discuss the recent development of an automatic method for evaluating definition questions based on n-gram overlap , a commonly - used technique in summarization evaluation .
SYSTRAN'S Chinese word segmentation is one important component of its Chinese-English machine translation system.	Chinese word segmentation	Chinese-English machine translation system	part_whole	{'e1': {'word': 'Chinese word segmentation', 'word_index': [(1, 3)], 'id': 'W03-1729.1'}, 'e2': {'word': 'Chinese-English machine translation system', 'word_index': [(10, 15)], 'id': 'W03-1729.2'}, 'entity_replacement': {'1:3': 'ENTITY', '10:15': 'ENTITYOTHER'}}	SYSTRAN'S Chinese word segmentation is one important component of its Chinese - English machine translation system .
The Chinese word segmentation module uses a rule-based approach, based on a large dictionary and fine-grained linguistic rules.	rule-based approach	Chinese word segmentation	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(7, 10)], 'id': 'W03-1729.4'}, 'e2': {'word': 'Chinese word segmentation', 'word_index': [(1, 3)], 'id': 'W03-1729.3'}, 'entity_replacement': {'1:3': 'ENTITYOTHER', '7:10': 'ENTITY', '15:16': 'ENTITYUNRELATED', '18:22': 'ENTITYUNRELATED'}}	The Chinese word segmentation module uses a rule - based approach , based on a large dictionary and fine - grained linguistic rules .
It works on general-purpose texts from different Chinese-speaking regions, with comparable performance.	Chinese-speaking regions	general-purpose texts	model-feature	{'e1': {'word': 'Chinese-speaking regions', 'word_index': [(9, 11)], 'id': 'W03-1729.8'}, 'e2': {'word': 'general-purpose texts', 'word_index': [(3, 6)], 'id': 'W03-1729.7'}, 'entity_replacement': {'3:6': 'ENTITYOTHER', '9:11': 'ENTITY', '15:15': 'ENTITYUNRELATED'}}	It works on general - purpose texts from different Chinese -speaking regions , with comparable performance .
ParaMor, our minimally supervised morphology induction algorithm, retrusses the word forms of raw text corpora back onto their paradigmatic skeletons; performing on par with state-of-the-art minimally supervised morphology induction algorithms at morphological analysis of English and German.	morphological analysis	English	usage	{'e1': {'word': 'morphological analysis', 'word_index': [(40, 41)], 'id': 'W07-1315.6'}, 'e2': {'word': 'English', 'word_index': [(43, 43)], 'id': 'W07-1315.7'}, 'entity_replacement': {'3:7': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '14:16': 'ENTITYUNRELATED', '34:38': 'ENTITYUNRELATED', '40:41': 'ENTITY', '43:43': 'ENTITYOTHER', '45:45': 'ENTITYUNRELATED'}}	ParaMor , our minimally supervised morphology induction algorithm , retrusses the word forms of raw text corpora back onto their paradigmatic skeletons ; performing on par with state - of - the - art minimally supervised morphology induction algorithms at morphological analysis of English and German .
And with these structures in hand, ParaMor then annotates word forms with morpheme boundaries.	morpheme boundaries	word forms	model-feature	{'e1': {'word': 'morpheme boundaries', 'word_index': [(14, 15)], 'id': 'W07-1315.12'}, 'e2': {'word': 'word forms', 'word_index': [(11, 12)], 'id': 'W07-1315.11'}, 'entity_replacement': {'11:12': 'ENTITYOTHER', '14:15': 'ENTITY'}}	And with these structures in hand , Para Mor then annotates word forms with morpheme boundaries .
To set ParaMor 's few free parameters we analyze a training corpus of Spanish.	Spanish	training corpus	part_whole	{'e1': {'word': 'Spanish', 'word_index': [(13, 13)], 'id': 'W07-1315.14'}, 'e2': {'word': 'training corpus', 'word_index': [(10, 11)], 'id': 'W07-1315.13'}, 'entity_replacement': {'10:11': 'ENTITYOTHER', '13:13': 'ENTITY'}}	To set ParaMor 's few free parameters we analyze a training corpus of Spanish .
Without adjusting parameters, we induce the morphological structure of English and German.	morphological structure	English	model-feature	{'e1': {'word': 'morphological structure', 'word_index': [(7, 8)], 'id': 'W07-1315.15'}, 'e2': {'word': 'English', 'word_index': [(10, 10)], 'id': 'W07-1315.16'}, 'entity_replacement': {'7:8': 'ENTITY', '10:10': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED'}}	Without adjusting parameters , we induce the morphological structure of English and German .
The other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores.	distance features	multi-level confidence scores	usage	{'e1': {'word': 'distance features', 'word_index': [(12, 13)], 'id': 'P08-2055.8'}, 'e2': {'word': 'multi-level confidence scores', 'word_index': [(19, 21)], 'id': 'P08-2055.9'}, 'entity_replacement': {'12:13': 'ENTITY', '19:21': 'ENTITYOTHER'}}	The other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores .
Using Conditional Maximum Entropy (CME) classifier with all the selected features, we reached an annotation error rate of 26.0% in the SWBD corpus, compared with a subtree error rate of 41.91%, a closely related benchmark with the Charniak parser from ( Kahn et al., 2005 ).	Conditional Maximum Entropy (CME) classifier	annotation error rate	result	{'e1': {'word': 'Conditional Maximum Entropy (CME) classifier', 'word_index': [(1, 7)], 'id': 'P08-2055.10'}, 'e2': {'word': 'annotation error rate', 'word_index': [(17, 19)], 'id': 'P08-2055.11'}, 'entity_replacement': {'1:7': 'ENTITY', '17:19': 'ENTITYOTHER', '25:26': 'ENTITYUNRELATED', '31:33': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED'}}	Using Conditional Maximum Entropy ( CME ) classifier with all the selected features , we reached an annotation error rate of 26.0 % in the SWBD corpus , compared with a subtree error rate of 41.91 % , a closely related benchmark with the Charniak parser from ( Kahn et al. , 2005 ) .
Coreference resolution systems usually attempt to find a suitable antecedent for (almost) every noun phrase	Coreference resolution systems	noun phrase	usage	{'e1': {'word': 'Coreference resolution systems', 'word_index': [(0, 2)], 'id': 'P03-2012.1'}, 'e2': {'word': 'noun phrase', 'word_index': [(15, 16)], 'id': 'P03-2012.2'}, 'entity_replacement': {'0:2': 'ENTITY', '15:16': 'ENTITYOTHER'}}	Coreference resolution systems usually attempt to find a suitable antecedent for ( almost ) every noun phrase
We use a small training corpus (MUC-7), but also acquire some data from the Internet.	data	Internet	part_whole	{'e1': {'word': 'data', 'word_index': [(16, 16)], 'id': 'P03-2012.8'}, 'e2': {'word': 'Internet', 'word_index': [(19, 19)], 'id': 'P03-2012.9'}, 'entity_replacement': {'4:10': 'ENTITYUNRELATED', '16:16': 'ENTITY', '19:19': 'ENTITYOTHER'}}	We use a small training corpus ( MUC - 7 ) , but also acquire some data from the Internet .
Combining our classifiers sequentially, we achieve 88.9% precision and 84.6% recall for discourse new entities.	classifiers	precision	result	{'e1': {'word': 'classifiers', 'word_index': [(2, 2)], 'id': 'P03-2012.10'}, 'e2': {'word': 'precision', 'word_index': [(9, 9)], 'id': 'P03-2012.11'}, 'entity_replacement': {'2:2': 'ENTITY', '9:9': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '15:17': 'ENTITYUNRELATED'}}	Combining our classifiers sequentially , we achieve 88.9 % precision and 84.6 % recall for discourse new entities .
We expect our classifiers to provide a good prefiltering for coreference resolution systems, improving both their speed and performance.	classifiers	coreference resolution systems	usage	{'e1': {'word': 'classifiers', 'word_index': [(3, 3)], 'id': 'P03-2012.14'}, 'e2': {'word': 'coreference resolution systems', 'word_index': [(10, 12)], 'id': 'P03-2012.15'}, 'entity_replacement': {'3:3': 'ENTITY', '10:12': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED'}}	We expect our classifiers to provide a good prefiltering for coreference resolution systems , improving both their speed and performance .
MBDP-1 is a knowledge-free segmentation algorithm that bootstraps its own lexicon, which starts out empty.	knowledge-free segmentation algorithm	lexicon	usage	{'e1': {'word': 'knowledge-free segmentation algorithm', 'word_index': [(4, 8)], 'id': 'P01-1013.3'}, 'e2': {'word': 'lexicon', 'word_index': [(13, 13)], 'id': 'P01-1013.4'}, 'entity_replacement': {'4:8': 'ENTITY', '13:13': 'ENTITYOTHER'}}	MBDP -1 is a knowledge - free segmentation algorithm that bootstraps its own lexicon , which starts out empty .
In this paper, we present methods that allow the users of a natural language processor (NLP) to define, inspect, and modify any case frame information associated with the words and phrases known to the system.	case frame information	words	model-feature	{'e1': {'word': 'case frame information', 'word_index': [(27, 29)], 'id': 'C86-1108.2'}, 'e2': {'word': 'words', 'word_index': [(33, 33)], 'id': 'C86-1108.3'}, 'entity_replacement': {'13:18': 'ENTITYUNRELATED', '27:29': 'ENTITY', '33:33': 'ENTITYOTHER', '35:35': 'ENTITYUNRELATED'}}	In this paper , we present methods that allow the users of a natural language processor ( NLP ) to define , inspect , and modify any case frame information associated with the words and phrases known to the system .
The number and sizes of parallel corpora keep growing, which makes it necessary to have automatic methods of processing them: combining, checking and improving corpora quality, etc.	corpora quality	parallel corpora	model-feature	{'e1': {'word': 'corpora quality', 'word_index': [(27, 28)], 'id': 'L08-1114.2'}, 'e2': {'word': 'parallel corpora', 'word_index': [(5, 6)], 'id': 'L08-1114.1'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '27:28': 'ENTITY'}}	The number and sizes of parallel corpora keep growing , which makes it necessary to have automatic methods of processing them : combining , checking and improving corpora quality , etc.
The method takes into consideration slight differences in the source documents, different levels of segmentation of the input corpora, encoding differences and other aspects of the task.	segmentation	input corpora	usage	{'e1': {'word': 'segmentation', 'word_index': [(15, 15)], 'id': 'L08-1114.10'}, 'e2': {'word': 'input corpora', 'word_index': [(18, 19)], 'id': 'L08-1114.11'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '15:15': 'ENTITY', '18:19': 'ENTITYOTHER'}}	The method takes into consideration slight differences in the source documents , different levels of segmentation of the input corpora , encoding differences and other aspects of the task .
In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts.	Estonian-English	JRC-Acquis corpus	part_whole	{'e1': {'word': 'Estonian-English', 'word_index': [(6, 8)], 'id': 'L08-1114.12'}, 'e2': {'word': 'JRC-Acquis corpus', 'word_index': [(12, 15)], 'id': 'L08-1114.13'}, 'entity_replacement': {'6:8': 'ENTITY', '12:15': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED'}}	In the first experiment , the Estonian - English part of the JRC - Acquis corpus was combined with another corpus of legislation texts .
In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts.	legislation texts	corpus	part_whole	{'e1': {'word': 'legislation texts', 'word_index': [(22, 23)], 'id': 'L08-1114.15'}, 'e2': {'word': 'corpus', 'word_index': [(20, 20)], 'id': 'L08-1114.14'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '12:15': 'ENTITYUNRELATED', '20:20': 'ENTITYOTHER', '22:23': 'ENTITY'}}	In the first experiment , the Estonian - English part of the JRC - Acquis corpus was combined with another corpus of legislation texts .
The generation module supports the seamless integration of full grammar rules, templates and canned text.	grammar rules	generation module	usage	{'e1': {'word': 'grammar rules', 'word_index': [(9, 10)], 'id': 'E03-1019.6'}, 'e2': {'word': 'generation module', 'word_index': [(1, 2)], 'id': 'E03-1019.5'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '9:10': 'ENTITY', '12:12': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED'}}	The generation module supports the seamless integration of full grammar rules , templates and canned text .
Ambiguity is the fundamental property of natural language	Ambiguity	natural language	model-feature	{'e1': {'word': 'Ambiguity', 'word_index': [(0, 0)], 'id': 'C02-1079.1'}, 'e2': {'word': 'natural language', 'word_index': [(6, 7)], 'id': 'C02-1079.2'}, 'entity_replacement': {'0:0': 'ENTITY', '6:7': 'ENTITYOTHER'}}	Ambiguity is the fundamental property of natural language
Perhaps, the most burdensome case of ambiguity manifests itself on the syntactic level of analysis.	ambiguity	syntactic level of analysis	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(7, 7)], 'id': 'C02-1079.3'}, 'e2': {'word': 'syntactic level of analysis', 'word_index': [(12, 15)], 'id': 'C02-1079.4'}, 'entity_replacement': {'7:7': 'ENTITY', '12:15': 'ENTITYOTHER'}}	Perhaps , the most burdensome case of ambiguity manifests itself on the syntactic level of analysis .
The presented methods are based on language specific features of synthetical languages and they improve the results of simple stochastic approaches.	language specific features	synthetical languages	model-feature	{'e1': {'word': 'language specific features', 'word_index': [(6, 8)], 'id': 'C02-1079.7'}, 'e2': {'word': 'synthetical languages', 'word_index': [(10, 11)], 'id': 'C02-1079.8'}, 'entity_replacement': {'6:8': 'ENTITY', '10:11': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED'}}	The presented methods are based on language specific features of synthetical languages and they improve the results of simple stochastic approaches .
The texts are in English and Czech.	texts	English	model-feature	{'e1': {'word': 'texts', 'word_index': [(1, 1)], 'id': 'L08-1197.3'}, 'e2': {'word': 'English', 'word_index': [(4, 4)], 'id': 'L08-1197.4'}, 'entity_replacement': {'1:1': 'ENTITY', '4:4': 'ENTITYOTHER', '6:6': 'ENTITYUNRELATED'}}	The texts are in English and Czech .
This paper presents techniques for multimedia annotation and their application to video summarization and translation.	multimedia annotation	video summarization and translation	usage	{'e1': {'word': 'multimedia annotation', 'word_index': [(5, 6)], 'id': 'C02-1098.2'}, 'e2': {'word': 'video summarization and translation', 'word_index': [(11, 14)], 'id': 'C02-1098.3'}, 'entity_replacement': {'5:6': 'ENTITY', '11:14': 'ENTITYOTHER'}}	This paper presents techniques for multimedia annotation and their application to video summarization and translation .
A video scene description consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes.	semi-automatically detected keyframes	video scene description	part_whole	{'e1': {'word': 'semi-automatically detected keyframes', 'word_index': [(6, 8)], 'id': 'C02-1098.12'}, 'e2': {'word': 'video scene description', 'word_index': [(1, 3)], 'id': 'C02-1098.11'}, 'entity_replacement': {'1:3': 'ENTITYOTHER', '6:8': 'ENTITY'}}	A video scene description consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes .
The text data in the multimedia annotation are syntactically and semantically structured using linguistic annotation.	text data	syntactically and semantically structured	model-feature	{'e1': {'word': 'text data', 'word_index': [(1, 2)], 'id': 'C02-1098.14'}, 'e2': {'word': 'syntactically and semantically structured', 'word_index': [(8, 11)], 'id': 'C02-1098.16'}, 'entity_replacement': {'1:2': 'ENTITY', '5:6': 'ENTITYUNRELATED', '8:11': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED'}}	The text data in the multimedia annotation are syntactically and semantically structured using linguistic annotation .
The proposed multimedia summarization works upon a multimodal document that consists of a video, keyframes of scenes, and transcripts of the scenes.	 multimedia summarization	multimodal document	usage	{'e1': {'word': ' multimedia summarization', 'word_index': [(2, 3)], 'id': 'C02-1098.18'}, 'e2': {'word': 'multimodal document', 'word_index': [(7, 8)], 'id': 'C02-1098.19'}, 'entity_replacement': {'2:3': 'ENTITY', '7:8': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED'}}	The proposed multimedia summarization works upon a multimodal document that consists of a video , keyframes of scenes , and transcripts of the scenes .
The multimedia translation automatically generates several versions of multimedia content in different languages.	languages	multimedia content	model-feature	{'e1': {'word': 'languages', 'word_index': [(12, 12)], 'id': 'C02-1098.26'}, 'e2': {'word': 'multimedia content', 'word_index': [(8, 9)], 'id': 'C02-1098.25'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '8:9': 'ENTITYOTHER', '12:12': 'ENTITY'}}	The multimedia translation automatically generates several versions of multimedia content in different languages .
When machine translation (MT) knowledge is automatically constructed from bilingual corpora, redundant rules are acquired due to translation variety.	bilingual corpora	machine translation (MT) knowledge	usage	{'e1': {'word': 'bilingual corpora', 'word_index': [(11, 12)], 'id': 'E03-1029.2'}, 'e2': {'word': 'machine translation (MT) knowledge', 'word_index': [(1, 6)], 'id': 'E03-1029.1'}, 'entity_replacement': {'1:6': 'ENTITYOTHER', '11:12': 'ENTITY', '14:15': 'ENTITYUNRELATED'}}	When machine translation ( MT ) knowledge is automatically constructed from bilingual corpora , redundant rules are acquired due to translation variety .
These rules increase ambiguity or cause incorrect MT results.	rules	ambiguity	result	{'e1': {'word': 'rules', 'word_index': [(1, 1)], 'id': 'E03-1029.4'}, 'e2': {'word': 'ambiguity', 'word_index': [(3, 3)], 'id': 'E03-1029.5'}, 'entity_replacement': {'1:1': 'ENTITY', '3:3': 'ENTITYOTHER', '7:8': 'ENTITYUNRELATED'}}	These rules increase ambiguity or cause incorrect MT results .
"To overcome this problem, we constrain the sentences used for knowledge extraction to ""the appropriate bilingual sentences for the MT""."	sentences	knowledge extraction	usage	{'e1': {'word': 'sentences', 'word_index': [(8, 8)], 'id': 'E03-1029.7'}, 'e2': {'word': 'knowledge extraction', 'word_index': [(11, 12)], 'id': 'E03-1029.8'}, 'entity_replacement': {'8:8': 'ENTITY', '11:12': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED'}}	"To overcome this problem , we constrain the sentences used for knowledge extraction to "" the appropriate bilingual sentences for the MT "" ."
"To overcome this problem, we constrain the sentences used for knowledge extraction to ""the appropriate bilingual sentences for the MT""."	bilingual sentences	MT	usage	{'e1': {'word': 'bilingual sentences', 'word_index': [(17, 18)], 'id': 'E03-1029.9'}, 'e2': {'word': 'MT', 'word_index': [(21, 21)], 'id': 'E03-1029.10'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '17:18': 'ENTITY', '21:21': 'ENTITYOTHER'}}	"To overcome this problem , we constrain the sentences used for knowledge extraction to "" the appropriate bilingual sentences for the MT "" ."
For the creation of these scholarly digital editions the AAC edition philosophy and edition principles have been applied whereby new corpus research methods have been made use of for questions of computational philology and textual studies in a digital environment.	AAC edition philosophy and edition principles	scholarly digital editions	usage	{'e1': {'word': 'AAC edition philosophy and edition principles', 'word_index': [(9, 14)], 'id': 'L08-1405.9'}, 'e2': {'word': 'scholarly digital editions', 'word_index': [(5, 7)], 'id': 'L08-1405.8'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '9:14': 'ENTITY', '20:22': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED'}}	For the creation of these scholarly digital editions the AAC edition philosophy and edition principles have been applied whereby new corpus research methods have been made use of for questions of computational philology and textual studies in a digital environment .
The evaluation results show our system can achieve an F measure of 0.9400.967 for different testing corpora.	system	F measure	result	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'I05-3026.5'}, 'e2': {'word': 'F measure', 'word_index': [(9, 10)], 'id': 'I05-3026.6'}, 'entity_replacement': {'5:5': 'ENTITY', '9:10': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	The evaluation results show our system can achieve an F measure of 0.9400.967 for different testing corpora .
In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others.	machine learning algorithm	shallow semantic parsing	usage	{'e1': {'word': 'machine learning algorithm', 'word_index': [(7, 9)], 'id': 'N04-1030.1'}, 'e2': {'word': 'shallow semantic parsing', 'word_index': [(11, 13)], 'id': 'N04-1030.2'}, 'entity_replacement': {'7:9': 'ENTITY', '11:13': 'ENTITYOTHER'}}	In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al. ( 2003 ) and others .
Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers.	Support Vector Machines	algorithm	usage	{'e1': {'word': 'Support Vector Machines', 'word_index': [(5, 7)], 'id': 'N04-1030.4'}, 'e2': {'word': 'algorithm', 'word_index': [(1, 1)], 'id': 'N04-1030.3'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '5:7': 'ENTITY', '18:18': 'ENTITYUNRELATED'}}	Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .
We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.	test set	AQUAINT corpus	part_whole	{'e1': {'word': 'test set', 'word_index': [(19, 20)], 'id': 'N04-1030.7'}, 'e2': {'word': 'AQUAINT corpus', 'word_index': [(24, 25)], 'id': 'N04-1030.8'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '19:20': 'ENTITY', '24:25': 'ENTITYOTHER'}}	We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus .
Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences.	language model	speech understanding	usage	{'e1': {'word': 'language model', 'word_index': [(5, 6)], 'id': 'A94-1010.1'}, 'e2': {'word': 'speech understanding', 'word_index': [(9, 10)], 'id': 'A94-1010.2'}, 'entity_replacement': {'5:6': 'ENTITY', '9:10': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '16:18': 'ENTITYUNRELATED'}}	Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences .
I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into sub corpora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster.	sentences	training corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(11, 11)], 'id': 'A94-1010.5'}, 'e2': {'word': 'training corpus', 'word_index': [(14, 15)], 'id': 'A94-1010.6'}, 'entity_replacement': {'11:11': 'ENTITY', '14:15': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED'}}	I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into sub corpora on the criterion of entropy reduction , and calculating separate language model parameters for each cluster .
This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model.	clustering	contextual effects	model-feature	{'e1': {'word': 'clustering', 'word_index': [(3, 3)], 'id': 'A94-1010.11'}, 'e2': {'word': 'contextual effects', 'word_index': [(10, 11)], 'id': 'A94-1010.12'}, 'entity_replacement': {'3:3': 'ENTITY', '10:11': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED'}}	This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model .
It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclustered model.	clustering	model	result	{'e1': {'word': 'clustering', 'word_index': [(43, 43)], 'id': 'A94-1010.16'}, 'e2': {'word': 'model', 'word_index': [(49, 49)], 'id': 'A94-1010.17'}, 'entity_replacement': {'13:19': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '43:43': 'ENTITY', '49:49': 'ENTITYOTHER', '57:58': 'ENTITYUNRELATED', '64:65': 'ENTITYUNRELATED'}}	It also offers a reasonably automatic means to gather evidence on whether a more complex , context - sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it : if clustering improves the performance of a model , this proves the existence of further context dependencies , not exploited by the unclustered model .
As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain.	clustering	models	result	{'e1': {'word': 'clustering', 'word_index': [(11, 11)], 'id': 'A94-1010.20'}, 'e2': {'word': 'models', 'word_index': [(14, 14)], 'id': 'A94-1010.21'}, 'entity_replacement': {'11:11': 'ENTITY', '14:14': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .
This paper presents a parsing system for the detection of syntactic errors.	parsing system	detection of syntactic errors	usage	{'e1': {'word': 'parsing system', 'word_index': [(4, 5)], 'id': 'A00-3005.1'}, 'e2': {'word': 'detection of syntactic errors', 'word_index': [(8, 11)], 'id': 'A00-3005.2'}, 'entity_replacement': {'4:5': 'ENTITY', '8:11': 'ENTITYOTHER'}}	This paper presents a parsing system for the detection of syntactic errors .
It combines a robust partial parser which obtains the main sentence components and a finite-state parser used for the description of syntactic error patterns.	finite-state parser	syntactic error patterns	usage	{'e1': {'word': 'finite-state parser', 'word_index': [(14, 17)], 'id': 'A00-3005.5'}, 'e2': {'word': 'syntactic error patterns', 'word_index': [(23, 25)], 'id': 'A00-3005.6'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '9:11': 'ENTITYUNRELATED', '14:17': 'ENTITY', '23:25': 'ENTITYOTHER'}}	It combines a robust partial parser which obtains the main sentence components and a finite - state parser used for the description of syntactic error patterns .
The system has been tested on a corpus of real texts, containing both correct and incorrect sentences, with promising results.	texts	corpus	part_whole	{'e1': {'word': 'texts', 'word_index': [(10, 10)], 'id': 'A00-3005.9'}, 'e2': {'word': 'corpus', 'word_index': [(7, 7)], 'id': 'A00-3005.8'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:7': 'ENTITYOTHER', '10:10': 'ENTITY', '14:17': 'ENTITYUNRELATED'}}	The system has been tested on a corpus of real texts , containing both correct and incorrect sentences , with promising results .
The objectives of this project are to advance our understanding of the merits of current text analysis techniques, as applied to the performance of realistic text analysis tasks, and to achieve this understanding by means of a sound performance evaluation methodology.	text analysis techniques	realistic text analysis tasks	usage	{'e1': {'word': 'text analysis techniques', 'word_index': [(15, 17)], 'id': 'H92-1111.1'}, 'e2': {'word': 'realistic text analysis tasks', 'word_index': [(25, 28)], 'id': 'H92-1111.3'}, 'entity_replacement': {'15:17': 'ENTITY', '23:23': 'ENTITYUNRELATED', '25:28': 'ENTITYOTHER', '40:42': 'ENTITYUNRELATED'}}	The objectives of this project are to advance our understanding of the merits of current text analysis techniques , as applied to the performance of realistic text analysis tasks , and to achieve this understanding by means of a sound performance evaluation methodology .
Talk'n'Travel is a fully conversational, mixed-initiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English.	general spoken English	questions	model-feature	{'e1': {'word': 'general spoken English', 'word_index': [(35, 37)], 'id': 'A00-1010.8'}, 'e2': {'word': 'questions', 'word_index': [(30, 30)], 'id': 'A00-1010.7'}, 'entity_replacement': {'0:3': 'ENTITYUNRELATED', '7:12': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '30:30': 'ENTITYOTHER', '35:37': 'ENTITY'}}	Talk 'n ' Travel is a fully conversational , mixed - initiative system that allows the user to specify the constraints on his travel plan in arbitrary order , ask questions , etc. , in general spoken English .
The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met.	plan-based agenda mechanism	system	usage	{'e1': {'word': 'plan-based agenda mechanism', 'word_index': [(6, 10)], 'id': 'A00-1010.10'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'A00-1010.9'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '6:10': 'ENTITY', '15:17': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	The system operates according to a plan - based agenda mechanism , rather than a finite state network , and attempts to negotiate with the user when not all of his constraints can be met .
As mentioned in Mitkov (1996), solving the anaphora and extracting the antecedent are key issues in a correct translation.	anaphora	translation	part_whole	{'e1': {'word': 'anaphora', 'word_index': [(10, 10)], 'id': 'W99-0210.3'}, 'e2': {'word': 'translation', 'word_index': [(21, 21)], 'id': 'W99-0210.5'}, 'entity_replacement': {'10:10': 'ENTITY', '14:14': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER'}}	As mentioned in Mitkov ( 1996 ) , solving the anaphora and extracting the antecedent are key issues in a correct translation .
The SS stores the lexical, syntactic, morphologic and semantic information of every constituent of the grammar.	lexical, syntactic, morphologic and semantic information	constituent	model-feature	{'e1': {'word': 'lexical, syntactic, morphologic and semantic information', 'word_index': [(4, 11)], 'id': 'W99-0210.10'}, 'e2': {'word': 'constituent', 'word_index': [(14, 14)], 'id': 'W99-0210.11'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:11': 'ENTITY', '14:14': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED'}}	The SS stores the lexical , syntactic , morphologic and semantic information of every constituent of the grammar .
This mechanism could be added to a MT system such as an additional module to solve anaphora generation problem.	mechanism	MT system	usage	{'e1': {'word': 'mechanism', 'word_index': [(1, 1)], 'id': 'W99-0210.23'}, 'e2': {'word': 'MT system', 'word_index': [(7, 8)], 'id': 'W99-0210.24'}, 'entity_replacement': {'1:1': 'ENTITY', '7:8': 'ENTITYOTHER', '16:18': 'ENTITYUNRELATED'}}	This mechanism could be added to a MT system such as an additional module to solve anaphora generation problem .
This paper addresses language engineering infrastructure issues by considering whether standard V&amp;V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&amp;V in the context of language processing systems.	standard V&amp;V methods	evaluation practices	compare	{'e1': {'word': 'standard V&amp;V methods', 'word_index': [(10, 14)], 'id': 'W01-0906.6'}, 'e2': {'word': 'evaluation practices', 'word_index': [(20, 21)], 'id': 'W01-0906.7'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '10:14': 'ENTITY', '20:21': 'ENTITYOTHER', '25:26': 'ENTITYUNRELATED', '34:36': 'ENTITYUNRELATED', '41:43': 'ENTITYUNRELATED'}}	This paper addresses language engineering infrastructure issues by considering whether standard V&amp ; V methods are fundamentally different than the evaluation practices commonly used for NLP systems , and proposes practical approaches for applying V&amp ; V in the context of language processing systems .
This paper addresses language engineering infrastructure issues by considering whether standard V&amp;V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&amp;V in the context of language processing systems.	V&amp;V	language processing systems	usage	{'e1': {'word': 'V&amp;V', 'word_index': [(34, 36)], 'id': 'W01-0906.9'}, 'e2': {'word': 'language processing systems', 'word_index': [(41, 43)], 'id': 'W01-0906.10'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '10:14': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '34:36': 'ENTITY', '41:43': 'ENTITYOTHER'}}	This paper addresses language engineering infrastructure issues by considering whether standard V&amp ; V methods are fundamentally different than the evaluation practices commonly used for NLP systems , and proposes practical approaches for applying V&amp ; V in the context of language processing systems .
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text.	paragraphs	document	part_whole	{'e1': {'word': 'paragraphs', 'word_index': [(14, 14)], 'id': 'W03-1102.1'}, 'e2': {'word': 'document', 'word_index': [(18, 18)], 'id': 'W03-1102.2'}, 'entity_replacement': {'14:14': 'ENTITY', '18:18': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	In this paper , we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text .
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text.	summary	Thai text	model-feature	{'e1': {'word': 'summary', 'word_index': [(22, 22)], 'id': 'W03-1102.3'}, 'e2': {'word': 'Thai text', 'word_index': [(24, 25)], 'id': 'W03-1102.4'}, 'entity_replacement': {'14:14': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '22:22': 'ENTITY', '24:25': 'ENTITYOTHER'}}	In this paper , we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text .
The idea of our approach is to exploit both the local and global properties of paragraphs.	local and global properties	paragraphs	model-feature	{'e1': {'word': 'local and global properties', 'word_index': [(10, 13)], 'id': 'W03-1102.5'}, 'e2': {'word': 'paragraphs', 'word_index': [(15, 15)], 'id': 'W03-1102.6'}, 'entity_replacement': {'10:13': 'ENTITY', '15:15': 'ENTITYOTHER'}}	The idea of our approach is to exploit both the local and global properties of paragraphs .
The local property can be considered as clusters of significant words within each paragraph, while the global property can be thought of as relations of all paragraphs in a document.	clusters	significant words	model-feature	{'e1': {'word': 'clusters', 'word_index': [(7, 7)], 'id': 'W03-1102.8'}, 'e2': {'word': 'significant words', 'word_index': [(9, 10)], 'id': 'W03-1102.9'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '7:7': 'ENTITY', '9:10': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED'}}	The local property can be considered as clusters of significant words within each paragraph , while the global property can be thought of as relations of all paragraphs in a document .
Syntax-based Machine Translation systems have recently become a focus of research with much hope that they will outperform traditional Phrase- Based Statistical Machine Translation (PBSMT)	Syntax-based Machine Translation systems	traditional Phrase- Based Statistical Machine Translation (PBSMT)	compare	{'e1': {'word': 'Syntax-based Machine Translation systems', 'word_index': [(0, 5)], 'id': 'W08-0410.1'}, 'e2': {'word': 'traditional Phrase- Based Statistical Machine Translation (PBSMT)', 'word_index': [(20, 29)], 'id': 'W08-0410.2'}, 'entity_replacement': {'0:5': 'ENTITY', '20:29': 'ENTITYOTHER'}}	Syntax - based Machine Translation systems have recently become a focus of research with much hope that they will outperform traditional Phrase - Based Statistical Machine Translation ( PBSMT )
Toward this goal, we present a method for analyzing the morphosyntactic content of language from an Elicitation Corpus such as the one included in the LDC's upcoming LCTL language packs.	morphosyntactic content	Elicitation Corpus	part_whole	{'e1': {'word': 'morphosyntactic content', 'word_index': [(11, 12)], 'id': 'W08-0410.3'}, 'e2': {'word': 'Elicitation Corpus', 'word_index': [(17, 18)], 'id': 'W08-0410.5'}, 'entity_replacement': {'11:12': 'ENTITY', '14:14': 'ENTITYUNRELATED', '17:18': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED', '29:31': 'ENTITYUNRELATED'}}	Toward this goal , we present a method for analyzing the morphosyntactic content of language from an Elicitation Corpus such as the one included in the LDC 's upcoming LCTL language packs .
By providing this tool that can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved.	rich features	structure-based MT models	usage	{'e1': {'word': 'rich features', 'word_index': [(14, 15)], 'id': 'W08-0410.12'}, 'e2': {'word': 'structure-based MT models', 'word_index': [(7, 11)], 'id': 'W08-0410.11'}, 'entity_replacement': {'7:11': 'ENTITYOTHER', '14:15': 'ENTITY', '20:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	By providing this tool that can augment structure - based MT models with these rich features , we believe the discriminative power of current models can be improved .
This article outlines a quantitative method for segmenting texts into thematically coherent units.	quantitative method	texts	usage	{'e1': {'word': 'quantitative method', 'word_index': [(4, 5)], 'id': 'P98-2243.1'}, 'e2': {'word': 'texts', 'word_index': [(8, 8)], 'id': 'P98-2243.2'}, 'entity_replacement': {'4:5': 'ENTITY', '8:8': 'ENTITYOTHER', '10:12': 'ENTITYUNRELATED'}}	This article outlines a quantitative method for segmenting texts into thematically coherent units .
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.	network of lexical collocations	method	usage	{'e1': {'word': 'network of lexical collocations', 'word_index': [(5, 8)], 'id': 'P98-2243.5'}, 'e2': {'word': 'method', 'word_index': [(1, 1)], 'id': 'P98-2243.4'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '5:8': 'ENTITY', '12:13': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words .
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.	lexical cohesiveness	words	model-feature	{'e1': {'word': 'lexical cohesiveness', 'word_index': [(23, 24)], 'id': 'P98-2243.8'}, 'e2': {'word': 'words', 'word_index': [(27, 27)], 'id': 'P98-2243.9'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:8': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:24': 'ENTITY', '27:27': 'ENTITYOTHER'}}	This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words .
We also present the results of an experiment about locating boundaries between a series of concatened texts.	boundaries	texts	part_whole	{'e1': {'word': 'boundaries', 'word_index': [(10, 10)], 'id': 'P98-2243.10'}, 'e2': {'word': 'texts', 'word_index': [(16, 16)], 'id': 'P98-2243.11'}, 'entity_replacement': {'10:10': 'ENTITY', '16:16': 'ENTITYOTHER'}}	We also present the results of an experiment about locating boundaries between a series of concatened texts .
In this work, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves.	model	sense assignment	usage	{'e1': {'word': 'model', 'word_index': [(7, 7)], 'id': 'W04-1908.1'}, 'e2': {'word': 'sense assignment', 'word_index': [(9, 10)], 'id': 'W04-1908.2'}, 'entity_replacement': {'7:7': 'ENTITY', '9:10': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	In this work , we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear , rather than to the words themselves .
In this work, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves.	words	contexts	model-feature	{'e1': {'word': 'words', 'word_index': [(21, 21)], 'id': 'W04-1908.5'}, 'e2': {'word': 'contexts', 'word_index': [(18, 18)], 'id': 'W04-1908.4'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '18:18': 'ENTITYOTHER', '21:21': 'ENTITY', '28:28': 'ENTITYUNRELATED'}}	In this work , we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear , rather than to the words themselves .
In this paper we describe a morphological analysis method based on a maximum entropy model.	maximum entropy model	morphological analysis method	usage	{'e1': {'word': 'maximum entropy model', 'word_index': [(12, 14)], 'id': 'W01-0512.2'}, 'e2': {'word': 'morphological analysis method', 'word_index': [(6, 8)], 'id': 'W01-0512.1'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '12:14': 'ENTITY'}}	In this paper we describe a morphological analysis method based on a maximum entropy model .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	model	method	usage	{'e1': {'word': 'model', 'word_index': [(4, 4)], 'id': 'W01-0512.4'}, 'e2': {'word': 'method', 'word_index': [(1, 1)], 'id': 'W01-0512.3'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:4': 'ENTITY', '11:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	lexical information	dictionary	part_whole	{'e1': {'word': 'lexical information', 'word_index': [(17, 18)], 'id': 'W01-0512.6'}, 'e2': {'word': 'dictionary', 'word_index': [(11, 11)], 'id': 'W01-0512.5'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '11:11': 'ENTITYOTHER', '17:18': 'ENTITY', '23:24': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	characteristics	unknown words	model-feature	{'e1': {'word': 'characteristics', 'word_index': [(28, 28)], 'id': 'W01-0512.8'}, 'e2': {'word': 'unknown words', 'word_index': [(23, 24)], 'id': 'W01-0512.7'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER', '28:28': 'ENTITY'}}	This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics .
Finally, we present Corporator, an Open Source software which was designed for collecting corpus from RSS feeds.	corpus	RSS feeds	part_whole	{'e1': {'word': 'corpus', 'word_index': [(15, 15)], 'id': 'W06-1707.10'}, 'e2': {'word': 'RSS feeds', 'word_index': [(17, 18)], 'id': 'W06-1707.11'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '7:9': 'ENTITYUNRELATED', '15:15': 'ENTITY', '17:18': 'ENTITYOTHER'}}	Finally , we present Corporator , an Open Source software which was designed for collecting corpus from RSS feeds .
Several SVMs are trained using information from pyramids of summary content units.	summary content units	SVMs	usage	{'e1': {'word': 'summary content units', 'word_index': [(9, 11)], 'id': 'P07-2015.4'}, 'e2': {'word': 'SVMs', 'word_index': [(1, 1)], 'id': 'P07-2015.3'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '9:11': 'ENTITY'}}	Several SVMs are trained using information from pyramids of summary content units .
Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.	performance	DUC-2005	compare	{'e1': {'word': 'performance', 'word_index': [(1, 1)], 'id': 'P07-2015.5'}, 'e2': {'word': 'DUC-2005', 'word_index': [(10, 10)], 'id': 'P07-2015.6'}, 'entity_replacement': {'1:1': 'ENTITY', '10:10': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '20:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	Their performance is compared with the best performing systems in DUC-2005 , using both ROUGE and auto Pan , an automatic scoring method for pyramid evaluation .
Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.	automatic scoring method	pyramid evaluation	usage	{'e1': {'word': 'automatic scoring method', 'word_index': [(20, 22)], 'id': 'P07-2015.9'}, 'e2': {'word': 'pyramid evaluation', 'word_index': [(24, 25)], 'id': 'P07-2015.10'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '20:22': 'ENTITY', '24:25': 'ENTITYOTHER'}}	Their performance is compared with the best performing systems in DUC-2005 , using both ROUGE and auto Pan , an automatic scoring method for pyramid evaluation .
We present a novel unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees.	unsupervised method	sentence compression	usage	{'e1': {'word': 'unsupervised method', 'word_index': [(4, 5)], 'id': 'W08-1105.1'}, 'e2': {'word': 'sentence compression', 'word_index': [(7, 8)], 'id': 'W08-1105.2'}, 'entity_replacement': {'4:5': 'ENTITY', '7:8': 'ENTITYOTHER', '13:15': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED'}}	We present a novel unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees .
We demonstrate that the choice of the parser affects the performance of the system.	parser	performance	result	{'e1': {'word': 'parser', 'word_index': [(7, 7)], 'id': 'W08-1105.8'}, 'e2': {'word': 'performance', 'word_index': [(10, 10)], 'id': 'W08-1105.9'}, 'entity_replacement': {'7:7': 'ENTITY', '10:10': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED'}}	We demonstrate that the choice of the parser affects the performance of the system .
We also apply the method to German and report the results of an evaluation with humans.	method	German	usage	{'e1': {'word': 'method', 'word_index': [(4, 4)], 'id': 'W08-1105.11'}, 'e2': {'word': 'German', 'word_index': [(6, 6)], 'id': 'W08-1105.12'}, 'entity_replacement': {'4:4': 'ENTITY', '6:6': 'ENTITYOTHER'}}	We also apply the method to German and report the results of an evaluation with humans .
Our approach even outperformed the hand coded system on NER in Spanish, and it achieved high accuracies in Portuguese.	NER	Spanish	usage	{'e1': {'word': 'NER', 'word_index': [(9, 9)], 'id': 'P05-2005.5'}, 'e2': {'word': 'Spanish', 'word_index': [(11, 11)], 'id': 'P05-2005.6'}, 'entity_replacement': {'9:9': 'ENTITY', '11:11': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED'}}	Our approach even outperformed the hand coded system on NER in Spanish , and it achieved high accuracies in Portuguese .
A karaka based approach to parsing of Indian languages is described.	parsing	Indian languages	usage	{'e1': {'word': 'parsing', 'word_index': [(5, 5)], 'id': 'C90-3005.2'}, 'e2': {'word': 'Indian languages', 'word_index': [(7, 8)], 'id': 'C90-3005.3'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '5:5': 'ENTITY', '7:8': 'ENTITYOTHER'}}	A karaka based approach to parsing of Indian languages is described .
It has been used for building a parser of Hindi for a prototype Machine Translation system.	parser	Hindi	usage	{'e1': {'word': 'parser', 'word_index': [(7, 7)], 'id': 'C90-3005.4'}, 'e2': {'word': 'Hindi', 'word_index': [(9, 9)], 'id': 'C90-3005.5'}, 'entity_replacement': {'7:7': 'ENTITY', '9:9': 'ENTITYOTHER', '12:15': 'ENTITYUNRELATED'}}	It has been used for building a parser of Hindi for a prototype Machine Translation system .
This paper presents our work on the detection of temporal information in web pages.	detection	web pages	usage	{'e1': {'word': 'detection', 'word_index': [(7, 7)], 'id': 'L08-1559.1'}, 'e2': {'word': 'web pages', 'word_index': [(12, 13)], 'id': 'L08-1559.3'}, 'entity_replacement': {'7:7': 'ENTITY', '9:10': 'ENTITYUNRELATED', '12:13': 'ENTITYOTHER'}}	This paper presents our work on the detection of temporal information in web pages .
The pages examined within the scope of this study were taken from the tourism sector and the temporal information in question is thus particular to this area.	temporal information	pages	part_whole	{'e1': {'word': 'temporal information', 'word_index': [(17, 18)], 'id': 'L08-1559.5'}, 'e2': {'word': 'pages', 'word_index': [(1, 1)], 'id': 'L08-1559.4'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '17:18': 'ENTITY'}}	The pages examined within the scope of this study were taken from the tourism sector and the temporal information in question is thus particular to this area .
The differences that exist between extraction from plain textual data and extraction from the web are brought to light.	extraction	plain textual data	usage	{'e1': {'word': 'extraction', 'word_index': [(5, 5)], 'id': 'L08-1559.6'}, 'e2': {'word': 'plain textual data', 'word_index': [(7, 9)], 'id': 'L08-1559.7'}, 'entity_replacement': {'5:5': 'ENTITY', '7:9': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED'}}	The differences that exist between extraction from plain textual data and extraction from the web are brought to light .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	patterns	symbolic approach	usage	{'e1': {'word': 'patterns', 'word_index': [(7, 7)], 'id': 'L08-1559.16'}, 'e2': {'word': 'symbolic approach', 'word_index': [(3, 4)], 'id': 'L08-1559.15'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '7:7': 'ENTITY', '9:9': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	We adopt a symbolic approach relying on patterns and rules for the detection , extraction and annotation of temporal expressions ; our method is based on the use of transducers .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	rules	detection	usage	{'e1': {'word': 'rules', 'word_index': [(9, 9)], 'id': 'L08-1559.17'}, 'e2': {'word': 'detection', 'word_index': [(12, 12)], 'id': 'L08-1559.18'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '9:9': 'ENTITY', '12:12': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	We adopt a symbolic approach relying on patterns and rules for the detection , extraction and annotation of temporal expressions ; our method is based on the use of transducers .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	annotation	temporal expressions	usage	{'e1': {'word': 'annotation', 'word_index': [(16, 16)], 'id': 'L08-1559.20'}, 'e2': {'word': 'temporal expressions', 'word_index': [(18, 19)], 'id': 'L08-1559.21'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:16': 'ENTITY', '18:19': 'ENTITYOTHER', '29:29': 'ENTITYUNRELATED'}}	We adopt a symbolic approach relying on patterns and rules for the detection , extraction and annotation of temporal expressions ; our method is based on the use of transducers .
The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German.	read and spontaneous speech	German Ph@ttSessionz speech database	part_whole	{'e1': {'word': 'read and spontaneous speech', 'word_index': [(10, 13)], 'id': 'L08-1196.2'}, 'e2': {'word': 'German Ph@ttSessionz speech database', 'word_index': [(5, 8)], 'id': 'L08-1196.1'}, 'entity_replacement': {'5:8': 'ENTITYOTHER', '10:13': 'ENTITY', '16:17': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German .
In this paper, we present a cross-sectional study of f0 measurements on this database.	cross-sectional study	f0 measurements	topic	{'e1': {'word': 'cross-sectional study', 'word_index': [(7, 9)], 'id': 'L08-1196.7'}, 'e2': {'word': 'f0 measurements', 'word_index': [(11, 12)], 'id': 'L08-1196.8'}, 'entity_replacement': {'7:9': 'ENTITY', '11:12': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED'}}	In this paper , we present a cross -sectional study of f0 measurements on this database .
Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers.	relative f0 variability	male and female speakers	model-feature	{'e1': {'word': 'relative f0 variability', 'word_index': [(16, 18)], 'id': 'L08-1196.11'}, 'e2': {'word': 'male and female speakers', 'word_index': [(20, 23)], 'id': 'L08-1196.12'}, 'entity_replacement': {'16:18': 'ENTITY', '20:23': 'ENTITYOTHER'}}	Furthermore , it shows that on a perceptive mel-scale , there is little difference in the relative f0 variability for male and female speakers .
The study provides statistically reliable voice parameters of adolescent speakers for German.	voice parameters	adolescent speakers	model-feature	{'e1': {'word': 'voice parameters', 'word_index': [(5, 6)], 'id': 'L08-1196.17'}, 'e2': {'word': 'adolescent speakers', 'word_index': [(8, 9)], 'id': 'L08-1196.18'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:6': 'ENTITY', '8:9': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED'}}	The study provides statistically reliable voice parameters of adolescent speakers for German .
The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability.	utterances	user input	part_whole	{'e1': {'word': 'utterances', 'word_index': [(16, 16)], 'id': 'L08-1196.22'}, 'e2': {'word': 'user input', 'word_index': [(13, 14)], 'id': 'L08-1196.21'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '16:16': 'ENTITY', '19:20': 'ENTITYUNRELATED'}}	The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability .
The platform will support researchers and engineers with well-developed and standardized resources and application tools thereby avoiding duplicate activities from scratch and amplifying overall effort on the domain.	standardized resources	platform	usage	{'e1': {'word': 'standardized resources', 'word_index': [(12, 13)], 'id': 'C96-2185.9'}, 'e2': {'word': 'platform', 'word_index': [(1, 1)], 'id': 'C96-2185.8'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '12:13': 'ENTITY', '15:16': 'ENTITYUNRELATED'}}	The platform will support researchers and engineers with well - developed and standardized resources and application tools thereby avoiding duplicate activities from scratch and amplifying overall effort on the domain .
We present in this article, as a part of aspectual operation system, a generation system of iterative expressions using a set of operators called iterative operators.	generation system	aspectual operation system	part_whole	{'e1': {'word': 'generation system', 'word_index': [(15, 16)], 'id': 'E83-1003.2'}, 'e2': {'word': 'aspectual operation system', 'word_index': [(10, 12)], 'id': 'E83-1003.1'}, 'entity_replacement': {'10:12': 'ENTITYOTHER', '15:16': 'ENTITY', '18:19': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	We present in this article , as a part of aspectual operation system , a generation system of iterative expressions using a set of operators called iterative operators .
The classification has been carried out especially in consideration of the durative / non-durative character of the denoted events and also in consideration of existence / non-existence of a culmination point (or a boundary) in the events.	durative / non-durative character	events	model-feature	{'e1': {'word': 'durative / non-durative character', 'word_index': [(11, 14)], 'id': 'E83-1003.11'}, 'e2': {'word': 'events', 'word_index': [(18, 18)], 'id': 'E83-1003.12'}, 'entity_replacement': {'11:14': 'ENTITY', '18:18': 'ENTITYOTHER', '34:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED'}}	The classification has been carried out especially in consideration of the durative / non-durative character of the denoted events and also in consideration of existence / non-existence of a culmination point ( or a boundary ) in the events .
In this paper, we show that time-synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.	cross-word triphones	time-synchronous one-pass decoding	usage	{'e1': {'word': 'cross-word triphones', 'word_index': [(15, 18)], 'id': 'H94-1080.16'}, 'e2': {'word': 'time-synchronous one-pass decoding', 'word_index': [(7, 13)], 'id': 'H94-1080.15'}, 'entity_replacement': {'7:13': 'ENTITYOTHER', '15:18': 'ENTITY', '21:23': 'ENTITYUNRELATED', '31:34': 'ENTITYUNRELATED'}}	In this paper , we show that time - synchronous one - pass decoding using cross - word triphones and a trigram language model can be implemented using a dynamically built tree - structured network .
In this paper, we show that time-synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.	tree-structured network	trigram language model	usage	{'e1': {'word': 'tree-structured network', 'word_index': [(31, 34)], 'id': 'H94-1080.18'}, 'e2': {'word': 'trigram language model', 'word_index': [(21, 23)], 'id': 'H94-1080.17'}, 'entity_replacement': {'7:13': 'ENTITYUNRELATED', '15:18': 'ENTITYUNRELATED', '21:23': 'ENTITYOTHER', '31:34': 'ENTITY'}}	In this paper , we show that time - synchronous one - pass decoding using cross - word triphones and a trigram language model can be implemented using a dynamically built tree - structured network .
It was included in the HTK large vocabulary speech recognition system used for the 1993 ARPA WSJ evaluation and experimental results are presented for that task.	HTK large vocabulary speech recognition system	1993 ARPA WSJ evaluation	usage	{'e1': {'word': 'HTK large vocabulary speech recognition system', 'word_index': [(5, 10)], 'id': 'H94-1080.20'}, 'e2': {'word': '1993 ARPA WSJ evaluation', 'word_index': [(14, 17)], 'id': 'H94-1080.21'}, 'entity_replacement': {'5:10': 'ENTITY', '14:17': 'ENTITYOTHER'}}	It was included in the HTK large vocabulary speech recognition system used for the 1993 ARPA WSJ evaluation and experimental results are presented for that task .
Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model.	parallel bi-lingual data	statistical machine translation systems	usage	{'e1': {'word': 'parallel bi-lingual data', 'word_index': [(9, 11)], 'id': 'D08-1090.2'}, 'e2': {'word': 'statistical machine translation systems', 'word_index': [(2, 5)], 'id': 'D08-1090.1'}, 'entity_replacement': {'2:5': 'ENTITYOTHER', '9:11': 'ENTITY', '15:16': 'ENTITYUNRELATED'}}	Traditionally , statistical machine translation systems have relied on parallel bi-lingual data to train a translation model .
While bi-lingual parallel data are expensive to generate, monolingual data are relatively common.	bi-lingual parallel data	monolingual data	compare	{'e1': {'word': 'bi-lingual parallel data', 'word_index': [(1, 3)], 'id': 'D08-1090.4'}, 'e2': {'word': 'monolingual data', 'word_index': [(9, 10)], 'id': 'D08-1090.5'}, 'entity_replacement': {'1:3': 'ENTITY', '9:10': 'ENTITYOTHER'}}	While bi-lingual parallel data are expensive to generate , monolingual data are relatively common .
Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language.	monolingual data	language model	usage	{'e1': {'word': 'monolingual data', 'word_index': [(1, 2)], 'id': 'D08-1090.6'}, 'e2': {'word': 'language model', 'word_index': [(14, 15)], 'id': 'D08-1090.7'}, 'entity_replacement': {'1:2': 'ENTITY', '14:15': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	Yet monolingual data have been under-utilized , having been used primarily for training a language model in the target language .
This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories.	monolingual target data	statistical machine translation system	usage	{'e1': {'word': 'monolingual target data', 'word_index': [(8, 10)], 'id': 'D08-1090.9'}, 'e2': {'word': 'statistical machine translation system', 'word_index': [(17, 20)], 'id': 'D08-1090.10'}, 'entity_replacement': {'8:10': 'ENTITY', '17:20': 'ENTITYOTHER'}}	This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories .
For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to thesource documents.	documents	source documents	compare	{'e1': {'word': 'documents', 'word_index': [(22, 22)], 'id': 'D08-1090.18'}, 'e2': {'word': 'source documents', 'word_index': [(29, 30)], 'id': 'D08-1090.19'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '22:22': 'ENTITY', '29:30': 'ENTITYOTHER'}}	For every source document that is to be translated , a large monolingual data set in the target language is searched for documents that might be comparable to the source documents .
These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document.	documents	MT system	usage	{'e1': {'word': 'documents', 'word_index': [(1, 1)], 'id': 'D08-1090.20'}, 'e2': {'word': 'MT system', 'word_index': [(8, 9)], 'id': 'D08-1090.21'}, 'entity_replacement': {'1:1': 'ENTITY', '8:9': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document .
Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system.	language and translation models	baseline system	compare	{'e1': {'word': 'language and translation models', 'word_index': [(7, 10)], 'id': 'D08-1090.23'}, 'e2': {'word': 'baseline system', 'word_index': [(16, 17)], 'id': 'D08-1090.24'}, 'entity_replacement': {'7:10': 'ENTITY', '16:17': 'ENTITYOTHER'}}	Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system .
This paper describes an unsupervised knowledge-lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus.	ambiguous word	corpus	part_whole	{'e1': {'word': 'ambiguous word', 'word_index': [(19, 20)], 'id': 'E06-2007.3'}, 'e2': {'word': 'corpus', 'word_index': [(26, 26)], 'id': 'E06-2007.4'}, 'entity_replacement': {'4:8': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '19:20': 'ENTITY', '26:26': 'ENTITYOTHER'}}	This paper describes an unsupervised knowledge - lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus .
This paper describes the Unisys MUC-3 text understanding system, a system based upon a three-tiered approach to text processing in which a powerful knowledge-based form of information retrieval plays a central role.	three-tiered approach	system	usage	{'e1': {'word': 'three-tiered approach', 'word_index': [(17, 20)], 'id': 'M91-1032.3'}, 'e2': {'word': 'system', 'word_index': [(13, 13)], 'id': 'M91-1032.2'}, 'entity_replacement': {'4:10': 'ENTITYUNRELATED', '13:13': 'ENTITYOTHER', '17:20': 'ENTITY', '22:23': 'ENTITYUNRELATED', '28:31': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED'}}	This paper describes the Unisys MUC - 3 text understanding system , a system based upon a three - tiered approach to text processing in which a powerful knowledge - based form of information retrieval plays a central role .
A decision was made to focus on the development of a knowledge-based information retrieval component, and this precluded the integration of Pundit into the prototype.	Pundit	prototype	part_whole	{'e1': {'word': 'Pundit', 'word_index': [(24, 24)], 'id': 'M91-1032.23'}, 'e2': {'word': 'prototype', 'word_index': [(27, 27)], 'id': 'M91-1032.24'}, 'entity_replacement': {'11:16': 'ENTITYUNRELATED', '24:24': 'ENTITY', '27:27': 'ENTITYOTHER'}}	A decision was made to focus on the development of a knowledge - based information retrieval component , and this precluded the integration of Pundit into the prototype .
ARBITER is a Prolog program that extracts assertions about macromolecular binding relationships from biomedical text.	macromolecular binding relationships	biomedical text	part_whole	{'e1': {'word': 'macromolecular binding relationships', 'word_index': [(9, 11)], 'id': 'A00-1026.3'}, 'e2': {'word': 'biomedical text', 'word_index': [(13, 14)], 'id': 'A00-1026.4'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '9:11': 'ENTITY', '13:14': 'ENTITYOTHER'}}	ARBITER is a Prolog program that extracts assertions about macromolecular binding relationships from biomedical text .
After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted.	ARBITER	MEDLINE abstracts	usage	{'e1': {'word': 'ARBITER', 'word_index': [(6, 6)], 'id': 'A00-1026.8'}, 'e2': {'word': 'MEDLINE abstracts', 'word_index': [(15, 16)], 'id': 'A00-1026.9'}, 'entity_replacement': {'6:6': 'ENTITY', '15:16': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED'}}	After discussing a formal evaluation of ARBITER , we report on its application to 491,000 MEDLINE abstracts , during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted .
After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted.	macro-molecular function	database	part_whole	{'e1': {'word': 'macro-molecular function', 'word_index': [(31, 32)], 'id': 'A00-1026.12'}, 'e2': {'word': 'database', 'word_index': [(29, 29)], 'id': 'A00-1026.11'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '29:29': 'ENTITYOTHER', '31:32': 'ENTITY'}}	After discussing a formal evaluation of ARBITER , we report on its application to 491,000 MEDLINE abstracts , during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted .
The resolution of lexical ambiguity is important for most natural language processing tasks, and a range of computational techniques have been proposed for its solution.	lexical ambiguity	natural language processing tasks	part_whole	{'e1': {'word': 'lexical ambiguity', 'word_index': [(3, 4)], 'id': 'H92-1046.2'}, 'e2': {'word': 'natural language processing tasks', 'word_index': [(9, 12)], 'id': 'H92-1046.3'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '3:4': 'ENTITY', '9:12': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	The resolution of lexical ambiguity is important for most natural language processing tasks , and a range of computational techniques have been proposed for its solution .
In this paper, we describe a method for lexical disambiguation of text using the definitions in a machine-readable dictionary together with the technique of simulated annealing.	definitions	lexical disambiguation	usage	{'e1': {'word': 'definitions', 'word_index': [(15, 15)], 'id': 'H92-1046.7'}, 'e2': {'word': 'lexical disambiguation', 'word_index': [(9, 10)], 'id': 'H92-1046.5'}, 'entity_replacement': {'9:10': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '15:15': 'ENTITY', '18:21': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	In this paper , we describe a method for lexical disambiguation of text using the definitions in a machine - readable dictionary together with the technique of simulated annealing .
Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.	hand coding	lexical entries	usage	{'e1': {'word': 'hand coding', 'word_index': [(25, 26)], 'id': 'H92-1046.23'}, 'e2': {'word': 'lexical entries', 'word_index': [(28, 29)], 'id': 'H92-1046.24'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '25:26': 'ENTITY', '28:29': 'ENTITYOTHER', '32:33': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED'}}	Our initial results on a sample set of 50 sentences are comparable to those of other researchers , and the fully automatic method requires no hand coding of lexical entries , or hand tagging of text .
Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.	hand tagging	text	usage	{'e1': {'word': 'hand tagging', 'word_index': [(32, 33)], 'id': 'H92-1046.25'}, 'e2': {'word': 'text', 'word_index': [(35, 35)], 'id': 'H92-1046.26'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '32:33': 'ENTITY', '35:35': 'ENTITYOTHER'}}	Our initial results on a sample set of 50 sentences are comparable to those of other researchers , and the fully automatic method requires no hand coding of lexical entries , or hand tagging of text .
To date, this array of formal and natural language processing technologies has been used to perform mass changes to legacy textual databases and to facilitate user interfacing to relational databases and software applications.	formal and natural language processing technologies	legacy textual databases	usage	{'e1': {'word': 'formal and natural language processing technologies', 'word_index': [(6, 11)], 'id': 'W97-0909.3'}, 'e2': {'word': 'legacy textual databases', 'word_index': [(20, 22)], 'id': 'W97-0909.4'}, 'entity_replacement': {'6:11': 'ENTITY', '20:22': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	To date , this array of formal and natural language processing technologies has been used to perform mass changes to legacy textual databases and to facilitate user interfacing to relational databases and software applications .
We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations.	Rhetorical Structure trees	parallel corpus	part_whole	{'e1': {'word': 'Rhetorical Structure trees', 'word_index': [(12, 14)], 'id': 'W00-1403.3'}, 'e2': {'word': 'parallel corpus', 'word_index': [(9, 10)], 'id': 'W00-1403.2'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '9:10': 'ENTITYOTHER', '12:14': 'ENTITY', '19:20': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations .
The approach to knowledge representation taken in a multi-modal multi-domain dialogue system - SmartKom - is presented.	knowledge representation	multi-modal multi-domain dialogue system - SmartKom -	usage	{'e1': {'word': 'knowledge representation', 'word_index': [(3, 4)], 'id': 'W03-0903.1'}, 'e2': {'word': 'multi-modal multi-domain dialogue system - SmartKom -', 'word_index': [(8, 14)], 'id': 'W03-0903.2'}, 'entity_replacement': {'3:4': 'ENTITY', '8:14': 'ENTITYOTHER'}}	The approach to knowledge representation taken in a multi-modal multi-domain dialogue system - SmartKom - is presented .
This paper presents intial work on a system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admission Test (LSAT) and the analytic section of the Graduate Record Exam (GRE).	logic puzzles	Law School Admission Test (LSAT)	part_whole	{'e1': {'word': 'logic puzzles', 'word_index': [(29, 30)], 'id': 'W04-0902.5'}, 'e2': {'word': 'Law School Admission Test (LSAT)', 'word_index': [(37, 43)], 'id': 'W04-0902.6'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '11:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '29:30': 'ENTITY', '37:43': 'ENTITYOTHER', '50:55': 'ENTITYUNRELATED'}}	This paper presents intial work on a system that bridges from robust , broad - coverage natural language processing to precise semantics and automated reasoning , focusing on solving logic puzzles drawn from sources such as the Law School Admission Test ( LSAT ) and the analytic section of the Graduate Record Exam ( GRE ) .
We highlight key challenges, and discuss the representations and performance of the prototype system.	performance	prototype system	model-feature	{'e1': {'word': 'performance', 'word_index': [(10, 10)], 'id': 'W04-0902.9'}, 'e2': {'word': 'prototype system', 'word_index': [(13, 14)], 'id': 'W04-0902.10'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '10:10': 'ENTITY', '13:14': 'ENTITYOTHER'}}	We highlight key challenges , and discuss the representations and performance of the prototype system .
In this paper, a new parsing model is proposed to formulate the complete chunking problem as a series of boundary detection subtasks.	chunking problem	parsing model	model-feature	{'e1': {'word': 'chunking problem', 'word_index': [(14, 15)], 'id': 'W06-0113.11'}, 'e2': {'word': 'parsing model', 'word_index': [(6, 7)], 'id': 'W06-0113.10'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '14:15': 'ENTITY', '20:21': 'ENTITYUNRELATED'}}	In this paper , a new parsing model is proposed to formulate the complete chunking problem as a series of boundary detection subtasks .
By applying SVM algorithm to these subtasks, we have achieved the best F-Score of 76.56% and 82.26% respectively.	SVM algorithm	F-Score	result	{'e1': {'word': 'SVM algorithm', 'word_index': [(2, 3)], 'id': 'W06-0113.17'}, 'e2': {'word': 'F-Score', 'word_index': [(13, 14)], 'id': 'W06-0113.18'}, 'entity_replacement': {'2:3': 'ENTITY', '13:14': 'ENTITYOTHER'}}	By applying SVM algorithm to these subtasks , we have achieved the best F- Score of 76.56 % and 82.26 % respectively .
In this paper, we introduce a WordNet-based measure of semantic relatedness by combining the structure and content of WordNet with co-occurrence information derived from raw text.	co-occurrence information	raw text	part_whole	{'e1': {'word': 'co-occurrence information', 'word_index': [(23, 24)], 'id': 'W06-2501.4'}, 'e2': {'word': 'raw text', 'word_index': [(27, 28)], 'id': 'W06-2501.5'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '23:24': 'ENTITY', '27:28': 'ENTITYOTHER'}}	In this paper , we introduce a WordNet - based measure of semantic relatedness by combining the structure and content of WordNet with co-occurrence information derived from raw text .
We use the co-occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet.	gloss vectors	concept	model-feature	{'e1': {'word': 'gloss vectors', 'word_index': [(13, 14)], 'id': 'W06-2501.8'}, 'e2': {'word': 'concept', 'word_index': [(18, 18)], 'id': 'W06-2501.9'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '8:10': 'ENTITYUNRELATED', '13:14': 'ENTITY', '18:18': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	We use the co-occurrence information along with the Word Net definitions to build gloss vectors corresponding to each concept in Word Net .
Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors.	Numeric scores of relatedness	concepts	model-feature	{'e1': {'word': 'Numeric scores of relatedness', 'word_index': [(0, 3)], 'id': 'W06-2501.11'}, 'e2': {'word': 'concepts', 'word_index': [(10, 10)], 'id': 'W06-2501.12'}, 'entity_replacement': {'0:3': 'ENTITY', '10:10': 'ENTITYOTHER', '14:17': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED'}}	Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors .
We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness.	semantic relatedness	word sense disambiguation algorithm	usage	{'e1': {'word': 'semantic relatedness', 'word_index': [(35, 36)], 'id': 'W06-2501.18'}, 'e2': {'word': 'word sense disambiguation algorithm', 'word_index': [(28, 31)], 'id': 'W06-2501.17'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '13:17': 'ENTITYUNRELATED', '28:31': 'ENTITYOTHER', '35:36': 'ENTITY'}}	We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness , and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness .
In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the cooccurrence information.	cooccurrence information	plain text corpus	part_whole	{'e1': {'word': 'cooccurrence information', 'word_index': [(22, 23)], 'id': 'W06-2501.23'}, 'e2': {'word': 'plain text corpus', 'word_index': [(13, 15)], 'id': 'W06-2501.22'}, 'entity_replacement': {'13:15': 'ENTITYOTHER', '22:23': 'ENTITY'}}	In addition , it can be adapted to different domains , since any plain text corpus can be used to derive the cooccurrence information .
This paper describes our system as used in the RTE3 task.	system	RTE3 task	usage	{'e1': {'word': 'system', 'word_index': [(4, 4)], 'id': 'W07-1403.1'}, 'e2': {'word': 'RTE3 task', 'word_index': [(9, 10)], 'id': 'W07-1403.2'}, 'entity_replacement': {'4:4': 'ENTITY', '9:10': 'ENTITYOTHER'}}	This paper describes our system as used in the RTE3 task .
The system maps premise and hypothesis pairs into an abstract knowledge representation (AKR) and then performs entailment and contradiction detection (ecd) on the resulting AKRs.	abstract knowledge representation (AKR)	premise and hypothesis pairs	model-feature	{'e1': {'word': 'abstract knowledge representation (AKR)', 'word_index': [(9, 14)], 'id': 'W07-1403.5'}, 'e2': {'word': 'premise and hypothesis pairs', 'word_index': [(3, 6)], 'id': 'W07-1403.4'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '3:6': 'ENTITYOTHER', '9:14': 'ENTITY', '18:24': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	The system maps premise and hypothesis pairs into an abstract knowledge representation ( AKR ) and then performs entailment and contradiction detection ( ecd ) on the resulting AKRs .
Two versions of ECD were used in RTE3, one with strict ECD and one with looser ECD.	ECD	RTE3	usage	{'e1': {'word': 'ECD', 'word_index': [(3, 3)], 'id': 'W07-1403.8'}, 'e2': {'word': 'RTE3', 'word_index': [(7, 7)], 'id': 'W07-1403.9'}, 'entity_replacement': {'3:3': 'ENTITY', '7:7': 'ENTITYOTHER', '11:12': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED'}}	Two versions of ECD were used in RTE3 , one with strict ECD and one with looser ECD .
We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax.	natural language strings	semantics	model-feature	{'e1': {'word': 'natural language strings', 'word_index': [(17, 19)], 'id': 'P07-1105.3'}, 'e2': {'word': 'semantics', 'word_index': [(23, 23)], 'id': 'P07-1105.4'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '17:19': 'ENTITY', '23:23': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED'}}	We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics , along with basic assumptions about natural language syntax .
In this paper, we propose a novel graph based sentence ranking algorithm, namely PNR2, for update summarization.	graph based sentence ranking algorithm	update summarization	usage	{'e1': {'word': 'graph based sentence ranking algorithm', 'word_index': [(8, 12)], 'id': 'C08-1062.7'}, 'e2': {'word': 'update summarization', 'word_index': [(18, 19)], 'id': 'C08-1062.9'}, 'entity_replacement': {'8:12': 'ENTITY', '15:15': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER'}}	In this paper , we propose a novel graph based sentence ranking algorithm , namely PNR2 , for update summarization .
This paper discusses the application of the Expectation-Maximization (EM) clustering algorithm to the task of Chinese verb sense discrimination.	Expectation-Maximization (EM) clustering algorithm	Chinese verb sense discrimination	usage	{'e1': {'word': 'Expectation-Maximization (EM) clustering algorithm', 'word_index': [(7, 14)], 'id': 'P04-1038.1'}, 'e2': {'word': 'Chinese verb sense discrimination', 'word_index': [(19, 22)], 'id': 'P04-1038.2'}, 'entity_replacement': {'7:14': 'ENTITY', '19:22': 'ENTITYOTHER'}}	This paper discusses the application of the Expectation - Maximization ( EM ) clustering algorithm to the task of Chinese verb sense discrimination .
The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.	rich linguistic features	model	usage	{'e1': {'word': 'rich linguistic features', 'word_index': [(3, 5)], 'id': 'P04-1038.4'}, 'e2': {'word': 'model', 'word_index': [(1, 1)], 'id': 'P04-1038.3'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '3:5': 'ENTITY', '8:12': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED'}}	The model utilized rich linguistic features that capture predicate - argument structure information of the target verbs .
The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.	predicate-argument structure information	target verbs	model-feature	{'e1': {'word': 'predicate-argument structure information', 'word_index': [(8, 12)], 'id': 'P04-1038.5'}, 'e2': {'word': 'target verbs', 'word_index': [(15, 16)], 'id': 'P04-1038.6'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '3:5': 'ENTITYUNRELATED', '8:12': 'ENTITY', '15:16': 'ENTITYOTHER'}}	The model utilized rich linguistic features that capture predicate - argument structure information of the target verbs .
A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.	semantic taxonomy	Chinese nouns	model-feature	{'e1': {'word': 'semantic taxonomy', 'word_index': [(1, 2)], 'id': 'P04-1038.7'}, 'e2': {'word': 'Chinese nouns', 'word_index': [(4, 5)], 'id': 'P04-1038.8'}, 'entity_replacement': {'1:2': 'ENTITY', '4:5': 'ENTITYOTHER', '14:17': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	A semantic taxonomy for Chinese nouns , which was built semi-automatically based on two electronic Chinese semantic dictionaries , was used to provide semantic features for the model .
A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.	semantic features	model	usage	{'e1': {'word': 'semantic features', 'word_index': [(23, 24)], 'id': 'P04-1038.10'}, 'e2': {'word': 'model', 'word_index': [(27, 27)], 'id': 'P04-1038.11'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:5': 'ENTITYUNRELATED', '14:17': 'ENTITYUNRELATED', '23:24': 'ENTITY', '27:27': 'ENTITYOTHER'}}	A semantic taxonomy for Chinese nouns , which was built semi-automatically based on two electronic Chinese semantic dictionaries , was used to provide semantic features for the model .
We further enhanced the model with certain fine-grained semantic categories called lexical sets.	fine-grained semantic categories	model	usage	{'e1': {'word': 'fine-grained semantic categories', 'word_index': [(7, 11)], 'id': 'P04-1038.20'}, 'e2': {'word': 'model', 'word_index': [(4, 4)], 'id': 'P04-1038.19'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '7:11': 'ENTITY', '13:14': 'ENTITYUNRELATED'}}	We further enhanced the model with certain fine - grained semantic categories called lexical sets .
Our results indicate that these lexical sets improve the model's performance for the three most challenging verbs chosen from the first set of experiments.	lexical sets	model	result	{'e1': {'word': 'lexical sets', 'word_index': [(5, 6)], 'id': 'P04-1038.22'}, 'e2': {'word': 'model', 'word_index': [(9, 9)], 'id': 'P04-1038.23'}, 'entity_replacement': {'5:6': 'ENTITY', '9:9': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED'}}	Our results indicate that these lexical sets improve the model 's performance for the three most challenging verbs chosen from the first set of experiments .
This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar.	dependency structure grammar	speech recognition and understanding	usage	{'e1': {'word': 'dependency structure grammar', 'word_index': [(15, 17)], 'id': 'C88-1082.3'}, 'e2': {'word': 'speech recognition and understanding', 'word_index': [(9, 12)], 'id': 'C88-1082.2'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '9:12': 'ENTITYOTHER', '15:17': 'ENTITY'}}	This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar .
After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships.	phoneme recognition	phrase recognition	usage	{'e1': {'word': 'phoneme recognition', 'word_index': [(8, 9)], 'id': 'C88-1082.9'}, 'e2': {'word': 'phrase recognition', 'word_index': [(4, 5)], 'id': 'C88-1082.8'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:5': 'ENTITYOTHER', '8:9': 'ENTITY', '12:12': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED', '35:38': 'ENTITYUNRELATED'}}	After speech processing and phrase recognition based on phoneme recognition , the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships .
A fast parsing algorithm using breadth-first search is also proposed.	breadth-first search	parsing algorithm	usage	{'e1': {'word': 'breadth-first search', 'word_index': [(5, 7)], 'id': 'C88-1082.18'}, 'e2': {'word': 'parsing algorithm', 'word_index': [(2, 3)], 'id': 'C88-1082.17'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '5:7': 'ENTITY'}}	A fast parsing algorithm using breadth- first search is also proposed .
The predictor pre-selects the phrase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing.	transition rules	predictor	usage	{'e1': {'word': 'transition rules', 'word_index': [(7, 8)], 'id': 'C88-1082.21'}, 'e2': {'word': 'predictor', 'word_index': [(1, 1)], 'id': 'C88-1082.19'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:5': 'ENTITYUNRELATED', '7:8': 'ENTITY', '12:13': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	The predictor pre-selects the phrase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing .
The experimental results show that it greatly increases the accuracy of speech recognitions, and the breadth-first parsing algorithm and predictor increase processing speed.	predictor	processing speed	result	{'e1': {'word': 'predictor', 'word_index': [(21, 21)], 'id': 'C88-1082.29'}, 'e2': {'word': 'processing speed', 'word_index': [(23, 24)], 'id': 'C88-1082.30'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '16:19': 'ENTITYUNRELATED', '21:21': 'ENTITY', '23:24': 'ENTITYOTHER'}}	The experimental results show that it greatly increases the accuracy of speech recognitions , and the breadth- first parsing algorithm and predictor increase processing speed .
In the EU-funded project, QALL-ME, a domain-specific ontology was developed and applied for question answering in the domain of tourism, along with the assistance of two upper ontologies for concept expansion and reasoning.	domain-specific ontology	question answering	usage	{'e1': {'word': 'domain-specific ontology', 'word_index': [(12, 13)], 'id': 'L08-1178.6'}, 'e2': {'word': 'question answering', 'word_index': [(19, 20)], 'id': 'L08-1178.7'}, 'entity_replacement': {'7:9': 'ENTITYUNRELATED', '12:13': 'ENTITY', '19:20': 'ENTITYOTHER', '34:34': 'ENTITYUNRELATED', '36:39': 'ENTITYUNRELATED'}}	In the EU - funded project , QALL - ME , a domain-specific ontology was developed and applied for question answering in the domain of tourism , along with the assistance of two upper ontologies for concept expansion and reasoning .
The design of the ontology is presented in the paper, and a semi-automatic alignment procedure is described with some alignment results given as well.	semi-automatic alignment procedure	alignment results	result	{'e1': {'word': 'semi-automatic alignment procedure', 'word_index': [(13, 15)], 'id': 'L08-1178.14'}, 'e2': {'word': 'alignment results', 'word_index': [(20, 21)], 'id': 'L08-1178.15'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '13:15': 'ENTITY', '20:21': 'ENTITYOTHER'}}	The design of the ontology is presented in the paper , and a semi-automatic alignment procedure is described with some alignment results given as well .
Furthermore, the aligned ontology was used to semantically annotate original data obtained from the tourism web sites and natural language questions.	natural language questions	data	part_whole	{'e1': {'word': 'natural language questions', 'word_index': [(19, 21)], 'id': 'L08-1178.18'}, 'e2': {'word': 'data', 'word_index': [(11, 11)], 'id': 'L08-1178.17'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '11:11': 'ENTITYOTHER', '19:21': 'ENTITY'}}	Furthermore , the aligned ontology was used to semantically annotate original data obtained from the tourism web sites and natural language questions .
The storage schema of the annotated data and the data access method for retrieving answers from the annotated data are also reported in the paper.	data access method	annotated data	usage	{'e1': {'word': 'data access method', 'word_index': [(9, 11)], 'id': 'L08-1178.21'}, 'e2': {'word': 'annotated data', 'word_index': [(17, 18)], 'id': 'L08-1178.22'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '9:11': 'ENTITY', '17:18': 'ENTITYOTHER'}}	The storage schema of the annotated data and the data access method for retrieving answers from the annotated data are also reported in the paper .
In particular we discuss a natural language generation system that is composed of SPoT, a trainable sentence planner, and FERGUS, a stochastic surface realizer.	SPoT	natural language generation system	part_whole	{'e1': {'word': 'SPoT', 'word_index': [(13, 13)], 'id': 'C02-1138.9'}, 'e2': {'word': 'natural language generation system', 'word_index': [(5, 8)], 'id': 'C02-1138.8'}, 'entity_replacement': {'5:8': 'ENTITYOTHER', '13:13': 'ENTITY', '17:18': 'ENTITYUNRELATED', '21:26': 'ENTITYUNRELATED'}}	In particular we discuss a natural language generation system that is composed of SPoT , a trainable sentence planner , and FERGUS , a stochastic surface realizer .
We show how these stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system.	NLG components	real-time dialog system	part_whole	{'e1': {'word': 'NLG components', 'word_index': [(29, 30)], 'id': 'C02-1138.13'}, 'e2': {'word': 'real-time dialog system', 'word_index': [(36, 40)], 'id': 'C02-1138.14'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '29:30': 'ENTITY', '36:40': 'ENTITYOTHER'}}	We show how these stochastic NLG components can be made to work together , that they can be ported to new domains with apparent ease , and that such NLG components can be integrated in a real - time dialog system .
In the current work, we produce a manual segmentation of laughter in a large corpus of interactive multi-party seminars, which promises to be a valuable resource for acoustic modeling purposes.	interactive multi-party seminars	corpus	part_whole	{'e1': {'word': 'interactive multi-party seminars', 'word_index': [(17, 19)], 'id': 'L08-1016.10'}, 'e2': {'word': 'corpus', 'word_index': [(15, 15)], 'id': 'L08-1016.9'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '15:15': 'ENTITYOTHER', '17:19': 'ENTITY', '29:30': 'ENTITYUNRELATED'}}	In the current work , we produce a manual segmentation of laughter in a large corpus of interactive multi-party seminars , which promises to be a valuable resource for acoustic modeling purposes .
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further.	grammar PCFG*	extended GLR parsing algorithm	usage	{'e1': {'word': 'grammar PCFG*', 'word_index': [(9, 11)], 'id': 'C02-2028.2'}, 'e2': {'word': 'extended GLR parsing algorithm', 'word_index': [(4, 7)], 'id': 'C02-2028.1'}, 'entity_replacement': {'4:7': 'ENTITYOTHER', '9:11': 'ENTITY', '16:20': 'ENTITYUNRELATED'}}	This paper presents an extended GLR parsing algorithm with grammar PCFG * that is based on Tomita 's GLR parsing algorithm and extends it further .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	PCFG	grammar PCFG*	usage	{'e1': {'word': 'PCFG', 'word_index': [(12, 12)], 'id': 'C02-2028.5'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(5, 7)], 'id': 'C02-2028.4'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '12:12': 'ENTITY', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	We also define a new grammar PCFG * that is based on PCFG and assigns not only probability but also frequency associated with each rule .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	frequency	rule	model-feature	{'e1': {'word': 'frequency', 'word_index': [(20, 20)], 'id': 'C02-2028.7'}, 'e2': {'word': 'rule', 'word_index': [(24, 24)], 'id': 'C02-2028.8'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITY', '24:24': 'ENTITYOTHER'}}	We also define a new grammar PCFG * that is based on PCFG and assigns not only probability but also frequency associated with each rule .
So our syntactic parsing system is implemented based on rule-based approach and statistics approach.	rule-based approach	syntactic parsing system	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(9, 12)], 'id': 'C02-2028.10'}, 'e2': {'word': 'syntactic parsing system', 'word_index': [(2, 4)], 'id': 'C02-2028.9'}, 'entity_replacement': {'2:4': 'ENTITYOTHER', '9:12': 'ENTITY', '14:15': 'ENTITYUNRELATED'}}	So our syntactic parsing system is implemented based on rule - based approach and statistics approach .
In this paper, we discuss lemma identification in Japanese morphological analysis, which is crucial for a proper formulation of morphological analysis that benefits not only NLP researchers but also corpus linguists.	lemma identification	Japanese morphological analysis	usage	{'e1': {'word': 'lemma identification', 'word_index': [(6, 7)], 'id': 'L08-1535.1'}, 'e2': {'word': 'Japanese morphological analysis', 'word_index': [(9, 11)], 'id': 'L08-1535.2'}, 'entity_replacement': {'6:7': 'ENTITY', '9:11': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED'}}	In this paper , we discuss lemma identification in Japanese morphological analysis , which is crucial for a proper formulation of morphological analysis that benefits not only NLP researchers but also corpus linguists .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	words	vocabulary	part_whole	{'e1': {'word': 'words', 'word_index': [(15, 15)], 'id': 'L08-1535.8'}, 'e2': {'word': 'vocabulary', 'word_index': [(10, 10)], 'id': 'L08-1535.6'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '10:10': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '15:15': 'ENTITY', '28:29': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED', '46:46': 'ENTITYUNRELATED', '49:49': 'ENTITYUNRELATED', '53:53': 'ENTITYUNRELATED'}}	Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins , it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and / or meanings .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	writing form	lemma	model-feature	{'e1': {'word': 'writing form', 'word_index': [(28, 29)], 'id': 'L08-1535.9'}, 'e2': {'word': 'lemma', 'word_index': [(34, 34)], 'id': 'L08-1535.10'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '28:29': 'ENTITY', '34:34': 'ENTITYOTHER', '39:40': 'ENTITYUNRELATED', '46:46': 'ENTITYUNRELATED', '49:49': 'ENTITYUNRELATED', '53:53': 'ENTITYUNRELATED'}}	Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins , it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and / or meanings .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	writing form	lemmas	model-feature	{'e1': {'word': 'writing form', 'word_index': [(39, 40)], 'id': 'L08-1535.11'}, 'e2': {'word': 'lemmas', 'word_index': [(46, 46)], 'id': 'L08-1535.12'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '39:40': 'ENTITY', '46:46': 'ENTITYOTHER', '49:49': 'ENTITYUNRELATED', '53:53': 'ENTITYUNRELATED'}}	Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins , it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and / or meanings .
The mapping from a writing form onto a lemma is important in linguistic analysis of corpora.	linguistic analysis	corpora	topic	{'e1': {'word': 'linguistic analysis', 'word_index': [(12, 13)], 'id': 'L08-1535.17'}, 'e2': {'word': 'corpora', 'word_index': [(15, 15)], 'id': 'L08-1535.18'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '12:13': 'ENTITY', '15:15': 'ENTITYOTHER'}}	The mapping from a writing form onto a lemma is important in linguistic analysis of corpora .
The current study focuses on disambiguation of heteronyms, words with the same writing form but with different word forms.	disambiguation	heteronyms	usage	{'e1': {'word': 'disambiguation', 'word_index': [(5, 5)], 'id': 'L08-1535.19'}, 'e2': {'word': 'heteronyms', 'word_index': [(7, 7)], 'id': 'L08-1535.20'}, 'entity_replacement': {'5:5': 'ENTITY', '7:7': 'ENTITYOTHER', '9:9': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED'}}	The current study focuses on disambiguation of heteronyms , words with the same writing form but with different word forms .
To resolve heteronym ambiguity, we make use of goshu information, the classification of words based on their origin.	origin	words	model-feature	{'e1': {'word': 'origin', 'word_index': [(19, 19)], 'id': 'L08-1535.28'}, 'e2': {'word': 'words', 'word_index': [(15, 15)], 'id': 'L08-1535.27'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:15': 'ENTITYOTHER', '19:19': 'ENTITY'}}	To resolve heteronym ambiguity , we make use of goshu information , the classification of words based on their origin .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	words	goshu classes	part_whole	{'e1': {'word': 'words', 'word_index': [(5, 5)], 'id': 'L08-1535.29'}, 'e2': {'word': 'goshu classes', 'word_index': [(8, 9)], 'id': 'L08-1535.30'}, 'entity_replacement': {'5:5': 'ENTITY', '8:9': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED'}}	Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes , we employ a statistical model based on CRFs using goshu information .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	words	classes	part_whole	{'e1': {'word': 'words', 'word_index': [(19, 19)], 'id': 'L08-1535.32'}, 'e2': {'word': 'classes', 'word_index': [(22, 22)], 'id': 'L08-1535.33'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '19:19': 'ENTITY', '22:22': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED'}}	Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes , we employ a statistical model based on CRFs using goshu information .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	CRFs	statistical model	usage	{'e1': {'word': 'CRFs', 'word_index': [(31, 31)], 'id': 'L08-1535.35'}, 'e2': {'word': 'statistical model', 'word_index': [(27, 28)], 'id': 'L08-1535.34'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '27:28': 'ENTITYOTHER', '31:31': 'ENTITY', '33:34': 'ENTITYUNRELATED'}}	Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes , we employ a statistical model based on CRFs using goshu information .
Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.	goshu information	performance	result	{'e1': {'word': 'goshu information', 'word_index': [(7, 8)], 'id': 'L08-1535.38'}, 'e2': {'word': 'performance', 'word_index': [(12, 12)], 'id': 'L08-1535.39'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '7:8': 'ENTITY', '12:12': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:28': 'ENTITYUNRELATED'}}	Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification , suggesting that goshu information solves the lemma identification task very effectively .
Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.	information	lemma identification task	usage	{'e1': {'word': 'information', 'word_index': [(23, 23)], 'id': 'L08-1535.42'}, 'e2': {'word': 'lemma identification task', 'word_index': [(26, 28)], 'id': 'L08-1535.43'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '23:23': 'ENTITY', '26:28': 'ENTITYOTHER'}}	Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification , suggesting that goshu information solves the lemma identification task very effectively .
An event detection algorithm identifies the collocations that may cause an event in a specific timestamp.	event detection algorithm	collocations	usage	{'e1': {'word': 'event detection algorithm', 'word_index': [(1, 3)], 'id': 'L08-1003.6'}, 'e2': {'word': 'collocations', 'word_index': [(6, 6)], 'id': 'L08-1003.7'}, 'entity_replacement': {'1:3': 'ENTITY', '6:6': 'ENTITYOTHER'}}	An event detection algorithm identifies the collocations that may cause an event in a specific timestamp .
An event summarization algorithm retrieves a set of collocations which describe an event.	event summarization algorithm	collocations	usage	{'e1': {'word': 'event summarization algorithm', 'word_index': [(1, 3)], 'id': 'L08-1003.8'}, 'e2': {'word': 'collocations', 'word_index': [(8, 8)], 'id': 'L08-1003.9'}, 'entity_replacement': {'1:3': 'ENTITY', '8:8': 'ENTITYOTHER'}}	An event summarization algorithm retrieves a set of collocations which describe an event .
We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis (LSA) to predict team performance.	Latent Semantic Analysis (LSA)	tagging	usage	{'e1': {'word': 'Latent Semantic Analysis (LSA)', 'word_index': [(11, 16)], 'id': 'N04-4025.2'}, 'e2': {'word': 'tagging', 'word_index': [(7, 7)], 'id': 'N04-4025.1'}, 'entity_replacement': {'7:7': 'ENTITYOTHER', '11:16': 'ENTITY'}}	We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis ( LSA ) to predict team performance .
A huge amount of translation work needs to be done when creating and updating technical documentation.	translation work	technical documentation	usage	{'e1': {'word': 'translation work', 'word_index': [(4, 5)], 'id': 'A94-1044.3'}, 'e2': {'word': 'technical documentation', 'word_index': [(14, 15)], 'id': 'A94-1044.4'}, 'entity_replacement': {'4:5': 'ENTITY', '14:15': 'ENTITYOTHER'}}	A huge amount of translation work needs to be done when creating and updating technical documentation .
The objective of this project is a pilot study of several new ideas for the automatic adaptation and improvement of natural language processing (NLP) systems.	pilot study	automatic adaptation	topic	{'e1': {'word': 'pilot study', 'word_index': [(7, 8)], 'id': 'H91-1079.1'}, 'e2': {'word': 'automatic adaptation', 'word_index': [(15, 16)], 'id': 'H91-1079.2'}, 'entity_replacement': {'7:8': 'ENTITY', '15:16': 'ENTITYOTHER', '18:18': 'ENTITYUNRELATED', '20:26': 'ENTITYUNRELATED'}}	The objective of this project is a pilot study of several new ideas for the automatic adaptation and improvement of natural language processing ( NLP ) systems .
The effort focuses particularly on automatically inferring the meaning of new words in context and on developing partial interpretations of language that is either fragmentary or beyond the capability of the NLP system to understand.	meaning	words	model-feature	{'e1': {'word': 'meaning', 'word_index': [(8, 8)], 'id': 'H91-1079.5'}, 'e2': {'word': 'words', 'word_index': [(11, 11)], 'id': 'H91-1079.6'}, 'entity_replacement': {'8:8': 'ENTITY', '11:11': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED'}}	The effort focuses particularly on automatically inferring the meaning of new words in context and on developing partial interpretations of language that is either fragmentary or beyond the capability of the NLP system to understand .
The NLP system uses large annotated corpora, such as those being developed under the DARPA-funded TREE-BANK project at the University of Pennsylvania, to adapt by acquiring syntactic and semantic information from the annotated examples.	large annotated corpora	NLP system	usage	{'e1': {'word': 'large annotated corpora', 'word_index': [(4, 6)], 'id': 'H91-1079.13'}, 'e2': {'word': 'NLP system', 'word_index': [(1, 2)], 'id': 'H91-1079.12'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '4:6': 'ENTITY', '15:19': 'ENTITYUNRELATED', '30:33': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED'}}	The NLP system uses large annotated corpora , such as those being developed under the DARPA - funded TREE-BANK project at the University of Pennsylvania , to adapt by acquiring syntactic and semantic information from the annotated examples .
The NLP system uses large annotated corpora, such as those being developed under the DARPA-funded TREE-BANK project at the University of Pennsylvania, to adapt by acquiring syntactic and semantic information from the annotated examples.	syntactic and semantic information	annotated examples	part_whole	{'e1': {'word': 'syntactic and semantic information', 'word_index': [(30, 33)], 'id': 'H91-1079.15'}, 'e2': {'word': 'annotated examples', 'word_index': [(36, 37)], 'id': 'H91-1079.16'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:6': 'ENTITYUNRELATED', '15:19': 'ENTITYUNRELATED', '30:33': 'ENTITY', '36:37': 'ENTITYOTHER'}}	The NLP system uses large annotated corpora , such as those being developed under the DARPA - funded TREE-BANK project at the University of Pennsylvania , to adapt by acquiring syntactic and semantic information from the annotated examples .
Statistical language modeling, based on probability estimates derived from the large corpora, will provide a means of ranking alternative interpretations of fragments.	probability estimates	Statistical language modeling	usage	{'e1': {'word': 'probability estimates', 'word_index': [(6, 7)], 'id': 'H91-1079.18'}, 'e2': {'word': 'Statistical language modeling', 'word_index': [(0, 2)], 'id': 'H91-1079.17'}, 'entity_replacement': {'0:2': 'ENTITYOTHER', '6:7': 'ENTITY', '11:12': 'ENTITYUNRELATED'}}	Statistical language modeling , based on probability estimates derived from the large corpora , will provide a means of ranking alternative interpretations of fragments .
Lexicalized concepts are organized by semantic relations (synonymy, antonymy, hyponymy, meronymy, etc.)	semantic relations	Lexicalized concepts	model-feature	{'e1': {'word': 'semantic relations', 'word_index': [(5, 6)], 'id': 'H92-1116.6'}, 'e2': {'word': 'Lexicalized concepts', 'word_index': [(0, 1)], 'id': 'H92-1116.5'}, 'entity_replacement': {'0:1': 'ENTITYOTHER', '5:6': 'ENTITY', '8:8': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED'}}	Lexicalized concepts are organized by semantic relations ( synonymy , antonymy , hyponymy , meronymy , etc. )
Work under this grant is intended to extend and upgrade WordNet, to make it generally available, and to develop it as a tool for use in practical applications.	WordNet	applications	usage	{'e1': {'word': 'WordNet', 'word_index': [(10, 10)], 'id': 'H92-1116.14'}, 'e2': {'word': 'applications', 'word_index': [(29, 29)], 'id': 'H92-1116.15'}, 'entity_replacement': {'10:10': 'ENTITY', '29:29': 'ENTITYOTHER'}}	Work under this grant is intended to extend and upgrade WordNet , to make it generally available , and to develop it as a tool for use in practical applications .
In order to make it available for information retrieval and machine translation, a system is being developed English text as input and automatically gives as output the same text augmented by syntactic and semantic anotations that disambiguate all of the substantive words.	syntactic and semantic anotations	substantive words	model-feature	{'e1': {'word': 'syntactic and semantic anotations', 'word_index': [(32, 35)], 'id': 'H92-1116.20'}, 'e2': {'word': 'substantive words', 'word_index': [(41, 42)], 'id': 'H92-1116.21'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '32:35': 'ENTITY', '41:42': 'ENTITYOTHER'}}	In order to make it available for information retrieval and machine translation , a system is being developed English text as input and automatically gives as output the same text augmented by syntactic and semantic anotations that disambiguate all of the substantive words .
Initially, the semantic tagging is being done manually so that we can (1) obtain extensive experience with the tagging process and (2) create a database of correctly tagged text for use in testing proposals for automatic sense disambiguation.	text	database	part_whole	{'e1': {'word': 'text', 'word_index': [(33, 33)], 'id': 'H92-1116.25'}, 'e2': {'word': 'database', 'word_index': [(29, 29)], 'id': 'H92-1116.24'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '29:29': 'ENTITYOTHER', '33:33': 'ENTITY', '40:42': 'ENTITYUNRELATED'}}	Initially , the semantic tagging is being done manually so that we can ( 1 ) obtain extensive experience with the tagging process and ( 2 ) create a database of correctly tagged text for use in testing proposals for automatic sense disambiguation .
Review previous designs involving TIPSTER technology, to support you design process.Determine if your application can benefit from upgrading to advanced TIPSTER technology that has been developed since your application was implemented.	TIPSTER technology	application	usage	{'e1': {'word': 'TIPSTER technology', 'word_index': [(23, 24)], 'id': 'X96-1060.8'}, 'e2': {'word': 'application', 'word_index': [(16, 16)], 'id': 'X96-1060.7'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '16:16': 'ENTITYOTHER', '23:24': 'ENTITY', '31:31': 'ENTITYUNRELATED'}}	Review previous designs involving TIPSTER technology , to support you design process . Determine if your application can benefit from upgrading to advanced TIPSTER technology that has been developed since your application was implemented .
Accurate lemmatization of German nouns mandates the use of a lexicon.	lemmatization	German nouns	usage	{'e1': {'word': 'lemmatization', 'word_index': [(1, 1)], 'id': 'H05-1080.1'}, 'e2': {'word': 'German nouns', 'word_index': [(3, 4)], 'id': 'H05-1080.2'}, 'entity_replacement': {'1:1': 'ENTITY', '3:4': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED'}}	Accurate lemmatization of German nouns mandates the use of a lexicon .
We present a self-learning lemmatizer capable of automatically creating a full-form lexicon by processing German documents.	self-learning lemmatizer	German documents	usage	{'e1': {'word': 'self-learning lemmatizer', 'word_index': [(3, 6)], 'id': 'H05-1080.5'}, 'e2': {'word': 'German documents', 'word_index': [(18, 19)], 'id': 'H05-1080.7'}, 'entity_replacement': {'3:6': 'ENTITY', '12:15': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER'}}	We present a self - learning lemmatizer capable of automatically creating a full - form lexicon by processing German documents .
In this paper we describe how Why-Atlas creates and utilizes a proof-based representation of student essays.	proof-based representation	Why-Atlas	usage	{'e1': {'word': 'proof-based representation', 'word_index': [(13, 16)], 'id': 'W02-0211.6'}, 'e2': {'word': 'Why-Atlas', 'word_index': [(6, 8)], 'id': 'W02-0211.5'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '13:16': 'ENTITY'}}	In this paper we describe how Why - Atlas creates and utilizes a proof - based representation of student essays .
We describe how it creates the proof given the output of sentence-level understanding, how it uses the proofs to give students feedback, some preliminary runtime measures, and the work we are currently doing to derive additional benefits from a proof-based approach for tutoring applications.	proof-based approach	tutoring applications	usage	{'e1': {'word': 'proof-based approach', 'word_index': [(44, 47)], 'id': 'W02-0211.10'}, 'e2': {'word': 'tutoring applications', 'word_index': [(49, 50)], 'id': 'W02-0211.11'}, 'entity_replacement': {'11:14': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '44:47': 'ENTITY', '49:50': 'ENTITYOTHER'}}	We describe how it creates the proof given the output of sentence - level understanding , how it uses the proofs to give students feedback , some preliminary runtime measures , and the work we are currently doing to derive additional benefits from a proof - based approach for tutoring applications .
Compared with syntactic knowledge, semantic knowledge is more difficult to annotate, for ambiguity problem is more serious.	syntactic knowledge	semantic knowledge	compare	{'e1': {'word': 'syntactic knowledge', 'word_index': [(2, 3)], 'id': 'W03-1712.13'}, 'e2': {'word': 'semantic knowledge', 'word_index': [(5, 6)], 'id': 'W03-1712.14'}, 'entity_replacement': {'2:3': 'ENTITY', '5:6': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED'}}	Compared with syntactic knowledge , semantic knowledge is more difficult to annotate , for ambiguity problem is more serious .
Finally, we will compare our corpus with other well-known corpora.	corpus	corpora	compare	{'e1': {'word': 'corpus', 'word_index': [(6, 6)], 'id': 'W03-1712.17'}, 'e2': {'word': 'corpora', 'word_index': [(12, 12)], 'id': 'W03-1712.18'}, 'entity_replacement': {'6:6': 'ENTITY', '12:12': 'ENTITYOTHER'}}	Finally , we will compare our corpus with other well - known corpora .
These techniques, developed within the Augmented Transition Network (ATN) model, are shown to be adequate to handle many of these cases.	techniques	Augmented Transition Network (ATN) model	usage	{'e1': {'word': 'techniques', 'word_index': [(1, 1)], 'id': 'J81-2002.7'}, 'e2': {'word': 'Augmented Transition Network (ATN) model', 'word_index': [(6, 12)], 'id': 'J81-2002.8'}, 'entity_replacement': {'1:1': 'ENTITY', '6:12': 'ENTITYOTHER'}}	These techniques , developed within the Augmented Transition Network ( ATN ) model , are shown to be adequate to handle many of these cases .
Once identified, reference chains can be extended across segment boundaries, thus enabling the application of CT over the entire discourse.	CT	discourse	usage	{'e1': {'word': 'CT', 'word_index': [(17, 17)], 'id': 'P98-1044.13'}, 'e2': {'word': 'discourse', 'word_index': [(21, 21)], 'id': 'P98-1044.14'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '17:17': 'ENTITY', '21:21': 'ENTITYOTHER'}}	Once identified , reference chains can be extended across segment boundaries , thus enabling the application of CT over the entire discourse .
We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains.	CT	global discourse	usage	{'e1': {'word': 'CT', 'word_index': [(15, 15)], 'id': 'P98-1044.17'}, 'e2': {'word': 'global discourse', 'word_index': [(20, 21)], 'id': 'P98-1044.18'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '15:15': 'ENTITY', '20:21': 'ENTITYOTHER', '25:25': 'ENTITYUNRELATED'}}	We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains .
We also define a discourse smoothness index which can be used to compare different discourse structures and interpretations, and show how VT can be used to abstract a span of text in the context of the whole discourse.	discourse smoothness index	discourse structures and interpretations	usage	{'e1': {'word': 'discourse smoothness index', 'word_index': [(4, 6)], 'id': 'P98-1044.20'}, 'e2': {'word': 'discourse structures and interpretations', 'word_index': [(14, 17)], 'id': 'P98-1044.21'}, 'entity_replacement': {'4:6': 'ENTITY', '14:17': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED'}}	We also define a discourse smoothness index which can be used to compare different discourse structures and interpretations , and show how VT can be used to abstract a span of text in the context of the whole discourse .
HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports in order to satisfy a given scenario.	natural language	exploratory questions	model-feature	{'e1': {'word': 'natural language', 'word_index': [(18, 19)], 'id': 'W04-2507.7'}, 'e2': {'word': 'exploratory questions', 'word_index': [(15, 16)], 'id': 'W04-2507.6'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '3:7': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '15:16': 'ENTITYOTHER', '18:19': 'ENTITY', '23:24': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED'}}	HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports in order to satisfy a given scenario .
The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space.	data-driven semantics	system	usage	{'e1': {'word': 'data-driven semantics', 'word_index': [(4, 5)], 'id': 'W04-2507.12'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'W04-2507.11'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:5': 'ENTITY', '13:13': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space .
One is that it resulted in the first freely distributable corpus of fully anonymized clinical text.	fully anonymized clinical text	corpus	part_whole	{'e1': {'word': 'fully anonymized clinical text', 'word_index': [(12, 15)], 'id': 'W07-1013.4'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'W07-1013.3'}, 'entity_replacement': {'10:10': 'ENTITYOTHER', '12:15': 'ENTITY'}}	One is that it resulted in the first freely distributable corpus of fully anonymized clinical text .
The other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels.	set of labels	categorization	usage	{'e1': {'word': 'set of labels', 'word_index': [(20, 22)], 'id': 'W07-1013.7'}, 'e2': {'word': 'categorization', 'word_index': [(11, 11)], 'id': 'W07-1013.6'}, 'entity_replacement': {'11:11': 'ENTITYOTHER', '20:22': 'ENTITY'}}	The other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels .
Many systems performed at levels approaching the inter-coder agreement, suggesting that human-like performance on this task is within the reach of currently available technologies.	human-like performance	currently available technologies	compare	{'e1': {'word': 'human-like performance', 'word_index': [(14, 17)], 'id': 'W07-1013.12'}, 'e2': {'word': 'currently available technologies', 'word_index': [(26, 28)], 'id': 'W07-1013.13'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '14:17': 'ENTITY', '26:28': 'ENTITYOTHER'}}	Many systems performed at levels approaching the inter - coder agreement , suggesting that human - like performance on this task is within the reach of currently available technologies .
In this paper we describe automatic information nuggetization and its application to text comparison.	automatic information nuggetization	text comparison	usage	{'e1': {'word': 'automatic information nuggetization', 'word_index': [(5, 7)], 'id': 'N07-2055.1'}, 'e2': {'word': 'text comparison', 'word_index': [(12, 13)], 'id': 'N07-2055.2'}, 'entity_replacement': {'5:7': 'ENTITY', '12:13': 'ENTITYOTHER'}}	In this paper we describe automatic information nuggetization and its application to text comparison .
More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material.	machine-generated nuggets	evaluation material	usage	{'e1': {'word': 'machine-generated nuggets', 'word_index': [(10, 13)], 'id': 'N07-2055.3'}, 'e2': {'word': 'evaluation material', 'word_index': [(19, 20)], 'id': 'N07-2055.4'}, 'entity_replacement': {'10:13': 'ENTITY', '19:20': 'ENTITYOTHER'}}	More specifically , we take a close look at how machine - generated nuggets can be used to create evaluation material .
A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.	semiautomatic annotation scheme	gold-standard data	usage	{'e1': {'word': 'semiautomatic annotation scheme', 'word_index': [(1, 3)], 'id': 'N07-2055.5'}, 'e2': {'word': 'gold-standard data', 'word_index': [(8, 11)], 'id': 'N07-2055.6'}, 'entity_replacement': {'1:3': 'ENTITY', '8:11': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	A semiautomatic annotation scheme is designed to produce gold - standard data with exceptionally high inter-human agreement .
This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts.	features	retrieval	usage	{'e1': {'word': 'features', 'word_index': [(14, 14)], 'id': 'P07-3007.1'}, 'e2': {'word': 'retrieval', 'word_index': [(16, 16)], 'id': 'P07-3007.2'}, 'entity_replacement': {'14:14': 'ENTITY', '16:16': 'ENTITYOTHER', '18:20': 'ENTITYUNRELATED'}}	This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts .
We assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR, but with some distinct features.	retrieval	IR	part_whole	{'e1': {'word': 'retrieval', 'word_index': [(6, 6)], 'id': 'P07-3007.4'}, 'e2': {'word': 'IR', 'word_index': [(21, 21)], 'id': 'P07-3007.6'}, 'entity_replacement': {'6:6': 'ENTITY', '8:12': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER', '27:27': 'ENTITYUNRELATED'}}	We assume that the task of retrieval of opinionated texts ( OIR ) can be regarded as a subtask of general IR , but with some distinct features .
This paper discusses the supervised learning of morphology using stochastic transducers, trained using the Expectation-Maximization (EM) algorithm.	stochastic transducers	supervised learning	usage	{'e1': {'word': 'stochastic transducers', 'word_index': [(9, 10)], 'id': 'P02-1065.3'}, 'e2': {'word': 'supervised learning', 'word_index': [(4, 5)], 'id': 'P02-1065.1'}, 'entity_replacement': {'4:5': 'ENTITYOTHER', '7:7': 'ENTITYUNRELATED', '9:10': 'ENTITY', '15:21': 'ENTITYUNRELATED'}}	This paper discusses the supervised learning of morphology using stochastic transducers , trained using the Expectation - Maximization ( EM ) algorithm .
These are evaluated and compared ondata sets from English, German, Slovene and Arabic.	data sets	English	part_whole	{'e1': {'word': 'data sets', 'word_index': [(6, 7)], 'id': 'P02-1065.9'}, 'e2': {'word': 'English', 'word_index': [(9, 9)], 'id': 'P02-1065.10'}, 'entity_replacement': {'6:7': 'ENTITY', '9:9': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED'}}	These are evaluated and compared on data sets from English , German , Slovene and Arabic .
Speech recognition problems are a reality in current spoken dialogue systems	Speech recognition problems	spoken dialogue systems	part_whole	{'e1': {'word': 'Speech recognition problems', 'word_index': [(0, 2)], 'id': 'P06-1025.1'}, 'e2': {'word': 'spoken dialogue systems', 'word_index': [(8, 10)], 'id': 'P06-1025.2'}, 'entity_replacement': {'0:2': 'ENTITY', '8:10': 'ENTITYOTHER'}}	Speech recognition problems are a reality in current spoken dialogue systems
We apply Chi Square (%2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns.	Chi Square (%2) analysis	corpus of speech-based computer tutoring dialogues	usage	{'e1': {'word': 'Chi Square (%2) analysis', 'word_index': [(2, 8)], 'id': 'P06-1025.5'}, 'e2': {'word': 'corpus of speech-based computer tutoring dialogues', 'word_index': [(11, 18)], 'id': 'P06-1025.6'}, 'entity_replacement': {'2:8': 'ENTITY', '11:18': 'ENTITYOTHER'}}	We apply Chi Square ( % 2 ) analysis to a corpus of speech - based computer tutoring dialogues to discover these dependencies both within and across turns .
In an interlingual knowledge-based machine translation system, ambiguity arises when the source language analyzer produces more than one interlingua expression for a source sentence.	interlingua expression	source sentence	model-feature	{'e1': {'word': 'interlingua expression', 'word_index': [(21, 22)], 'id': 'C94-1012.4'}, 'e2': {'word': 'source sentence', 'word_index': [(25, 26)], 'id': 'C94-1012.5'}, 'entity_replacement': {'2:8': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '14:16': 'ENTITYUNRELATED', '21:22': 'ENTITY', '25:26': 'ENTITYOTHER'}}	In an interlingual knowledge - based machine translation system , ambiguity arises when the source language analyzer produces more than one interlingua expression for a source sentence .
We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence.	test sentences	corpus	part_whole	{'e1': {'word': 'test sentences', 'word_index': [(10, 11)], 'id': 'C94-1012.12'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'C94-1012.11'}, 'entity_replacement': {'8:8': 'ENTITYOTHER', '10:11': 'ENTITY', '20:21': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	We also test these methods on a large corpus of test sentences , in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence .
We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence.	disambiguation methods	parses	result	{'e1': {'word': 'disambiguation methods', 'word_index': [(20, 21)], 'id': 'C94-1012.13'}, 'e2': {'word': 'parses', 'word_index': [(27, 27)], 'id': 'C94-1012.14'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '20:21': 'ENTITY', '27:27': 'ENTITYOTHER', '29:29': 'ENTITYUNRELATED'}}	We also test these methods on a large corpus of test sentences , in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence .
The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding.	dynamic programs	machine translation decoding	usage	{'e1': {'word': 'dynamic programs', 'word_index': [(16, 17)], 'id': 'D08-1012.3'}, 'e2': {'word': 'machine translation decoding', 'word_index': [(19, 21)], 'id': 'D08-1012.4'}, 'entity_replacement': {'3:8': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '16:17': 'ENTITY', '19:21': 'ENTITYOTHER'}}	The intersection of tree transducer - based translation models with n-gram language models results in huge dynamic programs for machine translation decoding .
In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language.	clustered encoding	encoding-based methods	usage	{'e1': {'word': 'clustered encoding', 'word_index': [(24, 25)], 'id': 'D08-1012.10'}, 'e2': {'word': 'encoding-based methods', 'word_index': [(16, 19)], 'id': 'D08-1012.9'}, 'entity_replacement': {'4:11': 'ENTITYUNRELATED', '16:19': 'ENTITYOTHER', '24:25': 'ENTITY', '28:29': 'ENTITYUNRELATED'}}	In contrast to previous order - based bigram- to - trigram approaches , we focus on encoding - based methods , which use a clustered encoding of the target language .
Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.	decoding cascade for trigram language models	bigram-to-trigram decoder	compare	{'e1': {'word': 'decoding cascade for trigram language models', 'word_index': [(4, 9)], 'id': 'D08-1012.16'}, 'e2': {'word': 'bigram-to-trigram decoder', 'word_index': [(20, 23)], 'id': 'D08-1012.18'}, 'entity_replacement': {'4:9': 'ENTITY', '15:16': 'ENTITYUNRELATED', '20:23': 'ENTITYOTHER'}}	Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram- to -trigram decoder .
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain.	pronoun resolution	biomedical texts	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(10, 11)], 'id': 'L08-1071.1'}, 'e2': {'word': 'biomedical texts', 'word_index': [(13, 14)], 'id': 'L08-1071.2'}, 'entity_replacement': {'10:11': 'ENTITY', '13:14': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts , in comparison with conducting pronoun resolution for the newswire domain .
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain.	pronoun resolution	newswire domain	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(20, 21)], 'id': 'L08-1071.3'}, 'e2': {'word': 'newswire domain', 'word_index': [(24, 25)], 'id': 'L08-1071.4'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '20:21': 'ENTITY', '24:25': 'ENTITYOTHER'}}	This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts , in comparison with conducting pronoun resolution for the newswire domain .
Sharing portions of grammars across languages greatly reduces the costs of multilingual grammar engineering.	grammars	multilingual grammar engineering	result	{'e1': {'word': 'grammars', 'word_index': [(3, 3)], 'id': 'C00-1005.1'}, 'e2': {'word': 'multilingual grammar engineering', 'word_index': [(11, 13)], 'id': 'C00-1005.3'}, 'entity_replacement': {'3:3': 'ENTITY', '5:5': 'ENTITYUNRELATED', '11:13': 'ENTITYOTHER'}}	Sharing portions of grammars across languages greatly reduces the costs of multilingual grammar engineering .
Taking grammatical relatedness seriously, we are particularly interested in designing linguistically motivated grammatical resources for Slavic languages to be used in applied and theoretical computational linguistics.	linguistically motivated grammatical resources	applied and theoretical computational linguistics	usage	{'e1': {'word': 'linguistically motivated grammatical resources', 'word_index': [(11, 14)], 'id': 'C00-1005.8'}, 'e2': {'word': 'applied and theoretical computational linguistics', 'word_index': [(22, 26)], 'id': 'C00-1005.10'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '11:14': 'ENTITY', '16:17': 'ENTITYUNRELATED', '22:26': 'ENTITYOTHER'}}	Taking grammatical relatedness seriously , we are particularly interested in designing linguistically motivated grammatical resources for Slavic languages to be used in applied and theoretical computational linguistics .
"On the basis of Slavic data, we show how a domain ontology conceptualising morpho-syntactic ""building blocks"" can serve as a basis of a shared grammar of Slavic."	domain ontology	shared grammar of Slavic	usage	{'e1': {'word': 'domain ontology', 'word_index': [(11, 12)], 'id': 'C00-1005.18'}, 'e2': {'word': 'shared grammar of Slavic', 'word_index': [(26, 29)], 'id': 'C00-1005.20'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '11:12': 'ENTITY', '14:18': 'ENTITYUNRELATED', '26:29': 'ENTITYOTHER'}}	"On the basis of Slavic data , we show how a domain ontology conceptualising morpho-syntactic "" building blocks "" can serve as a basis of a shared grammar of Slavic ."
We are currently developing MiniSTEx, a spatiotemporal annotation system to handle temporal and/or geospatial information directly and indirectly expressed in texts.	temporal and/or geospatial information	texts	part_whole	{'e1': {'word': 'temporal and/or geospatial information', 'word_index': [(12, 17)], 'id': 'L08-1561.6'}, 'e2': {'word': 'texts', 'word_index': [(23, 23)], 'id': 'L08-1561.7'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '7:9': 'ENTITYUNRELATED', '12:17': 'ENTITY', '23:23': 'ENTITYOTHER'}}	We are currently developing MiniSTEx , a spatiotemporal annotation system to handle temporal and / or geospatial information directly and indirectly expressed in texts .
A first version of MiniSTEx was originally developed for Dutch, keeping in mind that it should also be useful for other European languages, and for multilingual applications.	MiniSTEx	Dutch	usage	{'e1': {'word': 'MiniSTEx', 'word_index': [(4, 4)], 'id': 'L08-1561.11'}, 'e2': {'word': 'Dutch', 'word_index': [(9, 9)], 'id': 'L08-1561.12'}, 'entity_replacement': {'4:4': 'ENTITY', '9:9': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	A first version of MiniSTEx was originally developed for Dutch , keeping in mind that it should also be useful for other European languages , and for multilingual applications .
In this paper we describe a technique for improving the performance of an information extraction system for speech data by explicitly modeling the errors in the recognizer output.	information extraction system	speech data	usage	{'e1': {'word': 'information extraction system', 'word_index': [(13, 15)], 'id': 'H01-1034.4'}, 'e2': {'word': 'speech data', 'word_index': [(17, 18)], 'id': 'H01-1034.5'}, 'entity_replacement': {'13:15': 'ENTITY', '17:18': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED'}}	In this paper we describe a technique for improving the performance of an information extraction system for speech data by explicitly modeling the errors in the recognizer output .
In this paper, we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list.	QA system	type questions	usage	{'e1': {'word': 'QA system', 'word_index': [(7, 8)], 'id': 'I05-2006.1'}, 'e2': {'word': 'type questions', 'word_index': [(13, 14)], 'id': 'I05-2006.2'}, 'entity_replacement': {'7:8': 'ENTITY', '13:14': 'ENTITYOTHER', '18:20': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	In this paper , we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list .
In this paper, we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list.	confirmed knowledge base	mails	part_whole	{'e1': {'word': 'confirmed knowledge base', 'word_index': [(18, 20)], 'id': 'I05-2006.3'}, 'e2': {'word': 'mails', 'word_index': [(26, 26)], 'id': 'I05-2006.4'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '18:20': 'ENTITY', '26:26': 'ENTITYOTHER', '30:31': 'ENTITYUNRELATED'}}	In this paper , we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list .
We first discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents.	knowledge base	natural language documents	part_whole	{'e1': {'word': 'knowledge base', 'word_index': [(8, 9)], 'id': 'I05-2006.6'}, 'e2': {'word': 'natural language documents', 'word_index': [(12, 14)], 'id': 'I05-2006.7'}, 'entity_replacement': {'8:9': 'ENTITY', '12:14': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED'}}	We first discuss a problem of developing a knowledge base by using natural language documents : wrong information in natural language documents .
We first discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents.	wrong information	natural language documents	part_whole	{'e1': {'word': 'wrong information', 'word_index': [(16, 17)], 'id': 'I05-2006.8'}, 'e2': {'word': 'natural language documents', 'word_index': [(19, 21)], 'id': 'I05-2006.9'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '16:17': 'ENTITY', '19:21': 'ENTITYOTHER'}}	We first discuss a problem of developing a knowledge base by using natural language documents : wrong information in natural language documents .
Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails.	wrong information	mails	part_whole	{'e1': {'word': 'wrong information', 'word_index': [(8, 9)], 'id': 'I05-2006.10'}, 'e2': {'word': 'mails', 'word_index': [(11, 11)], 'id': 'I05-2006.11'}, 'entity_replacement': {'8:9': 'ENTITY', '11:11': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	Then , we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails .
Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails.	knowledge base	mails	part_whole	{'e1': {'word': 'knowledge base', 'word_index': [(20, 21)], 'id': 'I05-2006.13'}, 'e2': {'word': 'mails', 'word_index': [(25, 25)], 'id': 'I05-2006.14'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '20:21': 'ENTITY', '25:25': 'ENTITYOTHER'}}	Then , we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails .
Finally, we show that question and answer mails posted to a mailing list can be used as a knowledge base for a QA system.	knowledge base	QA system	usage	{'e1': {'word': 'knowledge base', 'word_index': [(19, 20)], 'id': 'I05-2006.17'}, 'e2': {'word': 'QA system', 'word_index': [(23, 24)], 'id': 'I05-2006.18'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '19:20': 'ENTITY', '23:24': 'ENTITYOTHER'}}	Finally , we show that question and answer mails posted to a mailing list can be used as a knowledge base for a QA system .
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.	reading difficulty	text passage	model-feature	{'e1': {'word': 'reading difficulty', 'word_index': [(12, 13)], 'id': 'N04-1025.1'}, 'e2': {'word': 'text passage', 'word_index': [(16, 17)], 'id': 'N04-1025.2'}, 'entity_replacement': {'12:13': 'ENTITY', '16:17': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage , by recasting readability in terms of statistical language modeling .
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.	statistical language modeling	readability	model-feature	{'e1': {'word': 'statistical language modeling', 'word_index': [(25, 27)], 'id': 'N04-1025.4'}, 'e2': {'word': 'readability', 'word_index': [(21, 21)], 'id': 'N04-1025.3'}, 'entity_replacement': {'12:13': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER', '25:27': 'ENTITY'}}	We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage , by recasting readability in terms of statistical language modeling .
We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.	language models	multinomial naive Bayes classification	usage	{'e1': {'word': 'language models', 'word_index': [(16, 17)], 'id': 'N04-1025.6'}, 'e2': {'word': 'multinomial naive Bayes classification', 'word_index': [(9, 12)], 'id': 'N04-1025.5'}, 'entity_replacement': {'9:12': 'ENTITYOTHER', '16:17': 'ENTITY', '28:28': 'ENTITYUNRELATED'}}	We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage .
We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.	semantic variables	readability measures	part_whole	{'e1': {'word': 'semantic variables', 'word_index': [(17, 18)], 'id': 'N04-1025.10'}, 'e2': {'word': 'readability measures', 'word_index': [(21, 22)], 'id': 'N04-1025.11'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '17:18': 'ENTITY', '21:22': 'ENTITYOTHER'}}	We perform predictions for individual Web pages in English and compare our performance to widely - used semantic variables from traditional readability measures .
Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).	language modeling approach	Web documents	usage	{'e1': {'word': 'language modeling approach', 'word_index': [(21, 23)], 'id': 'N04-1025.17'}, 'e2': {'word': 'Web documents', 'word_index': [(28, 29)], 'id': 'N04-1025.18'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '6:8': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '21:23': 'ENTITY', '28:29': 'ENTITYOTHER', '33:33': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED'}}	Some traditional semantic variables such as type- token ratio gave the best performance on commercial calibrated test passages , while our language modeling approach gave better accuracy for Web documents and very short passages ( less than 10 words ) .
Syntactic and semantic information are both represented in the grammar in a uniform manner, similar to HPSG ( Pollard and Sag, 1987 ).	grammar	Syntactic and semantic information	model-feature	{'e1': {'word': 'grammar', 'word_index': [(9, 9)], 'id': 'M93-1024.7'}, 'e2': {'word': 'Syntactic and semantic information', 'word_index': [(0, 3)], 'id': 'M93-1024.6'}, 'entity_replacement': {'0:3': 'ENTITYOTHER', '9:9': 'ENTITY', '17:17': 'ENTITYUNRELATED'}}	Syntactic and semantic information are both represented in the grammar in a uniform manner , similar to HPSG ( Pollard and Sag , 1987 ) .
LINK has been used in several information extraction applications.	LINK	information extraction applications	usage	{'e1': {'word': 'LINK', 'word_index': [(0, 0)], 'id': 'M93-1024.9'}, 'e2': {'word': 'information extraction applications', 'word_index': [(6, 8)], 'id': 'M93-1024.10'}, 'entity_replacement': {'0:0': 'ENTITY', '6:8': 'ENTITYOTHER'}}	LINK has been used in several information extraction applications .
In a project with General Motors, LINK was used to process terse free-form descriptions of symptoms displayed by malfunctioning automobiles, and the repairs which fixed them.	LINK	free-form descriptions	usage	{'e1': {'word': 'LINK', 'word_index': [(7, 7)], 'id': 'M93-1024.11'}, 'e2': {'word': 'free-form descriptions', 'word_index': [(13, 16)], 'id': 'M93-1024.12'}, 'entity_replacement': {'7:7': 'ENTITY', '13:16': 'ENTITYOTHER'}}	In a project with General Motors , LINK was used to process terse free - form descriptions of symptoms displayed by malfunctioning automobiles , and the repairs which fixed them .
Reduplication, a central instance of prosodic morphology, is particularly challenging for state-of-the-art computational morphology, since it involves copying of some part of a phonological string	Reduplication	prosodic morphology	part_whole	{'e1': {'word': 'Reduplication', 'word_index': [(0, 0)], 'id': 'A00-2039.1'}, 'e2': {'word': 'prosodic morphology', 'word_index': [(6, 7)], 'id': 'A00-2039.2'}, 'entity_replacement': {'0:0': 'ENTITY', '6:7': 'ENTITYOTHER', '13:21': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	Reduplication , a central instance of prosodic morphology , is particularly challenging for state - of - the - art computational morphology , since it involves copying of some part of a phonological string
In this paper I advocate a finite-state method that combines enriched lexical representations via intersection to implement the copying.	enriched lexical representations	finite-state method	usage	{'e1': {'word': 'enriched lexical representations', 'word_index': [(12, 14)], 'id': 'A00-2039.6'}, 'e2': {'word': 'finite-state method', 'word_index': [(6, 9)], 'id': 'A00-2039.5'}, 'entity_replacement': {'6:9': 'ENTITYOTHER', '12:14': 'ENTITY'}}	In this paper I advocate a finite - state method that combines enriched lexical representations via intersection to implement the copying .
The proposal includes a resource-conscious variant of automata and can benefit from the existence of lazy algorithms.	lazy algorithms	resource-conscious variant of automata	usage	{'e1': {'word': 'lazy algorithms', 'word_index': [(17, 18)], 'id': 'A00-2039.8'}, 'e2': {'word': 'resource-conscious variant of automata', 'word_index': [(4, 9)], 'id': 'A00-2039.7'}, 'entity_replacement': {'4:9': 'ENTITYOTHER', '17:18': 'ENTITY'}}	The proposal includes a resource - conscious variant of automata and can benefit from the existence of lazy algorithms .
"These quick relevancy judgements require two steps: (1) recognizing an expression that is highly relevant to the given domain, e.g. ""were killed"" in the domain of terrorism, and (2) verifying that the context surrounding the expression is consistent with the relevancy guidelines for the domain, e.g. ""5 soldiers were killed by guerrillas"" is not consistent with the terrorism domain since victims of terrorist acts must be civilians."	expression	given domain	model-feature	{'e1': {'word': 'expression', 'word_index': [(13, 13)], 'id': 'H92-1094.3'}, 'e2': {'word': 'given domain', 'word_index': [(20, 21)], 'id': 'H92-1094.4'}, 'entity_replacement': {'13:13': 'ENTITY', '20:21': 'ENTITYOTHER', '30:32': 'ENTITYUNRELATED', '41:41': 'ENTITYUNRELATED', '44:44': 'ENTITYUNRELATED', '49:50': 'ENTITYUNRELATED', '53:53': 'ENTITYUNRELATED', '69:70': 'ENTITYUNRELATED'}}	"These quick relevancy judgements require two steps : ( 1 ) recognizing an expression that is highly relevant to the given domain , e.g. "" were killed "" in the domain of terrorism , and ( 2 ) verifying that the context surrounding the expression is consistent with the relevancy guidelines for the domain , e.g. "" 5 soldiers were killed by guerrillas "" is not consistent with the terrorism domain since victims of terrorist acts must be civilians ."
The Relevancy Signatures Algorithm attempts to simulate the first step in this process by deriving reliable relevancy cues from a corpus of training texts and using these cues to quickly identify new texts that are highly likely to be relevant.	reliable relevancy cues	corpus of training texts	part_whole	{'e1': {'word': 'reliable relevancy cues', 'word_index': [(15, 17)], 'id': 'H92-1094.12'}, 'e2': {'word': 'corpus of training texts', 'word_index': [(20, 23)], 'id': 'H92-1094.13'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '15:17': 'ENTITY', '20:23': 'ENTITYOTHER', '31:32': 'ENTITYUNRELATED'}}	The Relevancy Signatures Algorithm attempts to simulate the first step in this process by deriving reliable relevancy cues from a corpus of training texts and using these cues to quickly identify new texts that are highly likely to be relevant .
Our system accepts an entire English news story (text) as query, and retrieves relevant Chinese broadcast news stories (audio) from the document collection.	relevant Chinese broadcast news stories (audio)	document collection	part_whole	{'e1': {'word': 'relevant Chinese broadcast news stories (audio)', 'word_index': [(16, 23)], 'id': 'H01-1050.6'}, 'e2': {'word': 'document collection', 'word_index': [(26, 27)], 'id': 'H01-1050.7'}, 'entity_replacement': {'5:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '16:23': 'ENTITY', '26:27': 'ENTITYOTHER'}}	Our system accepts an entire English news story ( text ) as query , and retrieves relevant Chinese broadcast news stories ( audio ) from the document collection .
The English queries are translated into Chinese by means of a dictionary-based approach, where we have integrated phrase-based translation with word-by-word translation.	phrase-based translation	dictionary-based approach	usage	{'e1': {'word': 'phrase-based translation', 'word_index': [(20, 23)], 'id': 'H01-1050.16'}, 'e2': {'word': 'dictionary-based approach', 'word_index': [(11, 14)], 'id': 'H01-1050.15'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '11:14': 'ENTITYOTHER', '20:23': 'ENTITY', '25:30': 'ENTITYUNRELATED'}}	The English queries are translated into Chinese by means of a dictionary - based approach , where we have integrated phrase - based translation with word - by - word translation .
Untranslatable named entities are transliterated by a novel subword translation technique.	novel subword translation technique	Untranslatable named entities	usage	{'e1': {'word': 'novel subword translation technique', 'word_index': [(7, 10)], 'id': 'H01-1050.19'}, 'e2': {'word': 'Untranslatable named entities', 'word_index': [(0, 2)], 'id': 'H01-1050.18'}, 'entity_replacement': {'0:2': 'ENTITYOTHER', '7:10': 'ENTITY'}}	Untranslatable named entities are transliterated by a novel subword translation technique .
Experimental results demonstrate that the use of phrase-based translation and subword translation gave performance gains, and multi-scale retrieval outperforms word-based retrieval.	multi-scale retrieval	word-based retrieval	compare	{'e1': {'word': 'multi-scale retrieval', 'word_index': [(19, 20)], 'id': 'H01-1050.26'}, 'e2': {'word': 'word-based retrieval', 'word_index': [(22, 25)], 'id': 'H01-1050.27'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '19:20': 'ENTITY', '22:25': 'ENTITYOTHER'}}	Experimental results demonstrate that the use of phrase - based translation and subword translation gave performance gains , and multi-scale retrieval outperforms word - based retrieval .
But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English.	features	English past tense	model-feature	{'e1': {'word': 'features', 'word_index': [(9, 9)], 'id': 'W98-1240.10'}, 'e2': {'word': 'English past tense', 'word_index': [(3, 5)], 'id': 'W98-1240.9'}, 'entity_replacement': {'3:5': 'ENTITYOTHER', '9:9': 'ENTITY', '14:15': 'ENTITYUNRELATED', '17:20': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED', '37:38': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED', '43:44': 'ENTITYUNRELATED'}}	But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns , it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English .
But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English.	vestigal nature	inflectional morphology	model-feature	{'e1': {'word': 'vestigal nature', 'word_index': [(37, 38)], 'id': 'W98-1240.14'}, 'e2': {'word': 'inflectional morphology', 'word_index': [(40, 41)], 'id': 'W98-1240.15'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '17:20': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED', '37:38': 'ENTITY', '40:41': 'ENTITYOTHER', '43:44': 'ENTITYUNRELATED'}}	But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns , it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English .
We define, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages.	shallow syntactic analysis	statistical machine translation	usage	{'e1': {'word': 'shallow syntactic analysis', 'word_index': [(18, 20)], 'id': 'W03-1002.2'}, 'e2': {'word': 'statistical machine translation', 'word_index': [(10, 12)], 'id': 'W03-1002.1'}, 'entity_replacement': {'10:12': 'ENTITYOTHER', '18:20': 'ENTITY', '22:26': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '34:37': 'ENTITYUNRELATED'}}	We define , implement and evaluate a novel model for statistical machine translation , which is based on shallow syntactic analysis ( part -of - speech tagging and phrase chunking ) in both the source and target languages .
This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.	machine translation systems	language pairs	usage	{'e1': {'word': 'machine translation systems', 'word_index': [(7, 9)], 'id': 'W08-0309.2'}, 'e2': {'word': 'language pairs', 'word_index': [(12, 13)], 'id': 'W08-0309.3'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '7:9': 'ENTITY', '12:13': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech , English , French , German , Hungarian , and Spanish .
We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information.	intra- and inter-annotator agreement	manual evaluation methodology	usage	{'e1': {'word': 'intra- and inter-annotator agreement', 'word_index': [(8, 12)], 'id': 'W08-0309.18'}, 'e2': {'word': 'manual evaluation methodology', 'word_index': [(3, 5)], 'id': 'W08-0309.17'}, 'entity_replacement': {'3:5': 'ENTITYOTHER', '8:12': 'ENTITY'}}	We validate our manual evaluation methodology by measuring intra - and inter-annotator agreement , and collecting timing information .
In this paper, we describe issues in the translation of proper names from English to Chinese which we have faced in constructing a system for multilingual text generation supporting both languages.	translation	proper names	usage	{'e1': {'word': 'translation', 'word_index': [(9, 9)], 'id': 'P98-2220.1'}, 'e2': {'word': 'proper names', 'word_index': [(11, 12)], 'id': 'P98-2220.2'}, 'entity_replacement': {'9:9': 'ENTITY', '11:12': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '26:28': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	In this paper , we describe issues in the translation of proper names from English to Chinese which we have faced in constructing a system for multilingual text generation supporting both languages .
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.	backward maximum matching	CWS	usage	{'e1': {'word': 'backward maximum matching', 'word_index': [(5, 7)], 'id': 'W06-0119.6'}, 'e2': {'word': 'CWS', 'word_index': [(1, 1)], 'id': 'W06-0119.5'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '5:7': 'ENTITY', '9:14': 'ENTITYUNRELATED', '16:22': 'ENTITYUNRELATED'}}	Our CWS is based on backward maximum matching with word support model ( WSM ) and contextual - based Chinese unknown word identification .
The Arabic language has far richer systems of inflection and derivation than English which has very little morphology.	systems of inflection and derivation	Arabic language	model-feature	{'e1': {'word': 'systems of inflection and derivation', 'word_index': [(6, 10)], 'id': 'W06-3103.2'}, 'e2': {'word': 'Arabic language', 'word_index': [(1, 2)], 'id': 'W06-3103.1'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '6:10': 'ENTITY', '12:12': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED'}}	The Arabic language has far richer systems of inflection and derivation than English which has very little morphology .
Segmentation of inflected Arabic words is a way to smooth its highly morphological nature.	Segmentation	inflected Arabic words	usage	{'e1': {'word': 'Segmentation', 'word_index': [(0, 0)], 'id': 'W06-3103.8'}, 'e2': {'word': 'inflected Arabic words', 'word_index': [(2, 4)], 'id': 'W06-3103.9'}, 'entity_replacement': {'0:0': 'ENTITY', '2:4': 'ENTITYOTHER', '11:13': 'ENTITYUNRELATED'}}	Segmentation of inflected Arabic words is a way to smooth its highly morphological nature .
In this paper, we describe some statistically and linguistically motivated methods for Arabic word segmentation.	statistically and linguistically motivated methods	Arabic word segmentation	usage	{'e1': {'word': 'statistically and linguistically motivated methods', 'word_index': [(7, 11)], 'id': 'W06-3103.11'}, 'e2': {'word': 'Arabic word segmentation', 'word_index': [(13, 15)], 'id': 'W06-3103.12'}, 'entity_replacement': {'7:11': 'ENTITY', '13:15': 'ENTITYOTHER'}}	In this paper , we describe some statistically and linguistically motivated methods for Arabic word segmentation .
Then, we show the efficiency of proposed methods on the Arabic-English BTEC and NIST tasks.	proposed methods	Arabic-English BTEC and NIST tasks	usage	{'e1': {'word': 'proposed methods', 'word_index': [(7, 8)], 'id': 'W06-3103.13'}, 'e2': {'word': 'Arabic-English BTEC and NIST tasks', 'word_index': [(11, 15)], 'id': 'W06-3103.14'}, 'entity_replacement': {'7:8': 'ENTITY', '11:15': 'ENTITYOTHER'}}	Then , we show the efficiency of proposed methods on the Arabic-English BTEC and NIST tasks .
Developing a high-quality lexicon is often the first step towards building a POS tagger, which is in turn the front-end to many NLP systems.	high-quality lexicon	POS tagger	usage	{'e1': {'word': 'high-quality lexicon', 'word_index': [(2, 5)], 'id': 'W06-1647.4'}, 'e2': {'word': 'POS tagger', 'word_index': [(14, 15)], 'id': 'W06-1647.5'}, 'entity_replacement': {'2:5': 'ENTITY', '14:15': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED'}}	Developing a high - quality lexicon is often the first step towards building a POS tagger , which is in turn the front - end to many NLP systems .
We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Transductive SVMs, Spectral Graph Transducers, and a novel Transductive Clustering method.	Transductive SVMs	Spectral Graph Transducers	compare	{'e1': {'word': 'Transductive SVMs', 'word_index': [(20, 21)], 'id': 'W06-1647.10'}, 'e2': {'word': 'Spectral Graph Transducers', 'word_index': [(23, 25)], 'id': 'W06-1647.11'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '8:10': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '20:21': 'ENTITY', '23:25': 'ENTITYOTHER', '29:32': 'ENTITYUNRELATED'}}	We frame the lexicon acquisition problem as a transductive learning problem , and perform comparisons on three transductive algorithms : Transductive SVMs , Spectral Graph Transducers , and a novel Transductive Clustering method .
We present an API for computing the semantic relatedness of words in Wikipedia.	API	semantic relatedness	usage	{'e1': {'word': 'API', 'word_index': [(3, 3)], 'id': 'P07-2013.1'}, 'e2': {'word': 'semantic relatedness', 'word_index': [(7, 8)], 'id': 'P07-2013.2'}, 'entity_replacement': {'3:3': 'ENTITY', '7:8': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED'}}	We present an API for computing the semantic relatedness of words in Wikipedia .
This paper presents a general platform, namely synchronous tree sequence substitution grammar (STSSG), for the grammar comparison study in Translational Equivalence Modeling (TEM) and Statistical Machine Translation (SMT).	synchronous tree sequence substitution grammar (STSSG)	grammar comparison study	usage	{'e1': {'word': 'synchronous tree sequence substitution grammar (STSSG)', 'word_index': [(8, 15)], 'id': 'C08-1138.1'}, 'e2': {'word': 'grammar comparison study', 'word_index': [(19, 21)], 'id': 'C08-1138.2'}, 'entity_replacement': {'8:15': 'ENTITY', '19:21': 'ENTITYOTHER', '23:28': 'ENTITYUNRELATED', '30:35': 'ENTITYUNRELATED'}}	This paper presents a general platform , namely synchronous tree sequence substitution grammar ( STSSG ) , for the grammar comparison study in Translational Equivalence Modeling ( TEM ) and Statistical Machine Translation ( SMT ) .
Experimental results show that the STSSG is able to better explain the data in parallel corpora than other grammars.	STSSG	other grammars	compare	{'e1': {'word': 'STSSG', 'word_index': [(5, 5)], 'id': 'C08-1138.11'}, 'e2': {'word': 'other grammars', 'word_index': [(17, 18)], 'id': 'C08-1138.13'}, 'entity_replacement': {'5:5': 'ENTITY', '14:15': 'ENTITYUNRELATED', '17:18': 'ENTITYOTHER'}}	Experimental results show that the STSSG is able to better explain the data in parallel corpora than other grammars .
Our study further finds that the complexity of structure divergence is much higher than suggested in literature, which imposes a big challenge to syntactic transformation-based SMT.	structure divergence	syntactic transformation-based SMT	result	{'e1': {'word': 'structure divergence', 'word_index': [(8, 9)], 'id': 'C08-1138.14'}, 'e2': {'word': 'syntactic transformation-based SMT', 'word_index': [(24, 28)], 'id': 'C08-1138.15'}, 'entity_replacement': {'8:9': 'ENTITY', '24:28': 'ENTITYOTHER'}}	Our study further finds that the complexity of structure divergence is much higher than suggested in literature , which imposes a big challenge to syntactic transformation - based SMT .
This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms.	unsupervised method	automatic sentence extraction	usage	{'e1': {'word': 'unsupervised method', 'word_index': [(5, 6)], 'id': 'P04-3020.1'}, 'e2': {'word': 'automatic sentence extraction', 'word_index': [(8, 10)], 'id': 'P04-3020.2'}, 'entity_replacement': {'5:6': 'ENTITY', '8:10': 'ENTITYOTHER', '12:16': 'ENTITYUNRELATED'}}	This paper presents an innovative unsupervised method for automatic sentence extraction using graph - based ranking algorithms .
We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.	method	text summarization task	usage	{'e1': {'word': 'method', 'word_index': [(3, 3)], 'id': 'P04-3020.4'}, 'e2': {'word': 'text summarization task', 'word_index': [(9, 11)], 'id': 'P04-3020.5'}, 'entity_replacement': {'3:3': 'ENTITY', '9:11': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED'}}	We evaluate the method in the context of a text summarization task , and show that the results obtained compare favorably with previously published results on established benchmarks .
Preferred antecedents are a subset of the possible antecedents, selected by the application of extralinguistic knowledge.	Preferred antecedents	possible antecedents	part_whole	{'e1': {'word': 'Preferred antecedents', 'word_index': [(0, 1)], 'id': 'C90-2017.7'}, 'e2': {'word': 'possible antecedents', 'word_index': [(7, 8)], 'id': 'C90-2017.8'}, 'entity_replacement': {'0:1': 'ENTITY', '7:8': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	Preferred antecedents are a subset of the possible antecedents , selected by the application of extralinguistic knowledge .
This paper presents a syntactic description of a fragment of German that has been worked out within the machine translation project Eurotra.	syntactic description	German	model-feature	{'e1': {'word': 'syntactic description', 'word_index': [(4, 5)], 'id': 'C88-2123.1'}, 'e2': {'word': 'German', 'word_index': [(10, 10)], 'id': 'C88-2123.2'}, 'entity_replacement': {'4:5': 'ENTITY', '10:10': 'ENTITYOTHER', '18:21': 'ENTITYUNRELATED'}}	This paper presents a syntactic description of a fragment of German that has been worked out within the machine translation project Eurotra .
It represents the syntactic part of the German module of this multilingual translation system.	syntactic part	German module	part_whole	{'e1': {'word': 'syntactic part', 'word_index': [(3, 4)], 'id': 'C88-2123.4'}, 'e2': {'word': 'German module', 'word_index': [(7, 8)], 'id': 'C88-2123.5'}, 'entity_replacement': {'3:4': 'ENTITY', '7:8': 'ENTITYOTHER', '11:13': 'ENTITYUNRELATED'}}	It represents the syntactic part of the German module of this multilingual translation system .
We present an approach for querying collections of heterogeneous linguistic corpora that are annotated on multiple layers using arbitrary XML-based markup languages.	multiple layers	heterogeneous linguistic corpora	model-feature	{'e1': {'word': 'multiple layers', 'word_index': [(15, 16)], 'id': 'L08-1190.2'}, 'e2': {'word': 'heterogeneous linguistic corpora', 'word_index': [(8, 10)], 'id': 'L08-1190.1'}, 'entity_replacement': {'8:10': 'ENTITYOTHER', '15:16': 'ENTITY', '19:23': 'ENTITYUNRELATED'}}	We present an approach for querying collections of heterogeneous linguistic corpora that are annotated on multiple layers using arbitrary XML - based markup languages .
An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion.	OWL ontology	markup languages	usage	{'e1': {'word': 'OWL ontology', 'word_index': [(1, 2)], 'id': 'L08-1190.4'}, 'e2': {'word': 'markup languages', 'word_index': [(11, 12)], 'id': 'L08-1190.5'}, 'entity_replacement': {'1:2': 'ENTITY', '11:12': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '26:30': 'ENTITYUNRELATED'}}	An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology - based query expansion .
An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion.	ontology-based query expansion	querying framework	usage	{'e1': {'word': 'ontology-based query expansion', 'word_index': [(26, 30)], 'id': 'L08-1190.7'}, 'e2': {'word': 'querying framework', 'word_index': [(17, 18)], 'id': 'L08-1190.6'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '17:18': 'ENTITYOTHER', '26:30': 'ENTITY'}}	An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology - based query expansion .
This interface can also be used for ontology-based querying of multiple corpora simultaneously.	ontology-based querying	corpora	usage	{'e1': {'word': 'ontology-based querying', 'word_index': [(7, 10)], 'id': 'L08-1190.12'}, 'e2': {'word': 'corpora', 'word_index': [(13, 13)], 'id': 'L08-1190.13'}, 'entity_replacement': {'7:10': 'ENTITY', '13:13': 'ENTITYOTHER'}}	This interface can also be used for ontology - based querying of multiple corpora simultaneously .
We also evaluate our model on the related role-labelling task, and compare it with a standard role labeller.	model	standard role labeller	compare	{'e1': {'word': 'model', 'word_index': [(4, 4)], 'id': 'E06-1044.5'}, 'e2': {'word': 'standard role labeller', 'word_index': [(18, 20)], 'id': 'E06-1044.7'}, 'entity_replacement': {'4:4': 'ENTITY', '8:11': 'ENTITYUNRELATED', '18:20': 'ENTITYOTHER'}}	We also evaluate our model on the related role - labelling task , and compare it with a standard role labeller .
For both tasks, our model benefits from class-based smoothing, which allows it to make correct argument-specific predictions despite a severe sparse data problem.	class-based smoothing	model	result	{'e1': {'word': 'class-based smoothing', 'word_index': [(8, 11)], 'id': 'E06-1044.10'}, 'e2': {'word': 'model', 'word_index': [(5, 5)], 'id': 'E06-1044.9'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '5:5': 'ENTITYOTHER', '8:11': 'ENTITY', '19:21': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	For both tasks , our model benefits from class - based smoothing , which allows it to make correct argument -specific predictions despite a severe sparse data problem .
The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.	sparse data	standard labeller	result	{'e1': {'word': 'sparse data', 'word_index': [(5, 6)], 'id': 'E06-1044.14'}, 'e2': {'word': 'standard labeller', 'word_index': [(1, 2)], 'id': 'E06-1044.13'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '5:6': 'ENTITY', '12:13': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED'}}	The standard labeller suffers from sparse data and a strong reliance on syntactic cues , especially in the prediction task .
The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.	syntactic cues	prediction task	usage	{'e1': {'word': 'syntactic cues', 'word_index': [(12, 13)], 'id': 'E06-1044.15'}, 'e2': {'word': 'prediction task', 'word_index': [(18, 19)], 'id': 'E06-1044.16'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '12:13': 'ENTITY', '18:19': 'ENTITYOTHER'}}	The standard labeller suffers from sparse data and a strong reliance on syntactic cues , especially in the prediction task .
In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step.	'Pendulum Algorithm'	word sets	usage	"{'e1': {'word': ""'Pendulum Algorithm'"", 'word_index': [(9, 12)], 'id': 'C04-1178.6'}, 'e2': {'word': 'word sets', 'word_index': [(15, 16)], 'id': 'C04-1178.7'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '9:12': 'ENTITY', '15:16': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED'}}"	In a bootstrapping fashion , the so - called ' Pendulum Algorithm ' operates on word sets obtained by co-occurrence statistics on a large un- annotated corpus and keeps error propagation low by a verification step .
In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step.	co-occurrence statistics	large un-annotated corpus	part_whole	{'e1': {'word': 'co-occurrence statistics', 'word_index': [(19, 20)], 'id': 'C04-1178.8'}, 'e2': {'word': 'large un-annotated corpus', 'word_index': [(23, 26)], 'id': 'C04-1178.9'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '9:12': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '19:20': 'ENTITY', '23:26': 'ENTITYOTHER', '29:30': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED'}}	In a bootstrapping fashion , the so - called ' Pendulum Algorithm ' operates on word sets obtained by co-occurrence statistics on a large un- annotated corpus and keeps error propagation low by a verification step .
The first algorithm, phone-dependent cepstral compensation, is similar in concept to the previously-described MFCDCN method, except that cepstral compensation vectors are selected according to the current phonetic hypothesis, rather than on the basis of SNR or VQ codeword identity.	phone-dependent cepstral compensation	MFCDCN method	compare	{'e1': {'word': 'phone-dependent cepstral compensation', 'word_index': [(4, 8)], 'id': 'H94-1066.4'}, 'e2': {'word': 'MFCDCN method', 'word_index': [(19, 20)], 'id': 'H94-1066.5'}, 'entity_replacement': {'4:8': 'ENTITY', '19:20': 'ENTITYOTHER', '24:26': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '42:46': 'ENTITYUNRELATED'}}	The first algorithm , phone - dependent cepstral compensation , is similar in concept to the previously - described MFCDCN method , except that cepstral compensation vectors are selected according to the current phonetic hypothesis , rather than on the basis of SNR or VQ codeword identity .
Use of the various compensation algorithms in consort produces a reduction of error rates for SPHINX-II by as much as 40 percent relative to the rate achieved with cepstral mean normalization alone, in both development test sets and in the context of the 1993 ARPA CSR evaluations.	compensation algorithms	reduction of error rates	result	{'e1': {'word': 'compensation algorithms', 'word_index': [(4, 5)], 'id': 'H94-1066.12'}, 'e2': {'word': 'reduction of error rates', 'word_index': [(10, 13)], 'id': 'H94-1066.13'}, 'entity_replacement': {'4:5': 'ENTITY', '10:13': 'ENTITYOTHER', '15:17': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED', '37:39': 'ENTITYUNRELATED', '46:49': 'ENTITYUNRELATED'}}	Use of the various compensation algorithms in consort produces a reduction of error rates for SPHINX - II by as much as 40 percent relative to the rate achieved with cepstral mean normalization alone , in both development test sets and in the context of the 1993 ARPA CSR evaluations .
One of the critical components of the CLT is a speech recognition system which is used to track the child's progress during oral reading and to provide sufficient information to detect reading miscues.	reading miscues	oral reading	part_whole	{'e1': {'word': 'reading miscues', 'word_index': [(32, 33)], 'id': 'C04-1182.7'}, 'e2': {'word': 'oral reading', 'word_index': [(23, 24)], 'id': 'C04-1182.6'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER', '32:33': 'ENTITY'}}	One of the critical components of the CLT is a speech recognition system which is used to track the child 's progress during oral reading and to provide sufficient information to detect reading miscues .
In this paper, we extend on prior work by examining a novel labeling of children's oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors.	labeling	oral reading audio data	model-feature	{'e1': {'word': 'labeling', 'word_index': [(13, 13)], 'id': 'C04-1182.8'}, 'e2': {'word': 'oral reading audio data', 'word_index': [(17, 20)], 'id': 'C04-1182.9'}, 'entity_replacement': {'13:13': 'ENTITY', '17:20': 'ENTITYOTHER', '33:35': 'ENTITYUNRELATED'}}	In this paper , we extend on prior work by examining a novel labeling of children 's oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors .
Next, we consider the problem of detecting miscues during oral reading.	miscues	oral reading	part_whole	{'e1': {'word': 'miscues', 'word_index': [(8, 8)], 'id': 'C04-1182.14'}, 'e2': {'word': 'oral reading', 'word_index': [(10, 11)], 'id': 'C04-1182.15'}, 'entity_replacement': {'8:8': 'ENTITY', '10:11': 'ENTITYOTHER'}}	Next , we consider the problem of detecting miscues during oral reading .
