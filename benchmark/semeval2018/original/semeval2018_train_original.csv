original_sentence	e1	e2	relation_type	metadata	tokenized_sentence
This paper introduces a simple mixture language model that attempts to capture long distance constraints in a sentence or paragraph.	long distance constraints	sentence	part_whole	{'e1': {'word': 'long distance constraints', 'word_index': [(12, 14)], 'id': 'H94-1014.2'}, 'e2': {'word': 'sentence', 'word_index': [(17, 17)], 'id': 'H94-1014.3'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '12:14': 'ENTITY', '17:17': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED'}}	This paper introduces a simple mixture language model that attempts to capture long distance constraints in a sentence or paragraph .
Using the BU recognition system, experiments show a 7% improvement in recognition accuracy with the mixture digram models as compared to using a Digram model.	BU recognition system	recognition accuracy	result	{'e1': {'word': 'BU recognition system', 'word_index': [(2, 4)], 'id': 'H94-1014.9'}, 'e2': {'word': 'recognition accuracy', 'word_index': [(13, 14)], 'id': 'H94-1014.10'}, 'entity_replacement': {'2:4': 'ENTITY', '13:14': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED'}}	Using the BU recognition system , experiments show a 7 % improvement in recognition accuracy with the mixture digram models as compared to using a Digram model .
Using the BU recognition system, experiments show a 7% improvement in recognition accuracy with the mixture digram models as compared to using a Digram model.	mixture digram models	Digram model	compare	{'e1': {'word': 'mixture digram models', 'word_index': [(17, 19)], 'id': 'H94-1014.11'}, 'e2': {'word': 'Digram model', 'word_index': [(25, 26)], 'id': 'H94-1014.12'}, 'entity_replacement': {'2:4': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '17:19': 'ENTITY', '25:26': 'ENTITYOTHER'}}	Using the BU recognition system , experiments show a 7 % improvement in recognition accuracy with the mixture digram models as compared to using a Digram model .
The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text.	proper names	text	part_whole	{'e1': {'word': 'proper names', 'word_index': [(4, 5)], 'id': 'W93-0105.1'}, 'e2': {'word': 'text', 'word_index': [(7, 7)], 'id': 'W93-0105.2'}, 'entity_replacement': {'4:5': 'ENTITY', '7:7': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text .
The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text.	NLP systems	unrestricted text	usage	{'e1': {'word': 'NLP systems', 'word_index': [(13, 14)], 'id': 'W93-0105.3'}, 'e2': {'word': 'unrestricted text', 'word_index': [(17, 18)], 'id': 'W93-0105.4'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '13:14': 'ENTITY', '17:18': 'ENTITYOTHER'}}	The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text .
A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction.	name references	documents	model-feature	{'e1': {'word': 'name references', 'word_index': [(7, 8)], 'id': 'W93-0105.6'}, 'e2': {'word': 'documents', 'word_index': [(4, 4)], 'id': 'W93-0105.5'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '7:8': 'ENTITY', '13:14': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '21:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction .
This paper describes a system which uses text skimming techniques for deriving proper names and their semantic attributes automatically from newswire text, without relying on any listing of name elements.	semantic attributes	proper names	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(16, 17)], 'id': 'W93-0105.13'}, 'e2': {'word': 'proper names', 'word_index': [(12, 13)], 'id': 'W93-0105.12'}, 'entity_replacement': {'7:9': 'ENTITYUNRELATED', '12:13': 'ENTITYOTHER', '16:17': 'ENTITY', '20:21': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	This paper describes a system which uses text skimming techniques for deriving proper names and their semantic attributes automatically from newswire text , without relying on any listing of name elements .
In order to identify new names, the system treats proper names as (potentially) context-dependent linguistic expressions.	context-dependent linguistic expressions	proper names	model-feature	{'e1': {'word': 'context-dependent linguistic expressions', 'word_index': [(16, 18)], 'id': 'W93-0105.17'}, 'e2': {'word': 'proper names', 'word_index': [(10, 11)], 'id': 'W93-0105.16'}, 'entity_replacement': {'10:11': 'ENTITYOTHER', '16:18': 'ENTITY'}}	In order to identify new names , the system treats proper names as ( potentially ) context-dependent linguistic expressions .
In addition to using information in the local context, the system exploits a computational model of discourse which identifies individuals based on the way they are described in the text, instead of relying on their description in a pre-existing knowledge base.	information	local context	part_whole	{'e1': {'word': 'information', 'word_index': [(4, 4)], 'id': 'W93-0105.18'}, 'e2': {'word': 'local context', 'word_index': [(7, 8)], 'id': 'W93-0105.19'}, 'entity_replacement': {'4:4': 'ENTITY', '7:8': 'ENTITYOTHER', '14:17': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED'}}	In addition to using information in the local context , the system exploits a computational model of discourse which identifies individuals based on the way they are described in the text , instead of relying on their description in a pre-existing knowledge base .
Some entities belong more or less to a class.	entities	class	part_whole	{'e1': {'word': 'entities', 'word_index': [(1, 1)], 'id': 'L08-1274.2'}, 'e2': {'word': 'class', 'word_index': [(8, 8)], 'id': 'L08-1274.3'}, 'entity_replacement': {'1:1': 'ENTITY', '8:8': 'ENTITYOTHER'}}	Some entities belong more or less to a class .
To specify whether an individual entity belonging to a class is typical or not, we borrow the topological concepts of interior, border, closure, and exterior.	entity	class	part_whole	{'e1': {'word': 'entity', 'word_index': [(5, 5)], 'id': 'L08-1274.8'}, 'e2': {'word': 'class', 'word_index': [(9, 9)], 'id': 'L08-1274.9'}, 'entity_replacement': {'5:5': 'ENTITY', '9:9': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	To specify whether an individual entity belonging to a class is typical or not , we borrow the topological concepts of interior , border , closure , and exterior .
It enables to define levels of typicality where individual entities are more or less typical elements of a concept.	entities	concept	model-feature	{'e1': {'word': 'entities', 'word_index': [(9, 9)], 'id': 'L08-1274.16'}, 'e2': {'word': 'concept', 'word_index': [(18, 18)], 'id': 'L08-1274.17'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:9': 'ENTITY', '18:18': 'ENTITYOTHER'}}	It enables to define levels of typicality where individual entities are more or less typical elements of a concept .
WordNet has been used extensively as a resource for the Word Sense Disambiguation (WSD) task, both as a sense inventory and a repository of semantic relationships.	WordNet	Word Sense Disambiguation (WSD) task	usage	{'e1': {'word': 'WordNet', 'word_index': [(0, 0)], 'id': 'L08-1598.1'}, 'e2': {'word': 'Word Sense Disambiguation (WSD) task', 'word_index': [(10, 16)], 'id': 'L08-1598.2'}, 'entity_replacement': {'0:0': 'ENTITY', '10:16': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	WordNet has been used extensively as a resource for the Word Sense Disambiguation ( WSD ) task , both as a sense inventory and a repository of semantic relationships .
We found that it would be very useful to assign to geographical entities in WordNet their coordinates, especially in order to implement geometric shape-based disambiguation methods.	geographical entities	WordNet	part_whole	{'e1': {'word': 'geographical entities', 'word_index': [(11, 12)], 'id': 'L08-1598.9'}, 'e2': {'word': 'WordNet', 'word_index': [(14, 15)], 'id': 'L08-1598.10'}, 'entity_replacement': {'11:12': 'ENTITY', '14:15': 'ENTITYOTHER', '24:29': 'ENTITYUNRELATED'}}	We found that it would be very useful to assign to geographical entities in Word Net their coordinates , especially in order to implement geometric shape - based disambiguation methods .
The annotation has been carried out by extracting geographical synsets from WordNet, together with their holonyms and hypernyms, and comparing them to the entries in the Wikipedia-World geographical database.	geographical synsets	WordNet	part_whole	{'e1': {'word': 'geographical synsets', 'word_index': [(8, 9)], 'id': 'L08-1598.17'}, 'e2': {'word': 'WordNet', 'word_index': [(11, 11)], 'id': 'L08-1598.18'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '8:9': 'ENTITY', '11:11': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '28:32': 'ENTITYUNRELATED'}}	The annotation has been carried out by extracting geographical synsets from WordNet , together with their holonyms and hypernyms , and comparing them to the entries in the Wikipedia - World geographical database .
A weight was calculated for each of the candidate annotations, on the basis of matches found between the database entries and synset gloss, holonyms and hypernyms.	weight	candidate annotations	model-feature	{'e1': {'word': 'weight', 'word_index': [(1, 1)], 'id': 'L08-1598.22'}, 'e2': {'word': 'candidate annotations', 'word_index': [(8, 9)], 'id': 'L08-1598.23'}, 'entity_replacement': {'1:1': 'ENTITY', '8:9': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	A weight was calculated for each of the candidate annotations , on the basis of matches found between the database entries and synset gloss , holonyms and hypernyms .
"""We present a robust summarisation system developed within the GATE architecture that makes use of robust components for semantic tagging and coreference resolution provided by GATE."	robust components	semantic tagging	usage	{'e1': {'word': 'robust components', 'word_index': [(16, 17)], 'id': 'E03-2013.3'}, 'e2': {'word': 'semantic tagging', 'word_index': [(19, 20)], 'id': 'E03-2013.4'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '16:17': 'ENTITY', '19:20': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	""" We present a robust summarisation system developed within the GATE architecture that makes use of robust components for semantic tagging and coreference resolution provided by GATE ."
Our system combines GATE components with well established statistical techniques developed for the purpose of text summarisation research.	statistical techniques	text summarisation research	usage	{'e1': {'word': 'statistical techniques', 'word_index': [(8, 9)], 'id': 'E03-2013.9'}, 'e2': {'word': 'text summarisation research', 'word_index': [(15, 17)], 'id': 'E03-2013.10'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '8:9': 'ENTITY', '15:17': 'ENTITYOTHER'}}	Our system combines GATE components with well established statistical techniques developed for the purpose of text summarisation research .
Prior to MUC-4, LINK had been used to extract information from free-form texts in two narrow application domains.	information	free-form texts	part_whole	{'e1': {'word': 'information', 'word_index': [(11, 11)], 'id': 'M92-1039.6'}, 'e2': {'word': 'free-form texts', 'word_index': [(13, 16)], 'id': 'M92-1039.7'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '11:11': 'ENTITY', '13:16': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	Prior to MUC -4 , LINK had been used to extract information from free - form texts in two narrow application domains .
One application corpus contained terse descriptions of symptoms displayed by malfunctioning automobiles, and the repairs which fixed them.	terse descriptions	application corpus	part_whole	{'e1': {'word': 'terse descriptions', 'word_index': [(4, 5)], 'id': 'M92-1039.10'}, 'e2': {'word': 'application corpus', 'word_index': [(1, 2)], 'id': 'M92-1039.9'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '4:5': 'ENTITY'}}	One application corpus contained terse descriptions of symptoms displayed by malfunctioning automobiles , and the repairs which fixed them .
In empirical testing in these two domains, LINK correctly processed 70% of previously unseen descriptions.	LINK	descriptions	usage	{'e1': {'word': 'LINK', 'word_index': [(8, 8)], 'id': 'M92-1039.15'}, 'e2': {'word': 'descriptions', 'word_index': [(16, 16)], 'id': 'M92-1039.16'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '8:8': 'ENTITY', '16:16': 'ENTITYOTHER'}}	In empirical testing in these two domains , LINK correctly processed 70 % of previously unseen descriptions .
A template was counted as correct only if all of the fillers in the template were filled correctly.	fillers	template	part_whole	{'e1': {'word': 'fillers', 'word_index': [(11, 11)], 'id': 'M92-1039.18'}, 'e2': {'word': 'template', 'word_index': [(14, 14)], 'id': 'M92-1039.19'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '11:11': 'ENTITY', '14:14': 'ENTITYOTHER'}}	A template was counted as correct only if all of the fillers in the template were filled correctly .
These previous domains were much narrower than the MUC-4 terrorism domain.	domains	MUC-4 terrorism domain	compare	{'e1': {'word': 'domains', 'word_index': [(2, 2)], 'id': 'M92-1039.23'}, 'e2': {'word': 'MUC-4 terrorism domain', 'word_index': [(8, 12)], 'id': 'M92-1039.24'}, 'entity_replacement': {'2:2': 'ENTITY', '8:12': 'ENTITYOTHER'}}	These previous domains were much narrower than the MUC - 4 terrorism domain .
As a comparison, the lexicons for the previous domains contained only 300-500 words, compared with 6700 words in our MUC-4 test configuration.	words	lexicons	part_whole	{'e1': {'word': 'words', 'word_index': [(15, 15)], 'id': 'M92-1039.27'}, 'e2': {'word': 'lexicons', 'word_index': [(5, 5)], 'id': 'M92-1039.25'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '9:9': 'ENTITYUNRELATED', '15:15': 'ENTITY', '20:20': 'ENTITYUNRELATED', '23:27': 'ENTITYUNRELATED'}}	As a comparison , the lexicons for the previous domains contained only 300 - 500 words , compared with 6700 words in our MUC - 4 test configuration .
Previous grammar size ranged from 75-100 rules, compared with over 500 rules in the MUC-4 knowledge base.	grammar size	MUC-4 knowledge base	compare	{'e1': {'word': 'grammar size', 'word_index': [(1, 2)], 'id': 'M92-1039.30'}, 'e2': {'word': 'MUC-4 knowledge base', 'word_index': [(17, 21)], 'id': 'M92-1039.33'}, 'entity_replacement': {'1:2': 'ENTITY', '8:8': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '17:21': 'ENTITYOTHER'}}	Previous grammar size ranged from 75 - 100 rules , compared with over 500 rules in the MUC - 4 knowledge base .
Thus, the integration of information from multiple sentences was not an issue in our previous work.	information	sentences	part_whole	{'e1': {'word': 'information', 'word_index': [(5, 5)], 'id': 'M92-1039.36'}, 'e2': {'word': 'sentences', 'word_index': [(8, 8)], 'id': 'M92-1039.37'}, 'entity_replacement': {'5:5': 'ENTITY', '8:8': 'ENTITYOTHER'}}	Thus , the integration of information from multiple sentences was not an issue in our previous work .
Brief Summary of Objectives: There are three objectives of the contract: to perform research and development in parallel parsing, semantic representation, ill-formed input, discourse, and tools for linguistic knowledge acquisition, and to integrate software components from BBN and elsewhere to produce Janus, DARPA's New Generation Natural Language Interface, and to demonstrate state-of-the-art natural language technology in DARPA applications.	language technology	DARPA applications	usage	{'e1': {'word': 'language technology', 'word_index': [(71, 72)], 'id': 'H89-2057.8'}, 'e2': {'word': 'DARPA applications', 'word_index': [(74, 75)], 'id': 'H89-2057.9'}, 'entity_replacement': {'19:20': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '25:28': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '35:37': 'ENTITYUNRELATED', '42:43': 'ENTITYUNRELATED', '50:58': 'ENTITYUNRELATED', '71:72': 'ENTITY', '74:75': 'ENTITYOTHER'}}	Brief Summary of Objectives : There are three objectives of the contract : to perform research and development in parallel parsing , semantic representation , ill - formed input , discourse , and tools for linguistic knowledge acquisition , and to integrate software components from BBN and elsewhere to produce Janus , DARPA 's New Generation Natural Language Interface , and to demonstrate state - of - the - art natural language technology in DARPA applications .
In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence.	dependency tree	sentence	model-feature	{'e1': {'word': 'dependency tree', 'word_index': [(21, 22)], 'id': 'H01-1009.11'}, 'e2': {'word': 'sentence', 'word_index': [(25, 25)], 'id': 'H01-1009.12'}, 'entity_replacement': {'6:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '21:22': 'ENTITY', '25:25': 'ENTITYOTHER'}}	In this paper , we introduce Tree - Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence .
We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text.	Tree-Based Patterns	un-annotated text	part_whole	{'e1': {'word': 'Tree-Based Patterns', 'word_index': [(6, 9)], 'id': 'H01-1009.13'}, 'e2': {'word': 'un-annotated text', 'word_index': [(13, 16)], 'id': 'H01-1009.15'}, 'entity_replacement': {'6:9': 'ENTITY', '11:11': 'ENTITYUNRELATED', '13:16': 'ENTITYOTHER'}}	We outline the procedure to acquire Tree - Based Patterns in Japanese from un - annotated text .
The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns.	sentences	training data	part_whole	{'e1': {'word': 'sentences', 'word_index': [(5, 5)], 'id': 'H01-1009.16'}, 'e2': {'word': 'training data', 'word_index': [(8, 9)], 'id': 'H01-1009.17'}, 'entity_replacement': {'5:5': 'ENTITY', '8:9': 'ENTITYOTHER', '12:15': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	The system extracts the relevant sentences from the training data based on TF / IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns .
The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns.	parse tree	sentences	model-feature	{'e1': {'word': 'parse tree', 'word_index': [(22, 23)], 'id': 'H01-1009.20'}, 'e2': {'word': 'sentences', 'word_index': [(26, 26)], 'id': 'H01-1009.21'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '12:15': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '22:23': 'ENTITY', '26:26': 'ENTITYOTHER', '31:31': 'ENTITYUNRELATED'}}	The system extracts the relevant sentences from the training data based on TF / IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns .
To overcome this problem, we present in this paper a procedure for the automatic extraction of application-tuned consistent subgrammars from proved large-scale generation grammars.	application-tuned consistent subgrammars	large-scale generation grammars	part_whole	{'e1': {'word': 'application-tuned consistent subgrammars', 'word_index': [(17, 21)], 'id': 'W97-1507.6'}, 'e2': {'word': 'large-scale generation grammars', 'word_index': [(24, 28)], 'id': 'W97-1507.7'}, 'entity_replacement': {'14:15': 'ENTITYUNRELATED', '17:21': 'ENTITY', '24:28': 'ENTITYOTHER'}}	To overcome this problem , we present in this paper a procedure for the automatic extraction of application - tuned consistent subgrammars from proved large - scale generation grammars .
The procedure has been implemented for large-scale systemic grammars and builds on the formal equivalence between systemic grammars and typed unification based grammars.	systemic grammars	typed unification based grammars	compare	{'e1': {'word': 'systemic grammars', 'word_index': [(18, 19)], 'id': 'W97-1507.10'}, 'e2': {'word': 'typed unification based grammars', 'word_index': [(21, 24)], 'id': 'W97-1507.11'}, 'entity_replacement': {'6:10': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '18:19': 'ENTITY', '21:24': 'ENTITYOTHER'}}	The procedure has been implemented for large - scale systemic grammars and builds on the formal equivalence between systemic grammars and typed unification based grammars .
First, the task has been performed traditionally using heuristics from the domain.	heuristics	task	usage	{'e1': {'word': 'heuristics', 'word_index': [(9, 9)], 'id': 'W00-0719.4'}, 'e2': {'word': 'task', 'word_index': [(3, 3)], 'id': 'W00-0719.3'}, 'entity_replacement': {'3:3': 'ENTITYOTHER', '9:9': 'ENTITY', '12:12': 'ENTITYUNRELATED'}}	First , the task has been performed traditionally using heuristics from the domain .
We present a comparative evaluation of several machine learning algorithms applied to spam filtering, considering the text of the messages and a set of heuristics for the task.	machine learning algorithms	spam filtering	usage	{'e1': {'word': 'machine learning algorithms', 'word_index': [(7, 9)], 'id': 'W00-0719.10'}, 'e2': {'word': 'spam filtering', 'word_index': [(12, 13)], 'id': 'W00-0719.11'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:9': 'ENTITY', '12:13': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	We present a comparative evaluation of several machine learning algorithms applied to spam filtering , considering the text of the messages and a set of heuristics for the task .
We present a comparative evaluation of several machine learning algorithms applied to spam filtering, considering the text of the messages and a set of heuristics for the task.	heuristics	task	usage	{'e1': {'word': 'heuristics', 'word_index': [(25, 25)], 'id': 'W00-0719.14'}, 'e2': {'word': 'task', 'word_index': [(28, 28)], 'id': 'W00-0719.15'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:9': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '25:25': 'ENTITY', '28:28': 'ENTITYOTHER'}}	We present a comparative evaluation of several machine learning algorithms applied to spam filtering , considering the text of the messages and a set of heuristics for the task .
The framework includes a dialog history that tracks input, output, and results.	dialog history	framework	part_whole	{'e1': {'word': 'dialog history', 'word_index': [(4, 5)], 'id': 'W02-0209.11'}, 'e2': {'word': 'framework', 'word_index': [(1, 1)], 'id': 'W02-0209.10'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:5': 'ENTITY'}}	The framework includes a dialog history that tracks input , output , and results .
We present the framework and preliminary results in two application domains.	framework	application domains	usage	{'e1': {'word': 'framework', 'word_index': [(3, 3)], 'id': 'W02-0209.12'}, 'e2': {'word': 'application domains', 'word_index': [(9, 10)], 'id': 'W02-0209.13'}, 'entity_replacement': {'3:3': 'ENTITY', '9:10': 'ENTITYOTHER'}}	We present the framework and preliminary results in two application domains .
Near-perfect automatic accent assignment is attainable for citation-style speech, but better computational models are needed to predict accent in extended, spontaneous discourses.	automatic accent assignment	citation-style speech	usage	{'e1': {'word': 'automatic accent assignment', 'word_index': [(1, 3)], 'id': 'P98-2155.1'}, 'e2': {'word': 'citation-style speech', 'word_index': [(7, 10)], 'id': 'P98-2155.2'}, 'entity_replacement': {'1:3': 'ENTITY', '7:10': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '22:25': 'ENTITYUNRELATED'}}	Near-perfect automatic accent assignment is attainable for citation - style speech , but better computational models are needed to predict accent in extended , spontaneous discourses .
Near-perfect automatic accent assignment is attainable for citation-style speech, but better computational models are needed to predict accent in extended, spontaneous discourses.	computational models	extended, spontaneous discourses	usage	{'e1': {'word': 'computational models', 'word_index': [(14, 15)], 'id': 'P98-2155.3'}, 'e2': {'word': 'extended, spontaneous discourses', 'word_index': [(22, 25)], 'id': 'P98-2155.5'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '7:10': 'ENTITYUNRELATED', '14:15': 'ENTITY', '20:20': 'ENTITYUNRELATED', '22:25': 'ENTITYOTHER'}}	Near-perfect automatic accent assignment is attainable for citation - style speech , but better computational models are needed to predict accent in extended , spontaneous discourses .
Machine learning experiments on 1031 noun phrases from eighteen spontaneous direction-giving monologues show that accent assignment can be significantly improved by up to 4%-6% relative to a hypothetical baseline system that would produce only citation-form accentuation, giving error rate reductions of 11%-25%.	Machine learning experiments	noun phrases	usage	{'e1': {'word': 'Machine learning experiments', 'word_index': [(0, 2)], 'id': 'P98-2155.16'}, 'e2': {'word': 'noun phrases', 'word_index': [(5, 6)], 'id': 'P98-2155.17'}, 'entity_replacement': {'0:2': 'ENTITY', '5:6': 'ENTITYOTHER', '9:13': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '40:43': 'ENTITYUNRELATED', '46:48': 'ENTITYUNRELATED'}}	Machine learning experiments on 1031 noun phrases from eighteen spontaneous direction - giving monologues show that accent assignment can be significantly improved by up to 4 % - 6 % relative to a hypothetical baseline system that would produce only citation - form accentuation , giving error rate reductions of 11 % - 25 % .
A key task in an extraction system for query-oriented multi-document summarisation, necessary for computing relevance and redundancy, is modelling text semantics.	extraction system	query-oriented multi-document summarisation	usage	{'e1': {'word': 'extraction system', 'word_index': [(5, 6)], 'id': 'W06-0701.1'}, 'e2': {'word': 'query-oriented multi-document summarisation', 'word_index': [(8, 10)], 'id': 'W06-0701.2'}, 'entity_replacement': {'5:6': 'ENTITY', '8:10': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED'}}	A key task in an extraction system for query-oriented multi-document summarisation , necessary for computing relevance and redundancy , is modelling text semantics .
Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (ccg).	Combinatory Categorial Grammar (ccg)	wide-coverage statistical parsers	usage	{'e1': {'word': 'Combinatory Categorial Grammar (ccg)', 'word_index': [(17, 22)], 'id': 'W04-3215.3'}, 'e2': {'word': 'wide-coverage statistical parsers', 'word_index': [(11, 15)], 'id': 'W04-3215.2'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '11:15': 'ENTITYOTHER', '17:22': 'ENTITY'}}	Accurate dependency recovery has recently been reported for a number of wide - coverage statistical parsers using Combinatory Categorial Grammar ( ccg ) .
However, overall figures give no indication of a parser's performance on specific constructions, nor how suitable a parser is for specific applications.	parser	applications	usage	{'e1': {'word': 'parser', 'word_index': [(20, 20)], 'id': 'W04-3215.6'}, 'e2': {'word': 'applications', 'word_index': [(24, 24)], 'id': 'W04-3215.7'}, 'entity_replacement': {'9:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '20:20': 'ENTITY', '24:24': 'ENTITYOTHER'}}	However , overall figures give no indication of a parser 's performance on specific constructions , nor how suitable a parser is for specific applications .
In this paper we give a detailed evaluation of a ccg parser on object extraction dependencies found in wsj text.	object extraction dependencies	wsj text	part_whole	{'e1': {'word': 'object extraction dependencies', 'word_index': [(13, 15)], 'id': 'W04-3215.9'}, 'e2': {'word': 'wsj text', 'word_index': [(18, 19)], 'id': 'W04-3215.10'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '13:15': 'ENTITY', '18:19': 'ENTITYOTHER'}}	In this paper we give a detailed evaluation of a ccg parser on object extraction dependencies found in wsj text .
We also show how the parser can be used to parse questions for Question Answering.	parser	questions	usage	{'e1': {'word': 'parser', 'word_index': [(5, 5)], 'id': 'W04-3215.11'}, 'e2': {'word': 'questions', 'word_index': [(11, 11)], 'id': 'W04-3215.12'}, 'entity_replacement': {'5:5': 'ENTITY', '11:11': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED'}}	We also show how the parser can be used to parse questions for Question Answering .
The accuracy of the original parser on questions is very poor, and we propose a novel technique for porting the parser to a new domain, by creating new labelled data at the lexical category level only.	parser	accuracy	result	{'e1': {'word': 'parser', 'word_index': [(5, 5)], 'id': 'W04-3215.15'}, 'e2': {'word': 'accuracy', 'word_index': [(1, 1)], 'id': 'W04-3215.14'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '5:5': 'ENTITY', '7:7': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '34:36': 'ENTITYUNRELATED'}}	The accuracy of the original parser on questions is very poor , and we propose a novel technique for porting the parser to a new domain , by creating new labelled data at the lexical category level only .
Using a supertagger to assign categories to words, trained on the new data, leads to a dramatic increase in question parsing accuracy.	categories	words	model-feature	{'e1': {'word': 'categories', 'word_index': [(5, 5)], 'id': 'W04-3215.22'}, 'e2': {'word': 'words', 'word_index': [(7, 7)], 'id': 'W04-3215.23'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '5:5': 'ENTITY', '7:7': 'ENTITYOTHER', '21:23': 'ENTITYUNRELATED'}}	Using a supertagger to assign categories to words , trained on the new data , leads to a dramatic increase in question parsing accuracy .
This paper addresses syntax-based paraphrasing methods for Recognizing Textual Entailment (RTE).	syntax-based paraphrasing methods	Recognizing Textual Entailment (RTE)	usage	{'e1': {'word': 'syntax-based paraphrasing methods', 'word_index': [(3, 7)], 'id': 'W07-1414.1'}, 'e2': {'word': 'Recognizing Textual Entailment (RTE)', 'word_index': [(9, 14)], 'id': 'W07-1414.2'}, 'entity_replacement': {'3:7': 'ENTITY', '9:14': 'ENTITYOTHER'}}	This paper addresses syntax - based paraphrasing methods for Recognizing Textual Entailment ( RTE ) .
In particular, we describe a dependency-based paraphrasing algorithm, using the DIRT data set, and its application in the context of a straightforward RTE system based on aligning dependency trees.	DIRT data set	dependency-based paraphrasing algorithm	usage	{'e1': {'word': 'DIRT data set', 'word_index': [(14, 16)], 'id': 'W07-1414.4'}, 'e2': {'word': 'dependency-based paraphrasing algorithm', 'word_index': [(6, 10)], 'id': 'W07-1414.3'}, 'entity_replacement': {'6:10': 'ENTITYOTHER', '14:16': 'ENTITY', '27:28': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	In particular , we describe a dependency - based paraphrasing algorithm , using the DIRT data set , and its application in the context of a straightforward RTE system based on aligning dependency trees .
The goal of the project is to provide a quantitative description of Polish preposition-pronoun contractions taking into consideration morphosyntactic properties of their components.	morphosyntactic properties	Polish preposition-pronoun contractions	model-feature	{'e1': {'word': 'morphosyntactic properties', 'word_index': [(20, 21)], 'id': 'W06-2103.6'}, 'e2': {'word': 'Polish preposition-pronoun contractions', 'word_index': [(12, 16)], 'id': 'W06-2103.5'}, 'entity_replacement': {'12:16': 'ENTITYOTHER', '20:21': 'ENTITY'}}	The goal of the project is to provide a quantitative description of Polish preposition - pronoun contractions taking into consideration morphosyntactic properties of their components .
The results of corpus-based investigations of the distribution of prepositions within preposition-pronoun contractions can be used for grammar-theoretical and lexicographic purposes.	distribution	prepositions	model-feature	{'e1': {'word': 'distribution', 'word_index': [(9, 9)], 'id': 'W06-2103.11'}, 'e2': {'word': 'prepositions', 'word_index': [(11, 11)], 'id': 'W06-2103.12'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '9:9': 'ENTITY', '11:11': 'ENTITYOTHER', '13:16': 'ENTITYUNRELATED'}}	The results of corpus - based investigations of the distribution of prepositions within preposition - pronoun contractions can be used for grammar-theoretical and lexicographic purposes .
This paper presents a new unsupervised algorithm (WordEnds) for inferring word boundaries from transcribed adult conversations.	unsupervised algorithm	word boundaries	usage	{'e1': {'word': 'unsupervised algorithm', 'word_index': [(5, 6)], 'id': 'P08-1016.1'}, 'e2': {'word': 'word boundaries', 'word_index': [(13, 14)], 'id': 'P08-1016.3'}, 'entity_replacement': {'5:6': 'ENTITY', '8:9': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '16:18': 'ENTITYUNRELATED'}}	This paper presents a new unsupervised algorithm ( Word Ends ) for inferring word boundaries from transcribed adult conversations .
This fast algorithm delivers high performance even on morphologically complex words in English and Arabic, and promising results on accurate phonetic transcriptions with extensive pronunciation variation.	algorithm	performance	result	{'e1': {'word': 'algorithm', 'word_index': [(2, 2)], 'id': 'P08-1016.8'}, 'e2': {'word': 'performance', 'word_index': [(5, 5)], 'id': 'P08-1016.9'}, 'entity_replacement': {'2:2': 'ENTITY', '5:5': 'ENTITYOTHER', '8:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED'}}	This fast algorithm delivers high performance even on morphologically complex words in English and Arabic , and promising results on accurate phonetic transcriptions with extensive pronunciation variation .
This suggests that WordEnds is a viable model of child language acquisition and might be useful in speech understanding.	model	speech understanding	usage	{'e1': {'word': 'model', 'word_index': [(8, 8)], 'id': 'P08-1016.19'}, 'e2': {'word': 'speech understanding', 'word_index': [(18, 19)], 'id': 'P08-1016.21'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '8:8': 'ENTITY', '10:12': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER'}}	This suggests that Word Ends is a viable model of child language acquisition and might be useful in speech understanding .
Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disfluencies, or difficulties in the accurate identification of sentence boundaries	output	Speech-to-text summarization systems	usage	{'e1': {'word': 'output', 'word_index': [(9, 9)], 'id': 'W08-1406.3'}, 'e2': {'word': 'Speech-to-text summarization systems', 'word_index': [(0, 3)], 'id': 'W08-1406.1'}, 'entity_replacement': {'0:3': 'ENTITYOTHER', '7:7': 'ENTITYUNRELATED', '9:9': 'ENTITY', '12:18': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '38:39': 'ENTITYUNRELATED'}}	Speech-to- text summarization systems usually take as input the output of an automatic speech recognition ( ASR ) system that is affected by issues like speech recognition errors , disfluencies , or difficulties in the accurate identification of sentence boundaries
We propose the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech-to-text summarization.	multi-document summarization techniques	single document speech-to-text summarization	usage	{'e1': {'word': 'multi-document summarization techniques', 'word_index': [(23, 25)], 'id': 'W08-1406.10'}, 'e2': {'word': 'single document speech-to-text summarization', 'word_index': [(27, 34)], 'id': 'W08-1406.11'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '23:25': 'ENTITY', '27:34': 'ENTITYOTHER'}}	We propose the inclusion of related , solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech - to - text summarization .
We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences, while avoiding the need for syntactic analysis of the less resource rich language, and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models.	automated word alignment	parallel texts	usage	{'e1': {'word': 'automated word alignment', 'word_index': [(6, 8)], 'id': 'P04-3014.1'}, 'e2': {'word': 'parallel texts', 'word_index': [(10, 11)], 'id': 'P04-3014.2'}, 'entity_replacement': {'6:8': 'ENTITY', '10:11': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '30:33': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '40:42': 'ENTITYUNRELATED', '46:49': 'ENTITYUNRELATED'}}	We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences , while avoiding the need for syntactic analysis of the less resource rich language , and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models .
We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences, while avoiding the need for syntactic analysis of the less resource rich language, and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models.	syntactic analysis	less resource rich language	topic	{'e1': {'word': 'syntactic analysis', 'word_index': [(26, 27)], 'id': 'P04-3014.4'}, 'e2': {'word': 'less resource rich language', 'word_index': [(30, 33)], 'id': 'P04-3014.5'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '26:27': 'ENTITY', '30:33': 'ENTITYOTHER', '38:38': 'ENTITYUNRELATED', '40:42': 'ENTITYUNRELATED', '46:49': 'ENTITYUNRELATED'}}	We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences , while avoiding the need for syntactic analysis of the less resource rich language , and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models .
We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences, while avoiding the need for syntactic analysis of the less resource rich language, and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models.	robustness	syntactically agnostic approaches	model-feature	{'e1': {'word': 'robustness', 'word_index': [(38, 38)], 'id': 'P04-3014.6'}, 'e2': {'word': 'syntactically agnostic approaches', 'word_index': [(40, 42)], 'id': 'P04-3014.7'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '30:33': 'ENTITYUNRELATED', '38:38': 'ENTITY', '40:42': 'ENTITYOTHER', '46:49': 'ENTITYUNRELATED'}}	We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences , while avoiding the need for syntactic analysis of the less resource rich language , and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models .
We achieve this by using simple, easily-elicited knowledge to produce syntax-based heuristics which transform the target language (e.g. English) into a form more closely resembling the source language, and then by using standard alignment methods to align the transformed bitext.	syntax-based heuristics	target language	usage	{'e1': {'word': 'syntax-based heuristics', 'word_index': [(13, 16)], 'id': 'P04-3014.9'}, 'e2': {'word': 'target language', 'word_index': [(20, 21)], 'id': 'P04-3014.10'}, 'entity_replacement': {'13:16': 'ENTITY', '20:21': 'ENTITYOTHER', '24:24': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '40:42': 'ENTITYUNRELATED', '46:47': 'ENTITYUNRELATED'}}	We achieve this by using simple , easily - elicited knowledge to produce syntax - based heuristics which transform the target language ( e.g. English ) into a form more closely resembling the source language , and then by using standard alignment methods to align the transformed bitext .
The knowledge representation method is introduced to be applied in the ICAI system to teach programming language.	knowledge representation method	ICAI system	usage	{'e1': {'word': 'knowledge representation method', 'word_index': [(1, 3)], 'id': 'C82-1003.1'}, 'e2': {'word': 'ICAI system', 'word_index': [(11, 12)], 'id': 'C82-1003.2'}, 'entity_replacement': {'1:3': 'ENTITY', '11:12': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	The knowledge representation method is introduced to be applied in the ICAI system to teach programming language .
The directed graph of concepts is mentioned as a method to represent an instructional structure of the domain knowledge.	directed graph	concepts	model-feature	{'e1': {'word': 'directed graph', 'word_index': [(1, 2)], 'id': 'C82-1003.10'}, 'e2': {'word': 'concepts', 'word_index': [(4, 4)], 'id': 'C82-1003.11'}, 'entity_replacement': {'1:2': 'ENTITY', '4:4': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED'}}	The directed graph of concepts is mentioned as a method to represent an instructional structure of the domain knowledge .
We consider the problem of extracting specified types of information from natural language text.	information	natural language text	part_whole	{'e1': {'word': 'information', 'word_index': [(9, 9)], 'id': 'C90-3071.1'}, 'e2': {'word': 'natural language text', 'word_index': [(11, 13)], 'id': 'C90-3071.2'}, 'entity_replacement': {'9:9': 'ENTITY', '11:13': 'ENTITYOTHER'}}	We consider the problem of extracting specified types of information from natural language text .
We describe a specific information extraction task, and report on the benefits of using preference semantics for this task.	preference semantics	information extraction task	usage	{'e1': {'word': 'preference semantics', 'word_index': [(15, 16)], 'id': 'C90-3071.10'}, 'e2': {'word': 'information extraction task', 'word_index': [(4, 6)], 'id': 'C90-3071.9'}, 'entity_replacement': {'4:6': 'ENTITYOTHER', '15:16': 'ENTITY'}}	We describe a specific information extraction task , and report on the benefits of using preference semantics for this task .
The studies were the result of semantic annotation of the corpus in this domain.	semantic annotation	corpus	usage	{'e1': {'word': 'semantic annotation', 'word_index': [(6, 7)], 'id': 'L08-1593.3'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'L08-1593.4'}, 'entity_replacement': {'6:7': 'ENTITY', '10:10': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED'}}	The studies were the result of semantic annotation of the corpus in this domain .
We begin by explaining the criteria involved in the annotation process, not only for the colour categories but also for the colour groups created in order to do finer-grained analyses, presenting also some quantitative data regarding these categories and groups.	quantitative data	categories	model-feature	{'e1': {'word': 'quantitative data', 'word_index': [(37, 38)], 'id': 'L08-1593.11'}, 'e2': {'word': 'categories', 'word_index': [(41, 41)], 'id': 'L08-1593.12'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '37:38': 'ENTITY', '41:41': 'ENTITYOTHER', '43:43': 'ENTITYUNRELATED'}}	We begin by explaining the criteria involved in the annotation process , not only for the colour categories but also for the colour groups created in order to do finer - grained analyses , presenting also some quantitative data regarding these categories and groups .
We end by explaining how any user who wants to do serious studies using the corpus can collaborate in enhancing the corpus and making their semantic annotations widely available as well.	semantic annotations	corpus	model-feature	{'e1': {'word': 'semantic annotations', 'word_index': [(25, 26)], 'id': 'L08-1593.20'}, 'e2': {'word': 'corpus', 'word_index': [(21, 21)], 'id': 'L08-1593.19'}, 'entity_replacement': {'15:15': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER', '25:26': 'ENTITY'}}	We end by explaining how any user who wants to do serious studies using the corpus can collaborate in enhancing the corpus and making their semantic annotations widely available as well .
The processing tasks involved in reconstructing the temporal structure of a narrative ( Webber 's e/s structure) are formulated in terms of these two notions.	temporal structure	narrative	model-feature	{'e1': {'word': 'temporal structure', 'word_index': [(7, 8)], 'id': 'E87-1042.14'}, 'e2': {'word': 'narrative', 'word_index': [(11, 11)], 'id': 'E87-1042.15'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '7:8': 'ENTITY', '11:11': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	The processing tasks involved in reconstructing the temporal structure of a narrative ( Webber 's e/s structure ) are formulated in terms of these two notions .
The remainder of the paper analyzes the durational and aspectual knowledge needed for those tasks.	durational and aspectual knowledge	tasks	usage	{'e1': {'word': 'durational and aspectual knowledge', 'word_index': [(7, 10)], 'id': 'E87-1042.17'}, 'e2': {'word': 'tasks', 'word_index': [(14, 14)], 'id': 'E87-1042.18'}, 'entity_replacement': {'7:10': 'ENTITY', '14:14': 'ENTITYOTHER'}}	The remainder of the paper analyzes the durational and aspectual knowledge needed for those tasks .
In this paper we present a project which aims to standardise the format of a set of bilingual lexicons in order to make them available to potential users, to facilitate the exchange of data (among the resources and with other (monolingual) resources ) and to enable reuse of these lexicons for NLP applications like machine translation and multilingual information retrieval.	lexicons	NLP applications	usage	{'e1': {'word': 'lexicons', 'word_index': [(53, 53)], 'id': 'L08-1432.8'}, 'e2': {'word': 'NLP applications', 'word_index': [(55, 56)], 'id': 'L08-1432.9'}, 'entity_replacement': {'17:18': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '42:45': 'ENTITYUNRELATED', '53:53': 'ENTITY', '55:56': 'ENTITYOTHER', '58:59': 'ENTITYUNRELATED', '61:63': 'ENTITYUNRELATED'}}	In this paper we present a project which aims to standardise the format of a set of bilingual lexicons in order to make them available to potential users , to facilitate the exchange of data ( among the resources and with other ( monolingual ) resources ) and to enable reuse of these lexicons for NLP applications like machine translation and multilingual information retrieval .
After exploiting the linguistic discrepancy between numbered arguments and ARGMs, we built a semantic role classifier based on a hierarchical feature selection strategy.	linguistic discrepancy	arguments	model-feature	{'e1': {'word': 'linguistic discrepancy', 'word_index': [(3, 4)], 'id': 'D08-1034.10'}, 'e2': {'word': 'arguments', 'word_index': [(7, 7)], 'id': 'D08-1034.11'}, 'entity_replacement': {'3:4': 'ENTITY', '7:7': 'ENTITYOTHER', '9:9': 'ENTITYUNRELATED', '14:16': 'ENTITYUNRELATED', '20:23': 'ENTITYUNRELATED'}}	After exploiting the linguistic discrepancy between numbered arguments and ARGMs , we built a semantic role classifier based on a hierarchical feature selection strategy .
After exploiting the linguistic discrepancy between numbered arguments and ARGMs, we built a semantic role classifier based on a hierarchical feature selection strategy.	hierarchical feature selection strategy	semantic role classifier	usage	{'e1': {'word': 'hierarchical feature selection strategy', 'word_index': [(20, 23)], 'id': 'D08-1034.14'}, 'e2': {'word': 'semantic role classifier', 'word_index': [(14, 16)], 'id': 'D08-1034.13'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '14:16': 'ENTITYOTHER', '20:23': 'ENTITY'}}	After exploiting the linguistic discrepancy between numbered arguments and ARGMs , we built a semantic role classifier based on a hierarchical feature selection strategy .
In recent years tree kernels have been proposed for the automatic learning of natural language applications.	tree kernels	automatic learning	usage	{'e1': {'word': 'tree kernels', 'word_index': [(3, 4)], 'id': 'E06-1015.1'}, 'e2': {'word': 'automatic learning', 'word_index': [(10, 11)], 'id': 'E06-1015.2'}, 'entity_replacement': {'3:4': 'ENTITY', '10:11': 'ENTITYOTHER', '13:15': 'ENTITYUNRELATED'}}	In recent years tree kernels have been proposed for the automatic learning of natural language applications .
In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.	tree kernels	processing of natural language	usage	{'e1': {'word': 'tree kernels', 'word_index': [(7, 8)], 'id': 'E06-1015.7'}, 'e2': {'word': 'processing of natural language', 'word_index': [(14, 17)], 'id': 'E06-1015.8'}, 'entity_replacement': {'7:8': 'ENTITY', '14:17': 'ENTITYOTHER', '29:30': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED', '48:49': 'ENTITYUNRELATED', '52:53': 'ENTITYUNRELATED'}}	In this paper , we show that tree kernels are very helpful in the processing of natural language as ( a ) we provide a simple algorithm to compute tree kernels in linear average running time and ( b ) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods .
In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.	classification properties	tree kernels	model-feature	{'e1': {'word': 'classification properties', 'word_index': [(44, 45)], 'id': 'E06-1015.11'}, 'e2': {'word': 'tree kernels', 'word_index': [(48, 49)], 'id': 'E06-1015.12'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '14:17': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED', '44:45': 'ENTITY', '48:49': 'ENTITYOTHER', '52:53': 'ENTITYUNRELATED'}}	In this paper , we show that tree kernels are very helpful in the processing of natural language as ( a ) we provide a simple algorithm to compute tree kernels in linear average running time and ( b ) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods .
Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.	Support Vector Machines	argument classification task	usage	{'e1': {'word': 'Support Vector Machines', 'word_index': [(2, 4)], 'id': 'E06-1015.14'}, 'e2': {'word': 'argument classification task', 'word_index': [(8, 10)], 'id': 'E06-1015.15'}, 'entity_replacement': {'2:4': 'ENTITY', '8:10': 'ENTITYOTHER'}}	Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis .
This paper presents a study of military applications of advanced speech processing technology which includes three major elements: (1) review and assessment of current efforts in military applications of speech technology ; (2) identification of opportunities for future military applications of advanced speech technology ; and (3) identification of problem areas where research in speech processing is needed to meet application requirements, and of current research thrusts which appear promising.	speech processing technology	military applications	usage	{'e1': {'word': 'speech processing technology', 'word_index': [(10, 12)], 'id': 'H90-1103.2'}, 'e2': {'word': 'military applications', 'word_index': [(6, 7)], 'id': 'H90-1103.1'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '10:12': 'ENTITY', '29:30': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED', '43:44': 'ENTITYUNRELATED', '47:48': 'ENTITYUNRELATED', '61:62': 'ENTITYUNRELATED', '67:68': 'ENTITYUNRELATED'}}	This paper presents a study of military applications of advanced speech processing technology which includes three major elements : ( 1 ) review and assessment of current efforts in military applications of speech technology ; ( 2 ) identification of opportunities for future military applications of advanced speech technology ; and ( 3 ) identification of problem areas where research in speech processing is needed to meet application requirements , and of current research thrusts which appear promising .
This paper presents a study of military applications of advanced speech processing technology which includes three major elements: (1) review and assessment of current efforts in military applications of speech technology ; (2) identification of opportunities for future military applications of advanced speech technology ; and (3) identification of problem areas where research in speech processing is needed to meet application requirements, and of current research thrusts which appear promising.	speech technology	military applications	usage	{'e1': {'word': 'speech technology', 'word_index': [(32, 33)], 'id': 'H90-1103.4'}, 'e2': {'word': 'military applications', 'word_index': [(29, 30)], 'id': 'H90-1103.3'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '29:30': 'ENTITYOTHER', '32:33': 'ENTITY', '43:44': 'ENTITYUNRELATED', '47:48': 'ENTITYUNRELATED', '61:62': 'ENTITYUNRELATED', '67:68': 'ENTITYUNRELATED'}}	This paper presents a study of military applications of advanced speech processing technology which includes three major elements : ( 1 ) review and assessment of current efforts in military applications of speech technology ; ( 2 ) identification of opportunities for future military applications of advanced speech technology ; and ( 3 ) identification of problem areas where research in speech processing is needed to meet application requirements , and of current research thrusts which appear promising .
This paper presents a study of military applications of advanced speech processing technology which includes three major elements: (1) review and assessment of current efforts in military applications of speech technology ; (2) identification of opportunities for future military applications of advanced speech technology ; and (3) identification of problem areas where research in speech processing is needed to meet application requirements, and of current research thrusts which appear promising.	speech technology	military applications	usage	{'e1': {'word': 'speech technology', 'word_index': [(47, 48)], 'id': 'H90-1103.6'}, 'e2': {'word': 'military applications', 'word_index': [(43, 44)], 'id': 'H90-1103.5'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED', '43:44': 'ENTITYOTHER', '47:48': 'ENTITY', '61:62': 'ENTITYUNRELATED', '67:68': 'ENTITYUNRELATED'}}	This paper presents a study of military applications of advanced speech processing technology which includes three major elements : ( 1 ) review and assessment of current efforts in military applications of speech technology ; ( 2 ) identification of opportunities for future military applications of advanced speech technology ; and ( 3 ) identification of problem areas where research in speech processing is needed to meet application requirements , and of current research thrusts which appear promising .
The relationship of this study to previous assessments of military applications of speech technology is discussed, and substantial recent progress is noted.	speech technology	military applications	usage	{'e1': {'word': 'speech technology', 'word_index': [(12, 13)], 'id': 'H90-1103.10'}, 'e2': {'word': 'military applications', 'word_index': [(9, 10)], 'id': 'H90-1103.9'}, 'entity_replacement': {'9:10': 'ENTITYOTHER', '12:13': 'ENTITY'}}	The relationship of this study to previous assessments of military applications of speech technology is discussed , and substantial recent progress is noted .
Current efforts in military applications of speech technology which are highlighted include : (1) narrowband (2400 b/s) and very low-rate (50-1200 b/s) secure voice communication ; (2) voice/data integration in computer networks ; (3) speech recognition in fighter aircraft, military helicopters, battle management, and air traffic control training systems ; and (4) noise and interference removal for human listeners.	speech technology	military applications	usage	{'e1': {'word': 'speech technology', 'word_index': [(6, 7)], 'id': 'H90-1103.12'}, 'e2': {'word': 'military applications', 'word_index': [(3, 4)], 'id': 'H90-1103.11'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '6:7': 'ENTITY', '16:36': 'ENTITYUNRELATED', '41:47': 'ENTITYUNRELATED', '52:53': 'ENTITYUNRELATED', '55:56': 'ENTITYUNRELATED', '68:69': 'ENTITYUNRELATED', '75:78': 'ENTITYUNRELATED', '80:81': 'ENTITYUNRELATED'}}	Current efforts in military applications of speech technology which are highlighted include : ( 1 ) narrowband ( 2400 b / s ) and very low-rate ( 50 - 1200 b / s ) secure voice communication ; ( 2 ) voice / data integration in computer networks ; ( 3 ) speech recognition in fighter aircraft , military helicopters , battle management , and air traffic control training systems ; and ( 4 ) noise and interference removal for human listeners .
"The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels (""uniform lexico-structural processing "")."	declarative lexico-structural transformations	generation and transfer	usage	{'e1': {'word': 'declarative lexico-structural transformations', 'word_index': [(12, 14)], 'id': 'A00-1009.4'}, 'e2': {'word': 'generation and transfer', 'word_index': [(7, 9)], 'id': 'A00-1009.3'}, 'entity_replacement': {'7:9': 'ENTITYOTHER', '12:14': 'ENTITY', '16:17': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	"The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels ( "" uniform lexico-structural processing "" ) ."
 Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text.	Lexical co-occurrence statistics	syntactic analysis	usage	{'e1': {'word': 'Lexical co-occurrence statistics', 'word_index': [(0, 2)], 'id': 'W93-0307.1'}, 'e2': {'word': 'syntactic analysis', 'word_index': [(9, 10)], 'id': 'W93-0307.2'}, 'entity_replacement': {'0:2': 'ENTITY', '9:10': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED'}}	Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text .
However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters.	sparseness	data	model-feature	{'e1': {'word': 'sparseness', 'word_index': [(10, 10)], 'id': 'W93-0307.5'}, 'e2': {'word': 'data', 'word_index': [(12, 12)], 'id': 'W93-0307.6'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '10:10': 'ENTITY', '12:12': 'ENTITYOTHER', '21:23': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	However , analyses based solely on lexical relationships suffer from sparseness of data : it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters .
However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters.	less informed model	statistical parameters	usage	{'e1': {'word': 'less informed model', 'word_index': [(21, 23)], 'id': 'W93-0307.7'}, 'e2': {'word': 'statistical parameters', 'word_index': [(29, 30)], 'id': 'W93-0307.8'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '21:23': 'ENTITY', '29:30': 'ENTITYOTHER'}}	However , analyses based solely on lexical relationships suffer from sparseness of data : it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters .
"For example, the ""lexical association"" strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth 1991] takes into account only the attachment site (a verb or its direct object ) and the preposition, ignoring the object of the preposition.We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account."	"""lexical association"""	resolving ambiguous prepositional phrase attachments	usage	"{'e1': {'word': '""lexical association""', 'word_index': [(4, 7)], 'id': 'W93-0307.9'}, 'e2': {'word': 'resolving ambiguous prepositional phrase attachments', 'word_index': [(10, 14)], 'id': 'W93-0307.10'}, 'entity_replacement': {'4:7': 'ENTITY', '10:14': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED', '53:54': 'ENTITYUNRELATED', '60:62': 'ENTITYUNRELATED', '67:67': 'ENTITYUNRELATED'}}"	"For example , the "" lexical association "" strategy for resolving ambiguous prepositional phrase attachments [ Hindle and Rooth 1991 ] takes into account only the attachment site ( a verb or its direct object ) and the preposition , ignoring the object of the preposition . We investigated an extension of the lexical association strategy to make use of noun class information , thus permitting a disambiguation strategy to take more information into account ."
"For example, the ""lexical association"" strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth 1991] takes into account only the attachment site (a verb or its direct object ) and the preposition, ignoring the object of the preposition.We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account."	noun class information	disambiguation	usage	{'e1': {'word': 'noun class information', 'word_index': [(60, 62)], 'id': 'W93-0307.18'}, 'e2': {'word': 'disambiguation', 'word_index': [(67, 67)], 'id': 'W93-0307.19'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '10:14': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED', '53:54': 'ENTITYUNRELATED', '60:62': 'ENTITY', '67:67': 'ENTITYOTHER'}}	"For example , the "" lexical association "" strategy for resolving ambiguous prepositional phrase attachments [ Hindle and Rooth 1991 ] takes into account only the attachment site ( a verb or its direct object ) and the preposition , ignoring the object of the preposition . We investigated an extension of the lexical association strategy to make use of noun class information , thus permitting a disambiguation strategy to take more information into account ."
Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone, a qualitative analysis of results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation.	qualitative analysis	results	topic	{'e1': {'word': 'qualitative analysis', 'word_index': [(18, 19)], 'id': 'W93-0307.21'}, 'e2': {'word': 'results', 'word_index': [(21, 21)], 'id': 'W93-0307.22'}, 'entity_replacement': {'13:14': 'ENTITYUNRELATED', '18:19': 'ENTITY', '21:21': 'ENTITYOTHER', '30:32': 'ENTITYUNRELATED', '38:40': 'ENTITYUNRELATED', '44:44': 'ENTITYUNRELATED', '49:50': 'ENTITYUNRELATED'}}	Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone , a qualitative analysis of results suggests that the problem lies not in the noun class information , but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation .
Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone, a qualitative analysis of results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation.	multiplicity of classes	noun	model-feature	{'e1': {'word': 'multiplicity of classes', 'word_index': [(38, 40)], 'id': 'W93-0307.24'}, 'e2': {'word': 'noun', 'word_index': [(44, 44)], 'id': 'W93-0307.25'}, 'entity_replacement': {'13:14': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED', '38:40': 'ENTITY', '44:44': 'ENTITYOTHER', '49:50': 'ENTITYUNRELATED'}}	Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone , a qualitative analysis of results suggests that the problem lies not in the noun class information , but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation .
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.	context-dependent lexicon	error-driven HMM-based text chunk tagger	part_whole	{'e1': {'word': 'context-dependent lexicon', 'word_index': [(12, 14)], 'id': 'W00-0737.2'}, 'e2': {'word': 'error-driven HMM-based text chunk tagger', 'word_index': [(4, 10)], 'id': 'W00-0737.1'}, 'entity_replacement': {'4:10': 'ENTITYOTHER', '12:14': 'ENTITY'}}	This paper proposes an error-driven HMM - based text chunk tagger with context -dependent lexicon .
Compared with standard HMM-based tagger, this tagger incorporates more contextual information into a lexical entry.	contextual information	tagger	usage	{'e1': {'word': 'contextual information', 'word_index': [(12, 13)], 'id': 'W00-0737.5'}, 'e2': {'word': 'tagger', 'word_index': [(9, 9)], 'id': 'W00-0737.4'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '9:9': 'ENTITYOTHER', '12:13': 'ENTITY', '16:17': 'ENTITYUNRELATED'}}	Compared with standard HMM - based tagger , this tagger incorporates more contextual information into a lexical entry .
Finally, memory-based learning is adopted to further improve the performance of the chunk tagger.	memory-based learning	chunk tagger	usage	{'e1': {'word': 'memory-based learning', 'word_index': [(2, 5)], 'id': 'W00-0737.11'}, 'e2': {'word': 'chunk tagger', 'word_index': [(15, 16)], 'id': 'W00-0737.12'}, 'entity_replacement': {'2:5': 'ENTITY', '15:16': 'ENTITYOTHER'}}	Finally , memory - based learning is adopted to further improve the performance of the chunk tagger .
However, recent evaluations of a prototype prediction system showed that it significantly decreased the productivity of most translators who used it.	productivity	translators	model-feature	{'e1': {'word': 'productivity', 'word_index': [(15, 15)], 'id': 'W02-1020.6'}, 'e2': {'word': 'translators', 'word_index': [(18, 18)], 'id': 'W02-1020.7'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '15:15': 'ENTITY', '18:18': 'ENTITYOTHER'}}	However , recent evaluations of a prototype prediction system showed that it significantly decreased the productivity of most translators who used it .
In this paper, we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected benefit to the translator, rather than just trying to anticipate some amount of upcoming text.	predictions	expected benefit	result	{'e1': {'word': 'predictions', 'word_index': [(18, 18)], 'id': 'W02-1020.8'}, 'e2': {'word': 'expected benefit', 'word_index': [(22, 23)], 'id': 'W02-1020.9'}, 'entity_replacement': {'18:18': 'ENTITY', '22:23': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '37:38': 'ENTITYUNRELATED'}}	In this paper , we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected benefit to the translator , rather than just trying to anticipate some amount of upcoming text .
Using a model of a “typical translator” constructed from data collected in the evaluations of the prediction prototype, we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator.	data	prediction prototype	part_whole	{'e1': {'word': 'data', 'word_index': [(11, 11)], 'id': 'W02-1020.13'}, 'e2': {'word': 'prediction prototype', 'word_index': [(18, 19)], 'id': 'W02-1020.14'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '11:11': 'ENTITY', '18:19': 'ENTITYOTHER', '31:32': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED'}}	Using a model of a “ typical translator ” constructed from data collected in the evaluations of the prediction prototype , we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator .
"""We describe an experimental text-to-speech system that uses information about syntactic constituency, adjacency to a verb, and constituent length to determine prosodic phrasing for synthetic speech."	syntactic constituency	text-to-speech system	usage	{'e1': {'word': 'syntactic constituency', 'word_index': [(15, 16)], 'id': 'J90-3003.2'}, 'e2': {'word': 'text-to-speech system', 'word_index': [(5, 10)], 'id': 'J90-3003.1'}, 'entity_replacement': {'5:10': 'ENTITYOTHER', '15:16': 'ENTITY', '18:21': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED'}}	""" We describe an experimental text - to - speech system that uses information about syntactic constituency , adjacency to a verb , and constituent length to determine prosodic phrasing for synthetic speech ."
"Results so far indicate that the current system performs well when measured against a corpus of judgments of prosodic phrasing."""	judgments of prosodic phrasing	corpus	part_whole	{'e1': {'word': 'judgments of prosodic phrasing', 'word_index': [(16, 19)], 'id': 'J90-3003.13'}, 'e2': {'word': 'corpus', 'word_index': [(14, 14)], 'id': 'J90-3003.12'}, 'entity_replacement': {'14:14': 'ENTITYOTHER', '16:19': 'ENTITY'}}	"Results so far indicate that the current system performs well when measured against a corpus of judgments of prosodic phrasing . """
In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars.	potential entries	on-line corpus	part_whole	{'e1': {'word': 'potential entries', 'word_index': [(10, 11)], 'id': 'P98-2182.2'}, 'e2': {'word': 'on-line corpus', 'word_index': [(17, 20)], 'id': 'P98-2182.3'}, 'entity_replacement': {'10:11': 'ENTITY', '17:20': 'ENTITYOTHER', '28:28': 'ENTITYUNRELATED'}}	In this paper , we present an algorithm for extracting potential entries for a category from an on - line corpus , based upon a small set of exemplars .
In this paper, we describe our experiments on statistical word sense disambiguation (WSD) using two systems based on different approaches : Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features.	word tokens	Naive Bayes	usage	{'e1': {'word': 'word tokens', 'word_index': [(27, 28)], 'id': 'W04-0833.3'}, 'e2': {'word': 'Naive Bayes', 'word_index': [(24, 25)], 'id': 'W04-0833.2'}, 'entity_replacement': {'9:15': 'ENTITYUNRELATED', '24:25': 'ENTITYOTHER', '27:28': 'ENTITY', '30:31': 'ENTITYUNRELATED', '33:37': 'ENTITYUNRELATED'}}	In this paper , we describe our experiments on statistical word sense disambiguation ( WSD ) using two systems based on different approaches : Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features .
In this paper, we describe our experiments on statistical word sense disambiguation (WSD) using two systems based on different approaches : Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features.	local syntactic and semantic features	Maximum Entropy	usage	{'e1': {'word': 'local syntactic and semantic features', 'word_index': [(33, 37)], 'id': 'W04-0833.5'}, 'e2': {'word': 'Maximum Entropy', 'word_index': [(30, 31)], 'id': 'W04-0833.4'}, 'entity_replacement': {'9:15': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '30:31': 'ENTITYOTHER', '33:37': 'ENTITY'}}	In this paper , we describe our experiments on statistical word sense disambiguation ( WSD ) using two systems based on different approaches : Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features .
In the first approach, we consider a context window and a sub-window within it around the word to disambiguate.	sub-window	context window	part_whole	{'e1': {'word': 'sub-window', 'word_index': [(12, 14)], 'id': 'W04-0833.7'}, 'e2': {'word': 'context window', 'word_index': [(8, 9)], 'id': 'W04-0833.6'}, 'entity_replacement': {'8:9': 'ENTITYOTHER', '12:14': 'ENTITY', '19:19': 'ENTITYUNRELATED'}}	In the first approach , we consider a context window and a sub - window within it around the word to disambiguate .
In the second system, sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner.	syntactic structure	sense resolution	usage	{'e1': {'word': 'syntactic structure', 'word_index': [(12, 13)], 'id': 'W04-0833.18'}, 'e2': {'word': 'sense resolution', 'word_index': [(5, 6)], 'id': 'W04-0833.17'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '12:13': 'ENTITY', '17:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	In the second system , sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner .
In the second system, sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner.	semantics	neighboring nouns	model-feature	{'e1': {'word': 'semantics', 'word_index': [(17, 17)], 'id': 'W04-0833.19'}, 'e2': {'word': 'neighboring nouns', 'word_index': [(19, 20)], 'id': 'W04-0833.20'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '17:17': 'ENTITY', '19:20': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	In the second system , sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner .
The Document Understanding Conference (DUC) 2005 evaluation had a single user-oriented, question-focused summarization task, which was to synthesize from a set of 25-50 documents a well-organized, fluent answer to a complex question.	user-oriented, question-focused summarization task	documents	usage	{'e1': {'word': 'user-oriented, question-focused summarization task', 'word_index': [(12, 18)], 'id': 'W06-0707.2'}, 'e2': {'word': 'documents', 'word_index': [(31, 31)], 'id': 'W06-0707.3'}, 'entity_replacement': {'1:8': 'ENTITYUNRELATED', '12:18': 'ENTITY', '31:31': 'ENTITYOTHER', '37:38': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED'}}	The Document Understanding Conference ( DUC ) 2005 evaluation had a single user-oriented , question - focused summarization task , which was to synthesize from a set of 25 - 50 documents a well - organized , fluent answer to a complex question .
The evaluation shows that the best summarization systems have difficulty extracting relevant sentences in response to complex questions (as opposed to representative sentences that might be appropriate to a generic summary ).	summarization systems	extracting relevant sentences	usage	{'e1': {'word': 'summarization systems', 'word_index': [(6, 7)], 'id': 'W06-0707.6'}, 'e2': {'word': 'extracting relevant sentences', 'word_index': [(10, 12)], 'id': 'W06-0707.7'}, 'entity_replacement': {'6:7': 'ENTITY', '10:12': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	The evaluation shows that the best summarization systems have difficulty extracting relevant sentences in response to complex questions ( as opposed to representative sentences that might be appropriate to a generic summary ) .
This paper presents two systems for textual entailment, both employing decision trees as a supervised learning algorithm.	decision trees	textual entailment	usage	{'e1': {'word': 'decision trees', 'word_index': [(11, 12)], 'id': 'W07-1420.2'}, 'e2': {'word': 'textual entailment', 'word_index': [(6, 7)], 'id': 'W07-1420.1'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '11:12': 'ENTITY', '15:17': 'ENTITYUNRELATED'}}	This paper presents two systems for textual entailment , both employing decision trees as a supervised learning algorithm .
The first one is based primarily on the concept of lexical overlap, considering a bag of words similarity overlap measure to form a mapping of terms in the hypothesis to the source text.	bag of words similarity overlap measure	mapping	usage	{'e1': {'word': 'bag of words similarity overlap measure', 'word_index': [(15, 20)], 'id': 'W07-1420.5'}, 'e2': {'word': 'mapping', 'word_index': [(24, 24)], 'id': 'W07-1420.6'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '15:20': 'ENTITY', '24:24': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	The first one is based primarily on the concept of lexical overlap , considering a bag of words similarity overlap measure to form a mapping of terms in the hypothesis to the source text .
We contrast the MT condition, for both text and audio data types, with high quality human reference Gold Standard (GS) translations.	MT condition	human reference Gold Standard (GS) translations	compare	{'e1': {'word': 'MT condition', 'word_index': [(3, 4)], 'id': 'N07-2020.9'}, 'e2': {'word': 'human reference Gold Standard (GS) translations', 'word_index': [(17, 24)], 'id': 'N07-2020.12'}, 'entity_replacement': {'3:4': 'ENTITY', '8:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '17:24': 'ENTITYOTHER'}}	We contrast the MT condition , for both text and audio data types , with high quality human reference Gold Standard ( GS ) translations .
Overall, subjects achieved 95% comprehension for GS and 74% for MT, across 4 genres and 3 difficulty levels.	GS	MT	compare	{'e1': {'word': 'GS', 'word_index': [(8, 8)], 'id': 'N07-2020.14'}, 'e2': {'word': 'MT', 'word_index': [(13, 13)], 'id': 'N07-2020.16'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '8:8': 'ENTITY', '10:11': 'ENTITYUNRELATED', '13:13': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED'}}	Overall , subjects achieved 95 % comprehension for GS and 74 % for MT , across 4 genres and 3 difficulty levels .
This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations.	features	event-matching strategy	usage	{'e1': {'word': 'features', 'word_index': [(8, 8)], 'id': 'P08-2037.2'}, 'e2': {'word': 'event-matching strategy', 'word_index': [(5, 6)], 'id': 'P08-2037.1'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '8:8': 'ENTITY', '12:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED'}}	This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations .
This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations.	transitive closure	dependency relations	usage	{'e1': {'word': 'transitive closure', 'word_index': [(12, 13)], 'id': 'P08-2037.3'}, 'e2': {'word': 'dependency relations', 'word_index': [(15, 16)], 'id': 'P08-2037.4'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '12:13': 'ENTITY', '15:16': 'ENTITYOTHER'}}	This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations .
The method yields a model capable of matching events with an F-measure of 66.5%.	model	matching events	usage	{'e1': {'word': 'model', 'word_index': [(4, 4)], 'id': 'P08-2037.5'}, 'e2': {'word': 'matching events', 'word_index': [(7, 8)], 'id': 'P08-2037.6'}, 'entity_replacement': {'4:4': 'ENTITY', '7:8': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED'}}	The method yields a model capable of matching events with an F-measure of 66.5 %.
The objective of this paper is to demonstrate how an existing web application can be modified using VoiceXML to enable non-visual access from any phone.	VoiceXML	web application	usage	{'e1': {'word': 'VoiceXML', 'word_index': [(17, 17)], 'id': 'W08-1504.7'}, 'e2': {'word': 'web application', 'word_index': [(11, 12)], 'id': 'W08-1504.6'}, 'entity_replacement': {'11:12': 'ENTITYOTHER', '17:17': 'ENTITY', '20:22': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	The objective of this paper is to demonstrate how an existing web application can be modified using VoiceXML to enable non- visual access from any phone .
In order to elucidate the entire process, we present a sample Package Tracking System application, which is based on an existing website and provides the same functionality as the website does.	website	Package Tracking System application	usage	{'e1': {'word': 'website', 'word_index': [(23, 23)], 'id': 'W08-1504.19'}, 'e2': {'word': 'Package Tracking System application', 'word_index': [(12, 15)], 'id': 'W08-1504.18'}, 'entity_replacement': {'12:15': 'ENTITYOTHER', '23:23': 'ENTITY', '31:31': 'ENTITYUNRELATED'}}	In order to elucidate the entire process , we present a sample Package Tracking System application , which is based on an existing website and provides the same functionality as the website does .
This paper reports the development of log-linear models for the disambiguation in wide-coverage HPSG parsing.	log-linear models	disambiguation in wide-coverage HPSG parsing	usage	{'e1': {'word': 'log-linear models', 'word_index': [(6, 7)], 'id': 'P05-1011.1'}, 'e2': {'word': 'disambiguation in wide-coverage HPSG parsing', 'word_index': [(10, 16)], 'id': 'P05-1011.2'}, 'entity_replacement': {'6:7': 'ENTITY', '10:16': 'ENTITYOTHER'}}	This paper reports the development of log-linear models for the disambiguation in wide - coverage HPSG parsing .
The estimation of log-linear models requires high computational cost, especially with wide-coverage grammars.	computational cost	log-linear models	model-feature	{'e1': {'word': 'computational cost', 'word_index': [(9, 10)], 'id': 'P05-1011.4'}, 'e2': {'word': 'log-linear models', 'word_index': [(3, 6)], 'id': 'P05-1011.3'}, 'entity_replacement': {'3:6': 'ENTITYOTHER', '9:10': 'ENTITY', '14:17': 'ENTITYUNRELATED'}}	The estimation of log - linear models requires high computational cost , especially with wide - coverage grammars .
A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences.	disambiguation models	parsing	usage	{'e1': {'word': 'disambiguation models', 'word_index': [(17, 18)], 'id': 'P05-1011.9'}, 'e2': {'word': 'parsing', 'word_index': [(21, 21)], 'id': 'P05-1011.10'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '17:18': 'ENTITY', '21:21': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED'}}	A series of experiments empirically evaluated the estimation techniques , and also examined the performance of the disambiguation models on the parsing of real - world sentences .
 Text normalization is an important aspect of successful information retrieval from medical documents such as clinical notes, radiology reports and discharge summaries	information retrieval	medical documents	usage	{'e1': {'word': 'information retrieval', 'word_index': [(8, 9)], 'id': 'P02-1021.2'}, 'e2': {'word': 'medical documents', 'word_index': [(11, 12)], 'id': 'P02-1021.3'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '8:9': 'ENTITY', '11:12': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED'}}	Text normalization is an important aspect of successful information retrieval from medical documents such as clinical notes , radiology reports and discharge summaries
In the medical domain, a significant part of the general problem of text normalization is abbreviation and acronym disambiguation.	abbreviation and acronym disambiguation	text normalization	part_whole	{'e1': {'word': 'abbreviation and acronym disambiguation', 'word_index': [(16, 19)], 'id': 'P02-1021.9'}, 'e2': {'word': 'text normalization', 'word_index': [(13, 14)], 'id': 'P02-1021.8'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '16:19': 'ENTITY'}}	In the medical domain , a significant part of the general problem of text normalization is abbreviation and acronym disambiguation .
Numerous abbreviations are used routinely throughout such texts and knowing their meaning is critical to data retrieval from the document.	abbreviations	texts	part_whole	{'e1': {'word': 'abbreviations', 'word_index': [(1, 1)], 'id': 'P02-1021.10'}, 'e2': {'word': 'texts', 'word_index': [(7, 7)], 'id': 'P02-1021.11'}, 'entity_replacement': {'1:1': 'ENTITY', '7:7': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED'}}	Numerous abbreviations are used routinely throughout such texts and knowing their meaning is critical to data retrieval from the document .
Numerous abbreviations are used routinely throughout such texts and knowing their meaning is critical to data retrieval from the document.	data retrieval	document	usage	{'e1': {'word': 'data retrieval', 'word_index': [(15, 16)], 'id': 'P02-1021.12'}, 'e2': {'word': 'document', 'word_index': [(19, 19)], 'id': 'P02-1021.13'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '15:16': 'ENTITY', '19:19': 'ENTITYOTHER'}}	Numerous abbreviations are used routinely throughout such texts and knowing their meaning is critical to data retrieval from the document .
In this paper I will demonstrate a method of automatically generating training data for Maximum Entropy (ME) modeling of abbreviations and acronyms and will show that using ME modeling is a promising technique for abbreviation and acronym normalization.	automatically generating training data	Maximum Entropy (ME) modeling	usage	{'e1': {'word': 'automatically generating training data', 'word_index': [(9, 12)], 'id': 'P02-1021.14'}, 'e2': {'word': 'Maximum Entropy (ME) modeling', 'word_index': [(14, 19)], 'id': 'P02-1021.15'}, 'entity_replacement': {'9:12': 'ENTITY', '14:19': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '36:39': 'ENTITYUNRELATED'}}	In this paper I will demonstrate a method of automatically generating training data for Maximum Entropy ( ME ) modeling of abbreviations and acronyms and will show that using ME modeling is a promising technique for abbreviation and acronym normalization .
In this paper I will demonstrate a method of automatically generating training data for Maximum Entropy (ME) modeling of abbreviations and acronyms and will show that using ME modeling is a promising technique for abbreviation and acronym normalization.	ME modeling	abbreviation and acronym normalization	usage	{'e1': {'word': 'ME modeling', 'word_index': [(29, 30)], 'id': 'P02-1021.18'}, 'e2': {'word': 'abbreviation and acronym normalization', 'word_index': [(36, 39)], 'id': 'P02-1021.19'}, 'entity_replacement': {'9:12': 'ENTITYUNRELATED', '14:19': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '29:30': 'ENTITY', '36:39': 'ENTITYOTHER'}}	In this paper I will demonstrate a method of automatically generating training data for Maximum Entropy ( ME ) modeling of abbreviations and acronyms and will show that using ME modeling is a promising technique for abbreviation and acronym normalization .
I report on the results of an experiment involving training a number of ME models used to normalize abbreviations and acronyms on a sample of 10,000 rheumatology notes with ~89% accuracy.	ME models	abbreviations	usage	{'e1': {'word': 'ME models', 'word_index': [(13, 14)], 'id': 'P02-1021.20'}, 'e2': {'word': 'abbreviations', 'word_index': [(18, 18)], 'id': 'P02-1021.21'}, 'entity_replacement': {'13:14': 'ENTITY', '18:18': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	I report on the results of an experiment involving training a number of ME models used to normalize abbreviations and acronyms on a sample of 10,000 rheumatology notes with ~ 89 % accuracy .
In this paper we describe the roots of Controlled English (CE), the analysis of several existing CE grammars, the development of a well-founded 150-rule CE grammar (COGRAM), the elaboration of an algorithmic variant (ALCOGRAM) as a basis for NLP applications, the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively, and the preparatory study into a Controlled English grammar and style checker within a desktop publishing (DTP) environment.	algorithmic variant (ALCOGRAM)	NLP applications	usage	{'e1': {'word': 'algorithmic variant (ALCOGRAM)', 'word_index': [(42, 46)], 'id': 'C92-2090.4'}, 'e2': {'word': 'NLP applications', 'word_index': [(51, 52)], 'id': 'C92-2090.5'}, 'entity_replacement': {'8:12': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '29:36': 'ENTITYUNRELATED', '42:46': 'ENTITY', '51:52': 'ENTITYOTHER', '57:57': 'ENTITYUNRELATED', '60:61': 'ENTITYUNRELATED', '63:63': 'ENTITYUNRELATED', '76:78': 'ENTITYUNRELATED', '80:81': 'ENTITYUNRELATED', '84:89': 'ENTITYUNRELATED'}}	In this paper we describe the roots of Controlled English ( CE ) , the analysis of several existing CE grammars , the development of a well - founded 150 - rule CE grammar ( COGRAM ) , the elaboration of an algorithmic variant ( ALCOGRAM ) as a basis for NLP applications , the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively , and the preparatory study into a Controlled English grammar and style checker within a desktop publishing ( DTP ) environment .
In this paper we describe the roots of Controlled English (CE), the analysis of several existing CE grammars, the development of a well-founded 150-rule CE grammar (COGRAM), the elaboration of an algorithmic variant (ALCOGRAM) as a basis for NLP applications, the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively, and the preparatory study into a Controlled English grammar and style checker within a desktop publishing (DTP) environment.	ALCOGRAM	CA1 program	usage	{'e1': {'word': 'ALCOGRAM', 'word_index': [(57, 57)], 'id': 'C92-2090.6'}, 'e2': {'word': 'CA1 program', 'word_index': [(60, 61)], 'id': 'C92-2090.7'}, 'entity_replacement': {'8:12': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '29:36': 'ENTITYUNRELATED', '42:46': 'ENTITYUNRELATED', '51:52': 'ENTITYUNRELATED', '57:57': 'ENTITY', '60:61': 'ENTITYOTHER', '63:63': 'ENTITYUNRELATED', '76:78': 'ENTITYUNRELATED', '80:81': 'ENTITYUNRELATED', '84:89': 'ENTITYUNRELATED'}}	In this paper we describe the roots of Controlled English ( CE ) , the analysis of several existing CE grammars , the development of a well - founded 150 - rule CE grammar ( COGRAM ) , the elaboration of an algorithmic variant ( ALCOGRAM ) as a basis for NLP applications , the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively , and the preparatory study into a Controlled English grammar and style checker within a desktop publishing ( DTP ) environment .
In this paper we describe the roots of Controlled English (CE), the analysis of several existing CE grammars, the development of a well-founded 150-rule CE grammar (COGRAM), the elaboration of an algorithmic variant (ALCOGRAM) as a basis for NLP applications, the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively, and the preparatory study into a Controlled English grammar and style checker within a desktop publishing (DTP) environment.	style checker	desktop publishing (DTP) environment	part_whole	{'e1': {'word': 'style checker', 'word_index': [(80, 81)], 'id': 'C92-2090.10'}, 'e2': {'word': 'desktop publishing (DTP) environment', 'word_index': [(84, 89)], 'id': 'C92-2090.11'}, 'entity_replacement': {'8:12': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '29:36': 'ENTITYUNRELATED', '42:46': 'ENTITYUNRELATED', '51:52': 'ENTITYUNRELATED', '57:57': 'ENTITYUNRELATED', '60:61': 'ENTITYUNRELATED', '63:63': 'ENTITYUNRELATED', '76:78': 'ENTITYUNRELATED', '80:81': 'ENTITY', '84:89': 'ENTITYOTHER'}}	In this paper we describe the roots of Controlled English ( CE ) , the analysis of several existing CE grammars , the development of a well - founded 150 - rule CE grammar ( COGRAM ) , the elaboration of an algorithmic variant ( ALCOGRAM ) as a basis for NLP applications , the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively , and the preparatory study into a Controlled English grammar and style checker within a desktop publishing ( DTP ) environment .
We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement.	discriminative, latent variable approach	syntactic parsing	usage	{'e1': {'word': 'discriminative, latent variable approach', 'word_index': [(3, 7)], 'id': 'D08-1091.1'}, 'e2': {'word': 'syntactic parsing', 'word_index': [(9, 10)], 'id': 'D08-1091.2'}, 'entity_replacement': {'3:7': 'ENTITY', '9:10': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED'}}	We present a discriminative , latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement .
We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement.	refinement	rules	model-feature	{'e1': {'word': 'refinement', 'word_index': [(19, 19)], 'id': 'D08-1091.4'}, 'e2': {'word': 'rules', 'word_index': [(13, 13)], 'id': 'D08-1091.3'}, 'entity_replacement': {'3:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '13:13': 'ENTITYOTHER', '19:19': 'ENTITY'}}	We present a discriminative , latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement .
The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories ).	grammar productions	latent variable CRF grammar	usage	{'e1': {'word': 'grammar productions', 'word_index': [(16, 17)], 'id': 'D08-1091.7'}, 'e2': {'word': 'latent variable CRF grammar', 'word_index': [(5, 8)], 'id': 'D08-1091.5'}, 'entity_replacement': {'5:8': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '16:17': 'ENTITY', '20:20': 'ENTITYUNRELATED'}}	The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .
On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.	method	parsing accuracies	result	{'e1': {'word': 'method', 'word_index': [(9, 9)], 'id': 'D08-1091.19'}, 'e2': {'word': 'parsing accuracies', 'word_index': [(14, 15)], 'id': 'D08-1091.20'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '9:9': 'ENTITY', '14:15': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED'}}	On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further.	Tomita's GLR parsing algorithm	grammar PCFG*	usage	"{'e1': {'word': ""Tomita's GLR parsing algorithm"", 'word_index': [(16, 20)], 'id': 'C02-2028.3'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(9, 11)], 'id': 'C02-2028.2'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '9:11': 'ENTITYOTHER', '16:20': 'ENTITY'}}"	This paper presents an extended GLR parsing algorithm with grammar PCFG * that is based on Tomita 's GLR parsing algorithm and extends it further .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	PCFG	grammar PCFG*	usage	{'e1': {'word': 'PCFG', 'word_index': [(12, 12)], 'id': 'C02-2028.5'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(5, 7)], 'id': 'C02-2028.4'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '12:12': 'ENTITY', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	We also define a new grammar PCFG * that is based on PCFG and assigns not only probability but also frequency associated with each rule .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	frequency	rule	model-feature	{'e1': {'word': 'frequency', 'word_index': [(20, 20)], 'id': 'C02-2028.7'}, 'e2': {'word': 'rule', 'word_index': [(24, 24)], 'id': 'C02-2028.8'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITY', '24:24': 'ENTITYOTHER'}}	We also define a new grammar PCFG * that is based on PCFG and assigns not only probability but also frequency associated with each rule .
So our syntactic parsing system is implemented based on rule-based approach and statistics approach.	rule-based approach	syntactic parsing system	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(9, 12)], 'id': 'C02-2028.10'}, 'e2': {'word': 'syntactic parsing system', 'word_index': [(2, 4)], 'id': 'C02-2028.9'}, 'entity_replacement': {'2:4': 'ENTITYOTHER', '9:12': 'ENTITY', '14:15': 'ENTITYUNRELATED'}}	So our syntactic parsing system is implemented based on rule - based approach and statistics approach .
In this paper a method for controlling the dialog in a natural language (NL) system is presented.	controlling the dialog	natural language (NL) system	part_whole	{'e1': {'word': 'controlling the dialog', 'word_index': [(6, 8)], 'id': 'E89-1004.1'}, 'e2': {'word': 'natural language (NL) system', 'word_index': [(11, 16)], 'id': 'E89-1004.2'}, 'entity_replacement': {'6:8': 'ENTITY', '11:16': 'ENTITYOTHER'}}	In this paper a method for controlling the dialog in a natural language ( NL ) system is presented .
It provides a deep modeling of information processing based on time dependent propositional attitudes of the interacting agents.	time dependent propositional attitudes	modeling	usage	{'e1': {'word': 'time dependent propositional attitudes', 'word_index': [(10, 13)], 'id': 'E89-1004.5'}, 'e2': {'word': 'modeling', 'word_index': [(4, 4)], 'id': 'E89-1004.3'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '6:7': 'ENTITYUNRELATED', '10:13': 'ENTITY', '16:17': 'ENTITYUNRELATED'}}	It provides a deep modeling of information processing based on time dependent propositional attitudes of the interacting agents .
Knowledge about the state of the dialog is represented in a dedicated language and changes of this state are described by a compact set of rules.	state	dialog	model-feature	{'e1': {'word': 'state', 'word_index': [(3, 3)], 'id': 'E89-1004.8'}, 'e2': {'word': 'dialog', 'word_index': [(6, 6)], 'id': 'E89-1004.9'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '3:3': 'ENTITY', '6:6': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	Knowledge about the state of the dialog is represented in a dedicated language and changes of this state are described by a compact set of rules .
An appropriate organization of rule application is introduced including the initiation of an adequate system reaction.	initiation	rule application	part_whole	{'e1': {'word': 'initiation', 'word_index': [(10, 10)], 'id': 'E89-1004.14'}, 'e2': {'word': 'rule application', 'word_index': [(4, 5)], 'id': 'E89-1004.13'}, 'entity_replacement': {'4:5': 'ENTITYOTHER', '10:10': 'ENTITY', '13:15': 'ENTITYUNRELATED'}}	An appropriate organization of rule application is introduced including the initiation of an adequate system reaction .
This paper reports the principles behind designing a tagset to cover Russian morphosyntactic phenomena, modifications of the core tagset, and its evaluation.	tagset	Russian morphosyntactic phenomena	model-feature	{'e1': {'word': 'tagset', 'word_index': [(8, 8)], 'id': 'L08-1539.1'}, 'e2': {'word': 'Russian morphosyntactic phenomena', 'word_index': [(11, 13)], 'id': 'L08-1539.2'}, 'entity_replacement': {'8:8': 'ENTITY', '11:13': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	This paper reports the principles behind designing a tagset to cover Russian morphosyntactic phenomena , modifications of the core tagset , and its evaluation .
The final tagset contains about 600 tags and achieves about 95% accuracy on the disambiguated portion of the Russian National Corpus.	tags	tagset	part_whole	{'e1': {'word': 'tags', 'word_index': [(6, 6)], 'id': 'L08-1539.9'}, 'e2': {'word': 'tagset', 'word_index': [(2, 2)], 'id': 'L08-1539.8'}, 'entity_replacement': {'2:2': 'ENTITYOTHER', '6:6': 'ENTITY', '10:12': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED'}}	The final tagset contains about 600 tags and achieves about 95 % accuracy on the disambiguated portion of the Russian National Corpus .
Within the framework of translation knowledge acquisition from WWW news sites, this paper studies issues on the effect of cross-language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora.	translation knowledge acquisition	WWW news sites	usage	{'e1': {'word': 'translation knowledge acquisition', 'word_index': [(4, 6)], 'id': 'E03-1023.1'}, 'e2': {'word': 'WWW news sites', 'word_index': [(8, 10)], 'id': 'E03-1023.2'}, 'entity_replacement': {'4:6': 'ENTITY', '8:10': 'ENTITYOTHER', '20:22': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:29': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED'}}	Within the framework of translation knowledge acquisition from WWW news sites , this paper studies issues on the effect of cross -language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora .
Within the framework of translation knowledge acquisition from WWW news sites, this paper studies issues on the effect of cross-language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora.	texts	comparable corpora	part_whole	{'e1': {'word': 'texts', 'word_index': [(25, 25)], 'id': 'E03-1023.4'}, 'e2': {'word': 'comparable corpora', 'word_index': [(31, 32)], 'id': 'E03-1023.6'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '8:10': 'ENTITYUNRELATED', '20:22': 'ENTITYUNRELATED', '25:25': 'ENTITY', '27:29': 'ENTITYUNRELATED', '31:32': 'ENTITYOTHER'}}	Within the framework of translation knowledge acquisition from WWW news sites , this paper studies issues on the effect of cross -language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora .
We experimentally show that it is quite effective to reduce the candidate bilingual term pairs against which bilingual term correspondences are estimated, in terms of both computational complexity and the performance of precise estimation of bilingual term correspondences.	estimation	bilingual term correspondences	model-feature	{'e1': {'word': 'estimation', 'word_index': [(34, 34)], 'id': 'E03-1023.10'}, 'e2': {'word': 'bilingual term correspondences', 'word_index': [(36, 38)], 'id': 'E03-1023.11'}, 'entity_replacement': {'11:14': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '34:34': 'ENTITY', '36:38': 'ENTITYOTHER'}}	We experimentally show that it is quite effective to reduce the candidate bilingual term pairs against which bilingual term correspondences are estimated , in terms of both computational complexity and the performance of precise estimation of bilingual term correspondences .
We present an approach to building a test collection of research papers.	research papers	test collection	part_whole	{'e1': {'word': 'research papers', 'word_index': [(10, 11)], 'id': 'N06-1050.2'}, 'e2': {'word': 'test collection', 'word_index': [(7, 8)], 'id': 'N06-1050.1'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '10:11': 'ENTITY'}}	We present an approach to building a test collection of research papers .
The resultant test collection is different from TREC's in that it comprises scientific articles rather than newspaper text and, thus, allows for IR experiments that include citation information.	scientific articles	test collection	part_whole	{'e1': {'word': 'scientific articles', 'word_index': [(13, 14)], 'id': 'N06-1050.9'}, 'e2': {'word': 'test collection', 'word_index': [(2, 3)], 'id': 'N06-1050.7'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '7:8': 'ENTITYUNRELATED', '13:14': 'ENTITY', '17:18': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	The resultant test collection is different from TREC 's in that it comprises scientific articles rather than newspaper text and , thus , allows for IR experiments that include citation information .
The test collection currently consists of 170 queries with relevance judgements; the document collection is the ACL Anthology.	queries	test collection	part_whole	{'e1': {'word': 'queries', 'word_index': [(7, 7)], 'id': 'N06-1050.14'}, 'e2': {'word': 'test collection', 'word_index': [(1, 2)], 'id': 'N06-1050.13'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '7:7': 'ENTITY', '9:10': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	The test collection currently consists of 170 queries with relevance judgements ; the document collection is the ACL Anthology .
This paper presents a model for generating prosodically appropriate synthesized responses to database queries using Combinatory Categorial Grammar (CCG - cf.	Combinatory Categorial Grammar	prosodically appropriate synthesized responses	usage	{'e1': {'word': 'Combinatory Categorial Grammar', 'word_index': [(15, 17)], 'id': 'H94-1035.3'}, 'e2': {'word': 'prosodically appropriate synthesized responses', 'word_index': [(7, 10)], 'id': 'H94-1035.1'}, 'entity_replacement': {'7:10': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '15:17': 'ENTITY'}}	This paper presents a model for generating prosodically appropriate synthesized responses to database queries using Combinatory Categorial Grammar ( CCG - cf.
As part of our TIPSTER III research program, we have continued our research into strategies to resolve coreferences within a free text document; this research was begun during our TIPSTER II research program.	coreferences	free text document	part_whole	{'e1': {'word': 'coreferences', 'word_index': [(18, 18)], 'id': 'X98-1010.2'}, 'e2': {'word': 'free text document', 'word_index': [(21, 23)], 'id': 'X98-1010.3'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '18:18': 'ENTITY', '21:23': 'ENTITYOTHER', '31:34': 'ENTITYUNRELATED'}}	As part of our TIPSTER III research program , we have continued our research into strategies to resolve coreferences within a free text document ; this research was begun during our TIPSTER II research program .
It also has raised the importance of understanding the structure of a document in order to guide the coreference resolution process.	structure	document	model-feature	{'e1': {'word': 'structure', 'word_index': [(9, 9)], 'id': 'X98-1010.19'}, 'e2': {'word': 'document', 'word_index': [(12, 12)], 'id': 'X98-1010.20'}, 'entity_replacement': {'9:9': 'ENTITY', '12:12': 'ENTITYOTHER', '18:20': 'ENTITYUNRELATED'}}	It also has raised the importance of understanding the structure of a document in order to guide the coreference resolution process .
We describe the design and implementation of the dialogue management module in a voice operated car-driver information system.	dialogue management module	voice operated car-driver information system	part_whole	{'e1': {'word': 'dialogue management module', 'word_index': [(8, 10)], 'id': 'W97-0616.1'}, 'e2': {'word': 'voice operated car-driver information system', 'word_index': [(13, 17)], 'id': 'W97-0616.2'}, 'entity_replacement': {'8:10': 'ENTITY', '13:17': 'ENTITYOTHER'}}	We describe the design and implementation of the dialogue management module in a voice operated car-driver information system .
In this paper, we show how these constraints influence the design and subsequent implementation of the Dialogue Manager module, and how the additional requirements fit in with the 7 commandments.	constraints	Dialogue Manager module	result	{'e1': {'word': 'constraints', 'word_index': [(8, 8)], 'id': 'W97-0616.13'}, 'e2': {'word': 'Dialogue Manager module', 'word_index': [(17, 19)], 'id': 'W97-0616.14'}, 'entity_replacement': {'8:8': 'ENTITY', '17:19': 'ENTITYOTHER', '31:31': 'ENTITYUNRELATED'}}	In this paper , we show how these constraints influence the design and subsequent implementation of the Dialogue Manager module , and how the additional requirements fit in with the 7 commandments .
HMM-based models are developed for the alignment of words and phrases in bitext	HMM-based models	alignment	usage	{'e1': {'word': 'HMM-based models', 'word_index': [(0, 3)], 'id': 'H05-1022.1'}, 'e2': {'word': 'alignment', 'word_index': [(8, 8)], 'id': 'H05-1022.2'}, 'entity_replacement': {'0:3': 'ENTITY', '8:8': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED'}}	HMM - based models are developed for the alignment of words and phrases in bitext
We find that Chinese-English word alignment performance is comparable to that of IBM Model-4 even over large training bitexts.	Chinese-English word alignment performance	IBM Model-4	compare	{'e1': {'word': 'Chinese-English word alignment performance', 'word_index': [(3, 8)], 'id': 'H05-1022.9'}, 'e2': {'word': 'IBM Model-4', 'word_index': [(14, 16)], 'id': 'H05-1022.10'}, 'entity_replacement': {'3:8': 'ENTITY', '14:16': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	We find that Chinese - English word alignment performance is comparable to that of IBM Model -4 even over large training bitexts .
Phrase pairs extracted from word alignments generated under the model can also be used for phrase-based translation, and in Chinese to English and Arabic to English translation, performance is comparable to systems based on Model-4 alignments.	Phrase pairs	word alignments	part_whole	{'e1': {'word': 'Phrase pairs', 'word_index': [(0, 1)], 'id': 'H05-1022.12'}, 'e2': {'word': 'word alignments', 'word_index': [(4, 5)], 'id': 'H05-1022.13'}, 'entity_replacement': {'0:1': 'ENTITY', '4:5': 'ENTITYOTHER', '9:9': 'ENTITYUNRELATED', '15:18': 'ENTITYUNRELATED', '22:29': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '38:41': 'ENTITYUNRELATED'}}	Phrase pairs extracted from word alignments generated under the model can also be used for phrase - based translation , and in Chinese to English and Arabic to English translation , performance is comparable to systems based on Model - 4 alignments .
Phrase pairs extracted from word alignments generated under the model can also be used for phrase-based translation, and in Chinese to English and Arabic to English translation, performance is comparable to systems based on Model-4 alignments.	Model-4 alignments	systems	usage	{'e1': {'word': 'Model-4 alignments', 'word_index': [(38, 41)], 'id': 'H05-1022.19'}, 'e2': {'word': 'systems', 'word_index': [(35, 35)], 'id': 'H05-1022.18'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '4:5': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '15:18': 'ENTITYUNRELATED', '22:29': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '35:35': 'ENTITYOTHER', '38:41': 'ENTITY'}}	Phrase pairs extracted from word alignments generated under the model can also be used for phrase - based translation , and in Chinese to English and Arabic to English translation , performance is comparable to systems based on Model - 4 alignments .
This paper proposed a new query translation method based on the mutual information matrices of terms in the Chinese and English corpora.	mutual information matrices	query translation method	usage	{'e1': {'word': 'mutual information matrices', 'word_index': [(11, 13)], 'id': 'W00-1313.2'}, 'e2': {'word': 'query translation method', 'word_index': [(5, 7)], 'id': 'W00-1313.1'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '11:13': 'ENTITY', '15:15': 'ENTITYUNRELATED', '18:21': 'ENTITYUNRELATED'}}	This paper proposed a new query translation method based on the mutual information matrices of terms in the Chinese and English corpora .
A novel selection method for translations of query terms is also presented in detail.	selection method	translations	usage	{'e1': {'word': 'selection method', 'word_index': [(2, 3)], 'id': 'W00-1313.13'}, 'e2': {'word': 'translations', 'word_index': [(5, 5)], 'id': 'W00-1313.14'}, 'entity_replacement': {'2:3': 'ENTITY', '5:5': 'ENTITYOTHER', '7:8': 'ENTITYUNRELATED'}}	A novel selection method for translations of query terms is also presented in detail .
The evaluation results show that the retrieval performance achieved by our query translation method is about 73% of monolingual information retrieval and is about 28% higher than that of simple word-by-word translation way.	query translation method	retrieval performance	result	{'e1': {'word': 'query translation method', 'word_index': [(11, 13)], 'id': 'W00-1313.21'}, 'e2': {'word': 'retrieval performance', 'word_index': [(6, 7)], 'id': 'W00-1313.20'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '11:13': 'ENTITY', '19:21': 'ENTITYUNRELATED', '32:37': 'ENTITYUNRELATED'}}	The evaluation results show that the retrieval performance achieved by our query translation method is about 73 % of monolingual information retrieval and is about 28 % higher than that of simple word - by - word translation way .
First, we describe a method of classifying facts (information) into categories or levels; where each level signifies a different degree of difficulty of extracting a fact from a piece of text containing it.	level	degree of difficulty	model-feature	{'e1': {'word': 'level', 'word_index': [(19, 19)], 'id': 'W00-0106.4'}, 'e2': {'word': 'degree of difficulty', 'word_index': [(23, 25)], 'id': 'W00-0106.5'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '19:19': 'ENTITY', '23:25': 'ENTITYOTHER', '29:29': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	First , we describe a method of classifying facts ( information ) into categories or levels ; where each level signifies a different degree of difficulty of extracting a fact from a piece of text containing it .
First, we describe a method of classifying facts (information) into categories or levels; where each level signifies a different degree of difficulty of extracting a fact from a piece of text containing it.	fact	text	part_whole	{'e1': {'word': 'fact', 'word_index': [(29, 29)], 'id': 'W00-0106.6'}, 'e2': {'word': 'text', 'word_index': [(34, 34)], 'id': 'W00-0106.7'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '23:25': 'ENTITYUNRELATED', '29:29': 'ENTITY', '34:34': 'ENTITYOTHER'}}	First , we describe a method of classifying facts ( information ) into categories or levels ; where each level signifies a different degree of difficulty of extracting a fact from a piece of text containing it .
The two main factors that characterize a text are its content and its style, and both can be used as a means of categorization.	categorization	text	usage	{'e1': {'word': 'categorization', 'word_index': [(24, 24)], 'id': 'J00-4001.4'}, 'e2': {'word': 'text', 'word_index': [(7, 7)], 'id': 'J00-4001.1'}, 'entity_replacement': {'7:7': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '24:24': 'ENTITY'}}	The two main factors that characterize a text are its content and its style , and both can be used as a means of categorization .
In this paper we present an approach to text categorization in terms of genre and author for Modern Greek.	genre	text categorization	usage	{'e1': {'word': 'genre', 'word_index': [(13, 13)], 'id': 'J00-4001.6'}, 'e2': {'word': 'text categorization', 'word_index': [(8, 9)], 'id': 'J00-4001.5'}, 'entity_replacement': {'8:9': 'ENTITYOTHER', '13:13': 'ENTITY', '17:18': 'ENTITYUNRELATED'}}	In this paper we present an approach to text categorization in terms of genre and author for Modern Greek .
To this end, we propose a set of style markers including analysis-level measures that represent the way in which the input text has been analyzed and capture useful stylistic information without additional cost.	analysis-level measures	style markers	part_whole	{'e1': {'word': 'analysis-level measures', 'word_index': [(12, 13)], 'id': 'J00-4001.11'}, 'e2': {'word': 'style markers', 'word_index': [(9, 10)], 'id': 'J00-4001.10'}, 'entity_replacement': {'9:10': 'ENTITYOTHER', '12:13': 'ENTITY', '21:22': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	To this end , we propose a set of style markers including analysis-level measures that represent the way in which the input text has been analyzed and capture useful stylistic information without additional cost .
We present a set of small-scale but reasonable experiments in text genre detection, author identification, and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence of the most frequent words.	proposed method	distributional lexical measures	compare	{'e1': {'word': 'proposed method', 'word_index': [(27, 28)], 'id': 'J00-4001.17'}, 'e2': {'word': 'distributional lexical measures', 'word_index': [(35, 37)], 'id': 'J00-4001.18'}, 'entity_replacement': {'12:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '27:28': 'ENTITY', '35:37': 'ENTITYOTHER', '43:44': 'ENTITYUNRELATED', '46:48': 'ENTITYUNRELATED', '53:53': 'ENTITYUNRELATED'}}	We present a set of small - scale but reasonable experiments in text genre detection , author identification , and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures , i.e. , functions of vocabulary richness and frequencies of occurrence of the most frequent words .
We present a set of small-scale but reasonable experiments in text genre detection, author identification, and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence of the most frequent words.	frequencies of occurrence	words	model-feature	{'e1': {'word': 'frequencies of occurrence', 'word_index': [(46, 48)], 'id': 'J00-4001.20'}, 'e2': {'word': 'words', 'word_index': [(53, 53)], 'id': 'J00-4001.21'}, 'entity_replacement': {'12:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '35:37': 'ENTITYUNRELATED', '43:44': 'ENTITYUNRELATED', '46:48': 'ENTITY', '53:53': 'ENTITYOTHER'}}	We present a set of small - scale but reasonable experiments in text genre detection , author identification , and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures , i.e. , functions of vocabulary richness and frequencies of occurrence of the most frequent words .
All the presented experiments are based on unrestricted text downloaded from the World Wide Web without any manual text preprocessing or text sampling.	unrestricted text	World Wide Web	part_whole	{'e1': {'word': 'unrestricted text', 'word_index': [(7, 8)], 'id': 'J00-4001.22'}, 'e2': {'word': 'World Wide Web', 'word_index': [(12, 14)], 'id': 'J00-4001.23'}, 'entity_replacement': {'7:8': 'ENTITY', '12:14': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED'}}	All the presented experiments are based on unrestricted text downloaded from the World Wide Web without any manual text preprocessing or text sampling .
Our system can be used in any application that requires fast and easily adaptable text categorization in terms of stylistically homogeneous categories.	system	application	usage	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'J00-4001.28'}, 'e2': {'word': 'application', 'word_index': [(7, 7)], 'id': 'J00-4001.29'}, 'entity_replacement': {'1:1': 'ENTITY', '7:7': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED'}}	Our system can be used in any application that requires fast and easily adaptable text categorization in terms of stylistically homogeneous categories .
We first split a dataset consisting of pairs of sentences into clusters according to their similarities, and then construct a classifier for each cluster to identify equivalence relations.	sentences	dataset	part_whole	{'e1': {'word': 'sentences', 'word_index': [(9, 9)], 'id': 'I08-1019.19'}, 'e2': {'word': 'dataset', 'word_index': [(4, 4)], 'id': 'I08-1019.18'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '9:9': 'ENTITY', '11:11': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	We first split a dataset consisting of pairs of sentences into clusters according to their similarities , and then construct a classifier for each cluster to identify equivalence relations .
In this paper, we propose a new learning method to solve the sparse data problem in automatic extraction of bilingual word pairs from parallel corpora with various languages.	learning method	sparse data problem	usage	{'e1': {'word': 'learning method', 'word_index': [(8, 9)], 'id': 'W05-1010.1'}, 'e2': {'word': 'sparse data problem', 'word_index': [(13, 15)], 'id': 'W05-1010.2'}, 'entity_replacement': {'8:9': 'ENTITY', '13:15': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '20:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	In this paper , we propose a new learning method to solve the sparse data problem in automatic extraction of bilingual word pairs from parallel corpora with various languages .
In this paper, we propose a new learning method to solve the sparse data problem in automatic extraction of bilingual word pairs from parallel corpora with various languages.	bilingual word pairs	parallel corpora	part_whole	{'e1': {'word': 'bilingual word pairs', 'word_index': [(20, 22)], 'id': 'W05-1010.4'}, 'e2': {'word': 'parallel corpora', 'word_index': [(24, 25)], 'id': 'W05-1010.5'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '13:15': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '20:22': 'ENTITY', '24:25': 'ENTITYOTHER', '28:28': 'ENTITYUNRELATED'}}	In this paper , we propose a new learning method to solve the sparse data problem in automatic extraction of bilingual word pairs from parallel corpora with various languages .
Our learning method automatically acquires rules, which are effective to solve the sparse data problem, only from parallel corpora without any bilingual resource (e.g., a bilingual dictionary, machine translation systems) beforehand.	rules	sparse data problem	usage	{'e1': {'word': 'rules', 'word_index': [(5, 5)], 'id': 'W05-1010.8'}, 'e2': {'word': 'sparse data problem', 'word_index': [(13, 15)], 'id': 'W05-1010.9'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '5:5': 'ENTITY', '13:15': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '32:34': 'ENTITYUNRELATED'}}	Our learning method automatically acquires rules , which are effective to solve the sparse data problem , only from parallel corpora without any bilingual resource ( e.g. , a bilingual dictionary , machine translation systems ) beforehand .
Using ICL, the recall in three systems based on similarity measures improved respectively 8.0, 6.1 and 6.0 percentage points.	similarity measures	systems	usage	{'e1': {'word': 'similarity measures', 'word_index': [(10, 11)], 'id': 'W05-1010.22'}, 'e2': {'word': 'systems', 'word_index': [(7, 7)], 'id': 'W05-1010.21'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '7:7': 'ENTITYOTHER', '10:11': 'ENTITY'}}	Using ICL , the recall in three systems based on similarity measures improved respectively 8.0 , 6.1 and 6.0 percentage points .
NLP systems for tasks such as question answering and information extraction typically rely on statistical parsers	statistical parsers	NLP systems	usage	{'e1': {'word': 'statistical parsers', 'word_index': [(14, 15)], 'id': 'W06-1604.4'}, 'e2': {'word': 'NLP systems', 'word_index': [(0, 1)], 'id': 'W06-1604.1'}, 'entity_replacement': {'0:1': 'ENTITYOTHER', '6:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '14:15': 'ENTITY'}}	NLP systems for tasks such as question answering and information extraction typically rely on statistical parsers
But the efficacy of such parsers can be surprisingly low, particularly for sentences drawn from heterogeneous corpora such as the Web.	sentences	heterogeneous corpora	part_whole	{'e1': {'word': 'sentences', 'word_index': [(13, 13)], 'id': 'W06-1604.6'}, 'e2': {'word': 'heterogeneous corpora', 'word_index': [(16, 17)], 'id': 'W06-1604.7'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '13:13': 'ENTITY', '16:17': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED'}}	But the efficacy of such parsers can be surprisingly low , particularly for sentences drawn from heterogeneous corpora such as the Web .
We have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences, which can be detected automatically using semantic information obtained from the Web.	semantic interpretations	sentences	model-feature	{'e1': {'word': 'semantic interpretations', 'word_index': [(11, 12)], 'id': 'W06-1604.10'}, 'e2': {'word': 'sentences', 'word_index': [(14, 14)], 'id': 'W06-1604.11'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '11:12': 'ENTITY', '14:14': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	We have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences , which can be detected automatically using semantic information obtained from the Web .
We have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences, which can be detected automatically using semantic information obtained from the Web.	semantic information	Web	part_whole	{'e1': {'word': 'semantic information', 'word_index': [(22, 23)], 'id': 'W06-1604.12'}, 'e2': {'word': 'Web', 'word_index': [(27, 27)], 'id': 'W06-1604.13'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '22:23': 'ENTITY', '27:27': 'ENTITYOTHER'}}	We have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences , which can be detected automatically using semantic information obtained from the Web .
We demonstrate that a previously defined formal algebra applies to grammar engineering across a much greater range of frameworks than was originally envisaged.	formal algebra	grammar engineering	usage	{'e1': {'word': 'formal algebra', 'word_index': [(6, 7)], 'id': 'W07-1210.4'}, 'e2': {'word': 'grammar engineering', 'word_index': [(10, 11)], 'id': 'W07-1210.5'}, 'entity_replacement': {'6:7': 'ENTITY', '10:11': 'ENTITYOTHER', '18:18': 'ENTITYUNRELATED'}}	We demonstrate that a previously defined formal algebra applies to grammar engineering across a much greater range of frameworks than was originally envisaged .
We show how this algebra can be adapted to composition in grammar frameworks where a lexicon is not assumed, and how this underlies a practical implementation of semantic construction for the RASP system.	semantic construction	RASP system	usage	{'e1': {'word': 'semantic construction', 'word_index': [(28, 29)], 'id': 'W07-1210.11'}, 'e2': {'word': 'RASP system', 'word_index': [(32, 33)], 'id': 'W07-1210.12'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '28:29': 'ENTITY', '32:33': 'ENTITYOTHER'}}	We show how this algebra can be adapted to composition in grammar frameworks where a lexicon is not assumed , and how this underlies a practical implementation of semantic construction for the RASP system .
We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER).	Wikipedia	named entity recognition (NER)	usage	{'e1': {'word': 'Wikipedia', 'word_index': [(5, 5)], 'id': 'D07-1073.1'}, 'e2': {'word': 'named entity recognition (NER)', 'word_index': [(11, 16)], 'id': 'D07-1073.3'}, 'entity_replacement': {'5:5': 'ENTITY', '7:8': 'ENTITYUNRELATED', '11:16': 'ENTITYOTHER'}}	We explore the use of Wikipedia as external knowledge to improve named entity recognition ( NER ) .
Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry, which can be thought of as a definition part.	category label	sentence	part_whole	{'e1': {'word': 'category label', 'word_index': [(15, 16)], 'id': 'D07-1073.6'}, 'e2': {'word': 'sentence', 'word_index': [(20, 20)], 'id': 'D07-1073.7'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '9:11': 'ENTITYUNRELATED', '15:16': 'ENTITY', '20:20': 'ENTITYOTHER', '23:23': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry , which can be thought of as a definition part .
These category labels are used as features in a CRF-based NE tagger.	features	CRF-based NE tagger	usage	{'e1': {'word': 'features', 'word_index': [(6, 6)], 'id': 'D07-1073.11'}, 'e2': {'word': 'CRF-based NE tagger', 'word_index': [(9, 13)], 'id': 'D07-1073.12'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '6:6': 'ENTITY', '9:13': 'ENTITYOTHER'}}	These category labels are used as features in a CRF - based NE tagger .
We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT's peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages.	POS tagging	languages	usage	{'e1': {'word': 'POS tagging', 'word_index': [(54, 55)], 'id': 'P07-2053.7'}, 'e2': {'word': 'languages', 'word_index': [(61, 61)], 'id': 'P07-2053.8'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '13:16': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '54:55': 'ENTITY', '61:61': 'ENTITYOTHER'}}	We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT 's peculiar license , free but not open source , since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages .
We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT.	syntactic structural knowledge	word reordering	usage	{'e1': {'word': 'syntactic structural knowledge', 'word_index': [(11, 13)], 'id': 'C08-1027.2'}, 'e2': {'word': 'word reordering', 'word_index': [(6, 7)], 'id': 'C08-1027.1'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '11:13': 'ENTITY', '15:18': 'ENTITYUNRELATED'}}	We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase - based SMT .
This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules.	probabilistic syntactic rules	lattice of alternatives	usage	{'e1': {'word': 'probabilistic syntactic rules', 'word_index': [(13, 15)], 'id': 'C08-1027.5'}, 'e2': {'word': 'lattice of alternatives', 'word_index': [(6, 8)], 'id': 'C08-1027.4'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '13:15': 'ENTITY'}}	This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules .
In decoding, the alternatives are scored based on the output word order, not the order of the input.	output word order	alternatives	model-feature	{'e1': {'word': 'output word order', 'word_index': [(10, 12)], 'id': 'C08-1027.8'}, 'e2': {'word': 'alternatives', 'word_index': [(4, 4)], 'id': 'C08-1027.7'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:4': 'ENTITYOTHER', '10:12': 'ENTITY', '16:19': 'ENTITYUNRELATED'}}	In decoding , the alternatives are scored based on the output word order , not the order of the input .
Manual evaluation supports the claim that the present approach is significantly superior to previous approaches.	approach	approaches	compare	{'e1': {'word': 'approach', 'word_index': [(8, 8)], 'id': 'C08-1027.15'}, 'e2': {'word': 'approaches', 'word_index': [(14, 14)], 'id': 'C08-1027.16'}, 'entity_replacement': {'8:8': 'ENTITY', '14:14': 'ENTITYOTHER'}}	Manual evaluation supports the claim that the present approach is significantly superior to previous approaches .
Most of the previous Korean noun extraction systems use a morphological analyzer or a Part-of- Speech (POS) tagger.	morphological analyzer	Korean noun extraction systems	usage	{'e1': {'word': 'morphological analyzer', 'word_index': [(10, 11)], 'id': 'P03-1060.7'}, 'e2': {'word': 'Korean noun extraction systems', 'word_index': [(4, 7)], 'id': 'P03-1060.6'}, 'entity_replacement': {'4:7': 'ENTITYOTHER', '10:11': 'ENTITY', '14:20': 'ENTITYUNRELATED'}}	Most of the previous Korean noun extraction systems use a morphological analyzer or a Part-of - Speech ( POS ) tagger .
This paper proposes a new noun extraction method that uses the syllable based word recognition model.	syllable based word recognition model	noun extraction method	usage	{'e1': {'word': 'syllable based word recognition model', 'word_index': [(11, 15)], 'id': 'P03-1060.14'}, 'e2': {'word': 'noun extraction method', 'word_index': [(5, 7)], 'id': 'P03-1060.13'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '11:15': 'ENTITY'}}	This paper proposes a new noun extraction method that uses the syllable based word recognition model .
It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries.	syllable-tag sequence	input sentence	model-feature	{'e1': {'word': 'syllable-tag sequence', 'word_index': [(5, 8)], 'id': 'P03-1060.15'}, 'e2': {'word': 'input sentence', 'word_index': [(11, 12)], 'id': 'P03-1060.16'}, 'entity_replacement': {'5:8': 'ENTITY', '11:12': 'ENTITYOTHER', '15:18': 'ENTITYUNRELATED', '21:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	It finds the most probable syllable - tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries .
It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries.	automatically acquired statistical information	POS tagged corpus	part_whole	{'e1': {'word': 'automatically acquired statistical information', 'word_index': [(15, 18)], 'id': 'P03-1060.17'}, 'e2': {'word': 'POS tagged corpus', 'word_index': [(21, 23)], 'id': 'P03-1060.18'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '15:18': 'ENTITY', '21:23': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	It finds the most probable syllable - tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries .
The experimental results show that without morphological analysis or POS tagging, the proposed method achieves comparable performance with the previous methods.	proposed method	performance	result	{'e1': {'word': 'proposed method', 'word_index': [(13, 14)], 'id': 'P03-1060.25'}, 'e2': {'word': 'performance', 'word_index': [(17, 17)], 'id': 'P03-1060.26'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '13:14': 'ENTITY', '17:17': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED'}}	The experimental results show that without morphological analysis or POS tagging , the proposed method achieves comparable performance with the previous methods .
This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs.	word alignment	languages with scarce resources	usage	{'e1': {'word': 'word alignment', 'word_index': [(7, 8)], 'id': 'P06-2112.1'}, 'e2': {'word': 'languages with scarce resources', 'word_index': [(10, 13)], 'id': 'P06-2112.2'}, 'entity_replacement': {'7:8': 'ENTITY', '10:13': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs .
Based on these two additional corpora and with L3 as the pivot language, we build a word alignment model for L1 and L2.	word alignment model	L1 and L2	usage	{'e1': {'word': 'word alignment model', 'word_index': [(17, 19)], 'id': 'P06-2112.16'}, 'e2': {'word': 'L1 and L2', 'word_index': [(21, 23)], 'id': 'P06-2112.17'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '17:19': 'ENTITY', '21:23': 'ENTITYOTHER'}}	Based on these two additional corpora and with L3 as the pivot language , we build a word alignment model for L1 and L2 .
This approach can build a word alignment model for two languages even if no bilingual corpus is available in this language pair.	word alignment model	languages	usage	{'e1': {'word': 'word alignment model', 'word_index': [(5, 7)], 'id': 'P06-2112.19'}, 'e2': {'word': 'languages', 'word_index': [(10, 10)], 'id': 'P06-2112.20'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:7': 'ENTITY', '10:10': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED'}}	This approach can build a word alignment model for two languages even if no bilingual corpus is available in this language pair .
In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus.	L1-L2 bilingual corpus	word alignment model	usage	{'e1': {'word': 'L1-L2 bilingual corpus', 'word_index': [(16, 20)], 'id': 'P06-2112.25'}, 'e2': {'word': 'word alignment model', 'word_index': [(6, 8)], 'id': 'P06-2112.23'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '10:12': 'ENTITYUNRELATED', '16:20': 'ENTITY'}}	In addition , we build another word alignment model for L1 and L2 using the small L1 - L2 bilingual corpus .
This paper provides an approach to the semi-automatic extraction of collocations from corpora using statistics.	collocations	corpora	part_whole	{'e1': {'word': 'collocations', 'word_index': [(10, 10)], 'id': 'C96-1009.2'}, 'e2': {'word': 'corpora', 'word_index': [(12, 12)], 'id': 'C96-1009.3'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:10': 'ENTITY', '12:12': 'ENTITYOTHER'}}	This paper provides an approach to the semi-automatic extraction of collocations from corpora using statistics .
In this paper, we address the problem of nested collocations; that is, those being part of longer collocations.	nested collocations	collocations	part_whole	{'e1': {'word': 'nested collocations', 'word_index': [(9, 10)], 'id': 'C96-1009.6'}, 'e2': {'word': 'collocations', 'word_index': [(20, 20)], 'id': 'C96-1009.7'}, 'entity_replacement': {'9:10': 'ENTITY', '20:20': 'ENTITYOTHER'}}	In this paper , we address the problem of nested collocations ; that is , those being part of longer collocations .
Most approaches till now, treated substring of collocations as collocations only if they appeared frequently enough by themselves in the corpus.	collocations	corpus	part_whole	{'e1': {'word': 'collocations', 'word_index': [(10, 10)], 'id': 'C96-1009.10'}, 'e2': {'word': 'corpus', 'word_index': [(21, 21)], 'id': 'C96-1009.11'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '10:10': 'ENTITY', '21:21': 'ENTITYOTHER'}}	Most approaches till now , treated substring of collocations as collocations only if they appeared frequently enough by themselves in the corpus .
Surprisingly, students had initiative more of the time in the didactic dialogues (21% of the turns) than in the Socratic dialogues (10% of the turns), and there was no direct relationship between student initiative and learning.	initiative	didactic dialogues	model-feature	{'e1': {'word': 'initiative', 'word_index': [(4, 4)], 'id': 'E03-1072.13'}, 'e2': {'word': 'didactic dialogues', 'word_index': [(11, 12)], 'id': 'E03-1072.14'}, 'entity_replacement': {'4:4': 'ENTITY', '11:12': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED'}}	Surprisingly , students had initiative more of the time in the didactic dialogues ( 21 % of the turns ) than in the Socratic dialogues ( 10 % of the turns ) , and there was no direct relationship between student initiative and learning .
However, Socratic dialogues were more interactive than didactic dialogues as measured by percentage of tutor utterances that were questions and percentage of words in the dialogue uttered by the student, and interactivity had a positive correlation with learning.	Socratic dialogues	didactic dialogues	compare	{'e1': {'word': 'Socratic dialogues', 'word_index': [(2, 3)], 'id': 'E03-1072.18'}, 'e2': {'word': 'didactic dialogues', 'word_index': [(8, 9)], 'id': 'E03-1072.19'}, 'entity_replacement': {'2:3': 'ENTITY', '8:9': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED'}}	However , Socratic dialogues were more interactive than didactic dialogues as measured by percentage of tutor utterances that were questions and percentage of words in the dialogue uttered by the student , and interactivity had a positive correlation with learning .
We present several statistical models of syntactic constituent order for sentence realization.	statistical models	syntactic constituent order	model-feature	{'e1': {'word': 'statistical models', 'word_index': [(3, 4)], 'id': 'C04-1097.1'}, 'e2': {'word': 'syntactic constituent order', 'word_index': [(6, 8)], 'id': 'C04-1097.2'}, 'entity_replacement': {'3:4': 'ENTITY', '6:8': 'ENTITYOTHER', '10:11': 'ENTITYUNRELATED'}}	We present several statistical models of syntactic constituent order for sentence realization .
We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models.	joint models	conditional models	compare	{'e1': {'word': 'joint models', 'word_index': [(7, 8)], 'id': 'C04-1097.4'}, 'e2': {'word': 'conditional models', 'word_index': [(19, 20)], 'id': 'C04-1097.6'}, 'entity_replacement': {'7:8': 'ENTITY', '12:14': 'ENTITYUNRELATED', '19:20': 'ENTITYOTHER'}}	We compare several models , including simple joint models inspired by existing statistical parsing models , and several novel conditional models .
We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank.	model	unordered trees	usage	{'e1': {'word': 'model', 'word_index': [(6, 6)], 'id': 'C04-1097.14'}, 'e2': {'word': 'unordered trees', 'word_index': [(11, 12)], 'id': 'C04-1097.15'}, 'entity_replacement': {'6:6': 'ENTITY', '11:12': 'ENTITYOTHER', '15:17': 'ENTITYUNRELATED'}}	We employ a version of that model in an evaluation on unordered trees from the Penn Tree Bank .
A system would accomplish speech reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and content-preserving English, the message that the speaker intended to convey.	speech reconstruction	spontaneous speech input	usage	{'e1': {'word': 'speech reconstruction', 'word_index': [(4, 5)], 'id': 'L08-1530.4'}, 'e2': {'word': 'spontaneous speech input', 'word_index': [(8, 10)], 'id': 'L08-1530.5'}, 'entity_replacement': {'4:5': 'ENTITY', '8:10': 'ENTITYOTHER', '24:27': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED'}}	A system would accomplish speech reconstruction of its spontaneous speech input if its output were to represent , in flawless , fluent , and content - preserving English , the message that the speaker intended to convey .
These cleaner speech transcripts would allow for more accurate language processing as needed for NLP tasks such as machine translation and conversation summarization, which often rely on grammatical input.	grammatical input	conversation summarization	usage	{'e1': {'word': 'grammatical input', 'word_index': [(28, 29)], 'id': 'L08-1530.14'}, 'e2': {'word': 'conversation summarization', 'word_index': [(21, 22)], 'id': 'L08-1530.13'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '21:22': 'ENTITYOTHER', '28:29': 'ENTITY'}}	These cleaner speech transcripts would allow for more accurate language processing as needed for NLP tasks such as machine translation and conversation summarization , which often rely on grammatical input .
This small corpus of reconstructed and aligned conversational telephone speech transcriptions for the Fisher conversational telephone speech corpus ( Strassel and Walker, 2004 ) was annotated on several levels including string transformations and predicate-argument structure, and will be shared with the linguistic research community.	reconstructed and aligned conversational telephone speech transcriptions	corpus	part_whole	{'e1': {'word': 'reconstructed and aligned conversational telephone speech transcriptions', 'word_index': [(4, 10)], 'id': 'L08-1530.20'}, 'e2': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'L08-1530.19'}, 'entity_replacement': {'2:2': 'ENTITYOTHER', '4:10': 'ENTITY', '13:17': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED', '34:37': 'ENTITYUNRELATED'}}	This small corpus of reconstructed and aligned conversational telephone speech transcriptions for the Fisher conversational telephone speech corpus ( Strassel and Walker , 2004 ) was annotated on several levels including string transformations and predicate - argument structure , and will be shared with the linguistic research community .
The corpus contains recordings of approximately 77 hours of broadcast news shows from the Norwegian broadcasting company NRK.	recordings	corpus	part_whole	{'e1': {'word': 'recordings', 'word_index': [(3, 3)], 'id': 'L08-1122.3'}, 'e2': {'word': 'corpus', 'word_index': [(1, 1)], 'id': 'L08-1122.2'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '3:3': 'ENTITY', '9:11': 'ENTITYUNRELATED'}}	The corpus contains recordings of approximately 77 hours of broadcast news shows from the Norwegian broadcasting company NRK .
The corpus covers both read and spontaneous speech as well as spontaneous dialogues and multipart discussions, including frequent occurrences of non-speech material (e.g. music, jingles).	read and spontaneous speech	corpus	part_whole	{'e1': {'word': 'read and spontaneous speech', 'word_index': [(4, 7)], 'id': 'L08-1122.6'}, 'e2': {'word': 'corpus', 'word_index': [(1, 1)], 'id': 'L08-1122.5'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:7': 'ENTITY', '11:12': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '21:23': 'ENTITYUNRELATED'}}	The corpus covers both read and spontaneous speech as well as spontaneous dialogues and multipart discussions , including frequent occurrences of non- speech material ( e.g. music , jingles ) .
The RUNDKAST corpus is planned to be included in a future national Norwegian language resource bank.	RUNDKAST corpus	Norwegian language resource bank	part_whole	{'e1': {'word': 'RUNDKAST corpus', 'word_index': [(1, 2)], 'id': 'L08-1122.24'}, 'e2': {'word': 'Norwegian language resource bank', 'word_index': [(12, 15)], 'id': 'L08-1122.25'}, 'entity_replacement': {'1:2': 'ENTITY', '12:15': 'ENTITYOTHER'}}	The RUNDKAST corpus is planned to be included in a future national Norwegian language resource bank .
Statistical measures of word similarity have application in many areas of natural language processing, such as language modeling and information retrieval	Statistical measures	word similarity	model-feature	{'e1': {'word': 'Statistical measures', 'word_index': [(0, 1)], 'id': 'N03-1032.1'}, 'e2': {'word': 'word similarity', 'word_index': [(3, 4)], 'id': 'N03-1032.2'}, 'entity_replacement': {'0:1': 'ENTITY', '3:4': 'ENTITYOTHER', '11:13': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED'}}	Statistical measures of word similarity have application in many areas of natural language processing , such as language modeling and information retrieval
Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures.	Web data	corpus	part_whole	{'e1': {'word': 'Web data', 'word_index': [(12, 13)], 'id': 'N03-1032.10'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'N03-1032.9'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '10:10': 'ENTITYOTHER', '12:13': 'ENTITY', '21:22': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	Our frequency estimates are generated from a terabyte - sized corpus of Web data , and we study the impact of corpus size on the effectiveness of the measures .
Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures.	corpus size	measures	result	{'e1': {'word': 'corpus size', 'word_index': [(21, 22)], 'id': 'N03-1032.11'}, 'e2': {'word': 'measures', 'word_index': [(28, 28)], 'id': 'N03-1032.12'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '21:22': 'ENTITY', '28:28': 'ENTITYOTHER'}}	Our frequency estimates are generated from a terabyte - sized corpus of Web data , and we study the impact of corpus size on the effectiveness of the measures .
We base the evaluation on one TOEFL question set and two practice questions sets, each consisting of a number of multiple choice questions seeking the best synonym for a given target word.	multiple choice questions	practice questions sets	part_whole	{'e1': {'word': 'multiple choice questions', 'word_index': [(21, 23)], 'id': 'N03-1032.15'}, 'e2': {'word': 'practice questions sets', 'word_index': [(11, 13)], 'id': 'N03-1032.14'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '11:13': 'ENTITYOTHER', '21:23': 'ENTITY', '27:27': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED'}}	We base the evaluation on one TOEFL question set and two practice questions sets , each consisting of a number of multiple choice questions seeking the best synonym for a given target word .
The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer.	stack decoder	acoustic and language model	usage	{'e1': {'word': 'stack decoder', 'word_index': [(1, 2)], 'id': 'H92-1082.1'}, 'e2': {'word': 'acoustic and language model', 'word_index': [(10, 13)], 'id': 'H92-1082.2'}, 'entity_replacement': {'1:2': 'ENTITY', '10:13': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED'}}	The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer .
A previous paper described a near-optimal admissible Viterbi A* search algorithm for use with non-cross-word acoustic models and no-grammar language models [16].	non-cross-word acoustic models	Viterbi A* search algorithm	usage	{'e1': {'word': 'non-cross-word acoustic models', 'word_index': [(14, 19)], 'id': 'H92-1082.5'}, 'e2': {'word': 'Viterbi A* search algorithm', 'word_index': [(7, 10)], 'id': 'H92-1082.4'}, 'entity_replacement': {'7:10': 'ENTITYOTHER', '14:19': 'ENTITY', '21:23': 'ENTITYUNRELATED'}}	A previous paper described a near-optimal admissible Viterbi A* search algorithm for use with non- cross - word acoustic models and no-grammar language models [ 16 ] .
In addition, we make a proposal for organising intermodule communication in an NLG system by having a central server for this information.	NLG system	intermodule communication	usage	{'e1': {'word': 'NLG system', 'word_index': [(13, 14)], 'id': 'A00-1017.6'}, 'e2': {'word': 'intermodule communication', 'word_index': [(9, 10)], 'id': 'A00-1017.5'}, 'entity_replacement': {'9:10': 'ENTITYOTHER', '13:14': 'ENTITY', '18:19': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED'}}	In addition , we make a proposal for organising intermodule communication in an NLG system by having a central server for this information .
This paper describes a sense tagging technique for the automatic sense tagging of running Chinese text.	automatic sense tagging	running Chinese text	usage	{'e1': {'word': 'automatic sense tagging', 'word_index': [(9, 11)], 'id': 'W93-0312.2'}, 'e2': {'word': 'running Chinese text', 'word_index': [(13, 15)], 'id': 'W93-0312.3'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '9:11': 'ENTITY', '13:15': 'ENTITYOTHER'}}	This paper describes a sense tagging technique for the automatic sense tagging of running Chinese text .
Whereas previous work (Yarowsky, 1992; Gale et al., 1992, 1993) relies heavily on the role of statistics, the present system makes use of Machine Readable/Tractable Dictionaries (Wilks et al., 1990; Guo, in press) and an example-based reasoning technique (Nagao, 1984; Sumita et al., 1990) to treat novel words, compound words, and phrases found in the input text.	phrases	input text	part_whole	{'e1': {'word': 'phrases', 'word_index': [(74, 74)], 'id': 'W93-0312.10'}, 'e2': {'word': 'input text', 'word_index': [(78, 79)], 'id': 'W93-0312.11'}, 'entity_replacement': {'30:34': 'ENTITYUNRELATED', '49:53': 'ENTITYUNRELATED', '67:68': 'ENTITYUNRELATED', '70:71': 'ENTITYUNRELATED', '74:74': 'ENTITY', '78:79': 'ENTITYOTHER'}}	Whereas previous work ( Yarowsky , 1992 ; Gale et al. , 1992 , 1993 ) relies heavily on the role of statistics , the present system makes use of Machine Readable / Tractable Dictionaries ( Wilks et al. , 1990 ; Guo , in press ) and an example - based reasoning technique ( Nagao , 1984 ; Sumita et al. , 1990 ) to treat novel words , compound words , and phrases found in the input text .
A syntactic description is autonomous in the sense that it has certain explicit formal properties.	formal properties	syntactic description	model-feature	{'e1': {'word': 'formal properties', 'word_index': [(13, 14)], 'id': 'W98-0501.3'}, 'e2': {'word': 'syntactic description', 'word_index': [(1, 2)], 'id': 'W98-0501.2'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '13:14': 'ENTITY'}}	A syntactic description is autonomous in the sense that it has certain explicit formal properties .
Such a description relates to the semantic interpretation of the sentences, and to the surface text.	semantic interpretation	sentences	model-feature	{'e1': {'word': 'semantic interpretation', 'word_index': [(6, 7)], 'id': 'W98-0501.4'}, 'e2': {'word': 'sentences', 'word_index': [(10, 10)], 'id': 'W98-0501.5'}, 'entity_replacement': {'6:7': 'ENTITY', '10:10': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	Such a description relates to the semantic interpretation of the sentences , and to the surface text .
As the formalism is implemented in a broad-coverage syntactic parser, we concentrate on issues that must be resolved by any practical system that uses such models.	formalism	broad-coverage syntactic parser	usage	{'e1': {'word': 'formalism', 'word_index': [(2, 2)], 'id': 'W98-0501.7'}, 'e2': {'word': 'broad-coverage syntactic parser', 'word_index': [(7, 11)], 'id': 'W98-0501.8'}, 'entity_replacement': {'2:2': 'ENTITY', '7:11': 'ENTITYOTHER'}}	As the formalism is implemented in a broad - coverage syntactic parser , we concentrate on issues that must be resolved by any practical system that uses such models .
We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text.	argumentative structure	text	model-feature	{'e1': {'word': 'argumentative structure', 'word_index': [(24, 25)], 'id': 'W00-1302.7'}, 'e2': {'word': 'text', 'word_index': [(28, 28)], 'id': 'W00-1302.8'}, 'entity_replacement': {'17:18': 'ENTITYUNRELATED', '24:25': 'ENTITY', '28:28': 'ENTITYOTHER'}}	We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text .
"RE operates either interactively, allowing word-by-word evaluation of hypothesized sound changes and semantic shifts, or in a ""batch"" mode, processing entire multilingual lexicons."	word-by-word evaluation	hypothesized sound changes	usage	{'e1': {'word': 'word-by-word evaluation', 'word_index': [(6, 11)], 'id': 'J94-3004.22'}, 'e2': {'word': 'hypothesized sound changes', 'word_index': [(13, 15)], 'id': 'J94-3004.23'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '6:11': 'ENTITY', '13:15': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	"RE operates either interactively , allowing word - by - word evaluation of hypothesized sound changes and semantic shifts , or in a "" batch "" mode , processing entire multilingual lexicons ."
We describe the algorithms implemented in RE, specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time, the procedures for creating and consolidating cognate sets based on these projections, and the ad hoc techniques developed for handling the semantic component of the comparative method.	combinatorial techniques	projections	usage	{'e1': {'word': 'combinatorial techniques', 'word_index': [(12, 13)], 'id': 'J94-3004.28'}, 'e2': {'word': 'projections', 'word_index': [(17, 17)], 'id': 'J94-3004.29'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:13': 'ENTITY', '17:17': 'ENTITYOTHER', '33:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '49:50': 'ENTITYUNRELATED', '53:54': 'ENTITYUNRELATED'}}	We describe the algorithms implemented in RE , specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time , the procedures for creating and consolidating cognate sets based on these projections , and the ad hoc techniques developed for handling the semantic component of the comparative method .
We describe the algorithms implemented in RE, specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time, the procedures for creating and consolidating cognate sets based on these projections, and the ad hoc techniques developed for handling the semantic component of the comparative method.	projections	cognate sets	usage	{'e1': {'word': 'projections', 'word_index': [(38, 38)], 'id': 'J94-3004.31'}, 'e2': {'word': 'cognate sets', 'word_index': [(33, 34)], 'id': 'J94-3004.30'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '33:34': 'ENTITYOTHER', '38:38': 'ENTITY', '49:50': 'ENTITYUNRELATED', '53:54': 'ENTITYUNRELATED'}}	We describe the algorithms implemented in RE , specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time , the procedures for creating and consolidating cognate sets based on these projections , and the ad hoc techniques developed for handling the semantic component of the comparative method .
We describe the algorithms implemented in RE, specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time, the procedures for creating and consolidating cognate sets based on these projections, and the ad hoc techniques developed for handling the semantic component of the comparative method.	semantic component	comparative method	part_whole	{'e1': {'word': 'semantic component', 'word_index': [(49, 50)], 'id': 'J94-3004.32'}, 'e2': {'word': 'comparative method', 'word_index': [(53, 54)], 'id': 'J94-3004.33'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '49:50': 'ENTITY', '53:54': 'ENTITYOTHER'}}	We describe the algorithms implemented in RE , specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time , the procedures for creating and consolidating cognate sets based on these projections , and the ad hoc techniques developed for handling the semantic component of the comparative method .
Finally, we discuss features of RE that make it possible to handle the complex and sometimes imprecise representations of lexical items, and speculate on possible directions for future research.	representations	lexical items	model-feature	{'e1': {'word': 'representations', 'word_index': [(18, 18)], 'id': 'J94-3004.40'}, 'e2': {'word': 'lexical items', 'word_index': [(20, 21)], 'id': 'J94-3004.41'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '18:18': 'ENTITY', '20:21': 'ENTITYOTHER'}}	Finally , we discuss features of RE that make it possible to handle the complex and sometimes imprecise representations of lexical items , and speculate on possible directions for future research .
In this paper, we present a chunk based partial parsing system for spontaneous, conversational speech in unrestricted domains.	chunk based partial parsing system	spontaneous, conversational speech	usage	{'e1': {'word': 'chunk based partial parsing system', 'word_index': [(7, 11)], 'id': 'P98-2237.1'}, 'e2': {'word': 'spontaneous, conversational speech', 'word_index': [(13, 16)], 'id': 'P98-2237.2'}, 'entity_replacement': {'7:11': 'ENTITY', '13:16': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	In this paper , we present a chunk based partial parsing system for spontaneous , conversational speech in unrestricted domains .
The input for the system is N-best lists generated from speech recognizer lattices.	speech recognizer lattices	N-best lists	usage	{'e1': {'word': 'speech recognizer lattices', 'word_index': [(10, 12)], 'id': 'P98-2237.11'}, 'e2': {'word': 'N-best lists', 'word_index': [(6, 7)], 'id': 'P98-2237.10'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '10:12': 'ENTITY'}}	The input for the system is N-best lists generated from speech recognizer lattices .
"The hypotheses from the N-best lists are tagged for part of speech, ""cleaned up"" by a preprocessing pipe, parsed by a part of speech based chunk parser, and rescored using a backpropagation neural net trained on the chunk based scores."	chunk based scores	backpropagation neural net	usage	{'e1': {'word': 'chunk based scores', 'word_index': [(42, 44)], 'id': 'P98-2237.17'}, 'e2': {'word': 'backpropagation neural net', 'word_index': [(36, 38)], 'id': 'P98-2237.16'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '9:11': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '25:30': 'ENTITYUNRELATED', '36:38': 'ENTITYOTHER', '42:44': 'ENTITY'}}	"The hypotheses from the N-best lists are tagged for part of speech , "" cleaned up "" by a preprocessing pipe , parsed by a part of speech based chunk parser , and rescored using a backpropagation neural net trained on the chunk based scores ."
In this paper, we study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction.	features	data representations	part_whole	{'e1': {'word': 'features', 'word_index': [(9, 9)], 'id': 'W05-0409.1'}, 'e2': {'word': 'data representations', 'word_index': [(12, 13)], 'id': 'W05-0409.2'}, 'entity_replacement': {'9:9': 'ENTITY', '12:13': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	In this paper , we study how to generate features from various data representations , such as surface texts and parse trees , for answer extraction .
Besides the featuresgenerated from the surface texts, we mainly discuss the feature generation in the parse trees.	features	surface texts	part_whole	{'e1': {'word': 'features', 'word_index': [(2, 2)], 'id': 'W05-0409.6'}, 'e2': {'word': 'surface texts', 'word_index': [(6, 7)], 'id': 'W05-0409.7'}, 'entity_replacement': {'2:2': 'ENTITY', '6:7': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	Besides the features generated from the surface texts , we mainly discuss the feature generation in the parse trees .
We propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines.	syntactic features	Support Vector Machines	usage	{'e1': {'word': 'syntactic features', 'word_index': [(20, 21)], 'id': 'W05-0409.13'}, 'e2': {'word': 'Support Vector Machines', 'word_index': [(23, 25)], 'id': 'W05-0409.14'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '20:21': 'ENTITY', '23:25': 'ENTITYOTHER'}}	We propose and compare three methods , including feature vector , string kernel and tree kernel , to represent the syntactic features in Support Vector Machines .
The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts.	features	data representations	part_whole	{'e1': {'word': 'features', 'word_index': [(11, 11)], 'id': 'W05-0409.16'}, 'e2': {'word': 'data representations', 'word_index': [(17, 18)], 'id': 'W05-0409.17'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '11:11': 'ENTITY', '17:18': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts .
The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts.	features	surface texts	part_whole	{'e1': {'word': 'features', 'word_index': [(26, 26)], 'id': 'W05-0409.18'}, 'e2': {'word': 'surface texts', 'word_index': [(30, 31)], 'id': 'W05-0409.19'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '26:26': 'ENTITY', '30:31': 'ENTITYOTHER'}}	The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts .
In this paper, we explore the use of structured content as semantic constraints for enhancing the performance of traditional term-based document retrieval in special domains.	semantic constraints	term-based document retrieval	usage	{'e1': {'word': 'semantic constraints', 'word_index': [(12, 13)], 'id': 'W06-0805.2'}, 'e2': {'word': 'term-based document retrieval', 'word_index': [(20, 24)], 'id': 'W06-0805.3'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '12:13': 'ENTITY', '20:24': 'ENTITYOTHER'}}	In this paper , we explore the use of structured content as semantic constraints for enhancing the performance of traditional term - based document retrieval in special domains .
First, we describe a method for automatic extraction of semantic content in the form of attribute-value (AV) pairs from natural language texts based on domain modelsconstructed from a semi-structured web resource.	attribute-value (AV) pairs	semantic content	model-feature	{'e1': {'word': 'attribute-value (AV) pairs', 'word_index': [(16, 22)], 'id': 'W06-0805.6'}, 'e2': {'word': 'semantic content', 'word_index': [(10, 11)], 'id': 'W06-0805.5'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:11': 'ENTITYOTHER', '16:22': 'ENTITY', '24:26': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '34:36': 'ENTITYUNRELATED'}}	First , we describe a method for automatic extraction of semantic content in the form of attribute - value ( AV ) pairs from natural language texts based on domain models constructed from a semi-structured web resource .
First, we describe a method for automatic extraction of semantic content in the form of attribute-value (AV) pairs from natural language texts based on domain modelsconstructed from a semi-structured web resource.	domain models	semi-structured web resource	part_whole	{'e1': {'word': 'domain models', 'word_index': [(29, 30)], 'id': 'W06-0805.8'}, 'e2': {'word': 'semi-structured web resource', 'word_index': [(34, 36)], 'id': 'W06-0805.9'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '16:22': 'ENTITYUNRELATED', '24:26': 'ENTITYUNRELATED', '29:30': 'ENTITY', '34:36': 'ENTITYOTHER'}}	First , we describe a method for automatic extraction of semantic content in the form of attribute - value ( AV ) pairs from natural language texts based on domain models constructed from a semi-structured web resource .
Then, we explore the effect of combining a state-of-the-art term-based IR system and a simple constraint-based search system that uses the extracted AV pairs.	AV pairs	constraint-based search system	usage	{'e1': {'word': 'AV pairs', 'word_index': [(33, 34)], 'id': 'W06-0805.12'}, 'e2': {'word': 'constraint-based search system', 'word_index': [(24, 28)], 'id': 'W06-0805.11'}, 'entity_replacement': {'9:20': 'ENTITYUNRELATED', '24:28': 'ENTITYOTHER', '33:34': 'ENTITY'}}	Then , we explore the effect of combining a state - of - the - art term - based IR system and a simple constraint - based search system that uses the extracted AV pairs .
Our evaluation results have shown that such combination produces some improvement in IR performance over the term-based IR system on our test collection.	IR performance	term-based IR system	compare	{'e1': {'word': 'IR performance', 'word_index': [(12, 13)], 'id': 'W06-0805.13'}, 'e2': {'word': 'term-based IR system', 'word_index': [(16, 20)], 'id': 'W06-0805.14'}, 'entity_replacement': {'12:13': 'ENTITY', '16:20': 'ENTITYOTHER'}}	Our evaluation results have shown that such combination produces some improvement in IR performance over the term - based IR system on our test collection .
These networks are induced from treebanks: their vertices denote word forms which occur as nuclei of dependency trees.	networks	treebanks	part_whole	{'e1': {'word': 'networks', 'word_index': [(1, 1)], 'id': 'W07-0210.4'}, 'e2': {'word': 'treebanks', 'word_index': [(5, 5)], 'id': 'W07-0210.5'}, 'entity_replacement': {'1:1': 'ENTITY', '5:5': 'ENTITYOTHER', '8:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	These networks are induced from treebanks : their vertices denote word forms which occur as nuclei of dependency trees .
These networks are induced from treebanks: their vertices denote word forms which occur as nuclei of dependency trees.	vertices	word forms	model-feature	{'e1': {'word': 'vertices', 'word_index': [(8, 8)], 'id': 'W07-0210.6'}, 'e2': {'word': 'word forms', 'word_index': [(10, 11)], 'id': 'W07-0210.7'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '8:8': 'ENTITY', '10:11': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	These networks are induced from treebanks : their vertices denote word forms which occur as nuclei of dependency trees .
Their edges connect pairs of vertices if at least two instance nuclei of these vertices are linked in the dependency structure of a sentence.	dependency structure	sentence	model-feature	{'e1': {'word': 'dependency structure', 'word_index': [(19, 20)], 'id': 'W07-0210.14'}, 'e2': {'word': 'sentence', 'word_index': [(23, 23)], 'id': 'W07-0210.15'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '19:20': 'ENTITY', '23:23': 'ENTITYOTHER'}}	Their edges connect pairs of vertices if at least two instance nuclei of these vertices are linked in the dependency structure of a sentence .
We examine the syntactic dependency networks of seven languages.	syntactic dependency networks	languages	model-feature	{'e1': {'word': 'syntactic dependency networks', 'word_index': [(3, 5)], 'id': 'W07-0210.16'}, 'e2': {'word': 'languages', 'word_index': [(8, 8)], 'id': 'W07-0210.17'}, 'entity_replacement': {'3:5': 'ENTITY', '8:8': 'ENTITYOTHER'}}	We examine the syntactic dependency networks of seven languages .
Secondly, the mean clustering of vertices decreases with their degree - this finding suggests the presence of a hierarchical network organization.	mean clustering	vertices	model-feature	{'e1': {'word': 'mean clustering', 'word_index': [(3, 4)], 'id': 'W07-0210.21'}, 'e2': {'word': 'vertices', 'word_index': [(6, 6)], 'id': 'W07-0210.22'}, 'entity_replacement': {'3:4': 'ENTITY', '6:6': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED'}}	Secondly , the mean clustering of vertices decreases with their degree - this finding suggests the presence of a hierarchical network organization .
Thirdly, the mean degree of the nearest neighbors of a vertex x tends to decrease as the degree of x grows - this finding indicates disassortative mixing in the sense that links tend to connect vertices of dissimilar degrees.	mean degree	nearest neighbors	model-feature	{'e1': {'word': 'mean degree', 'word_index': [(3, 4)], 'id': 'W07-0210.25'}, 'e2': {'word': 'nearest neighbors', 'word_index': [(7, 8)], 'id': 'W07-0210.26'}, 'entity_replacement': {'3:4': 'ENTITY', '7:8': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED'}}	Thirdly , the mean degree of the nearest neighbors of a vertex x tends to decrease as the degree of x grows - this finding indicates disassortative mixing in the sense that links tend to connect vertices of dissimilar degrees .
Dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity	Dependency-based representations	natural language syntax	model-feature	{'e1': {'word': 'Dependency-based representations', 'word_index': [(0, 3)], 'id': 'P07-1021.1'}, 'e2': {'word': 'natural language syntax', 'word_index': [(5, 7)], 'id': 'P07-1021.2'}, 'entity_replacement': {'0:3': 'ENTITY', '5:7': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED'}}	Dependency - based representations of natural language syntax require a fine balance between structural flexibility and computational complexity
Most constraints are formulated on fully specified structures, which makes them hard to integrate into models where structures are composed from lexical information.	lexical information	structures	usage	{'e1': {'word': 'lexical information', 'word_index': [(22, 23)], 'id': 'P07-1021.10'}, 'e2': {'word': 'structures', 'word_index': [(18, 18)], 'id': 'P07-1021.9'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:7': 'ENTITYUNRELATED', '18:18': 'ENTITYOTHER', '22:23': 'ENTITY'}}	Most constraints are formulated on fully specified structures , which makes them hard to integrate into models where structures are composed from lexical information .
We show that morphological decomposition of the Arabic source is beneficial, especially for smaller-size corpora, and investigate different recombination techniques.	morphological decomposition	Arabic	usage	{'e1': {'word': 'morphological decomposition', 'word_index': [(3, 4)], 'id': 'P08-2039.2'}, 'e2': {'word': 'Arabic', 'word_index': [(7, 7)], 'id': 'P08-2039.3'}, 'entity_replacement': {'3:4': 'ENTITY', '7:7': 'ENTITYOTHER', '14:17': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED'}}	We show that morphological decomposition of the Arabic source is beneficial , especially for smaller - size corpora , and investigate different recombination techniques .
We also report on the use of Factored Translation Models for English-to-Arabic translation.	Factored Translation Models	English-to-Arabic translation	usage	{'e1': {'word': 'Factored Translation Models', 'word_index': [(7, 9)], 'id': 'P08-2039.6'}, 'e2': {'word': 'English-to-Arabic translation', 'word_index': [(11, 15)], 'id': 'P08-2039.7'}, 'entity_replacement': {'7:9': 'ENTITY', '11:15': 'ENTITYOTHER'}}	We also report on the use of Factored Translation Models for English -to - Arabic translation .
The first algorithm uses machine learning methods to identify the semantic dependencies in four stages: identification and labeling of predicates, identification and labeling of arguments.	labeling	predicates	usage	{'e1': {'word': 'labeling', 'word_index': [(18, 18)], 'id': 'W08-2131.8'}, 'e2': {'word': 'predicates', 'word_index': [(20, 20)], 'id': 'W08-2131.9'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '18:18': 'ENTITY', '20:20': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	The first algorithm uses machine learning methods to identify the semantic dependencies in four stages : identification and labeling of predicates , identification and labeling of arguments .
The first algorithm uses machine learning methods to identify the semantic dependencies in four stages: identification and labeling of predicates, identification and labeling of arguments.	labeling	arguments	usage	{'e1': {'word': 'labeling', 'word_index': [(24, 24)], 'id': 'W08-2131.11'}, 'e2': {'word': 'arguments', 'word_index': [(26, 26)], 'id': 'W08-2131.12'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:24': 'ENTITY', '26:26': 'ENTITYOTHER'}}	The first algorithm uses machine learning methods to identify the semantic dependencies in four stages : identification and labeling of predicates , identification and labeling of arguments .
A hybrid algorithm combining the best stages of the two algorithms attains 86.62% labeled syntactic attachment accuracy, 73.24% labeled semantic dependency F1 and 79.93% labeled macro Fl score for the combined WSJ and Brown test sets.	hybrid algorithm	labeled syntactic attachment accuracy	result	{'e1': {'word': 'hybrid algorithm', 'word_index': [(1, 2)], 'id': 'W08-2131.15'}, 'e2': {'word': 'labeled syntactic attachment accuracy', 'word_index': [(14, 17)], 'id': 'W08-2131.16'}, 'entity_replacement': {'1:2': 'ENTITY', '14:17': 'ENTITYOTHER', '21:24': 'ENTITYUNRELATED', '28:31': 'ENTITYUNRELATED', '35:39': 'ENTITYUNRELATED'}}	A hybrid algorithm combining the best stages of the two algorithms attains 86.62 % labeled syntactic attachment accuracy , 73.24 % labeled semantic dependency F1 and 79.93 % labeled macro Fl score for the combined WSJ and Brown test sets .
This paper presents experiments into modelling the substitutability of discourse connectives.	substitutability	discourse connectives	model-feature	{'e1': {'word': 'substitutability', 'word_index': [(7, 7)], 'id': 'P05-1019.6'}, 'e2': {'word': 'discourse connectives', 'word_index': [(9, 10)], 'id': 'P05-1019.7'}, 'entity_replacement': {'7:7': 'ENTITY', '9:10': 'ENTITYOTHER'}}	This paper presents experiments into modelling the substitutability of discourse connectives .
A new Theory of Names and Descriptions that offers a uniform treatment for many types of non-singular concepts found in natural language discourse is presented.	non-singular concepts	natural language discourse	part_whole	{'e1': {'word': 'non-singular concepts', 'word_index': [(16, 17)], 'id': 'C86-1086.2'}, 'e2': {'word': 'natural language discourse', 'word_index': [(20, 22)], 'id': 'C86-1086.3'}, 'entity_replacement': {'2:6': 'ENTITYUNRELATED', '16:17': 'ENTITY', '20:22': 'ENTITYOTHER'}}	A new Theory of Names and Descriptions that offers a uniform treatment for many types of non-singular concepts found in natural language discourse is presented .
This paper proposes a novel, corpus-based method for producing mappings between lexical resources.	corpus-based method	mappings	usage	{'e1': {'word': 'corpus-based method', 'word_index': [(6, 9)], 'id': 'E99-1050.1'}, 'e2': {'word': 'mappings', 'word_index': [(12, 12)], 'id': 'E99-1050.2'}, 'entity_replacement': {'6:9': 'ENTITY', '12:12': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED'}}	This paper proposes a novel , corpus - based method for producing mappings between lexical resources .
We propose a novel method to predict the inter-paragraph discourse structure of text, i.e.	inter-paragraph discourse structure	text	model-feature	{'e1': {'word': 'inter-paragraph discourse structure', 'word_index': [(8, 10)], 'id': 'C04-1007.1'}, 'e2': {'word': 'text', 'word_index': [(12, 12)], 'id': 'C04-1007.2'}, 'entity_replacement': {'8:10': 'ENTITY', '12:12': 'ENTITYOTHER'}}	We propose a novel method to predict the inter-paragraph discourse structure of text , i.e.
"Our method combines a clustering algorithm with a model of segment ""relatedness"" acquired in a machine learning step."	model	"segment ""relatedness"""	model-feature	"{'e1': {'word': 'model', 'word_index': [(8, 8)], 'id': 'C04-1007.5'}, 'e2': {'word': 'segment ""relatedness""', 'word_index': [(10, 13)], 'id': 'C04-1007.6'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '8:8': 'ENTITY', '10:13': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED'}}"	"Our method combines a clustering algorithm with a model of segment "" relatedness "" acquired in a machine learning step ."
It improves on other recent analyses in the computational linguistics literature in three respects: (i) it uses no tree- or logical-form rewriting devices in building meaning representations (ii) this results in a fully reversible linguistic description, equally suited for analysis or generation (iii) the analysis extends to types of elliptical comparative not elsewhere treated.	tree- or logical-form rewriting devices	meaning representations	usage	{'e1': {'word': 'tree- or logical-form rewriting devices', 'word_index': [(21, 28)], 'id': 'E91-1002.6'}, 'e2': {'word': 'meaning representations', 'word_index': [(31, 32)], 'id': 'E91-1002.7'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '21:28': 'ENTITY', '31:32': 'ENTITYOTHER', '40:43': 'ENTITYUNRELATED', '50:50': 'ENTITYUNRELATED', '60:61': 'ENTITYUNRELATED'}}	It improves on other recent analyses in the computational linguistics literature in three respects : ( i ) it uses no tree - or logical - form rewriting devices in building meaning representations ( ii ) this results in a fully reversible linguistic description , equally suited for analysis or generation ( iii ) the analysis extends to types of elliptical comparative not elsewhere treated .
In contrast to earlier dialectology, we seek a comprehensive characterization of (potentially gradual) differences between dialects, rather than a geographic delineation of (discrete) features of individual words or pronunciations.	(discrete) features	individual words	model-feature	{'e1': {'word': '(discrete) features', 'word_index': [(26, 29)], 'id': 'E99-1048.4'}, 'e2': {'word': 'individual words', 'word_index': [(31, 32)], 'id': 'E99-1048.5'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '26:29': 'ENTITY', '31:32': 'ENTITYOTHER', '34:34': 'ENTITYUNRELATED'}}	In contrast to earlier dialectology , we seek a comprehensive characterization of ( potentially gradual ) differences between dialects , rather than a geographic delineation of ( discrete ) features of individual words or pronunciations .
We measure phonetic (un)relatedness between dialects using Levenshtein distance, and classify by clustering distances but also by analysis through multidimensional scaling.	Levenshtein distance	phonetic (un)relatedness	usage	{'e1': {'word': 'Levenshtein distance', 'word_index': [(10, 11)], 'id': 'E99-1048.10'}, 'e2': {'word': 'phonetic (un)relatedness', 'word_index': [(2, 6)], 'id': 'E99-1048.8'}, 'entity_replacement': {'2:6': 'ENTITYOTHER', '8:8': 'ENTITYUNRELATED', '10:11': 'ENTITY', '16:17': 'ENTITYUNRELATED'}}	We measure phonetic ( un ) relatedness between dialects using Levenshtein distance , and classify by clustering distances but also by analysis through multidimensional scaling .
Experimental results using a threaded discussion corpus from an undergraduate class show that it achieves significant performance improvements compared with the baseline system.	performance improvements	baseline system	compare	{'e1': {'word': 'performance improvements', 'word_index': [(16, 17)], 'id': 'N06-1027.16'}, 'e2': {'word': 'baseline system', 'word_index': [(21, 22)], 'id': 'N06-1027.17'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '16:17': 'ENTITY', '21:22': 'ENTITYOTHER'}}	Experimental results using a threaded discussion corpus from an undergraduate class show that it achieves significant performance improvements compared with the baseline system .
A method of anaphoral resolution of zero pronouns in Japanese language texts using the verbal semantic attributes is suggested.	anaphoral resolution	Japanese language texts	usage	{'e1': {'word': 'anaphoral resolution', 'word_index': [(3, 4)], 'id': 'A92-1028.1'}, 'e2': {'word': 'Japanese language texts', 'word_index': [(9, 11)], 'id': 'A92-1028.3'}, 'entity_replacement': {'3:4': 'ENTITY', '6:7': 'ENTITYUNRELATED', '9:11': 'ENTITYOTHER', '14:16': 'ENTITYUNRELATED'}}	A method of anaphoral resolution of zero pronouns in Japanese language texts using the verbal semantic attributes is suggested .
This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents.	semantic attributes	verbs	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(6, 7)], 'id': 'A92-1028.5'}, 'e2': {'word': 'verbs', 'word_index': [(9, 9)], 'id': 'A92-1028.6'}, 'entity_replacement': {'6:7': 'ENTITY', '9:9': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents .
This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents.	semantic attributes	verbs	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(19, 20)], 'id': 'A92-1028.7'}, 'e2': {'word': 'verbs', 'word_index': [(22, 22)], 'id': 'A92-1028.8'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '19:20': 'ENTITY', '22:22': 'ENTITYOTHER', '24:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents .
This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents.	semantic attributes	verbs	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(28, 29)], 'id': 'A92-1028.10'}, 'e2': {'word': 'verbs', 'word_index': [(31, 31)], 'id': 'A92-1028.11'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:29': 'ENTITY', '31:31': 'ENTITYOTHER', '34:34': 'ENTITYUNRELATED'}}	This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents .
The semantic attributes of verbs are created using 2 different viewpoints: dynamic characteristics of verbs and the relationship of verbs to cases.	semantic attributes	verbs	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(1, 2)], 'id': 'A92-1028.13'}, 'e2': {'word': 'verbs', 'word_index': [(4, 4)], 'id': 'A92-1028.14'}, 'entity_replacement': {'1:2': 'ENTITY', '4:4': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED'}}	The semantic attributes of verbs are created using 2 different viewpoints : dynamic characteristics of verbs and the relationship of verbs to cases .
The semantic attributes of verbs are created using 2 different viewpoints: dynamic characteristics of verbs and the relationship of verbs to cases.	dynamic characteristics	verbs	model-feature	{'e1': {'word': 'dynamic characteristics', 'word_index': [(12, 13)], 'id': 'A92-1028.15'}, 'e2': {'word': 'verbs', 'word_index': [(15, 15)], 'id': 'A92-1028.16'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '12:13': 'ENTITY', '15:15': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED'}}	The semantic attributes of verbs are created using 2 different viewpoints : dynamic characteristics of verbs and the relationship of verbs to cases .
By using this method, it is shown that, in the case of translating newspaper articles, the major portion (93%) of anaphoral resolution of zero pronouns necessary for machine translation can be achieved by using only linguistic knowledge.Factors to be given special attention when incorporating this method into a machine translation system are examined, together with suggested conditions for the detection of zero pronouns and methods for their conversion.	linguistic knowledge	anaphoral resolution	usage	{'e1': {'word': 'linguistic knowledge', 'word_index': [(41, 42)], 'id': 'A92-1028.23'}, 'e2': {'word': 'anaphoral resolution', 'word_index': [(26, 27)], 'id': 'A92-1028.20'}, 'entity_replacement': {'14:16': 'ENTITYUNRELATED', '26:27': 'ENTITYOTHER', '29:30': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '41:42': 'ENTITY', '56:58': 'ENTITYUNRELATED', '70:71': 'ENTITYUNRELATED'}}	By using this method , it is shown that , in the case of translating newspaper articles , the major portion ( 93 % ) of anaphoral resolution of zero pronouns necessary for machine translation can be achieved by using only linguistic knowledge . Factors to be given special attention when incorporating this method into a machine translation system are examined , together with suggested conditions for the detection of zero pronouns and methods for their conversion .
Implementation of the proposed method with due consideration of these points leads to a viable method for anaphoral resolution of zero pronouns in a practical machine translation system.	anaphoral resolution	machine translation system	usage	{'e1': {'word': 'anaphoral resolution', 'word_index': [(17, 18)], 'id': 'A92-1028.33'}, 'e2': {'word': 'machine translation system', 'word_index': [(25, 27)], 'id': 'A92-1028.34'}, 'entity_replacement': {'17:18': 'ENTITY', '25:27': 'ENTITYOTHER'}}	Implementation of the proposed method with due consideration of these points leads to a viable method for anaphoral resolution of zero pronouns in a practical machine translation system .
The primary objective of this project is to develop a robust, high-performance parser for English by automatically extracting a grammar from an annotated corpus of bracketed sentences, called the Treebank.	high-performance parser	English	usage	{'e1': {'word': 'high-performance parser', 'word_index': [(12, 15)], 'id': 'H93-1092.1'}, 'e2': {'word': 'English', 'word_index': [(17, 17)], 'id': 'H93-1092.2'}, 'entity_replacement': {'12:15': 'ENTITY', '17:17': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED'}}	The primary objective of this project is to develop a robust , high - performance parser for English by automatically extracting a grammar from an annotated corpus of bracketed sentences , called the Treebank .
The primary objective of this project is to develop a robust, high-performance parser for English by automatically extracting a grammar from an annotated corpus of bracketed sentences, called the Treebank.	grammar	annotated corpus	part_whole	{'e1': {'word': 'grammar', 'word_index': [(22, 22)], 'id': 'H93-1092.3'}, 'e2': {'word': 'annotated corpus', 'word_index': [(25, 26)], 'id': 'H93-1092.4'}, 'entity_replacement': {'12:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '22:22': 'ENTITY', '25:26': 'ENTITYOTHER', '28:29': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED'}}	The primary objective of this project is to develop a robust , high - performance parser for English by automatically extracting a grammar from an annotated corpus of bracketed sentences , called the Treebank .
"The test collection consists of over 1 million documents from diverse full-text sources, 250 topics, and the set of relevant documents or ""right answers"" to those topics."	documents	test collection	part_whole	{'e1': {'word': 'documents', 'word_index': [(8, 8)], 'id': 'X96-1007.19'}, 'e2': {'word': 'test collection', 'word_index': [(1, 2)], 'id': 'X96-1007.18'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '8:8': 'ENTITY', '11:14': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	"The test collection consists of over 1 million documents from diverse full - text sources , 250 topics , and the set of relevant documents or "" right answers "" to those topics ."
The results from TREC-2 showed significant improvements over the TREC-1 results, and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection.	TREC-2	TREC-1 results	compare	{'e1': {'word': 'TREC-2', 'word_index': [(3, 4)], 'id': 'X96-1007.32'}, 'e2': {'word': 'TREC-1 results', 'word_index': [(10, 13)], 'id': 'X96-1007.33'}, 'entity_replacement': {'3:4': 'ENTITY', '10:13': 'ENTITYOTHER', '31:32': 'ENTITYUNRELATED'}}	The results from TREC -2 showed significant improvements over the TREC - 1 results , and should be viewed as the appropriate baseline representing state - of - the - art retrieval techniques as scaled up to handling a 2 gigabyte collection .
In this paper we outline a lexical organization for Turkish that makes use of lexical rules for inflections, derivations, and lexical category changes to control the proliferation of lexical entries.	lexical rules	inflections	usage	{'e1': {'word': 'lexical rules', 'word_index': [(14, 15)], 'id': 'W96-0311.2'}, 'e2': {'word': 'inflections', 'word_index': [(17, 17)], 'id': 'W96-0311.3'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '14:15': 'ENTITY', '17:17': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	In this paper we outline a lexical organization for Turkish that makes use of lexical rules for inflections , derivations , and lexical category changes to control the proliferation of lexical entries .
A lexical inheritance hierarchy facilitates the enforcement of type constraints.	lexical inheritance hierarchy	type constraints	usage	{'e1': {'word': 'lexical inheritance hierarchy', 'word_index': [(1, 3)], 'id': 'W96-0311.12'}, 'e2': {'word': 'type constraints', 'word_index': [(8, 9)], 'id': 'W96-0311.13'}, 'entity_replacement': {'1:3': 'ENTITY', '8:9': 'ENTITYOTHER'}}	A lexical inheritance hierarchy facilitates the enforcement of type constraints .
Semantic compositions  in inflections and derivations are constrained by the properties of the terms and predicates.The design has been tested as part of a HPSG grammar for Turkish.	Semantic compositions 	inflections	model-feature	{'e1': {'word': 'Semantic compositions ', 'word_index': [(0, 1)], 'id': 'W96-0311.14'}, 'e2': {'word': 'inflections', 'word_index': [(3, 3)], 'id': 'W96-0311.15'}, 'entity_replacement': {'0:1': 'ENTITY', '3:3': 'ENTITYOTHER', '5:5': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	Semantic compositions in inflections and derivations are constrained by the properties of the terms and predicates . The design has been tested as part of a HPSG grammar for Turkish .
The bigram language models are popular, in much language processing applications, in both Indo-European and Asian languages.	bigram language models	language processing applications	usage	{'e1': {'word': 'bigram language models', 'word_index': [(1, 3)], 'id': 'W00-1311.1'}, 'e2': {'word': 'language processing applications', 'word_index': [(9, 11)], 'id': 'W00-1311.2'}, 'entity_replacement': {'1:3': 'ENTITY', '9:11': 'ENTITYOTHER', '15:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	The bigram language models are popular , in much language processing applications , in both Indo - European and Asian languages .
However, when the language model for Chinese is applied in a novel domain, the accuracy is reduced significantly, from 96% to 78% in our evaluation.	language model	domain	usage	{'e1': {'word': 'language model', 'word_index': [(4, 5)], 'id': 'W00-1311.5'}, 'e2': {'word': 'domain', 'word_index': [(13, 13)], 'id': 'W00-1311.7'}, 'entity_replacement': {'4:5': 'ENTITY', '7:7': 'ENTITYUNRELATED', '13:13': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	However , when the language model for Chinese is applied in a novel domain , the accuracy is reduced significantly , from 96 % to 78 % in our evaluation .
In our evaluation, Bayesian classifiers produce the best recall performance of 80% but the precision is low (60%).	Bayesian classifiers	recall performance	result	{'e1': {'word': 'Bayesian classifiers', 'word_index': [(4, 5)], 'id': 'W00-1311.15'}, 'e2': {'word': 'recall performance', 'word_index': [(9, 10)], 'id': 'W00-1311.16'}, 'entity_replacement': {'4:5': 'ENTITY', '9:10': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	In our evaluation , Bayesian classifiers produce the best recall performance of 80 % but the precision is low ( 60 % ) .
Neural network produced good recall (75%) and precision (80%) but both Bayesian and Neural network have low skip ratio (65%).	Neural network	recall	result	{'e1': {'word': 'Neural network', 'word_index': [(0, 1)], 'id': 'W00-1311.18'}, 'e2': {'word': 'recall', 'word_index': [(4, 4)], 'id': 'W00-1311.19'}, 'entity_replacement': {'0:1': 'ENTITY', '4:4': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '17:20': 'ENTITYUNRELATED'}}	Neural network produced good recall ( 75 % ) and precision ( 80 % ) but both Bayesian and Neural network have low skip ratio ( 65 % ) .
The decision tree classifier produced the best precision (81%) and skip ratio (76%) but its recall is the lowest (73%).	decision tree classifier	precision	result	{'e1': {'word': 'decision tree classifier', 'word_index': [(1, 3)], 'id': 'W00-1311.22'}, 'e2': {'word': 'precision', 'word_index': [(7, 7)], 'id': 'W00-1311.23'}, 'entity_replacement': {'1:3': 'ENTITY', '7:7': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED'}}	The decision tree classifier produced the best precision ( 81 % ) and skip ratio ( 76 % ) but its recall is the lowest ( 73 % ) .
Tokenization is the process of mapping sentences from character strings into strings of words.	mapping sentences	character strings	usage	{'e1': {'word': 'mapping sentences', 'word_index': [(5, 6)], 'id': 'J97-4004.1'}, 'e2': {'word': 'character strings', 'word_index': [(8, 9)], 'id': 'J97-4004.2'}, 'entity_replacement': {'5:6': 'ENTITY', '8:9': 'ENTITYOTHER', '11:13': 'ENTITYUNRELATED'}}	Tokenization is the process of mapping sentences from character strings into strings of words .
This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework.	goodness measures	unsupervised segmentation	usage	{'e1': {'word': 'goodness measures', 'word_index': [(11, 12)], 'id': 'I08-1002.1'}, 'e2': {'word': 'unsupervised segmentation', 'word_index': [(14, 15)], 'id': 'I08-1002.2'}, 'entity_replacement': {'11:12': 'ENTITY', '14:15': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '20:24': 'ENTITYUNRELATED'}}	This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff - 3 data sets with a unified framework .
Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores.	word candidates	unlabeled texts	part_whole	{'e1': {'word': 'word candidates', 'word_index': [(16, 17)], 'id': 'I08-1002.8'}, 'e2': {'word': 'unlabeled texts', 'word_index': [(19, 20)], 'id': 'I08-1002.9'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '16:17': 'ENTITY', '19:20': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED'}}	Assuming no prior knowledge about Chinese , this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores .
Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores.	decoding algorithm	segmentation	usage	{'e1': {'word': 'decoding algorithm', 'word_index': [(26, 27)], 'id': 'I08-1002.10'}, 'e2': {'word': 'segmentation', 'word_index': [(32, 32)], 'id': 'I08-1002.11'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '26:27': 'ENTITY', '32:32': 'ENTITYOTHER', '35:35': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED'}}	Assuming no prior knowledge about Chinese , this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores .
Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores.	goodness scores	candidates	model-feature	{'e1': {'word': 'goodness scores', 'word_index': [(44, 45)], 'id': 'I08-1002.14'}, 'e2': {'word': 'candidates', 'word_index': [(38, 38)], 'id': 'I08-1002.13'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '38:38': 'ENTITYOTHER', '44:45': 'ENTITY'}}	Assuming no prior knowledge about Chinese , this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores .
Building on a state-of-the-art set of features, a binary classifier for each label is trained using AdaBoost with fixed depth decision trees.	decision trees	AdaBoost	usage	{'e1': {'word': 'decision trees', 'word_index': [(28, 29)], 'id': 'W05-0628.10'}, 'e2': {'word': 'AdaBoost', 'word_index': [(23, 24)], 'id': 'W05-0628.9'}, 'entity_replacement': {'10:12': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER', '28:29': 'ENTITY'}}	Building on a state - of - the - art set of features , a binary classifier for each label is trained using Ada Boost with fixed depth decision trees .
In this paper we propose two metrics to be used in various fields of computational linguistics area.	metrics	computational linguistics	usage	{'e1': {'word': 'metrics', 'word_index': [(6, 6)], 'id': 'W06-1114.1'}, 'e2': {'word': 'computational linguistics', 'word_index': [(14, 15)], 'id': 'W06-1114.2'}, 'entity_replacement': {'6:6': 'ENTITY', '14:15': 'ENTITYOTHER'}}	In this paper we propose two metrics to be used in various fields of computational linguistics area .
Finally, a short application is presented: we investigate the similarity of Romance languages by computing the scaled total rank distance between the digram rankings of each language.	scaled total rank distance	similarity	usage	{'e1': {'word': 'scaled total rank distance', 'word_index': [(18, 21)], 'id': 'W06-1114.9'}, 'e2': {'word': 'similarity', 'word_index': [(11, 11)], 'id': 'W06-1114.7'}, 'entity_replacement': {'11:11': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED', '18:21': 'ENTITY', '28:28': 'ENTITYUNRELATED'}}	Finally , a short application is presented : we investigate the similarity of Romance languages by computing the scaled total rank distance between the digram rankings of each language .
First, we formalize the task of identifying Japanese compound functional expressions in a text as a machine learning based chunking problem.	Japanese compound functional expressions	text	part_whole	{'e1': {'word': 'Japanese compound functional expressions', 'word_index': [(8, 11)], 'id': 'W07-1109.4'}, 'e2': {'word': 'text', 'word_index': [(14, 14)], 'id': 'W07-1109.5'}, 'entity_replacement': {'8:11': 'ENTITY', '14:14': 'ENTITYOTHER', '17:21': 'ENTITYUNRELATED'}}	First , we formalize the task of identifying Japanese compound functional expressions in a text as a machine learning based chunking problem .
Next, against the results of identifying compound functional expressions, we apply the method of dependency analysis based on the cascaded chunking model.	cascaded chunking model	dependency analysis	usage	{'e1': {'word': 'cascaded chunking model', 'word_index': [(21, 23)], 'id': 'W07-1109.9'}, 'e2': {'word': 'dependency analysis', 'word_index': [(16, 17)], 'id': 'W07-1109.8'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '16:17': 'ENTITYOTHER', '21:23': 'ENTITY'}}	Next , against the results of identifying compound functional expressions , we apply the method of dependency analysis based on the cascaded chunking model .
We introduce a relation extraction method to identify the sentences in biomedical text that indicate an interaction among the protein names mentioned.	relation extraction method	biomedical text	usage	{'e1': {'word': 'relation extraction method', 'word_index': [(3, 5)], 'id': 'D07-1024.1'}, 'e2': {'word': 'biomedical text', 'word_index': [(11, 12)], 'id': 'D07-1024.3'}, 'entity_replacement': {'3:5': 'ENTITY', '9:9': 'ENTITYUNRELATED', '11:12': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	We introduce a relation extraction method to identify the sentences in biomedical text that indicate an interaction among the protein names mentioned .
Our approach is based on the analysis of the paths between two protein names in the dependency parse trees of the sentences.	dependency parse trees	sentences	model-feature	{'e1': {'word': 'dependency parse trees', 'word_index': [(16, 18)], 'id': 'D07-1024.7'}, 'e2': {'word': 'sentences', 'word_index': [(21, 21)], 'id': 'D07-1024.8'}, 'entity_replacement': {'12:13': 'ENTITYUNRELATED', '16:18': 'ENTITY', '21:21': 'ENTITYOTHER'}}	Our approach is based on the analysis of the paths between two protein names in the dependency parse trees of the sentences .
Given two dependency trees, we define two separate similarity functions (kernels) based on cosine similarity and edit distance among the paths between the protein names.	cosine similarity	similarity functions (kernels)	usage	{'e1': {'word': 'cosine similarity', 'word_index': [(16, 17)], 'id': 'D07-1024.11'}, 'e2': {'word': 'similarity functions (kernels)', 'word_index': [(9, 13)], 'id': 'D07-1024.10'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '9:13': 'ENTITYOTHER', '16:17': 'ENTITY', '19:20': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	Given two dependency trees , we define two separate similarity functions ( kernels ) based on cosine similarity and edit distance among the paths between the protein names .
Semi-supervised algorithms perform better than their supervised version by a wide margin especially when the amount of labeled data is limited.	Semi-supervised algorithms	labeled data	usage	{'e1': {'word': 'Semi-supervised algorithms', 'word_index': [(0, 1)], 'id': 'D07-1024.19'}, 'e2': {'word': 'labeled data', 'word_index': [(17, 18)], 'id': 'D07-1024.20'}, 'entity_replacement': {'0:1': 'ENTITY', '17:18': 'ENTITYOTHER'}}	Semi-supervised algorithms perform better than their supervised version by a wide margin especially when the amount of labeled data is limited .
Language model (LM) adaptation is important for both speech and language processing	Language model (LM) adaptation	speech and language processing	usage	{'e1': {'word': 'Language model (LM) adaptation', 'word_index': [(0, 5)], 'id': 'P07-1085.1'}, 'e2': {'word': 'speech and language processing', 'word_index': [(10, 13)], 'id': 'P07-1085.2'}, 'entity_replacement': {'0:5': 'ENTITY', '10:13': 'ENTITYOTHER'}}	Language model ( LM ) adaptation is important for both speech and language processing
In addition, a new dynamically adapted weighting scheme for topic mixture models is proposed based on LDA topic analysis.	LDA topic analysis	weighting scheme	usage	{'e1': {'word': 'LDA topic analysis', 'word_index': [(17, 19)], 'id': 'P07-1085.15'}, 'e2': {'word': 'weighting scheme', 'word_index': [(7, 8)], 'id': 'P07-1085.13'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '10:12': 'ENTITYUNRELATED', '17:19': 'ENTITY'}}	In addition , a new dynamically adapted weighting scheme for topic mixture models is proposed based on LDA topic analysis .
Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline generic LM.	NE-driven LM adaptation	LM	compare	{'e1': {'word': 'NE-driven LM adaptation', 'word_index': [(6, 10)], 'id': 'P07-1085.16'}, 'e2': {'word': 'LM', 'word_index': [(16, 16)], 'id': 'P07-1085.17'}, 'entity_replacement': {'6:10': 'ENTITY', '16:16': 'ENTITYOTHER'}}	Our experimental results show that the NE - driven LM adaptation framework outperforms the baseline generic LM .
The state-of-the-art system combination method for machine translation (MT) is the word-based combination using confusion networks.	combination method	machine translation (MT)	usage	{'e1': {'word': 'combination method', 'word_index': [(9, 10)], 'id': 'C08-1005.1'}, 'e2': {'word': 'machine translation (MT)', 'word_index': [(12, 16)], 'id': 'C08-1005.2'}, 'entity_replacement': {'9:10': 'ENTITY', '12:16': 'ENTITYOTHER', '19:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	The state - of - the - art system combination method for machine translation ( MT ) is the word - based combination using confusion networks .
The state-of-the-art system combination method for machine translation (MT) is the word-based combination using confusion networks.	confusion networks	word-based combination	usage	{'e1': {'word': 'confusion networks', 'word_index': [(24, 25)], 'id': 'C08-1005.4'}, 'e2': {'word': 'word-based combination', 'word_index': [(19, 22)], 'id': 'C08-1005.3'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '12:16': 'ENTITYUNRELATED', '19:22': 'ENTITYOTHER', '24:25': 'ENTITY'}}	The state - of - the - art system combination method for machine translation ( MT ) is the word - based combination using confusion networks .
In this paper, we present new methods to improve alignment of hypotheses using word synonyms and a two-pass alignment strategy.	word synonyms	alignment	usage	{'e1': {'word': 'word synonyms', 'word_index': [(14, 15)], 'id': 'C08-1005.8'}, 'e2': {'word': 'alignment', 'word_index': [(10, 10)], 'id': 'C08-1005.7'}, 'entity_replacement': {'10:10': 'ENTITYOTHER', '14:15': 'ENTITY', '21:21': 'ENTITYUNRELATED'}}	In this paper , we present new methods to improve alignment of hypotheses using word synonyms and a two - pass alignment strategy .
We present a novel method for creating A* estimates for structured search problems.	A* estimates	structured search problems	usage	{'e1': {'word': 'A* estimates', 'word_index': [(7, 8)], 'id': 'N07-1052.1'}, 'e2': {'word': 'structured search problems', 'word_index': [(10, 12)], 'id': 'N07-1052.2'}, 'entity_replacement': {'7:8': 'ENTITY', '10:12': 'ENTITYOTHER'}}	We present a novel method for creating A* estimates for structured search problems .
The DPA algorithm works on the assumption of Direct Correspondence which simply means	assumption of Direct Correspondence	DPA algorithm	usage	{'e1': {'word': 'assumption of Direct Correspondence', 'word_index': [(6, 9)], 'id': 'P06-2039.11'}, 'e2': {'word': 'DPA algorithm', 'word_index': [(1, 2)], 'id': 'P06-2039.10'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '6:9': 'ENTITY'}}	The DPA algorithm works on the assumption of Direct Correspondence which simply means
This leads to wrong parsed structure of the target language sentence.	parsed structure 	target language sentence	model-feature	{'e1': {'word': 'parsed structure ', 'word_index': [(4, 5)], 'id': 'P06-2039.16'}, 'e2': {'word': 'target language sentence', 'word_index': [(8, 10)], 'id': 'P06-2039.17'}, 'entity_replacement': {'4:5': 'ENTITY', '8:10': 'ENTITYOTHER'}}	This leads to wrong parsed structure of the target language sentence .
A parser is an algorithm that assigns a structural description to a string according to a grammar.	structural description	string	model-feature	{'e1': {'word': 'structural description', 'word_index': [(8, 9)], 'id': 'C88-1049.2'}, 'e2': {'word': 'string', 'word_index': [(12, 12)], 'id': 'C88-1049.3'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '8:9': 'ENTITY', '12:12': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	A parser is an algorithm that assigns a structural description to a string according to a grammar .
Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition.	phrase structure descriptions	parsers	usage	{'e1': {'word': 'phrase structure descriptions', 'word_index': [(3, 5)], 'id': 'C88-1049.9'}, 'e2': {'word': 'parsers', 'word_index': [(1, 1)], 'id': 'C88-1049.8'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '3:5': 'ENTITY', '7:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:17': 'ENTITYUNRELATED'}}	Common parsers employ phrase structure descriptions , rule - based grammars , and derivation or transition oriented recognition .
the syntactical relationships are stated as part of the lexical descriptions of the elements of the language.	syntactical relationships	lexical descriptions	part_whole	{'e1': {'word': 'syntactical relationships', 'word_index': [(1, 2)], 'id': 'C88-1049.20'}, 'e2': {'word': 'lexical descriptions', 'word_index': [(9, 10)], 'id': 'C88-1049.21'}, 'entity_replacement': {'1:2': 'ENTITY', '9:10': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	the syntactical relationships are stated as part of the lexical descriptions of the elements of the language .
We are going to describe the design and implementation of a communication system for large AI projects, capable of supporting various software components in a heterogeneous hardware and programming-language environment.	communication system	AI	usage	{'e1': {'word': 'communication system', 'word_index': [(11, 12)], 'id': 'C96-1008.1'}, 'e2': {'word': 'AI', 'word_index': [(15, 15)], 'id': 'C96-1008.2'}, 'entity_replacement': {'11:12': 'ENTITY', '15:15': 'ENTITYOTHER'}}	We are going to describe the design and implementation of a communication system for large AI projects , capable of supporting various software components in a heterogeneous hardware and programming - language environment .
We present some preliminary results of a Czech-English translation system based on dependency trees.	dependency trees	Czech-English translation system	usage	{'e1': {'word': 'dependency trees', 'word_index': [(14, 15)], 'id': 'E03-1004.2'}, 'e2': {'word': 'Czech-English translation system', 'word_index': [(7, 11)], 'id': 'E03-1004.1'}, 'entity_replacement': {'7:11': 'ENTITYOTHER', '14:15': 'ENTITY'}}	We present some preliminary results of a Czech - English translation system based on dependency trees .
The fully automated process includes: morphological tagging, analytical and tectogrammatical parsing of Czech, tectogrammatical transfer based on lexical substitution using word-to-word translation dictionaries enhanced by the information from the English-Czech parallel corpus of WSJ, and a simple rule-based system for generation from English tectogrammatical representation.	word-to-word translation dictionaries	lexical substitution	usage	{'e1': {'word': 'word-to-word translation dictionaries', 'word_index': [(23, 29)], 'id': 'E03-1004.8'}, 'e2': {'word': 'lexical substitution', 'word_index': [(20, 21)], 'id': 'E03-1004.7'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '20:21': 'ENTITYOTHER', '23:29': 'ENTITY', '36:40': 'ENTITYUNRELATED', '46:46': 'ENTITYUNRELATED', '47:50': 'ENTITYUNRELATED', '52:52': 'ENTITYUNRELATED', '54:56': 'ENTITYUNRELATED'}}	The fully automated process includes : morphological tagging , analytical and tectogrammatical parsing of Czech , tectogrammatical transfer based on lexical substitution using word - to - word translation dictionaries enhanced by the information from the English - Czech parallel corpus of WSJ , and a simple rule - based system for generation from English tectogrammatical representation .
The fully automated process includes: morphological tagging, analytical and tectogrammatical parsing of Czech, tectogrammatical transfer based on lexical substitution using word-to-word translation dictionaries enhanced by the information from the English-Czech parallel corpus of WSJ, and a simple rule-based system for generation from English tectogrammatical representation.	rule-based system	generation	usage	{'e1': {'word': 'rule-based system', 'word_index': [(47, 50)], 'id': 'E03-1004.11'}, 'e2': {'word': 'generation', 'word_index': [(52, 52)], 'id': 'E03-1004.12'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '23:29': 'ENTITYUNRELATED', '36:40': 'ENTITYUNRELATED', '46:46': 'ENTITYUNRELATED', '47:50': 'ENTITY', '52:52': 'ENTITYOTHER', '54:56': 'ENTITYUNRELATED'}}	The fully automated process includes : morphological tagging , analytical and tectogrammatical parsing of Czech , tectogrammatical transfer based on lexical substitution using word - to - word translation dictionaries enhanced by the information from the English - Czech parallel corpus of WSJ , and a simple rule - based system for generation from English tectogrammatical representation .
ADOMIT is an algorithm for Automatic Detection of OMIssions in Translations.	Automatic Detection	Translations	usage	{'e1': {'word': 'Automatic Detection', 'word_index': [(5, 6)], 'id': 'C96-2129.2'}, 'e2': {'word': 'Translations', 'word_index': [(10, 10)], 'id': 'C96-2129.4'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '5:6': 'ENTITY', '8:8': 'ENTITYUNRELATED', '10:10': 'ENTITYOTHER'}}	ADOMIT is an algorithm for Automatic Detection of OMIssions in Translations .
The algorithm relies solely on geometric analysis of bitext maps and uses no linguistic information.	geometric analysis	bitext maps	topic	{'e1': {'word': 'geometric analysis', 'word_index': [(5, 6)], 'id': 'C96-2129.5'}, 'e2': {'word': 'bitext maps', 'word_index': [(8, 9)], 'id': 'C96-2129.6'}, 'entity_replacement': {'5:6': 'ENTITY', '8:9': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED'}}	The algorithm relies solely on geometric analysis of bitext maps and uses no linguistic information .
In the end, the aim is to locate all eventualities in a text on a time axis and/or a map to ensure an optimal base for automatic temporal and geospatial reasoning.	optimal base	automatic temporal and geospatial reasoning	usage	{'e1': {'word': 'optimal base', 'word_index': [(26, 27)], 'id': 'L08-1561.6'}, 'e2': {'word': 'automatic temporal and geospatial reasoning', 'word_index': [(29, 33)], 'id': 'L08-1561.7'}, 'entity_replacement': {'13:13': 'ENTITYUNRELATED', '26:27': 'ENTITY', '29:33': 'ENTITYOTHER'}}	In the end , the aim is to locate all eventualities in a text on a time axis and / or a map to ensure an optimal base for automatic temporal and geospatial reasoning .
The world knowledge MiniSTEx uses is contained in interconnected tables in a database.	interconnected tables	database	part_whole	{'e1': {'word': 'interconnected tables', 'word_index': [(8, 9)], 'id': 'L08-1561.20'}, 'e2': {'word': 'database', 'word_index': [(12, 12)], 'id': 'L08-1561.21'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '3:3': 'ENTITYUNRELATED', '8:9': 'ENTITY', '12:12': 'ENTITYOTHER'}}	The world knowledge MiniSTEx uses is contained in interconnected tables in a database .
We propose a semantic construction method for Feature-Based Tree Adjoining Grammar which is based on the derived tree, compare it with related proposals and briefly discuss some implementation possibilities.	derived tree	semantic construction method	usage	{'e1': {'word': 'derived tree', 'word_index': [(18, 19)], 'id': 'E03-1030.3'}, 'e2': {'word': 'semantic construction method', 'word_index': [(3, 5)], 'id': 'E03-1030.1'}, 'entity_replacement': {'3:5': 'ENTITYOTHER', '7:12': 'ENTITYUNRELATED', '18:19': 'ENTITY', '30:31': 'ENTITYUNRELATED'}}	We propose a semantic construction method for Feature - Based Tree Adjoining Grammar which is based on the derived tree , compare it with related proposals and briefly discuss some implementation possibilities .
Traditional approaches to the problem of extracting data from texts have emphasized handcrafted linguistic knowledge	data	texts	part_whole	{'e1': {'word': 'data', 'word_index': [(7, 7)], 'id': 'M91-1021.1'}, 'e2': {'word': 'texts', 'word_index': [(9, 9)], 'id': 'M91-1021.2'}, 'entity_replacement': {'7:7': 'ENTITY', '9:9': 'ENTITYOTHER', '12:14': 'ENTITYUNRELATED'}}	Traditional approaches to the problem of extracting data from texts have emphasized handcrafted linguistic knowledge
We have previously performed experiments on components of the system with texts from the Wall Street Journal, however, the MUC-3 task is the first end-to-end application of plum.	texts	Wall Street Journal	part_whole	{'e1': {'word': 'texts', 'word_index': [(11, 11)], 'id': 'M91-1021.14'}, 'e2': {'word': 'Wall Street Journal', 'word_index': [(14, 16)], 'id': 'M91-1021.15'}, 'entity_replacement': {'11:11': 'ENTITY', '14:16': 'ENTITYOTHER', '21:24': 'ENTITYUNRELATED', '28:33': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED'}}	We have previously performed experiments on components of the system with texts from the Wall Street Journal , however , the MUC - 3 task is the first end - to - end application of plum .
A central assumption of our approach is that in processing unrestricted text for data extraction, a non-trivial amount of the text will not be understood.	data extraction	unrestricted text	usage	{'e1': {'word': 'data extraction', 'word_index': [(13, 14)], 'id': 'M91-1021.24'}, 'e2': {'word': 'unrestricted text', 'word_index': [(10, 11)], 'id': 'M91-1021.23'}, 'entity_replacement': {'10:11': 'ENTITYOTHER', '13:14': 'ENTITY', '21:21': 'ENTITYUNRELATED'}}	A central assumption of our approach is that in processing unrestricted text for data extraction , a non-trivial amount of the text will not be understood .
We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose.	sentences	document	part_whole	{'e1': {'word': 'sentences', 'word_index': [(13, 13)], 'id': 'A00-1043.3'}, 'e2': {'word': 'document', 'word_index': [(19, 19)], 'id': 'A00-1043.4'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '13:13': 'ENTITY', '19:19': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED'}}	We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose .
The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed, including syntactic knowledge, context information, and statistics computed from a corpus which consists of examples written by human professionals.	statistics	corpus	part_whole	{'e1': {'word': 'statistics', 'word_index': [(27, 27)], 'id': 'A00-1043.11'}, 'e2': {'word': 'corpus', 'word_index': [(31, 31)], 'id': 'A00-1043.12'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '27:27': 'ENTITY', '31:31': 'ENTITYOTHER'}}	The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed , including syntactic knowledge , context information , and statistics computed from a corpus which consists of examples written by human professionals .
Reduction can significantly improve the conciseness of automatic summaries.	conciseness	automatic summaries	model-feature	{'e1': {'word': 'conciseness', 'word_index': [(5, 5)], 'id': 'A00-1043.14'}, 'e2': {'word': 'automatic summaries', 'word_index': [(7, 8)], 'id': 'A00-1043.15'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '5:5': 'ENTITY', '7:8': 'ENTITYOTHER'}}	Reduction can significantly improve the conciseness of automatic summaries .
The method is automatically trainable, acquiring information from both positive and negative examples.	information	positive and negative examples	part_whole	{'e1': {'word': 'information', 'word_index': [(7, 7)], 'id': 'H94-1040.5'}, 'e2': {'word': 'positive and negative examples', 'word_index': [(10, 13)], 'id': 'H94-1040.6'}, 'entity_replacement': {'7:7': 'ENTITY', '10:13': 'ENTITYOTHER'}}	The method is automatically trainable , acquiring information from both positive and negative examples .
This paper introduces an algorithm for automatically acquiring the conceptual structure of each word from corpus.	conceptual structure	word	model-feature	{'e1': {'word': 'conceptual structure', 'word_index': [(9, 10)], 'id': 'W93-0103.1'}, 'e2': {'word': 'word', 'word_index': [(13, 13)], 'id': 'W93-0103.2'}, 'entity_replacement': {'9:10': 'ENTITY', '13:13': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED'}}	This paper introduces an algorithm for automatically acquiring the conceptual structure of each word from corpus .
The lexical concept obtained from the Collocation Map best reflects the subdomain of language usage.	lexical concept	Collocation Map	part_whole	{'e1': {'word': 'lexical concept', 'word_index': [(1, 2)], 'id': 'W93-0103.24'}, 'e2': {'word': 'Collocation Map', 'word_index': [(6, 7)], 'id': 'W93-0103.25'}, 'entity_replacement': {'1:2': 'ENTITY', '6:7': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED'}}	The lexical concept obtained from the Collocation Map best reflects the subdomain of language usage .
The potential application of conditional probabilities the Collocation Map provides may extend to cover very diverse areas of language processing such as sense disambiguation, thesaurus construction, automatic indexing, and document classification.	conditional probabilities	language processing	usage	{'e1': {'word': 'conditional probabilities', 'word_index': [(4, 5)], 'id': 'W93-0103.28'}, 'e2': {'word': 'language processing', 'word_index': [(18, 19)], 'id': 'W93-0103.30'}, 'entity_replacement': {'4:5': 'ENTITY', '7:8': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	The potential application of conditional probabilities the Collocation Map provides may extend to cover very diverse areas of language processing such as sense disambiguation , thesaurus construction , automatic indexing , and document classification .
"We discuss such ""strapping"" methods in general, and exhibit a particular method for strapping word-sense classifiers for ambiguous words."	"""strapping"" methods"	word-sense classifiers	usage	"{'e1': {'word': '""strapping"" methods', 'word_index': [(3, 6)], 'id': 'H05-1050.8'}, 'e2': {'word': 'word-sense classifiers', 'word_index': [(17, 20)], 'id': 'H05-1050.9'}, 'entity_replacement': {'3:6': 'ENTITY', '17:20': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED'}}"	"We discuss such "" strapping "" methods in general , and exhibit a particular method for strapping word - sense classifiers for ambiguous words ."
"Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand (Yarowsky, 1995), which in turn is known to rival supervised methods. """	unsupervised technique	supervised methods	compare	{'e1': {'word': 'unsupervised technique', 'word_index': [(9, 10)], 'id': 'H05-1050.12'}, 'e2': {'word': 'supervised methods', 'word_index': [(33, 34)], 'id': 'H05-1050.14'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '9:10': 'ENTITY', '17:17': 'ENTITYUNRELATED', '33:34': 'ENTITYOTHER'}}	"Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand ( Yarowsky , 1995 ) , which in turn is known to rival supervised methods . """
This word association information is added to the system at the time of the automatic creation of our translation pattern database, thereby making this database more domain specific.	word association information	system	usage	{'e1': {'word': 'word association information', 'word_index': [(1, 3)], 'id': 'W01-1414.6'}, 'e2': {'word': 'system', 'word_index': [(8, 8)], 'id': 'W01-1414.7'}, 'entity_replacement': {'1:3': 'ENTITY', '8:8': 'ENTITYOTHER', '18:20': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	This word association information is added to the system at the time of the automatic creation of our translation pattern database , thereby making this database more domain specific .
This technique significantly improves the overall quality of translation, as measured in an independent blind evaluation.	technique	translation	result	{'e1': {'word': 'technique', 'word_index': [(1, 1)], 'id': 'W01-1414.10'}, 'e2': {'word': 'translation', 'word_index': [(8, 8)], 'id': 'W01-1414.11'}, 'entity_replacement': {'1:1': 'ENTITY', '8:8': 'ENTITYOTHER', '14:16': 'ENTITYUNRELATED'}}	This technique significantly improves the overall quality of translation , as measured in an independent blind evaluation .
We describe the experiments of the UC Berkeley team on improving English-Spanish machine translation of news text, as part of the WMT'08 Shared Translation Task.	English-Spanish machine translation	news text	usage	{'e1': {'word': 'English-Spanish machine translation', 'word_index': [(11, 15)], 'id': 'W08-0320.1'}, 'e2': {'word': 'news text', 'word_index': [(17, 18)], 'id': 'W08-0320.2'}, 'entity_replacement': {'11:15': 'ENTITY', '17:18': 'ENTITYOTHER', '24:27': 'ENTITYUNRELATED'}}	We describe the experiments of the UC Berkeley team on improving English - Spanish machine translation of news text , as part of the WMT'08 Shared Translation Task .
We further add a third phrase translation model trained on a version of the news bi-text augmented with monolingual sentence-level syntactic paraphrases on the source-language side, and we combine all models in a log-linear model using minimum error rate training.	monolingual sentence-level syntactic paraphrases	news bi-text	part_whole	{'e1': {'word': 'monolingual sentence-level syntactic paraphrases', 'word_index': [(18, 23)], 'id': 'W08-0320.11'}, 'e2': {'word': 'news bi-text', 'word_index': [(14, 15)], 'id': 'W08-0320.10'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '14:15': 'ENTITYOTHER', '18:23': 'ENTITY', '35:35': 'ENTITYUNRELATED', '38:41': 'ENTITYUNRELATED', '43:46': 'ENTITYUNRELATED'}}	We further add a third phrase translation model trained on a version of the news bi-text augmented with monolingual sentence - level syntactic paraphrases on the source - language side , and we combine all models in a log - linear model using minimum error rate training .
Several recently reported techniques for the automatic acquisition of Information Extraction (IE) systems have used dependency trees as the basis of their extraction pattern representation.	dependency trees	Information Extraction (IE) systems	usage	{'e1': {'word': 'dependency trees', 'word_index': [(17, 18)], 'id': 'W06-0202.3'}, 'e2': {'word': 'Information Extraction (IE) systems', 'word_index': [(9, 14)], 'id': 'W06-0202.2'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:14': 'ENTITYOTHER', '17:18': 'ENTITY', '24:26': 'ENTITYUNRELATED'}}	Several recently reported techniques for the automatic acquisition of Information Extraction ( IE ) systems have used dependency trees as the basis of their extraction pattern representation .
An appropriate model should be expressive enough to represent the information which is to be extracted from text without being overly complicated.	information	text	part_whole	{'e1': {'word': 'information', 'word_index': [(10, 10)], 'id': 'W06-0202.9'}, 'e2': {'word': 'text', 'word_index': [(17, 17)], 'id': 'W06-0202.10'}, 'entity_replacement': {'10:10': 'ENTITY', '17:17': 'ENTITYOTHER'}}	An appropriate model should be expressive enough to represent the information which is to be extracted from text without being overly complicated .
The number of control actions needed to switch languages was decreased over 93% when using TypeAny rather than a conventional method.	TypeAny	conventional method	compare	{'e1': {'word': 'TypeAny', 'word_index': [(16, 17)], 'id': 'I08-1058.23'}, 'e2': {'word': 'conventional method', 'word_index': [(21, 22)], 'id': 'I08-1058.24'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '16:17': 'ENTITY', '21:22': 'ENTITYOTHER'}}	The number of control actions needed to switch languages was decreased over 93 % when using Type Any rather than a conventional method .
It builds on an earlier system based on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.	word overlap	shallow component	usage	{'e1': {'word': 'word overlap', 'word_index': [(23, 24)], 'id': 'W07-1402.5'}, 'e2': {'word': 'shallow component', 'word_index': [(19, 20)], 'id': 'W07-1402.4'}, 'entity_replacement': {'10:12': 'ENTITYUNRELATED', '19:20': 'ENTITYOTHER', '23:24': 'ENTITY'}}	It builds on an earlier system based on a relatively deep linguistic analysis , which we complement with a shallow component based on word overlap .
However, earlier observations that the combination of features improves the overall accuracy could be replicated only partly.	features	overall accuracy	result	{'e1': {'word': 'features', 'word_index': [(8, 8)], 'id': 'W07-1402.7'}, 'e2': {'word': 'overall accuracy', 'word_index': [(11, 12)], 'id': 'W07-1402.8'}, 'entity_replacement': {'8:8': 'ENTITY', '11:12': 'ENTITYOTHER'}}	However , earlier observations that the combination of features improves the overall accuracy could be replicated only partly .
In this paper, we address the problem of extracting data records and their attributes from unstructured biomedical full text.	data records	unstructured biomedical full text	part_whole	{'e1': {'word': 'data records', 'word_index': [(10, 11)], 'id': 'D07-1088.1'}, 'e2': {'word': 'unstructured biomedical full text', 'word_index': [(16, 19)], 'id': 'D07-1088.3'}, 'entity_replacement': {'10:11': 'ENTITY', '14:14': 'ENTITYUNRELATED', '16:19': 'ENTITYOTHER'}}	In this paper , we address the problem of extracting data records and their attributes from unstructured biomedical full text .
We evaluate the approach from the perspective of Information Extraction and achieve significant improvements on system performance compared with other baseline systems.	system performance	baseline systems	compare	{'e1': {'word': 'system performance', 'word_index': [(15, 16)], 'id': 'D07-1088.12'}, 'e2': {'word': 'baseline systems', 'word_index': [(20, 21)], 'id': 'D07-1088.13'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '15:16': 'ENTITY', '20:21': 'ENTITYOTHER'}}	We evaluate the approach from the perspective of Information Extraction and achieve significant improvements on system performance compared with other baseline systems .
In this paper, we describe a system by which the multilingual characteristics of Wikipedia can be utilized to annotate a large corpus of text with Named Entity Recognition (NER) tags requiring minimal human intervention and no linguistic expertise.	multilingual characteristics	Wikipedia	model-feature	{'e1': {'word': 'multilingual characteristics', 'word_index': [(11, 12)], 'id': 'P08-1001.1'}, 'e2': {'word': 'Wikipedia', 'word_index': [(14, 14)], 'id': 'P08-1001.2'}, 'entity_replacement': {'11:12': 'ENTITY', '14:14': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '26:32': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED'}}	In this paper , we describe a system by which the multilingual characteristics of Wikipedia can be utilized to annotate a large corpus of text with Named Entity Recognition ( NER ) tags requiring minimal human intervention and no linguistic expertise .
In this paper, we describe a system by which the multilingual characteristics of Wikipedia can be utilized to annotate a large corpus of text with Named Entity Recognition (NER) tags requiring minimal human intervention and no linguistic expertise.	text	large corpus	part_whole	{'e1': {'word': 'text', 'word_index': [(24, 24)], 'id': 'P08-1001.4'}, 'e2': {'word': 'large corpus', 'word_index': [(21, 22)], 'id': 'P08-1001.3'}, 'entity_replacement': {'11:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '21:22': 'ENTITYOTHER', '24:24': 'ENTITY', '26:32': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED'}}	In this paper , we describe a system by which the multilingual characteristics of Wikipedia can be utilized to annotate a large corpus of text with Named Entity Recognition ( NER ) tags requiring minimal human intervention and no linguistic expertise .
We further describe the methods by which English language data can be used to bootstrap the NER process in other languages.	English language data	NER process	usage	{'e1': {'word': 'English language data', 'word_index': [(7, 9)], 'id': 'P08-1001.17'}, 'e2': {'word': 'NER process', 'word_index': [(16, 17)], 'id': 'P08-1001.18'}, 'entity_replacement': {'7:9': 'ENTITY', '16:17': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED'}}	We further describe the methods by which English language data can be used to bootstrap the NER process in other languages .
Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text.	entity-grid representation	discourse	model-feature	{'e1': {'word': 'entity-grid representation', 'word_index': [(6, 9)], 'id': 'J08-1001.2'}, 'e2': {'word': 'discourse', 'word_index': [(11, 11)], 'id': 'J08-1001.3'}, 'entity_replacement': {'6:9': 'ENTITY', '11:11': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED'}}	Central to this approach is the entity - grid representation of discourse , which captures patterns of entity distribution in a text .
The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities.	entity transition sequences	text	model-feature	{'e1': {'word': 'entity transition sequences', 'word_index': [(14, 16)], 'id': 'J08-1001.8'}, 'e2': {'word': 'text', 'word_index': [(9, 9)], 'id': 'J08-1001.7'}, 'entity_replacement': {'9:9': 'ENTITYOTHER', '14:16': 'ENTITY', '19:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional , syntactic , and referential information about discourse entities .
This paper describes discriminative language modeling for a large vocabulary speech recognition task.	discriminative language modeling	vocabulary speech recognition task	usage	{'e1': {'word': 'discriminative language modeling', 'word_index': [(3, 5)], 'id': 'P04-1007.1'}, 'e2': {'word': 'vocabulary speech recognition task', 'word_index': [(9, 12)], 'id': 'P04-1007.2'}, 'entity_replacement': {'3:5': 'ENTITY', '9:12': 'ENTITYOTHER'}}	This paper describes discriminative language modeling for a large vocabulary speech recognition task .
We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random fields (CRFs).	perceptron algorithm	conditional random fields (CRFs)	compare	{'e1': {'word': 'perceptron algorithm', 'word_index': [(8, 9)], 'id': 'P04-1007.4'}, 'e2': {'word': 'conditional random fields (CRFs)', 'word_index': [(16, 21)], 'id': 'P04-1007.5'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '8:9': 'ENTITY', '16:21': 'ENTITYOTHER'}}	We contrast two parameter estimation methods : the perceptron algorithm , and a method based on conditional random fields ( CRFs ) .
The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data.	feature set	training data	part_whole	{'e1': {'word': 'feature set', 'word_index': [(12, 13)], 'id': 'P04-1007.12'}, 'e2': {'word': 'training data', 'word_index': [(22, 23)], 'id': 'P04-1007.13'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '12:13': 'ENTITY', '22:23': 'ENTITYOTHER'}}	The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data .
However, using the feature set output from the perceptron algorithm (initialized with their weights), CRF training provides an additional 0.5% reduction in word error rate, for a total 1.8% absolute reduction from the baseline of 39.2%.	CRF training	word error rate	result	{'e1': {'word': 'CRF training', 'word_index': [(18, 19)], 'id': 'P04-1007.17'}, 'e2': {'word': 'word error rate', 'word_index': [(27, 29)], 'id': 'P04-1007.18'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '18:19': 'ENTITY', '27:29': 'ENTITYOTHER', '40:40': 'ENTITYUNRELATED'}}	However , using the feature set output from the perceptron algorithm ( initialized with their weights ) , CRF training provides an additional 0.5 % reduction in word error rate , for a total 1.8 % absolute reduction from the baseline of 39.2 %.
We briefly describe the design and implementation status of the system, and then focus on how this system is used to elicit useful data for supporting hypotheses about multimodal interaction in the domain of meeting retrieval and for developing NLP modules for this specific domain.	system	useful data	usage	{'e1': {'word': 'system', 'word_index': [(18, 18)], 'id': 'P06-4013.6'}, 'e2': {'word': 'useful data', 'word_index': [(23, 24)], 'id': 'P06-4013.7'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '18:18': 'ENTITY', '23:24': 'ENTITYOTHER', '27:27': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED'}}	We briefly describe the design and implementation status of the system , and then focus on how this system is used to elicit useful data for supporting hypotheses about multimodal interaction in the domain of meeting retrieval and for developing NLP modules for this specific domain .
We show that the possibility to label distinctions with names has major advantages both for the use of feature logic in computational linguistics and its implementation.	feature logic	computational linguistics	usage	{'e1': {'word': 'feature logic', 'word_index': [(18, 19)], 'id': 'C90-2018.7'}, 'e2': {'word': 'computational linguistics', 'word_index': [(21, 22)], 'id': 'C90-2018.8'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '18:19': 'ENTITY', '21:22': 'ENTITYOTHER'}}	We show that the possibility to label distinctions with names has major advantages both for the use of feature logic in computational linguistics and its implementation .
We give an open world semantics for feature terms, where the denotation of a term is determined in dependence on the disjunctive context, i.e.	denotation	term	model-feature	{'e1': {'word': 'denotation', 'word_index': [(12, 12)], 'id': 'C90-2018.11'}, 'e2': {'word': 'term', 'word_index': [(15, 15)], 'id': 'C90-2018.12'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '12:12': 'ENTITY', '15:15': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED'}}	We give an open world semantics for feature terms , where the denotation of a term is determined in dependence on the disjunctive context , i.e.
Acquiring source language documents for testing, creating training datasets for customized MT lexicons, and building parallel corpora for MT evaluation require translators and non-native speaking analysts to handle large document collections.	source language documents	training datasets	usage	{'e1': {'word': 'source language documents', 'word_index': [(1, 3)], 'id': 'L08-1588.4'}, 'e2': {'word': 'training datasets', 'word_index': [(8, 9)], 'id': 'L08-1588.5'}, 'entity_replacement': {'1:3': 'ENTITY', '8:9': 'ENTITYOTHER', '11:13': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '25:28': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	Acquiring source language documents for testing , creating training datasets for customized MT lexicons , and building parallel corpora for MT evaluation require translators and non- native speaking analysts to handle large document collections .
In particular, we will discuss the development and use of MTriage, an application environment that enables the translator to markup documents with metadata for MT parameterization and routing.	metadata	documents	model-feature	{'e1': {'word': 'metadata', 'word_index': [(24, 24)], 'id': 'L08-1588.24'}, 'e2': {'word': 'documents', 'word_index': [(22, 22)], 'id': 'L08-1588.23'}, 'entity_replacement': {'11:11': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '22:22': 'ENTITYOTHER', '24:24': 'ENTITY', '26:29': 'ENTITYUNRELATED'}}	In particular , we will discuss the development and use of MTriage , an application environment that enables the translator to markup documents with metadata for MT parameterization and routing .
The use of MTriage as a web-enabled front end to multiple MT engines has leveraged the capabilities of our human translators for creating lexicons from NFW (Not-Found-Word) lists, writing reference translations, and creating parallel corpora for MT development and evaluation.	lexicons	NFW (Not-Found-Word) lists	part_whole	{'e1': {'word': 'lexicons', 'word_index': [(23, 23)], 'id': 'L08-1588.30'}, 'e2': {'word': 'NFW (Not-Found-Word) lists', 'word_index': [(25, 31)], 'id': 'L08-1588.31'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '23:23': 'ENTITY', '25:31': 'ENTITYOTHER', '34:35': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED'}}	The use of MTriage as a web-enabled front end to multiple MT engines has leveraged the capabilities of our human translators for creating lexicons from NFW ( Not-Found - Word ) lists , writing reference translations , and creating parallel corpora for MT development and evaluation .
The use of MTriage as a web-enabled front end to multiple MT engines has leveraged the capabilities of our human translators for creating lexicons from NFW (Not-Found-Word) lists, writing reference translations, and creating parallel corpora for MT development and evaluation.	parallel corpora	MT	usage	{'e1': {'word': 'parallel corpora', 'word_index': [(39, 40)], 'id': 'L08-1588.33'}, 'e2': {'word': 'MT', 'word_index': [(42, 42)], 'id': 'L08-1588.34'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '25:31': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '39:40': 'ENTITY', '42:42': 'ENTITYOTHER'}}	The use of MTriage as a web-enabled front end to multiple MT engines has leveraged the capabilities of our human translators for creating lexicons from NFW ( Not-Found - Word ) lists , writing reference translations , and creating parallel corpora for MT development and evaluation .
This paper describes a method for analyzing Japanese double-subject construction having an adjective predicate based on the valency structure.	adjective predicate	Japanese double-subject construction	model-feature	{'e1': {'word': 'adjective predicate', 'word_index': [(14, 15)], 'id': 'C96-2146.2'}, 'e2': {'word': 'Japanese double-subject construction', 'word_index': [(7, 11)], 'id': 'C96-2146.1'}, 'entity_replacement': {'7:11': 'ENTITYOTHER', '14:15': 'ENTITY', '19:20': 'ENTITYUNRELATED'}}	This paper describes a method for analyzing Japanese double - subject construction having an adjective predicate based on the valency structure .
A simple sentence usually has only one subjective case in most languages.	subjective case	simple sentence	model-feature	{'e1': {'word': 'subjective case', 'word_index': [(7, 8)], 'id': 'C96-2146.5'}, 'e2': {'word': 'simple sentence', 'word_index': [(1, 2)], 'id': 'C96-2146.4'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '7:8': 'ENTITY', '11:11': 'ENTITYUNRELATED'}}	A simple sentence usually has only one subjective case in most languages .
This paper proposes a method for analyzing a Japanese double-subject construction having an adjective predicate in order to overcome thee problems described.	adjective predicate	Japanese double-subject construction	model-feature	{'e1': {'word': 'adjective predicate', 'word_index': [(15, 16)], 'id': 'C96-2146.16'}, 'e2': {'word': 'Japanese double-subject construction', 'word_index': [(8, 12)], 'id': 'C96-2146.15'}, 'entity_replacement': {'8:12': 'ENTITYOTHER', '15:16': 'ENTITY'}}	This paper proposes a method for analyzing a Japanese double - subject construction having an adjective predicate in order to overcome thee problems described .
Classification Hierarchies (CHs) are widely used to organize documents in a way that makes their retrieval easier	Classification Hierarchies (CHs)	retrieval	usage	{'e1': {'word': 'Classification Hierarchies (CHs)', 'word_index': [(0, 4)], 'id': 'C04-1163.1'}, 'e2': {'word': 'retrieval', 'word_index': [(17, 17)], 'id': 'C04-1163.3'}, 'entity_replacement': {'0:4': 'ENTITY', '10:10': 'ENTITYUNRELATED', '17:17': 'ENTITYOTHER'}}	Classification Hierarchies ( CHs ) are widely used to organize documents in a way that makes their retrieval easier
In this paper we discuss and evaluate CtxMatch, an approach to interoperability that discovers mappings among CHs considering the semantic interpretation of their nodes.	semantic interpretation	nodes	model-feature	{'e1': {'word': 'semantic interpretation', 'word_index': [(21, 22)], 'id': 'C04-1163.10'}, 'e2': {'word': 'nodes', 'word_index': [(25, 25)], 'id': 'C04-1163.11'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '21:22': 'ENTITY', '25:25': 'ENTITYOTHER'}}	In this paper we discuss and evaluate Ctx Match , an approach to interoperability that discovers mappings among CHs considering the semantic interpretation of their nodes .
CtxMatch performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation.	linguistic processing	labels	usage	{'e1': {'word': 'linguistic processing', 'word_index': [(3, 4)], 'id': 'C04-1163.13'}, 'e2': {'word': 'labels', 'word_index': [(7, 7)], 'id': 'C04-1163.14'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '3:4': 'ENTITY', '7:7': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:19': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '24:26': 'ENTITYUNRELATED'}}	CtxMatch performs a linguistic processing of the labels attached to the nodes , including tokenization , Part of Speech tagging , multiword recognition and word sense disambiguation .
A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient.	clustering	retrieval	usage	{'e1': {'word': 'clustering', 'word_index': [(7, 7)], 'id': 'C94-1014.7'}, 'e2': {'word': 'retrieval', 'word_index': [(10, 10)], 'id': 'C94-1014.8'}, 'entity_replacement': {'7:7': 'ENTITY', '10:10': 'ENTITYOTHER', '13:15': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED'}}	A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient .
A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient.	best matching example	database	part_whole	{'e1': {'word': 'best matching example', 'word_index': [(13, 15)], 'id': 'C94-1014.9'}, 'e2': {'word': 'database', 'word_index': [(18, 18)], 'id': 'C94-1014.10'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:15': 'ENTITY', '18:18': 'ENTITYOTHER'}}	A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient .
At the same time, higher level collective knowledge is often published using a graphical notation representing all the entities in a pathway and their interactions.	graphical notation	entities	model-feature	{'e1': {'word': 'graphical notation', 'word_index': [(14, 15)], 'id': 'L08-1302.5'}, 'e2': {'word': 'entities', 'word_index': [(19, 19)], 'id': 'L08-1302.6'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '14:15': 'ENTITY', '19:19': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	At the same time , higher level collective knowledge is often published using a graphical notation representing all the entities in a pathway and their interactions .
Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance.	keywords	information retrieval techniques	usage	{'e1': {'word': 'keywords', 'word_index': [(8, 8)], 'id': 'H01-1001.7'}, 'e2': {'word': 'information retrieval techniques', 'word_index': [(1, 3)], 'id': 'H01-1001.5'}, 'entity_replacement': {'1:3': 'ENTITYOTHER', '6:6': 'ENTITYUNRELATED', '8:8': 'ENTITY', '11:12': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED'}}	Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance .
Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance.	oral communication	indices	usage	{'e1': {'word': 'oral communication', 'word_index': [(14, 15)], 'id': 'H01-1001.9'}, 'e2': {'word': 'indices', 'word_index': [(19, 19)], 'id': 'H01-1001.10'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '14:15': 'ENTITY', '19:19': 'ENTITYOTHER'}}	Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance .
Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger database and detect those automatically which is shown on a large database of TV shows .	TV shows	database	part_whole	{'e1': {'word': 'TV shows', 'word_index': [(36, 37)], 'id': 'H01-1001.15'}, 'e2': {'word': 'database', 'word_index': [(34, 34)], 'id': 'H01-1001.14'}, 'entity_replacement': {'23:23': 'ENTITYUNRELATED', '34:34': 'ENTITYOTHER', '36:37': 'ENTITY'}}	Several extensions of this basic idea are being discussed and / or evaluated : Similar to activities one can define subsets of larger database and detect those automatically which is shown on a large database of TV shows .
 To support engaging human users in robust, mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems , the DARPA Communicator program [1] is funding the development of a distributed message-passing infrastructure for dialogue systems which all Communicator participants are using	distributed message-passing infrastructure	dialogue systems	model-feature	{'e1': {'word': 'distributed message-passing infrastructure', 'word_index': [(36, 40)], 'id': 'H01-1017.4'}, 'e2': {'word': 'dialogue systems', 'word_index': [(42, 43)], 'id': 'H01-1017.5'}, 'entity_replacement': {'8:13': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '24:26': 'ENTITYUNRELATED', '36:40': 'ENTITY', '42:43': 'ENTITYOTHER', '46:46': 'ENTITYUNRELATED'}}	To support engaging human users in robust , mixed - initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems , the DARPA Communicator program [ 1 ] is funding the development of a distributed message - passing infrastructure for dialogue systems which all Communicator participants are using
The CCLINC Korean-to-English translation system consists of two core modules , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame .	core modules	CCLINC Korean-to-English translation system	part_whole	{'e1': {'word': 'core modules', 'word_index': [(10, 11)], 'id': 'H01-1041.4'}, 'e2': {'word': 'CCLINC Korean-to-English translation system', 'word_index': [(1, 6)], 'id': 'H01-1041.3'}, 'entity_replacement': {'1:6': 'ENTITYOTHER', '10:11': 'ENTITY', '13:17': 'ENTITYUNRELATED', '21:24': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	The CCLINC Korean-to - English translation system consists of two core modules , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame .
The key features of the system include: (i) Robust efficient parsing of Korean (a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ).	parsing	Korean	usage	{'e1': {'word': 'parsing', 'word_index': [(12, 12)], 'id': 'H01-1041.8'}, 'e2': {'word': 'Korean', 'word_index': [(14, 14)], 'id': 'H01-1041.9'}, 'entity_replacement': {'12:12': 'ENTITY', '14:14': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED', '21:23': 'ENTITYUNRELATED', '26:28': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	The key features of the system include : ( i) Robust efficient parsing of Korean ( a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ) .
The key features of the system include: (i) Robust efficient parsing of Korean (a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ).	overt case markers	verb final language	model-feature	{'e1': {'word': 'overt case markers', 'word_index': [(21, 23)], 'id': 'H01-1041.11'}, 'e2': {'word': 'verb final language', 'word_index': [(17, 19)], 'id': 'H01-1041.10'}, 'entity_replacement': {'12:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '17:19': 'ENTITYOTHER', '21:23': 'ENTITY', '26:28': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	The key features of the system include : ( i) Robust efficient parsing of Korean ( a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ) .
(ii) High quality translation via word sense disambiguation and accurate word order generation of the target language .	word sense disambiguation	translation	usage	{'e1': {'word': 'word sense disambiguation', 'word_index': [(7, 9)], 'id': 'H01-1041.15'}, 'e2': {'word': 'translation', 'word_index': [(5, 5)], 'id': 'H01-1041.14'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '7:9': 'ENTITY', '12:14': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	( ii ) High quality translation via word sense disambiguation and accurate word order generation of the target language .
 The purpose of this research is to test the efficacy of applying automated evaluation techniques , originally devised for the evaluation of human language learners , to the output of machine translation (MT) systems 	automated evaluation techniques	output	usage	{'e1': {'word': 'automated evaluation techniques', 'word_index': [(12, 14)], 'id': 'H01-1042.1'}, 'e2': {'word': 'output', 'word_index': [(28, 28)], 'id': 'H01-1042.3'}, 'entity_replacement': {'12:14': 'ENTITY', '22:24': 'ENTITYUNRELATED', '28:28': 'ENTITYOTHER', '30:35': 'ENTITYUNRELATED'}}	The purpose of this research is to test the efficacy of applying automated evaluation techniques , originally devised for the evaluation of human language learners , to the output of machine translation ( MT ) systems
This, the first experiment in a series of experiments, looks at the intelligibility of MT output .	intelligibility	MT output	model-feature	{'e1': {'word': 'intelligibility', 'word_index': [(14, 14)], 'id': 'H01-1042.10'}, 'e2': {'word': 'MT output', 'word_index': [(16, 17)], 'id': 'H01-1042.11'}, 'entity_replacement': {'14:14': 'ENTITY', '16:17': 'ENTITYOTHER'}}	This , the first experiment in a series of experiments , looks at the intelligibility of MT output .
We integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources .	intelligent mobile agents	spoken language understanding system	part_whole	{'e1': {'word': 'intelligent mobile agents', 'word_index': [(8, 10)], 'id': 'H01-1049.4'}, 'e2': {'word': 'spoken language understanding system', 'word_index': [(3, 6)], 'id': 'H01-1049.3'}, 'entity_replacement': {'3:6': 'ENTITYOTHER', '8:10': 'ENTITY', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED'}}	We integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources .
We find that simple interpolation methods , like log-linear and linear interpolation , improve the performance but fall short of the performance of an oracle .	interpolation methods	performance	result	{'e1': {'word': 'interpolation methods', 'word_index': [(4, 5)], 'id': 'H01-1058.2'}, 'e2': {'word': 'performance', 'word_index': [(15, 15)], 'id': 'H01-1058.4'}, 'entity_replacement': {'4:5': 'ENTITY', '8:11': 'ENTITYUNRELATED', '15:15': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	We find that simple interpolation methods , like log-linear and linear interpolation , improve the performance but fall short of the performance of an oracle .
The oracle knows the reference word string and selects the word string with the best performance (typically, word or semantic error rate ) from a list of word strings , where each word string has been obtained by using a different LM .	word string	performance	result	{'e1': {'word': 'word string', 'word_index': [(10, 11)], 'id': 'H01-1058.9'}, 'e2': {'word': 'performance', 'word_index': [(15, 15)], 'id': 'H01-1058.10'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:6': 'ENTITYUNRELATED', '10:11': 'ENTITY', '15:15': 'ENTITYOTHER', '19:23': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED'}}	The oracle knows the reference word string and selects the word string with the best performance ( typically , word or semantic error rate ) from a list of word strings , where each word string has been obtained by using a different LM .
The oracle knows the reference word string and selects the word string with the best performance (typically, word or semantic error rate ) from a list of word strings , where each word string has been obtained by using a different LM .	LM	word string	result	{'e1': {'word': 'LM', 'word_index': [(43, 43)], 'id': 'H01-1058.14'}, 'e2': {'word': 'word string', 'word_index': [(34, 35)], 'id': 'H01-1058.13'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:6': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '19:23': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '34:35': 'ENTITYOTHER', '43:43': 'ENTITY'}}	The oracle knows the reference word string and selects the word string with the best performance ( typically , word or semantic error rate ) from a list of word strings , where each word string has been obtained by using a different LM .
Actually, the oracle acts like a dynamic combiner with hard decisions using the reference .	reference	dynamic combiner	usage	{'e1': {'word': 'reference', 'word_index': [(14, 14)], 'id': 'H01-1058.18'}, 'e2': {'word': 'dynamic combiner', 'word_index': [(7, 8)], 'id': 'H01-1058.16'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '7:8': 'ENTITYOTHER', '10:11': 'ENTITYUNRELATED', '14:14': 'ENTITY'}}	Actually , the oracle acts like a dynamic combiner with hard decisions using the reference .
We provide experimental results that clearly show the need for a dynamic language model combination to improve the performance further .	dynamic language model combination	performance	result	{'e1': {'word': 'dynamic language model combination', 'word_index': [(11, 14)], 'id': 'H01-1058.19'}, 'e2': {'word': 'performance', 'word_index': [(18, 18)], 'id': 'H01-1058.20'}, 'entity_replacement': {'11:14': 'ENTITY', '18:18': 'ENTITYOTHER'}}	We provide experimental results that clearly show the need for a dynamic language model combination to improve the performance further .
The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence .	confidence measures	LMs	model-feature	{'e1': {'word': 'confidence measures', 'word_index': [(7, 8)], 'id': 'H01-1058.25'}, 'e2': {'word': 'LMs', 'word_index': [(5, 5)], 'id': 'H01-1058.24'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '7:8': 'ENTITY', '13:13': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED'}}	The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence .
The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence .	confidence	LM	model-feature	{'e1': {'word': 'confidence', 'word_index': [(21, 21)], 'id': 'H01-1058.28'}, 'e2': {'word': 'LM', 'word_index': [(17, 17)], 'id': 'H01-1058.27'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '17:17': 'ENTITYOTHER', '21:21': 'ENTITY'}}	The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence .
 This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai-English language identification 	error-correction rules	Thai key prediction	usage	{'e1': {'word': 'error-correction rules', 'word_index': [(10, 11)], 'id': 'H01-1070.2'}, 'e2': {'word': 'Thai key prediction', 'word_index': [(13, 15)], 'id': 'H01-1070.3'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:11': 'ENTITY', '13:15': 'ENTITYOTHER', '17:20': 'ENTITYUNRELATED'}}	This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai -English language identification
The paper also proposes rule-reduction algorithm applying mutual information to reduce the error-correction rules .	mutual information	error-correction rules	usage	{'e1': {'word': 'mutual information', 'word_index': [(9, 10)], 'id': 'H01-1070.6'}, 'e2': {'word': 'error-correction rules', 'word_index': [(14, 15)], 'id': 'H01-1070.7'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '9:10': 'ENTITY', '14:15': 'ENTITYOTHER'}}	The paper also proposes rule - reduction algorithm applying mutual information to reduce the error-correction rules .
Our algorithm reported more than 99% accuracy in both language identification and key prediction .	language identification	accuracy	result	{'e1': {'word': 'language identification', 'word_index': [(10, 11)], 'id': 'H01-1070.9'}, 'e2': {'word': 'accuracy', 'word_index': [(7, 7)], 'id': 'H01-1070.8'}, 'entity_replacement': {'7:7': 'ENTITYOTHER', '10:11': 'ENTITY', '13:14': 'ENTITYUNRELATED'}}	Our algorithm reported more than 99 % accuracy in both language identification and key prediction .
First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of possible sentence plans for a given text-plan input .	sentence plans	text-plan input	model-feature	{'e1': {'word': 'sentence plans', 'word_index': [(20, 21)], 'id': 'N01-1003.12'}, 'e2': {'word': 'text-plan input', 'word_index': [(25, 28)], 'id': 'N01-1003.13'}, 'entity_replacement': {'6:12': 'ENTITYUNRELATED', '20:21': 'ENTITY', '25:28': 'ENTITYOTHER'}}	First , a very simple , randomized sentence - plan-generator ( SPG ) generates a potentially large list of possible sentence plans for a given text - plan input .
The SPR uses ranking rules automatically learned from training data .	training data	ranking rules	usage	{'e1': {'word': 'training data', 'word_index': [(8, 9)], 'id': 'N01-1003.19'}, 'e2': {'word': 'ranking rules', 'word_index': [(3, 4)], 'id': 'N01-1003.18'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '3:4': 'ENTITYOTHER', '8:9': 'ENTITY'}}	The SPR uses ranking rules automatically learned from training data .
We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan .	sentence plan	top human-ranked sentence plan	compare	{'e1': {'word': 'sentence plan', 'word_index': [(10, 11)], 'id': 'N01-1003.21'}, 'e2': {'word': 'top human-ranked sentence plan', 'word_index': [(23, 28)], 'id': 'N01-1003.22'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '10:11': 'ENTITY', '23:28': 'ENTITYOTHER'}}	We show that the trained SPR learns to select a sentence plan whose rating on average is only 5 % worse than the top human - ranked sentence plan .
 In this paper, we compare the relative effects of segment order , segmentation and segment contiguity on the retrieval performance of a translation memory system 	translation memory system	retrieval performance	result	{'e1': {'word': 'translation memory system', 'word_index': [(23, 25)], 'id': 'P01-1004.5'}, 'e2': {'word': 'retrieval performance', 'word_index': [(19, 20)], 'id': 'P01-1004.4'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '19:20': 'ENTITYOTHER', '23:25': 'ENTITY'}}	In this paper , we compare the relative effects of segment order , segmentation and segment contiguity on the retrieval performance of a translation memory system
We take a selection of both bag-of-words and segment order-sensitive string comparison methods , and run each over both character- and word-segmented data , in combination with a range of local segment contiguity models (in the form of N-grams ).	bag-of-words and segment order-sensitive string comparison methods	character- and word-segmented data	usage	{'e1': {'word': 'bag-of-words and segment order-sensitive string comparison methods', 'word_index': [(6, 18)], 'id': 'P01-1004.6'}, 'e2': {'word': 'character- and word-segmented data', 'word_index': [(25, 31)], 'id': 'P01-1004.7'}, 'entity_replacement': {'6:18': 'ENTITY', '25:31': 'ENTITYOTHER', '39:42': 'ENTITYUNRELATED', '48:48': 'ENTITYUNRELATED'}}	We take a selection of both bag - of - words and segment order - sensitive string comparison methods , and run each over both character - and word - segmented data , in combination with a range of local segment contiguity models ( in the form of N-grams ) .
Over two distinct datasets , we find that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word N-gram models .	character bigrams	indexing	usage	{'e1': {'word': 'character bigrams', 'word_index': [(12, 13)], 'id': 'P01-1004.12'}, 'e2': {'word': 'indexing', 'word_index': [(8, 8)], 'id': 'P01-1004.11'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '8:8': 'ENTITYOTHER', '12:13': 'ENTITY', '16:17': 'ENTITYUNRELATED', '24:26': 'ENTITYUNRELATED'}}	Over two distinct datasets , we find that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word N-gram models .
Further,in their optimum configuration , bag-of-words methods are shown to be equivalent to segment order-sensitive methods in terms of retrieval accuracy , but much faster.	bag-of-words methods	segment order-sensitive methods	compare	{'e1': {'word': 'bag-of-words methods', 'word_index': [(7, 12)], 'id': 'P01-1004.16'}, 'e2': {'word': 'segment order-sensitive methods', 'word_index': [(19, 23)], 'id': 'P01-1004.17'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '7:12': 'ENTITY', '19:23': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED'}}	Further , in their optimum configuration , bag - of - words methods are shown to be equivalent to segment order - sensitive methods in terms of retrieval accuracy , but much faster .
 The theoretical study of the range concatenation grammar [RCG] formalism has revealed many attractive properties which may be used in NLP 	range concatenation grammar [RCG] formalism	NLP	usage	{'e1': {'word': 'range concatenation grammar [RCG] formalism', 'word_index': [(5, 11)], 'id': 'P01-1007.1'}, 'e2': {'word': 'NLP', 'word_index': [(22, 22)], 'id': 'P01-1007.2'}, 'entity_replacement': {'5:11': 'ENTITY', '22:22': 'ENTITYOTHER'}}	The theoretical study of the range concatenation grammar [ RCG ] formalism has revealed many attractive properties which may be used in NLP
In particular, range concatenation languages [RCL] can be parsed in polynomial time and many classical grammatical formalisms can be translated into equivalent RCGs without increasing their worst-case parsing time complexity .	polynomial time	range concatenation languages [RCL]	model-feature	{'e1': {'word': 'polynomial time', 'word_index': [(13, 14)], 'id': 'P01-1007.4'}, 'e2': {'word': 'range concatenation languages [RCL]', 'word_index': [(3, 8)], 'id': 'P01-1007.3'}, 'entity_replacement': {'3:8': 'ENTITYOTHER', '13:14': 'ENTITY', '18:19': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '29:34': 'ENTITYUNRELATED'}}	In particular , range concatenation languages [ RCL ] can be parsed in polynomial time and many classical grammatical formalisms can be translated into equivalent RCGs without increasing their worst - case parsing time complexity .
For example, after translation into an equivalent RCG , any tree adjoining grammar can be parsed in O(n6) time .	O(n6) time	tree adjoining grammar	model-feature	{'e1': {'word': 'O(n6) time', 'word_index': [(18, 22)], 'id': 'P01-1007.11'}, 'e2': {'word': 'tree adjoining grammar', 'word_index': [(11, 13)], 'id': 'P01-1007.10'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:13': 'ENTITYOTHER', '18:22': 'ENTITY'}}	For example , after translation into an equivalent RCG , any tree adjoining grammar can be parsed in O ( n6 ) time .
In this paper, we study a parsing technique whose purpose is to improve the practical efficiency of RCL parsers .	parsing technique	RCL parsers	usage	{'e1': {'word': 'parsing technique', 'word_index': [(7, 8)], 'id': 'P01-1007.12'}, 'e2': {'word': 'RCL parsers', 'word_index': [(18, 19)], 'id': 'P01-1007.13'}, 'entity_replacement': {'7:8': 'ENTITY', '18:19': 'ENTITYOTHER'}}	In this paper , we study a parsing technique whose purpose is to improve the practical efficiency of RCL parsers .
The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L .	shared derivation forest	guide	usage	{'e1': {'word': 'shared derivation forest', 'word_index': [(20, 22)], 'id': 'P01-1007.18'}, 'e2': {'word': 'guide', 'word_index': [(16, 16)], 'id': 'P01-1007.17'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '6:7': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '16:16': 'ENTITYOTHER', '20:22': 'ENTITY', '27:28': 'ENTITYUNRELATED', '32:34': 'ENTITYUNRELATED'}}	The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L .
The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L .	RCL parser	superset of L	usage	{'e1': {'word': 'RCL parser', 'word_index': [(27, 28)], 'id': 'P01-1007.19'}, 'e2': {'word': 'superset of L', 'word_index': [(32, 34)], 'id': 'P01-1007.20'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '6:7': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '20:22': 'ENTITYUNRELATED', '27:28': 'ENTITY', '32:34': 'ENTITYOTHER'}}	The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L .
 While paraphrasing is critical both for interpretation and generation of natural language , current systems use manual or semi-automatic methods to collect paraphrases 	paraphrasing	interpretation and generation of natural language	usage	{'e1': {'word': 'paraphrasing', 'word_index': [(1, 1)], 'id': 'P01-1008.1'}, 'e2': {'word': 'interpretation and generation of natural language', 'word_index': [(6, 11)], 'id': 'P01-1008.2'}, 'entity_replacement': {'1:1': 'ENTITY', '6:11': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED'}}	While paraphrasing is critical both for interpretation and generation of natural language , current systems use manual or semi-automatic methods to collect paraphrases
We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text .	unsupervised learning algorithm	identification of paraphrases	usage	{'e1': {'word': 'unsupervised learning algorithm', 'word_index': [(3, 5)], 'id': 'P01-1008.4'}, 'e2': {'word': 'identification of paraphrases', 'word_index': [(7, 9)], 'id': 'P01-1008.5'}, 'entity_replacement': {'3:5': 'ENTITY', '7:9': 'ENTITYOTHER', '12:16': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED'}}	We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text .
 This paper presents a formal analysis for a large class of words called alternative markers , which includes other (than) , such (as) , and besides 	formal analysis	alternative markers	topic	{'e1': {'word': 'formal analysis', 'word_index': [(4, 5)], 'id': 'P01-1009.1'}, 'e2': {'word': 'alternative markers', 'word_index': [(13, 14)], 'id': 'P01-1009.3'}, 'entity_replacement': {'4:5': 'ENTITY', '11:11': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '18:21': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	This paper presents a formal analysis for a large class of words called alternative markers , which includes other ( than ) , such ( as ) , and besides
I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine &apos;s operational semantics .	formal analysis	performance	result	{'e1': {'word': 'formal analysis', 'word_index': [(19, 20)], 'id': 'P01-1009.14'}, 'e2': {'word': 'performance', 'word_index': [(4, 4)], 'id': 'P01-1009.12'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '7:8': 'ENTITYUNRELATED', '19:20': 'ENTITY', '26:27': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine &apos ;s operational semantics .
The value of this approach is that as the operational semantics of natural language applications improve, even larger improvements are possible.	operational semantics	natural language applications	part_whole	{'e1': {'word': 'operational semantics', 'word_index': [(9, 10)], 'id': 'P01-1009.17'}, 'e2': {'word': 'natural language applications', 'word_index': [(12, 14)], 'id': 'P01-1009.18'}, 'entity_replacement': {'9:10': 'ENTITY', '12:14': 'ENTITYOTHER'}}	The value of this approach is that as the operational semantics of natural language applications improve , even larger improvements are possible .
Our logical definition leads to a neat relation to categorial grammar , (yielding a treatment of Montague semantics ), a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data (based on a typing-algorithm and type-unification ).	structured data	learning algorithm	usage	{'e1': {'word': 'structured data', 'word_index': [(34, 35)], 'id': 'P01-1047.11'}, 'e2': {'word': 'learning algorithm', 'word_index': [(31, 32)], 'id': 'P01-1047.10'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED', '31:32': 'ENTITYOTHER', '34:35': 'ENTITY', '40:42': 'ENTITYUNRELATED', '44:44': 'ENTITYUNRELATED'}}	Our logical definition leads to a neat relation to categorial grammar , ( yielding a treatment of Montague semantics ) , a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data ( based on a typing - algorithm and type-unification ) .
 Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches 	quality	utterances	model-feature	{'e1': {'word': 'quality', 'word_index': [(22, 22)], 'id': 'P01-1056.3'}, 'e2': {'word': 'utterances', 'word_index': [(24, 24)], 'id': 'P01-1056.4'}, 'entity_replacement': {'0:3': 'ENTITYUNRELATED', '7:9': 'ENTITYUNRELATED', '22:22': 'ENTITY', '24:24': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED', '32:42': 'ENTITYUNRELATED'}}	Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand - crafted template - based or rule - based approaches
 Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches 	trainable components	hand-crafted template-based or rule-based approaches	compare	{'e1': {'word': 'trainable components', 'word_index': [(27, 28)], 'id': 'P01-1056.5'}, 'e2': {'word': 'hand-crafted template-based or rule-based approaches', 'word_index': [(32, 42)], 'id': 'P01-1056.6'}, 'entity_replacement': {'0:3': 'ENTITYUNRELATED', '7:9': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '27:28': 'ENTITY', '32:42': 'ENTITYOTHER'}}	Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand - crafted template - based or rule - based approaches
In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments .	trainable sentence planner	spoken dialogue system	usage	{'e1': {'word': 'trainable sentence planner', 'word_index': [(7, 9)], 'id': 'P01-1056.7'}, 'e2': {'word': 'spoken dialogue system', 'word_index': [(12, 14)], 'id': 'P01-1056.8'}, 'entity_replacement': {'7:9': 'ENTITY', '12:14': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED'}}	In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments .
We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .	trainable sentence planner	rule-based systems	compare	{'e1': {'word': 'trainable sentence planner', 'word_index': [(4, 6)], 'id': 'P01-1056.13'}, 'e2': {'word': 'rule-based systems', 'word_index': [(11, 14)], 'id': 'P01-1056.14'}, 'entity_replacement': {'4:6': 'ENTITY', '11:14': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED', '24:27': 'ENTITYUNRELATED'}}	We show that the trainable sentence planner performs better than the rule - based systems and the baselines , and as well as the hand - crafted system .
 We describe a set of supervised machine learning experiments centering on the construction of statistical models of WH-questions 	statistical models	WH-questions	model-feature	{'e1': {'word': 'statistical models', 'word_index': [(14, 15)], 'id': 'P01-1070.2'}, 'e2': {'word': 'WH-questions', 'word_index': [(17, 19)], 'id': 'P01-1070.3'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '14:15': 'ENTITY', '17:19': 'ENTITYOTHER'}}	We describe a set of supervised machine learning experiments centering on the construction of statistical models of WH - questions
These models , which are built from shallow linguistic features of questions , are employed to predict target variables which represent a user&apos;s informational goals .	shallow linguistic features	questions	model-feature	{'e1': {'word': 'shallow linguistic features', 'word_index': [(7, 9)], 'id': 'P01-1070.5'}, 'e2': {'word': 'questions', 'word_index': [(11, 11)], 'id': 'P01-1070.6'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:9': 'ENTITY', '11:11': 'ENTITYOTHER', '22:25': 'ENTITYUNRELATED'}}	These models , which are built from shallow linguistic features of questions , are employed to predict target variables which represent a user&apos ;s informational goals .
We report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables.	models	predictive performance	result	{'e1': {'word': 'models', 'word_index': [(11, 11)], 'id': 'P01-1070.9'}, 'e2': {'word': 'predictive performance', 'word_index': [(7, 8)], 'id': 'P01-1070.8'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '11:11': 'ENTITY', '18:21': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED'}}	We report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables .
We report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables.	training and testing factors	predictive performance	result	{'e1': {'word': 'training and testing factors', 'word_index': [(18, 21)], 'id': 'P01-1070.10'}, 'e2': {'word': 'predictive performance', 'word_index': [(23, 24)], 'id': 'P01-1070.11'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '18:21': 'ENTITY', '23:24': 'ENTITYOTHER'}}	We report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables .
 This paper describes a method for utterance classification that does not require manual transcription of training data 	manual transcription	utterance classification	usage	{'e1': {'word': 'manual transcription', 'word_index': [(12, 13)], 'id': 'N03-1001.2'}, 'e2': {'word': 'utterance classification', 'word_index': [(6, 7)], 'id': 'N03-1001.1'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '12:13': 'ENTITY', '15:16': 'ENTITYUNRELATED'}}	This paper describes a method for utterance classification that does not require manual transcription of training data
The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .	manual transcription	word-trigram recognition	usage	{'e1': {'word': 'manual transcription', 'word_index': [(35, 36)], 'id': 'N03-1001.8'}, 'e2': {'word': 'word-trigram recognition', 'word_index': [(30, 33)], 'id': 'N03-1001.7'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '16:18': 'ENTITYUNRELATED', '30:33': 'ENTITYOTHER', '35:36': 'ENTITY'}}	The method combines domain independent acoustic models with off - the - shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word - trigram recognition requiring manual transcription .
In our method, unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .	unsupervised training	phone n-gram model	usage	{'e1': {'word': 'unsupervised training', 'word_index': [(4, 5)], 'id': 'N03-1001.9'}, 'e2': {'word': 'phone n-gram model', 'word_index': [(12, 14)], 'id': 'N03-1001.10'}, 'entity_replacement': {'4:5': 'ENTITY', '12:14': 'ENTITYOTHER', '18:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED'}}	In our method , unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone - string classifier .
In our method, unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .	phone-string classifier	output	usage	{'e1': {'word': 'phone-string classifier', 'word_index': [(32, 35)], 'id': 'N03-1001.15'}, 'e2': {'word': 'output', 'word_index': [(21, 21)], 'id': 'N03-1001.12'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '32:35': 'ENTITY'}}	In our method , unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone - string classifier .
 Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora 	answering agents	multi-strategy and multi-source approach to question answering	usage	{'e1': {'word': 'answering agents', 'word_index': [(37, 38)], 'id': 'N03-1004.5'}, 'e2': {'word': 'multi-strategy and multi-source approach to question answering', 'word_index': [(21, 27)], 'id': 'N03-1004.4'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '14:16': 'ENTITYUNRELATED', '21:27': 'ENTITYOTHER', '37:38': 'ENTITY', '41:41': 'ENTITYUNRELATED', '44:44': 'ENTITYUNRELATED'}}	Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora
 Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora 	answers	corpora	part_whole	{'e1': {'word': 'answers', 'word_index': [(41, 41)], 'id': 'N03-1004.6'}, 'e2': {'word': 'corpora', 'word_index': [(44, 44)], 'id': 'N03-1004.7'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '14:16': 'ENTITYUNRELATED', '21:27': 'ENTITYUNRELATED', '37:38': 'ENTITYUNRELATED', '41:41': 'ENTITY', '44:44': 'ENTITYOTHER'}}	Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora
The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques .	knowledge-based mechanisms	answering agents	usage	{'e1': {'word': 'knowledge-based mechanisms', 'word_index': [(11, 14)], 'id': 'N03-1004.9'}, 'e2': {'word': 'answering agents', 'word_index': [(1, 2)], 'id': 'N03-1004.8'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '11:14': 'ENTITY', '19:20': 'ENTITYUNRELATED'}}	The answering agents adopt fundamentally different strategies , one utilizing primarily knowledge - based mechanisms and the other adopting statistical techniques .
Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered , and a 32.8% improvement according to the average precision metric .	answer resolution algorithm	baseline system	compare	{'e1': {'word': 'answer resolution algorithm', 'word_index': [(6, 8)], 'id': 'N03-1004.14'}, 'e2': {'word': 'baseline system', 'word_index': [(17, 18)], 'id': 'N03-1004.15'}, 'entity_replacement': {'6:8': 'ENTITY', '17:18': 'ENTITYOTHER', '23:25': 'ENTITYUNRELATED', '35:37': 'ENTITYUNRELATED'}}	Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0 % relative improvement over our baseline system in the number of questions correctly answered , and a 32.8 % improvement according to the average precision metric .
We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence .	scoring	speech recognition hypotheses (SRH)	usage	{'e1': {'word': 'scoring', 'word_index': [(8, 8)], 'id': 'N03-1012.4'}, 'e2': {'word': 'speech recognition hypotheses (SRH)', 'word_index': [(10, 15)], 'id': 'N03-1012.5'}, 'entity_replacement': {'8:8': 'ENTITY', '10:15': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	We apply our system to the task of scoring alternative speech recognition hypotheses ( SRH ) in terms of their semantic coherence .
An evaluation of our system against the annotated data shows that, it successfully classifies 73.2% in a German corpus of 2.284 SRHs as either coherent or incoherent (given a baseline of 54.55%).	SRHs	German corpus	part_whole	{'e1': {'word': 'SRHs', 'word_index': [(23, 23)], 'id': 'N03-1012.12'}, 'e2': {'word': 'German corpus', 'word_index': [(19, 20)], 'id': 'N03-1012.11'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '19:20': 'ENTITYOTHER', '23:23': 'ENTITY', '32:32': 'ENTITYUNRELATED'}}	An evaluation of our system against the annotated data shows that , it successfully classifies 73.2 % in a German corpus of 2.284 SRHs as either coherent or incoherent ( given a baseline of 54.55 % ) .
Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models .	phrase-based models	word-based models	compare	{'e1': {'word': 'phrase-based models', 'word_index': [(18, 21)], 'id': 'N03-1017.4'}, 'e2': {'word': 'word-based models', 'word_index': [(23, 26)], 'id': 'N03-1017.5'}, 'entity_replacement': {'18:21': 'ENTITY', '23:26': 'ENTITYOTHER'}}	Within our framework , we carry out a large number of experiments to understand better and explain why phrase - based models outperform word - based models .
Our empirical results, which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations .	word-based alignments	heuristic learning	usage	{'e1': {'word': 'word-based alignments', 'word_index': [(33, 36)], 'id': 'N03-1017.9'}, 'e2': {'word': 'heuristic learning', 'word_index': [(27, 28)], 'id': 'N03-1017.7'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '27:28': 'ENTITYOTHER', '30:31': 'ENTITYUNRELATED', '33:36': 'ENTITY', '38:39': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED'}}	Our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : heuristic learning of phrase translations from word - based alignments and lexical weighting of phrase translations .
Our empirical results, which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations .	lexical weighting	phrase translations	model-feature	{'e1': {'word': 'lexical weighting', 'word_index': [(38, 39)], 'id': 'N03-1017.10'}, 'e2': {'word': 'phrase translations', 'word_index': [(41, 42)], 'id': 'N03-1017.11'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '33:36': 'ENTITYUNRELATED', '38:39': 'ENTITY', '41:42': 'ENTITYOTHER'}}	Our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : heuristic learning of phrase translations from word - based alignments and lexical weighting of phrase translations .
Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance.	phrases	high-accuracy word-level alignment models	part_whole	{'e1': {'word': 'phrases', 'word_index': [(10, 10)], 'id': 'N03-1017.14'}, 'e2': {'word': 'high-accuracy word-level alignment models', 'word_index': [(12, 19)], 'id': 'N03-1017.15'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '10:10': 'ENTITY', '12:19': 'ENTITYOTHER'}}	Surprisingly , learning phrases longer than three words and learning phrases from high - accuracy word - level alignment models does not have a strong impact on performance .
The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks .	model	error correction	usage	{'e1': {'word': 'model', 'word_index': [(1, 1)], 'id': 'N03-1018.6'}, 'e2': {'word': 'error correction', 'word_index': [(7, 8)], 'id': 'N03-1018.7'}, 'entity_replacement': {'1:1': 'ENTITY', '7:8': 'ENTITYOTHER', '14:16': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED'}}	The model is designed for use in error correction , with a focus on post - processing the output of black - box OCR systems in order to make it more useful for NLP tasks .
The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks .	post-processing	output	usage	{'e1': {'word': 'post-processing', 'word_index': [(14, 16)], 'id': 'N03-1018.8'}, 'e2': {'word': 'output', 'word_index': [(18, 18)], 'id': 'N03-1018.9'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '14:16': 'ENTITY', '18:18': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED'}}	The model is designed for use in error correction , with a focus on post - processing the output of black - box OCR systems in order to make it more useful for NLP tasks .
We present an implementation of the model based on finite-state models , demonstrate the model &apos;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .	finite-state models	model	usage	{'e1': {'word': 'finite-state models', 'word_index': [(9, 12)], 'id': 'N03-1018.13'}, 'e2': {'word': 'model', 'word_index': [(6, 6)], 'id': 'N03-1018.12'}, 'entity_replacement': {'6:6': 'ENTITYOTHER', '9:12': 'ENTITY', '16:16': 'ENTITYUNRELATED', '23:27': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '37:38': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED'}}	We present an implementation of the model based on finite - state models , demonstrate the model &apos ;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .
We present an implementation of the model based on finite-state models , demonstrate the model &apos;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .	model	character and word error rate	result	{'e1': {'word': 'model', 'word_index': [(16, 16)], 'id': 'N03-1018.14'}, 'e2': {'word': 'character and word error rate', 'word_index': [(23, 27)], 'id': 'N03-1018.15'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:12': 'ENTITYUNRELATED', '16:16': 'ENTITY', '23:27': 'ENTITYOTHER', '34:35': 'ENTITYUNRELATED', '37:38': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED'}}	We present an implementation of the model based on finite - state models , demonstrate the model &apos ;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .
We present an implementation of the model based on finite-state models , demonstrate the model &apos;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .	automatic extraction	printed text	usage	{'e1': {'word': 'automatic extraction', 'word_index': [(34, 35)], 'id': 'N03-1018.16'}, 'e2': {'word': 'printed text', 'word_index': [(40, 41)], 'id': 'N03-1018.18'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:12': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '23:27': 'ENTITYUNRELATED', '34:35': 'ENTITY', '37:38': 'ENTITYUNRELATED', '40:41': 'ENTITYOTHER'}}	We present an implementation of the model based on finite - state models , demonstrate the model &apos ;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .
 We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation 	ambiguity packing and stochastic disambiguation techniques	Lexical-Functional Grammars (LFG)	usage	{'e1': {'word': 'ambiguity packing and stochastic disambiguation techniques', 'word_index': [(5, 10)], 'id': 'N03-1026.1'}, 'e2': {'word': 'Lexical-Functional Grammars (LFG)', 'word_index': [(12, 18)], 'id': 'N03-1026.2'}, 'entity_replacement': {'5:10': 'ENTITY', '12:18': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED'}}	We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical - Functional Grammars ( LFG ) to the domain of sentence condensation
Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	linguistic parser/generator	LFG	usage	{'e1': {'word': 'linguistic parser/generator', 'word_index': [(4, 7)], 'id': 'N03-1026.4'}, 'e2': {'word': 'LFG', 'word_index': [(9, 9)], 'id': 'N03-1026.5'}, 'entity_replacement': {'4:7': 'ENTITY', '9:9': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '25:28': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED'}}	Our system incorporates a linguistic parser / generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum - entropy model for stochastic output selection .
Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	transfer component	parse reduction	usage	{'e1': {'word': 'transfer component', 'word_index': [(12, 13)], 'id': 'N03-1026.6'}, 'e2': {'word': 'parse reduction', 'word_index': [(15, 16)], 'id': 'N03-1026.7'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '12:13': 'ENTITY', '15:16': 'ENTITYOTHER', '19:21': 'ENTITYUNRELATED', '25:28': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED'}}	Our system incorporates a linguistic parser / generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum - entropy model for stochastic output selection .
Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	stochastic output selection	maximum-entropy model	usage	{'e1': {'word': 'stochastic output selection', 'word_index': [(30, 32)], 'id': 'N03-1026.10'}, 'e2': {'word': 'maximum-entropy model', 'word_index': [(25, 28)], 'id': 'N03-1026.9'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '25:28': 'ENTITYOTHER', '30:32': 'ENTITY'}}	Our system incorporates a linguistic parser / generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum - entropy model for stochastic output selection .
An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings .	experimental evaluation	summarization	topic	{'e1': {'word': 'experimental evaluation', 'word_index': [(1, 2)], 'id': 'N03-1026.14'}, 'e2': {'word': 'summarization', 'word_index': [(4, 4)], 'id': 'N03-1026.15'}, 'entity_replacement': {'1:2': 'ENTITY', '4:4': 'ENTITYOTHER', '12:16': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED'}}	An experimental evaluation of summarization quality shows a close correlation between the automatic parse - based evaluation and a manual evaluation of generated strings .
Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator .	grammaticality	system output	model-feature	{'e1': {'word': 'grammaticality', 'word_index': [(18, 18)], 'id': 'N03-1026.20'}, 'e2': {'word': 'system output', 'word_index': [(21, 22)], 'id': 'N03-1026.21'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '18:18': 'ENTITY', '21:22': 'ENTITYOTHER', '29:34': 'ENTITYUNRELATED'}}	Overall summarization quality of the proposed system is state - of - the - art , with guaranteed grammaticality of the system output due to the use of a constraint - based parser / generator .
 We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation , (ii) broad use of lexical features , including jointly conditioning on multiple consecutive words , (iii) effective use of priors in conditional loglinear models , and (iv) fine-grained modeling of unknown word features 	priors	conditional loglinear models	usage	{'e1': {'word': 'priors', 'word_index': [(56, 56)], 'id': 'N03-1033.6'}, 'e2': {'word': 'conditional loglinear models', 'word_index': [(58, 60)], 'id': 'N03-1033.7'}, 'entity_replacement': {'4:8': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '29:31': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED', '43:48': 'ENTITYUNRELATED', '56:56': 'ENTITY', '58:60': 'ENTITYOTHER', '71:73': 'ENTITYUNRELATED'}}	We present a new part -of - speech tagger that demonstrates the following ideas : ( i ) explicit use of both preceding and following tag contexts via a dependency network representation , ( ii ) broad use of lexical features , including jointly conditioning on multiple consecutive words , ( iii ) effective use of priors in conditional loglinear models , and ( iv ) fine - grained modeling of unknown word features
Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ , an error reduction of 4.4% on the best previous single automatically learned tagging result.	tagger	accuracy	result	{'e1': {'word': 'tagger', 'word_index': [(7, 7)], 'id': 'N03-1033.9'}, 'e2': {'word': 'accuracy', 'word_index': [(12, 12)], 'id': 'N03-1033.10'}, 'entity_replacement': {'7:7': 'ENTITY', '12:12': 'ENTITYOTHER', '15:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	Using these ideas together , the resulting tagger gives a 97.24 % accuracy on the Penn Treebank WSJ , an error reduction of 4.4 % on the best previous single automatically learned tagging result .
 Sources of training data suitable for language modeling of conversational speech are limited	training data	language modeling	usage	{'e1': {'word': 'training data', 'word_index': [(2, 3)], 'id': 'N03-2003.1'}, 'e2': {'word': 'language modeling', 'word_index': [(6, 7)], 'id': 'N03-2003.2'}, 'entity_replacement': {'2:3': 'ENTITY', '6:7': 'ENTITYOTHER', '9:10': 'ENTITYUNRELATED'}}	Sources of training data suitable for language modeling of conversational speech are limited
In this paper, we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams .	text	web	part_whole	{'e1': {'word': 'text', 'word_index': [(13, 13)], 'id': 'N03-2003.5'}, 'e2': {'word': 'web', 'word_index': [(16, 16)], 'id': 'N03-2003.6'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '13:13': 'ENTITY', '16:16': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED', '48:50': 'ENTITYUNRELATED', '52:52': 'ENTITYUNRELATED'}}	In this paper , we show how training data can be supplemented with text from the web filtered to match the style and / or topic of the target recognition task , but also that it is possible to get bigger performance gains from the data by using class -dependent interpolation of N-grams .
 In order to boost the translation quality of EBMT based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and, in addition, the language model of an in-domain monolingual corpus 	EBMT	translation quality	result	{'e1': {'word': 'EBMT', 'word_index': [(8, 8)], 'id': 'N03-2006.2'}, 'e2': {'word': 'translation quality', 'word_index': [(5, 6)], 'id': 'N03-2006.1'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '8:8': 'ENTITY', '15:16': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED'}}	In order to boost the translation quality of EBMT based on a small - sized bilingual corpus , we use an out - of - domain bilingual corpus and , in addition , the language model of an in - domain monolingual corpus
 We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton 	hubs	automaton	part_whole	{'e1': {'word': 'hubs', 'word_index': [(11, 11)], 'id': 'N03-2015.3'}, 'e2': {'word': 'automaton', 'word_index': [(14, 14)], 'id': 'N03-2015.4'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:11': 'ENTITY', '14:14': 'ENTITYOTHER'}}	We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton
 We present a syntax-based constraint for word alignment , known as the cohesion constraint 	syntax-based constraint	word alignment	usage	{'e1': {'word': 'syntax-based constraint', 'word_index': [(3, 6)], 'id': 'N03-2017.1'}, 'e2': {'word': 'word alignment', 'word_index': [(8, 9)], 'id': 'N03-2017.2'}, 'entity_replacement': {'3:6': 'ENTITY', '8:9': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED'}}	We present a syntax - based constraint for word alignment , known as the cohesion constraint
 A novel bootstrapping approach to Named Entity (NE) tagging using concept-based seeds and successive learners is presented	bootstrapping approach	Named Entity (NE) tagging	usage	{'e1': {'word': 'bootstrapping approach', 'word_index': [(2, 3)], 'id': 'N03-2025.1'}, 'e2': {'word': 'Named Entity (NE) tagging', 'word_index': [(5, 10)], 'id': 'N03-2025.2'}, 'entity_replacement': {'2:3': 'ENTITY', '5:10': 'ENTITYOTHER', '12:15': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	A novel bootstrapping approach to Named Entity ( NE ) tagging using concept - based seeds and successive learners is presented
This approach only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE , e.g. he/she/man/woman for PERSON NE .	seeds	concept	model-feature	{'e1': {'word': 'seeds', 'word_index': [(10, 10)], 'id': 'N03-2025.7'}, 'e2': {'word': 'concept', 'word_index': [(15, 15)], 'id': 'N03-2025.8'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '10:10': 'ENTITY', '15:15': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	This approach only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE , e.g. he / she / man / woman for PERSON NE .
First, decision list is used to learn the parsing-based NE rules .	parsing-based NE rules	decision list	usage	{'e1': {'word': 'parsing-based NE rules', 'word_index': [(9, 13)], 'id': 'N03-2025.14'}, 'e2': {'word': 'decision list', 'word_index': [(2, 3)], 'id': 'N03-2025.13'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '9:13': 'ENTITY'}}	First , decision list is used to learn the parsing - based NE rules .
Then, a Hidden Markov Model is trained on a corpus automatically tagged by the first learner .	Hidden Markov Model	corpus	usage	{'e1': {'word': 'Hidden Markov Model', 'word_index': [(3, 5)], 'id': 'N03-2025.15'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'N03-2025.16'}, 'entity_replacement': {'3:5': 'ENTITY', '10:10': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	Then , a Hidden Markov Model is trained on a corpus automatically tagged by the first learner .
 In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models 	phrase-based unigram model	statistical machine translation	usage	{'e1': {'word': 'phrase-based unigram model', 'word_index': [(7, 11)], 'id': 'N03-2036.1'}, 'e2': {'word': 'statistical machine translation', 'word_index': [(13, 15)], 'id': 'N03-2036.2'}, 'entity_replacement': {'7:11': 'ENTITY', '13:15': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '27:30': 'ENTITYUNRELATED'}}	In this paper , we describe a phrase - based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase - based models
During training , the blocks are learned from source interval projections using an underlying word alignment .	blocks	source interval projections	part_whole	{'e1': {'word': 'blocks', 'word_index': [(4, 4)], 'id': 'N03-2036.12'}, 'e2': {'word': 'source interval projections', 'word_index': [(8, 10)], 'id': 'N03-2036.13'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:4': 'ENTITY', '8:10': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED'}}	During training , the blocks are learned from source interval projections using an underlying word alignment .
We show experimental results on block selection criteria based on unigram counts and phrase length.	unigram	block selection criteria	usage	{'e1': {'word': 'unigram', 'word_index': [(10, 10)], 'id': 'N03-2036.16'}, 'e2': {'word': 'block selection criteria', 'word_index': [(5, 7)], 'id': 'N03-2036.15'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '10:10': 'ENTITY', '13:13': 'ENTITYUNRELATED'}}	We show experimental results on block selection criteria based on unigram counts and phrase length .
 In this paper, we propose a novel Cooperative Model for natural language understanding in a dialogue system 	Cooperative Model	natural language understanding	usage	{'e1': {'word': 'Cooperative Model', 'word_index': [(8, 9)], 'id': 'N03-3010.1'}, 'e2': {'word': 'natural language understanding', 'word_index': [(11, 13)], 'id': 'N03-3010.2'}, 'entity_replacement': {'8:9': 'ENTITY', '11:13': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED'}}	In this paper , we propose a novel Cooperative Model for natural language understanding in a dialogue system
 The JAVELIN system integrates a flexible, planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text 	planning-based architecture	JAVELIN system	part_whole	{'e1': {'word': 'planning-based architecture', 'word_index': [(7, 10)], 'id': 'N03-4010.2'}, 'e2': {'word': 'JAVELIN system', 'word_index': [(1, 2)], 'id': 'N03-4010.1'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '7:10': 'ENTITY', '15:17': 'ENTITYUNRELATED', '21:24': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	The JAVELIN system integrates a flexible , planning - based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text
The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus .	JAVELIN	questions	usage	{'e1': {'word': 'JAVELIN', 'word_index': [(6, 6)], 'id': 'N03-4010.6'}, 'e2': {'word': 'questions', 'word_index': [(8, 8)], 'id': 'N03-4010.7'}, 'entity_replacement': {'6:6': 'ENTITY', '8:8': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus .
The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus .	answer candidates	text corpus	part_whole	{'e1': {'word': 'answer candidates', 'word_index': [(14, 15)], 'id': 'N03-4010.8'}, 'e2': {'word': 'text corpus', 'word_index': [(19, 20)], 'id': 'N03-4010.9'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '14:15': 'ENTITY', '19:20': 'ENTITYOTHER'}}	The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus .
The operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session .	data objects	repository	part_whole	{'e1': {'word': 'data objects', 'word_index': [(15, 16)], 'id': 'N03-4010.11'}, 'e2': {'word': 'repository', 'word_index': [(13, 13)], 'id': 'N03-4010.10'}, 'entity_replacement': {'13:13': 'ENTITYOTHER', '15:16': 'ENTITY', '23:25': 'ENTITYUNRELATED'}}	The operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session .
 In this paper we present a novel, customizable : IE paradigm that takes advantage of predicate-argument structures 	predicate-argument structures	IE paradigm	usage	{'e1': {'word': 'predicate-argument structures', 'word_index': [(16, 19)], 'id': 'P03-1002.2'}, 'e2': {'word': 'IE paradigm', 'word_index': [(10, 11)], 'id': 'P03-1002.1'}, 'entity_replacement': {'10:11': 'ENTITYOTHER', '16:19': 'ENTITY'}}	In this paper we present a novel , customizable : IE paradigm that takes advantage of predicate - argument structures
 This paper proposes the Hierarchical Directed Acyclic Graph (HDAG) Kernel for structured natural language data 	Hierarchical Directed Acyclic Graph (HDAG) Kernel	structured natural language data	usage	{'e1': {'word': 'Hierarchical Directed Acyclic Graph (HDAG) Kernel', 'word_index': [(4, 11)], 'id': 'P03-1005.1'}, 'e2': {'word': 'structured natural language data', 'word_index': [(13, 16)], 'id': 'P03-1005.2'}, 'entity_replacement': {'4:11': 'ENTITY', '13:16': 'ENTITYOTHER'}}	This paper proposes the Hierarchical Directed Acyclic Graph ( HDAG ) Kernel for structured natural language data
The HDAG Kernel directly accepts several levels of both chunks and their relations , and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs .	attribute sequences	HDAGs	model-feature	{'e1': {'word': 'attribute sequences', 'word_index': [(26, 27)], 'id': 'P03-1005.7'}, 'e2': {'word': 'HDAGs', 'word_index': [(30, 30)], 'id': 'P03-1005.8'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '26:27': 'ENTITY', '30:30': 'ENTITYOTHER'}}	The HDAG Kernel directly accepts several levels of both chunks and their relations , and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs .
The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods .	HDAG Kernel	kernel functions	compare	{'e1': {'word': 'HDAG Kernel', 'word_index': [(8, 9)], 'id': 'P03-1005.13'}, 'e2': {'word': 'kernel functions', 'word_index': [(14, 15)], 'id': 'P03-1005.14'}, 'entity_replacement': {'8:9': 'ENTITY', '14:15': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED'}}	The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods .
 Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data 	semantic verb classes	corpus data	model-feature	{'e1': {'word': 'semantic verb classes', 'word_index': [(10, 12)], 'id': 'P03-1009.2'}, 'e2': {'word': 'corpus data', 'word_index': [(15, 16)], 'id': 'P03-1009.3'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '10:12': 'ENTITY', '15:16': 'ENTITYOTHER'}}	Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data
We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods.	Information Bottleneck	subcategorization frame (SCF)	usage	{'e1': {'word': 'Information Bottleneck', 'word_index': [(16, 17)], 'id': 'P03-1009.5'}, 'e2': {'word': 'subcategorization frame (SCF)', 'word_index': [(8, 12)], 'id': 'P03-1009.4'}, 'entity_replacement': {'8:12': 'ENTITYOTHER', '16:17': 'ENTITY', '19:20': 'ENTITYUNRELATED'}}	We describe a new approach which involves clustering subcategorization frame ( SCF ) distributions using the Information Bottleneck and nearest neighbour methods .
A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters , offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data .	polysemy	clusters	model-feature	{'e1': {'word': 'polysemy', 'word_index': [(12, 12)], 'id': 'P03-1009.9'}, 'e2': {'word': 'clusters', 'word_index': [(15, 15)], 'id': 'P03-1009.10'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '12:12': 'ENTITY', '15:15': 'ENTITYOTHER', '28:29': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED'}}	A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters , offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data .
 We apply a decision tree based approach to pronoun resolution in spoken dialogue 	decision tree based approach	pronoun resolution	usage	{'e1': {'word': 'decision tree based approach', 'word_index': [(3, 6)], 'id': 'P03-1022.1'}, 'e2': {'word': 'pronoun resolution', 'word_index': [(8, 9)], 'id': 'P03-1022.2'}, 'entity_replacement': {'3:6': 'ENTITY', '8:9': 'ENTITYOTHER', '11:12': 'ENTITYUNRELATED'}}	We apply a decision tree based approach to pronoun resolution in spoken dialogue
Our system deals with pronouns with NP- and non-NP-antecedents .	NP- and non-NP-antecedents	pronouns	model-feature	{'e1': {'word': 'NP- and non-NP-antecedents', 'word_index': [(6, 10)], 'id': 'P03-1022.5'}, 'e2': {'word': 'pronouns', 'word_index': [(4, 4)], 'id': 'P03-1022.4'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '6:10': 'ENTITY'}}	Our system deals with pronouns with NP - and non-NP- antecedents .
We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features .	pronoun resolution	spoken dialogue	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(8, 9)], 'id': 'P03-1022.7'}, 'e2': {'word': 'spoken dialogue', 'word_index': [(11, 12)], 'id': 'P03-1022.8'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:9': 'ENTITY', '11:12': 'ENTITYOTHER', '18:18': 'ENTITYUNRELATED'}}	We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features .
 Link detection has been regarded as a core technology for the Topic Detection and Tracking tasks of new event detection 	Topic Detection and Tracking tasks	new event detection	part_whole	{'e1': {'word': 'Topic Detection and Tracking tasks', 'word_index': [(11, 15)], 'id': 'P03-1030.2'}, 'e2': {'word': 'new event detection', 'word_index': [(17, 19)], 'id': 'P03-1030.3'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '11:15': 'ENTITY', '17:19': 'ENTITYOTHER'}}	Link detection has been regarded as a core technology for the Topic Detection and Tracking tasks of new event detection
 This paper concerns the discourse understanding process in spoken dialogue systems 	discourse understanding process	spoken dialogue systems	usage	{'e1': {'word': 'discourse understanding process', 'word_index': [(4, 6)], 'id': 'P03-1031.1'}, 'e2': {'word': 'spoken dialogue systems', 'word_index': [(8, 10)], 'id': 'P03-1031.2'}, 'entity_replacement': {'4:6': 'ENTITY', '8:10': 'ENTITYOTHER'}}	This paper concerns the discourse understanding process in spoken dialogue systems
This process enables the system to understand user utterances based on the context of a dialogue .	context	dialogue	model-feature	{'e1': {'word': 'context', 'word_index': [(12, 12)], 'id': 'P03-1031.4'}, 'e2': {'word': 'dialogue', 'word_index': [(15, 15)], 'id': 'P03-1031.5'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '12:12': 'ENTITY', '15:15': 'ENTITYOTHER'}}	This process enables the system to understand user utterances based on the context of a dialogue .
Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance .	candidates	understanding	usage	{'e1': {'word': 'candidates', 'word_index': [(2, 2)], 'id': 'P03-1031.6'}, 'e2': {'word': 'understanding', 'word_index': [(5, 5)], 'id': 'P03-1031.7'}, 'entity_replacement': {'2:2': 'ENTITY', '5:5': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED'}}	Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance .
Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance .	ambiguity	speech understanding	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(17, 17)], 'id': 'P03-1031.9'}, 'e2': {'word': 'speech understanding', 'word_index': [(19, 20)], 'id': 'P03-1031.10'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '17:17': 'ENTITY', '19:20': 'ENTITYOTHER', '31:31': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED'}}	Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance .
By holding multiple candidates for understanding results and resolving the ambiguity as the dialogue progresses, the discourse understanding accuracy can be improved.	candidates	understanding	usage	{'e1': {'word': 'candidates', 'word_index': [(3, 3)], 'id': 'P03-1031.13'}, 'e2': {'word': 'understanding', 'word_index': [(5, 5)], 'id': 'P03-1031.14'}, 'entity_replacement': {'3:3': 'ENTITY', '5:5': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED'}}	By holding multiple candidates for understanding results and resolving the ambiguity as the dialogue progresses , the discourse understanding accuracy can be improved .
This paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpora .	statistical information	dialogue corpora	model-feature	{'e1': {'word': 'statistical information', 'word_index': [(11, 12)], 'id': 'P03-1031.19'}, 'e2': {'word': 'dialogue corpora', 'word_index': [(15, 16)], 'id': 'P03-1031.20'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '11:12': 'ENTITY', '15:16': 'ENTITYOTHER'}}	This paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpora .
Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple candidates for understanding results is effective.	candidates	understanding	usage	{'e1': {'word': 'candidates', 'word_index': [(18, 18)], 'id': 'P03-1031.23'}, 'e2': {'word': 'understanding', 'word_index': [(20, 20)], 'id': 'P03-1031.24'}, 'entity_replacement': {'18:18': 'ENTITY', '20:20': 'ENTITYOTHER'}}	Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple candidates for understanding results is effective .
 We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems 	user modeling	cooperative responses	usage	{'e1': {'word': 'user modeling', 'word_index': [(3, 4)], 'id': 'P03-1033.1'}, 'e2': {'word': 'cooperative responses', 'word_index': [(9, 10)], 'id': 'P03-1033.2'}, 'entity_replacement': {'3:4': 'ENTITY', '9:10': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '15:17': 'ENTITYUNRELATED'}}	We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems
Moreover, the models are automatically derived by decision tree learning using real dialogue data collected by the system.	decision tree learning	models	model-feature	{'e1': {'word': 'decision tree learning', 'word_index': [(8, 10)], 'id': 'P03-1033.15'}, 'e2': {'word': 'models', 'word_index': [(3, 3)], 'id': 'P03-1033.14'}, 'entity_replacement': {'3:3': 'ENTITYOTHER', '8:10': 'ENTITY', '13:14': 'ENTITYUNRELATED'}}	Moreover , the models are automatically derived by decision tree learning using real dialogue data collected by the system .
Dialogue strategies based on the user modeling are implemented in Kyoto city bus information system that has been developed at our laboratory.	user modeling	Dialogue strategies	usage	{'e1': {'word': 'user modeling', 'word_index': [(5, 6)], 'id': 'P03-1033.19'}, 'e2': {'word': 'Dialogue strategies', 'word_index': [(0, 1)], 'id': 'P03-1033.18'}, 'entity_replacement': {'0:1': 'ENTITYOTHER', '5:6': 'ENTITY', '10:14': 'ENTITYUNRELATED'}}	Dialogue strategies based on the user modeling are implemented in Kyoto city bus information system that has been developed at our laboratory .
 This paper presents an unsupervised learning approach to building a non-English (Arabic) stemmer 	unsupervised learning approach	non-English (Arabic) stemmer	usage	{'e1': {'word': 'unsupervised learning approach', 'word_index': [(4, 6)], 'id': 'P03-1050.1'}, 'e2': {'word': 'non-English (Arabic) stemmer', 'word_index': [(10, 15)], 'id': 'P03-1050.2'}, 'entity_replacement': {'4:6': 'ENTITY', '10:15': 'ENTITYOTHER'}}	This paper presents an unsupervised learning approach to building a non- English ( Arabic ) stemmer
The stemming model is based on statistical machine translation and it uses an English stemmer and a small (10K sentences) parallel corpus as its sole training resources .	statistical machine translation	stemming model	usage	{'e1': {'word': 'statistical machine translation', 'word_index': [(6, 8)], 'id': 'P03-1050.4'}, 'e2': {'word': 'stemming model', 'word_index': [(1, 2)], 'id': 'P03-1050.3'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '6:8': 'ENTITY', '13:14': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED'}}	The stemming model is based on statistical machine translation and it uses an English stemmer and a small ( 10 K sentences ) parallel corpus as its sole training resources .
Examples and results will be given for Arabic , but the approach is applicable to any language that needs affix removal .	affix removal	language	model-feature	{'e1': {'word': 'affix removal', 'word_index': [(19, 20)], 'id': 'P03-1050.16'}, 'e2': {'word': 'language', 'word_index': [(16, 16)], 'id': 'P03-1050.15'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '16:16': 'ENTITYOTHER', '19:20': 'ENTITY'}}	Examples and results will be given for Arabic , but the approach is applicable to any language that needs affix removal .
Our resource-frugal approach results in 87.5% agreement with a state of the art, proprietary Arabic stemmer built using rules , affix lists , and human annotated text , in addition to an unsupervised component .	resource-frugal approach	agreement	result	{'e1': {'word': 'resource-frugal approach', 'word_index': [(1, 4)], 'id': 'P03-1050.17'}, 'e2': {'word': 'agreement', 'word_index': [(9, 9)], 'id': 'P03-1050.18'}, 'entity_replacement': {'1:4': 'ENTITY', '9:9': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:30': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED'}}	Our resource - frugal approach results in 87.5 % agreement with a state of the art , proprietary Arabic stemmer built using rules , affix lists , and human annotated text , in addition to an unsupervised component .
Task-based evaluation using Arabic information retrieval indicates an improvement of 22-38% in average precision over unstemmed text , and 96% of the performance of the proprietary stemmer above.	Arabic information retrieval	Task-based evaluation	usage	{'e1': {'word': 'Arabic information retrieval', 'word_index': [(5, 7)], 'id': 'P03-1050.25'}, 'e2': {'word': 'Task-based evaluation', 'word_index': [(0, 3)], 'id': 'P03-1050.24'}, 'entity_replacement': {'0:3': 'ENTITYOTHER', '5:7': 'ENTITY', '16:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	Task - based evaluation using Arabic information retrieval indicates an improvement of 22- 38 % in average precision over unstemmed text , and 96 % of the performance of the proprietary stemmer above .
Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus .	manually segmented Arabic corpus	unsupervised algorithm	usage	{'e1': {'word': 'manually segmented Arabic corpus', 'word_index': [(7, 10)], 'id': 'P03-1051.8'}, 'e2': {'word': 'unsupervised algorithm', 'word_index': [(17, 18)], 'id': 'P03-1051.9'}, 'entity_replacement': {'7:10': 'ENTITY', '17:18': 'ENTITYOTHER', '22:24': 'ENTITYUNRELATED', '28:30': 'ENTITYUNRELATED'}}	Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus .
Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus .	unsegmented Arabic corpus	Arabic word segmenter	usage	{'e1': {'word': 'unsegmented Arabic corpus', 'word_index': [(28, 30)], 'id': 'P03-1051.11'}, 'e2': {'word': 'Arabic word segmenter', 'word_index': [(22, 24)], 'id': 'P03-1051.10'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '22:24': 'ENTITYOTHER', '28:30': 'ENTITY'}}	Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus .
The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input .	morpheme sequence	input	model-feature	{'e1': {'word': 'morpheme sequence', 'word_index': [(12, 13)], 'id': 'P03-1051.13'}, 'e2': {'word': 'input', 'word_index': [(17, 17)], 'id': 'P03-1051.14'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '12:13': 'ENTITY', '17:17': 'ENTITYOTHER'}}	The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input .
The language model is initially estimated from a small manually segmented corpus of about 110,000 words .	language model	manually segmented corpus	model-feature	{'e1': {'word': 'language model', 'word_index': [(1, 2)], 'id': 'P03-1051.15'}, 'e2': {'word': 'manually segmented corpus', 'word_index': [(9, 11)], 'id': 'P03-1051.16'}, 'entity_replacement': {'1:2': 'ENTITY', '9:11': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED'}}	The language model is initially estimated from a small manually segmented corpus of about 110,000 words .
To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus , and re-estimate the model parameters with the expanded vocabulary and training corpus .	stems	unsegmented corpus	part_whole	{'e1': {'word': 'stems', 'word_index': [(15, 15)], 'id': 'P03-1051.21'}, 'e2': {'word': 'unsegmented corpus', 'word_index': [(21, 22)], 'id': 'P03-1051.23'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '15:15': 'ENTITY', '20:20': 'ENTITYUNRELATED', '21:22': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED'}}	To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus , and re-estimate the model parameters with the expanded vocabulary and training corpus .
To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus , and re-estimate the model parameters with the expanded vocabulary and training corpus .	model parameters	vocabulary	usage	{'e1': {'word': 'model parameters', 'word_index': [(27, 28)], 'id': 'P03-1051.24'}, 'e2': {'word': 'vocabulary', 'word_index': [(32, 32)], 'id': 'P03-1051.25'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '27:28': 'ENTITY', '32:32': 'ENTITYOTHER', '34:35': 'ENTITYUNRELATED'}}	To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus , and re-estimate the model parameters with the expanded vocabulary and training corpus .
The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens .	Arabic word segmentation system	exact match accuracy	result	{'e1': {'word': 'Arabic word segmentation system', 'word_index': [(2, 5)], 'id': 'P03-1051.27'}, 'e2': {'word': 'exact match accuracy', 'word_index': [(10, 12)], 'id': 'P03-1051.28'}, 'entity_replacement': {'2:5': 'ENTITY', '10:12': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens .
The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens .	word tokens	test corpus	part_whole	{'e1': {'word': 'word tokens', 'word_index': [(19, 20)], 'id': 'P03-1051.30'}, 'e2': {'word': 'test corpus', 'word_index': [(15, 16)], 'id': 'P03-1051.29'}, 'entity_replacement': {'2:5': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '15:16': 'ENTITYOTHER', '19:20': 'ENTITY'}}	The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens .
We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest.	manually segmented corpus	language	model-feature	{'e1': {'word': 'manually segmented corpus', 'word_index': [(31, 33)], 'id': 'P03-1051.32'}, 'e2': {'word': 'language', 'word_index': [(36, 36)], 'id': 'P03-1051.33'}, 'entity_replacement': {'21:23': 'ENTITYUNRELATED', '31:33': 'ENTITY', '36:36': 'ENTITYOTHER'}}	We believe this is a state - of - the - art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest .
A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning .	manually sense-tagged data	word sense disambiguation (WSD)	usage	{'e1': {'word': 'manually sense-tagged data', 'word_index': [(14, 18)], 'id': 'P03-1058.2'}, 'e2': {'word': 'word sense disambiguation (WSD)', 'word_index': [(4, 9)], 'id': 'P03-1058.1'}, 'entity_replacement': {'4:9': 'ENTITYOTHER', '14:18': 'ENTITY', '21:22': 'ENTITYUNRELATED'}}	A central problem of word sense disambiguation ( WSD ) is the lack of manually sense - tagged data required for supervised learning .
In this paper, we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora , which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task .	sense-tagged training data	English-Chinese parallel corpora	model-feature	{'e1': {'word': 'sense-tagged training data', 'word_index': [(11, 13)], 'id': 'P03-1058.4'}, 'e2': {'word': 'English-Chinese parallel corpora', 'word_index': [(15, 19)], 'id': 'P03-1058.5'}, 'entity_replacement': {'11:13': 'ENTITY', '15:19': 'ENTITYOTHER', '28:28': 'ENTITYUNRELATED', '31:36': 'ENTITYUNRELATED'}}	In this paper , we evaluate an approach to automatically acquire sense-tagged training data from English - Chinese parallel corpora , which are then used for disambiguating the nouns in the SENSEVAL -2 English lexical sample task .
Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs .	domain dependence	WSD programs	model-feature	{'e1': {'word': 'domain dependence', 'word_index': [(10, 11)], 'id': 'P03-1058.13'}, 'e2': {'word': 'WSD programs', 'word_index': [(14, 15)], 'id': 'P03-1058.14'}, 'entity_replacement': {'10:11': 'ENTITY', '14:15': 'ENTITYOTHER'}}	Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs .
 We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic information , e.g. the construction of domain-independent lexica 	semantically annotated corpus	acquisition of word-semantic information	usage	{'e1': {'word': 'semantically annotated corpus', 'word_index': [(9, 11)], 'id': 'P03-1068.1'}, 'e2': {'word': 'acquisition of word-semantic information', 'word_index': [(21, 26)], 'id': 'P03-1068.2'}, 'entity_replacement': {'9:11': 'ENTITY', '21:26': 'ENTITYOTHER', '32:35': 'ENTITYUNRELATED'}}	We describe the ongoing construction of a large , semantically annotated corpus resource as reliable basis for the large - scale acquisition of word - semantic information , e.g. the construction of domain - independent lexica
On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation .	ambiguity	semantic annotation	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(11, 11)], 'id': 'P03-1068.9'}, 'e2': {'word': 'semantic annotation', 'word_index': [(13, 14)], 'id': 'P03-1068.10'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '11:11': 'ENTITY', '13:14': 'ENTITYOTHER'}}	On this basis , we discuss the problems of vagueness and ambiguity in semantic annotation .
We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task .	attentional focus	direction-giving task	model-feature	{'e1': {'word': 'attentional focus', 'word_index': [(8, 9)], 'id': 'P03-1070.9'}, 'e2': {'word': 'direction-giving task', 'word_index': [(15, 18)], 'id': 'P03-1070.10'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '8:9': 'ENTITY', '15:18': 'ENTITYOTHER'}}	We analyzed eye gaze , head nods and attentional focus in the context of a direction - giving task .
Based on these results, we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state .	verbal and nonverbal grounding acts	ECA	usage	{'e1': {'word': 'verbal and nonverbal grounding acts', 'word_index': [(11, 15)], 'id': 'P03-1070.15'}, 'e2': {'word': 'ECA', 'word_index': [(8, 8)], 'id': 'P03-1070.14'}, 'entity_replacement': {'8:8': 'ENTITYOTHER', '11:15': 'ENTITY', '18:19': 'ENTITYUNRELATED'}}	Based on these results , we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state .
We demonstrate that an approximation of HPSG produces a more effective CFG filter than that of LTAG .	HPSG	LTAG	compare	{'e1': {'word': 'HPSG', 'word_index': [(6, 6)], 'id': 'P03-2036.4'}, 'e2': {'word': 'LTAG', 'word_index': [(16, 16)], 'id': 'P03-2036.6'}, 'entity_replacement': {'6:6': 'ENTITY', '11:12': 'ENTITYUNRELATED', '16:16': 'ENTITYOTHER'}}	We demonstrate that an approximation of HPSG produces a more effective CFG filter than that of LTAG .
We report experiments conducted on a multilingual corpus to estimate the number of analogies among the sentences that it contains.	analogies	sentences	model-feature	{'e1': {'word': 'analogies', 'word_index': [(13, 13)], 'id': 'C04-1106.5'}, 'e2': {'word': 'sentences', 'word_index': [(16, 16)], 'id': 'C04-1106.6'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '13:13': 'ENTITY', '16:16': 'ENTITYOTHER'}}	We report experiments conducted on a multilingual corpus to estimate the number of analogies among the sentences that it contains .
 CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e.g., thesis statements )	CriterionSM Online Essay Evaluation Service	writing	usage	{'e1': {'word': 'CriterionSM Online Essay Evaluation Service', 'word_index': [(0, 4)], 'id': 'N04-1024.1'}, 'e2': {'word': 'writing', 'word_index': [(13, 13)], 'id': 'N04-1024.3'}, 'entity_replacement': {'0:4': 'ENTITY', '10:10': 'ENTITYUNRELATED', '13:13': 'ENTITYOTHER', '15:19': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED'}}	CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay - based discourse elements ( e.g. , thesis statements )
We describe a new system that enhances Criterion &apos;s capability, by evaluating multiple aspects of coherence in essays .	coherence	essays	model-feature	{'e1': {'word': 'coherence', 'word_index': [(17, 17)], 'id': 'N04-1024.7'}, 'e2': {'word': 'essays', 'word_index': [(19, 19)], 'id': 'N04-1024.8'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '17:17': 'ENTITY', '19:19': 'ENTITYOTHER'}}	We describe a new system that enhances Criterion &apos ;s capability , by evaluating multiple aspects of coherence in essays .
This system identifies features of sentences based on semantic similarity measures and discourse structure .	features	sentences	model-feature	{'e1': {'word': 'features', 'word_index': [(3, 3)], 'id': 'N04-1024.9'}, 'e2': {'word': 'sentences', 'word_index': [(5, 5)], 'id': 'N04-1024.10'}, 'entity_replacement': {'3:3': 'ENTITY', '5:5': 'ENTITYOTHER', '8:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED'}}	This system identifies features of sentences based on semantic similarity measures and discourse structure .
A support vector machine uses these features to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements .	features	support vector machine	usage	{'e1': {'word': 'features', 'word_index': [(6, 6)], 'id': 'N04-1024.14'}, 'e2': {'word': 'support vector machine', 'word_index': [(1, 3)], 'id': 'N04-1024.13'}, 'entity_replacement': {'1:3': 'ENTITYOTHER', '6:6': 'ENTITY', '9:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED'}}	A support vector machine uses these features to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements .
Intra-sentential quality is evaluated with rule-based heuristics .	rule-based heuristics	Intra-sentential quality	topic	{'e1': {'word': 'rule-based heuristics', 'word_index': [(5, 8)], 'id': 'N04-1024.19'}, 'e2': {'word': 'Intra-sentential quality', 'word_index': [(0, 1)], 'id': 'N04-1024.18'}, 'entity_replacement': {'0:1': 'ENTITYOTHER', '5:8': 'ENTITY'}}	Intra-sentential quality is evaluated with rule - based heuristics .
 In this paper, we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries 	information redundancy	multilingual input	model-feature	{'e1': {'word': 'information redundancy', 'word_index': [(7, 8)], 'id': 'H05-1005.1'}, 'e2': {'word': 'multilingual input', 'word_index': [(10, 11)], 'id': 'H05-1005.2'}, 'entity_replacement': {'7:8': 'ENTITY', '10:11': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	In this paper , we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries
We consider the case of multi-document summarization , where the input documents are in Arabic , and the output summary is in English .	Arabic	documents	model-feature	{'e1': {'word': 'Arabic', 'word_index': [(14, 14)], 'id': 'H05-1005.7'}, 'e2': {'word': 'documents', 'word_index': [(11, 11)], 'id': 'H05-1005.6'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '11:11': 'ENTITYOTHER', '14:14': 'ENTITY', '19:19': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED'}}	We consider the case of multi-document summarization , where the input documents are in Arabic , and the output summary is in English .
We consider the case of multi-document summarization , where the input documents are in Arabic , and the output summary is in English .	English	summary	model-feature	{'e1': {'word': 'English', 'word_index': [(22, 22)], 'id': 'H05-1005.9'}, 'e2': {'word': 'summary', 'word_index': [(19, 19)], 'id': 'H05-1005.8'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '19:19': 'ENTITYOTHER', '22:22': 'ENTITY'}}	We consider the case of multi-document summarization , where the input documents are in Arabic , and the output summary is in English .
Further, the use of multiple machine translation systems provides yet more redundancy , yielding different ways to realize that information in English .	English	information	model-feature	{'e1': {'word': 'English', 'word_index': [(22, 22)], 'id': 'H05-1005.16'}, 'e2': {'word': 'information', 'word_index': [(20, 20)], 'id': 'H05-1005.15'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '20:20': 'ENTITYOTHER', '22:22': 'ENTITY'}}	Further , the use of multiple machine translation systems provides yet more redundancy , yielding different ways to realize that information in English .
 This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data 	supervised training data	maximum entropy word alignment algorithm	usage	{'e1': {'word': 'supervised training data', 'word_index': [(13, 15)], 'id': 'H05-1012.3'}, 'e2': {'word': 'maximum entropy word alignment algorithm', 'word_index': [(4, 8)], 'id': 'H05-1012.1'}, 'entity_replacement': {'4:8': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '13:15': 'ENTITY'}}	This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data
We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance .	supervised and unsupervised methods	performance	result	{'e1': {'word': 'supervised and unsupervised methods', 'word_index': [(20, 23)], 'id': 'H05-1012.6'}, 'e2': {'word': 'performance', 'word_index': [(26, 26)], 'id': 'H05-1012.7'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '20:23': 'ENTITY', '26:26': 'ENTITYOTHER'}}	We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance .
The probabilistic model used in the alignment directly models the link decisions .	probabilistic model	alignment	usage	{'e1': {'word': 'probabilistic model', 'word_index': [(1, 2)], 'id': 'H05-1012.8'}, 'e2': {'word': 'alignment', 'word_index': [(6, 6)], 'id': 'H05-1012.9'}, 'entity_replacement': {'1:2': 'ENTITY', '6:6': 'ENTITYOTHER', '10:11': 'ENTITYUNRELATED'}}	The probabilistic model used in the alignment directly models the link decisions .
 This paper presents a phrase-based statistical machine translation method , based on non-contiguous phrases , i.e	non-contiguous phrases	phrase-based statistical machine translation method	usage	{'e1': {'word': 'non-contiguous phrases', 'word_index': [(14, 16)], 'id': 'H05-1095.2'}, 'e2': {'word': 'phrase-based statistical machine translation method', 'word_index': [(4, 10)], 'id': 'H05-1095.1'}, 'entity_replacement': {'4:10': 'ENTITYOTHER', '14:16': 'ENTITY'}}	This paper presents a phrase - based statistical machine translation method , based on non- contiguous phrases , i.e
A method for producing such phrases from a word-aligned corpora is proposed.	phrases	word-aligned corpora	part_whole	{'e1': {'word': 'phrases', 'word_index': [(5, 5)], 'id': 'H05-1095.4'}, 'e2': {'word': 'word-aligned corpora', 'word_index': [(8, 11)], 'id': 'H05-1095.5'}, 'entity_replacement': {'5:5': 'ENTITY', '8:11': 'ENTITYOTHER'}}	A method for producing such phrases from a word - aligned corpora is proposed .
A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .	statistical translation model	phrases	usage	{'e1': {'word': 'statistical translation model', 'word_index': [(1, 3)], 'id': 'H05-1095.6'}, 'e2': {'word': 'phrases', 'word_index': [(10, 10)], 'id': 'H05-1095.7'}, 'entity_replacement': {'1:3': 'ENTITY', '10:10': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED'}}	A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .
A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .	translation accuracy	training method	usage	{'e1': {'word': 'translation accuracy', 'word_index': [(23, 24)], 'id': 'H05-1095.9'}, 'e2': {'word': 'training method', 'word_index': [(16, 17)], 'id': 'H05-1095.8'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '16:17': 'ENTITYOTHER', '23:24': 'ENTITY', '30:32': 'ENTITYUNRELATED'}}	A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .
Translations are produced by means of a beam-search decoder .	beam-search decoder	Translations	usage	{'e1': {'word': 'beam-search decoder', 'word_index': [(7, 10)], 'id': 'H05-1095.12'}, 'e2': {'word': 'Translations', 'word_index': [(0, 0)], 'id': 'H05-1095.11'}, 'entity_replacement': {'0:0': 'ENTITYOTHER', '7:10': 'ENTITY'}}	Translations are produced by means of a beam - search decoder .
 Following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar approach, implemented in a measure called POURPRE , for automatically evaluating answers to definition questions 	automatic evaluation	machine translation	usage	{'e1': {'word': 'automatic evaluation', 'word_index': [(5, 6)], 'id': 'H05-1117.1'}, 'e2': {'word': 'machine translation', 'word_index': [(8, 9)], 'id': 'H05-1117.2'}, 'entity_replacement': {'5:6': 'ENTITY', '8:9': 'ENTITYOTHER', '11:12': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '28:33': 'ENTITYUNRELATED'}}	Following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar approach , implemented in a measure called POURPRE , for automatically evaluating answers to definition questions
Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings , and that POURPRE outperforms direct application of existing metrics.	rankings	official rankings	compare	{'e1': {'word': 'rankings', 'word_index': [(12, 12)], 'id': 'H05-1117.8'}, 'e2': {'word': 'official rankings', 'word_index': [(20, 21)], 'id': 'H05-1117.9'}, 'entity_replacement': {'3:9': 'ENTITYUNRELATED', '12:12': 'ENTITY', '20:21': 'ENTITYOTHER', '25:25': 'ENTITYUNRELATED'}}	Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings , and that POURPRE outperforms direct application of existing metrics .
 We describe a method for identifying systematic patterns in translation data using part-of-speech tag sequences 	patterns	translation data	part_whole	{'e1': {'word': 'patterns', 'word_index': [(7, 7)], 'id': 'H05-2007.1'}, 'e2': {'word': 'translation data', 'word_index': [(9, 10)], 'id': 'H05-2007.2'}, 'entity_replacement': {'7:7': 'ENTITY', '9:10': 'ENTITYOTHER', '12:16': 'ENTITYUNRELATED'}}	We describe a method for identifying systematic patterns in translation data using part-of - speech tag sequences
We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore patterns in machine translation output .	patterns	machine translation output	part_whole	{'e1': {'word': 'patterns', 'word_index': [(28, 28)], 'id': 'H05-2007.8'}, 'e2': {'word': 'machine translation output', 'word_index': [(30, 32)], 'id': 'H05-2007.9'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '28:28': 'ENTITY', '30:32': 'ENTITYOTHER'}}	We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore patterns in machine translation output .
At the same time, the recent improvements in the BLEU scores of statistical machine translation (SMT) suggests that SMT models are good at predicting the right translation of the words in source language sentences .	translation	words	model-feature	{'e1': {'word': 'translation', 'word_index': [(29, 29)], 'id': 'I05-2021.12'}, 'e2': {'word': 'words', 'word_index': [(32, 32)], 'id': 'I05-2021.13'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '13:18': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '29:29': 'ENTITY', '32:32': 'ENTITYOTHER', '34:36': 'ENTITYUNRELATED'}}	At the same time , the recent improvements in the BLEU scores of statistical machine translation ( SMT ) suggests that SMT models are good at predicting the right translation of the words in source language sentences .
Surprisingly however, the WSD accuracy of SMT models has never been evaluated and compared with that of the dedicated WSD models .	SMT models	accuracy	result	{'e1': {'word': 'SMT models', 'word_index': [(7, 8)], 'id': 'I05-2021.17'}, 'e2': {'word': 'accuracy', 'word_index': [(5, 5)], 'id': 'I05-2021.16'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '5:5': 'ENTITYOTHER', '7:8': 'ENTITY', '20:21': 'ENTITYUNRELATED'}}	Surprisingly however , the WSD accuracy of SMT models has never been evaluated and compared with that of the dedicated WSD models .
This tends to support the view that despite recent speculative claims to the contrary, current SMT models do have limitations in comparison with dedicated WSD models , and that SMT should benefit from the better predictions made by the WSD models .	SMT models	WSD models	compare	{'e1': {'word': 'SMT models', 'word_index': [(16, 17)], 'id': 'I05-2021.23'}, 'e2': {'word': 'WSD models', 'word_index': [(25, 26)], 'id': 'I05-2021.24'}, 'entity_replacement': {'16:17': 'ENTITY', '25:26': 'ENTITYOTHER', '30:30': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED'}}	This tends to support the view that despite recent speculative claims to the contrary , current SMT models do have limitations in comparison with dedicated WSD models , and that SMT should benefit from the better predictions made by the WSD models .
Over the last few years dramatic improvements have been made, and a number of comparative evaluations have shown, that SMT gives competitive results to rule-based translation systems , requiring significantly less development time.	SMT	rule-based translation systems	compare	{'e1': {'word': 'SMT', 'word_index': [(21, 21)], 'id': 'I05-2048.3'}, 'e2': {'word': 'rule-based translation systems', 'word_index': [(26, 30)], 'id': 'I05-2048.4'}, 'entity_replacement': {'21:21': 'ENTITY', '26:30': 'ENTITYOTHER'}}	Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that SMT gives competitive results to rule - based translation systems , requiring significantly less development time .
 In this paper we present our recent work on harvesting English-Chinese bitexts of the laws of Hong Kong from the Web and aligning them to the subparagraph level via utilizing the numbering system in the legal text hierarchy 	English-Chinese bitexts	Web	part_whole	{'e1': {'word': 'English-Chinese bitexts', 'word_index': [(10, 13)], 'id': 'I05-4010.1'}, 'e2': {'word': 'Web', 'word_index': [(22, 22)], 'id': 'I05-4010.2'}, 'entity_replacement': {'10:13': 'ENTITY', '22:22': 'ENTITYOTHER', '28:28': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '37:39': 'ENTITYUNRELATED'}}	In this paper we present our recent work on harvesting English - Chinese bitexts of the laws of Hong Kong from the Web and aligning them to the subparagraph level via utilizing the numbering system in the legal text hierarchy
This piece of work has also laid a foundation for exploring and harvesting English-Chinese bitexts in a larger volume from the Web .	English-Chinese bitexts	Web	part_whole	{'e1': {'word': 'English-Chinese bitexts', 'word_index': [(13, 16)], 'id': 'I05-4010.11'}, 'e2': {'word': 'Web', 'word_index': [(23, 23)], 'id': 'I05-4010.12'}, 'entity_replacement': {'13:16': 'ENTITY', '23:23': 'ENTITYOTHER'}}	This piece of work has also laid a foundation for exploring and harvesting English - Chinese bitexts in a larger volume from the Web .
 The task of machine translation (MT) evaluation is closely related to the task of sentence-level semantic equivalence classification 	machine translation (MT) evaluation	sentence-level semantic equivalence classification	compare	{'e1': {'word': 'machine translation (MT) evaluation', 'word_index': [(3, 8)], 'id': 'I05-5003.1'}, 'e2': {'word': 'sentence-level semantic equivalence classification', 'word_index': [(16, 21)], 'id': 'I05-5003.2'}, 'entity_replacement': {'3:8': 'ENTITY', '16:21': 'ENTITYOTHER'}}	The task of machine translation ( MT ) evaluation is closely related to the task of sentence - level semantic equivalence classification
This paper investigates the utility of applying standard MT evaluation methods (BLEU, NIST, WER and PER) to building classifiers to predict semantic equivalence and entailment .	MT evaluation methods (BLEU, NIST, WER and PER)	classifiers	usage	{'e1': {'word': 'MT evaluation methods (BLEU, NIST, WER and PER)', 'word_index': [(8, 19)], 'id': 'I05-5003.3'}, 'e2': {'word': 'classifiers', 'word_index': [(22, 22)], 'id': 'I05-5003.4'}, 'entity_replacement': {'8:19': 'ENTITY', '22:22': 'ENTITYOTHER', '25:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	This paper investigates the utility of applying standard MT evaluation methods ( BLEU , NIST , WER and PER ) to building classifiers to predict semantic equivalence and entailment .
We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence .	PER	classification method	usage	{'e1': {'word': 'PER', 'word_index': [(9, 9)], 'id': 'I05-5003.8'}, 'e2': {'word': 'classification method', 'word_index': [(5, 6)], 'id': 'I05-5003.7'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '9:9': 'ENTITY', '12:15': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '22:26': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non- matches in the sentence .
We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence .	part of speech information	words	model-feature	{'e1': {'word': 'part of speech information', 'word_index': [(12, 15)], 'id': 'I05-5003.9'}, 'e2': {'word': 'words', 'word_index': [(18, 18)], 'id': 'I05-5003.10'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '12:15': 'ENTITY', '18:18': 'ENTITYOTHER', '22:26': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non- matches in the sentence .
Our results show that MT evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment .	MT evaluation techniques	features	result	{'e1': {'word': 'MT evaluation techniques', 'word_index': [(4, 6)], 'id': 'I05-5003.13'}, 'e2': {'word': 'features', 'word_index': [(12, 12)], 'id': 'I05-5003.14'}, 'entity_replacement': {'4:6': 'ENTITY', '12:12': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED'}}	Our results show that MT evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment .
Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments.	technique	paraphrase classification accuracy	result	{'e1': {'word': 'technique', 'word_index': [(1, 1)], 'id': 'I05-5003.17'}, 'e2': {'word': 'paraphrase classification accuracy', 'word_index': [(7, 9)], 'id': 'I05-5003.18'}, 'entity_replacement': {'1:1': 'ENTITY', '7:9': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED'}}	Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .
 We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like BLEU and NIST 	paraphrase	seed sentences	part_whole	{'e1': {'word': 'paraphrase', 'word_index': [(7, 7)], 'id': 'I05-5008.1'}, 'e2': {'word': 'seed sentences', 'word_index': [(10, 11)], 'id': 'I05-5008.2'}, 'entity_replacement': {'7:7': 'ENTITY', '10:11': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '20:23': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like BLEU and NIST
We measured the quality of the paraphrases produced in an experiment, i.e., (i) their grammaticality : at least 99% correct sentences ; (ii) their equivalence in meaning : at least 96% correct paraphrases either by meaning equivalence or entailment ; and, (iii) the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .	lexical and syntactical variation	paraphrases	model-feature	{'e1': {'word': 'lexical and syntactical variation', 'word_index': [(57, 60)], 'id': 'I05-5008.14'}, 'e2': {'word': 'paraphrases', 'word_index': [(65, 65)], 'id': 'I05-5008.15'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '31:33': 'ENTITYUNRELATED', '40:40': 'ENTITYUNRELATED', '43:44': 'ENTITYUNRELATED', '46:46': 'ENTITYUNRELATED', '57:60': 'ENTITY', '65:65': 'ENTITYOTHER', '72:75': 'ENTITYUNRELATED'}}	We measured the quality of the paraphrases produced in an experiment , i.e. , ( i ) their grammaticality : at least 99 % correct sentences ; ( ii ) their equivalence in meaning : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , ( iii ) the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand - produced sets .
The paraphrase sets produced by this method thus seem adequate as reference sets to be used for MT evaluation .	reference sets	MT evaluation	usage	{'e1': {'word': 'reference sets', 'word_index': [(11, 12)], 'id': 'I05-5008.18'}, 'e2': {'word': 'MT evaluation', 'word_index': [(17, 18)], 'id': 'I05-5008.19'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '11:12': 'ENTITY', '17:18': 'ENTITYOTHER'}}	The paraphrase sets produced by this method thus seem adequate as reference sets to be used for MT evaluation .
 This paper proposes an annotating scheme that encodes honorifics (respectful words)	annotating scheme	honorifics	model-feature	{'e1': {'word': 'annotating scheme', 'word_index': [(4, 5)], 'id': 'I05-6011.1'}, 'e2': {'word': 'honorifics', 'word_index': [(8, 8)], 'id': 'I05-6011.2'}, 'entity_replacement': {'4:5': 'ENTITY', '8:8': 'ENTITYOTHER'}}	This paper proposes an annotating scheme that encodes honorifics ( respectful words )
This referential information is vital for resolving zero pronouns and improving machine translation outputs .	referential information	machine translation outputs	result	{'e1': {'word': 'referential information', 'word_index': [(1, 2)], 'id': 'I05-6011.6'}, 'e2': {'word': 'machine translation outputs', 'word_index': [(11, 13)], 'id': 'I05-6011.8'}, 'entity_replacement': {'1:2': 'ENTITY', '7:8': 'ENTITYUNRELATED', '11:13': 'ENTITYOTHER'}}	This referential information is vital for resolving zero pronouns and improving machine translation outputs .
Annotating honorifics is a complex task that involves identifying a predicate with honorifics , assigning ranks to referents of the predicate , calibrating the ranks , and connecting referents with their predicates .	ranks	referents	model-feature	{'e1': {'word': 'ranks', 'word_index': [(15, 15)], 'id': 'I05-6011.12'}, 'e2': {'word': 'referents', 'word_index': [(17, 17)], 'id': 'I05-6011.13'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '15:15': 'ENTITY', '17:17': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	Annotating honorifics is a complex task that involves identifying a predicate with honorifics , assigning ranks to referents of the predicate , calibrating the ranks , and connecting referents with their predicates .
The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .	candidate parses	sentence	model-feature	{'e1': {'word': 'candidate parses', 'word_index': [(7, 8)], 'id': 'J05-1003.3'}, 'e2': {'word': 'sentence', 'word_index': [(12, 12)], 'id': 'J05-1003.4'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '7:8': 'ENTITY', '12:12': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .
A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence.	features	tree	model-feature	{'e1': {'word': 'features', 'word_index': [(14, 14)], 'id': 'J05-1003.10'}, 'e2': {'word': 'tree', 'word_index': [(17, 17)], 'id': 'J05-1003.11'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '14:14': 'ENTITY', '17:17': 'ENTITYOTHER'}}	A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .
The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .	features	generative model	usage	{'e1': {'word': 'features', 'word_index': [(45, 45)], 'id': 'J05-1003.17'}, 'e2': {'word': 'generative model', 'word_index': [(40, 41)], 'id': 'J05-1003.16'}, 'entity_replacement': {'10:10': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '37:37': 'ENTITYUNRELATED', '40:41': 'ENTITYOTHER', '45:45': 'ENTITY'}}	The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .
We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al. (1998).	boosting approach	ranking problems	usage	{'e1': {'word': 'boosting approach', 'word_index': [(13, 14)], 'id': 'J05-1003.19'}, 'e2': {'word': 'ranking problems', 'word_index': [(16, 17)], 'id': 'J05-1003.20'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '13:14': 'ENTITY', '16:17': 'ENTITYOTHER'}}	We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al. ( 1998 ) .
We apply the boosting method to parsing the Wall Street Journal treebank .	parsing	Wall Street Journal treebank	usage	{'e1': {'word': 'parsing', 'word_index': [(6, 6)], 'id': 'J05-1003.22'}, 'e2': {'word': 'Wall Street Journal treebank', 'word_index': [(8, 11)], 'id': 'J05-1003.23'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '6:6': 'ENTITY', '8:11': 'ENTITYOTHER'}}	We apply the boosting method to parsing the Wall Street Journal treebank .
The new model achieved 89.75% F-measure , a 13% relative decrease in F-measure error over the baseline model's score of 88.2%.	model	F-measure	result	{'e1': {'word': 'model', 'word_index': [(2, 2)], 'id': 'J05-1003.29'}, 'e2': {'word': 'F-measure', 'word_index': [(6, 6)], 'id': 'J05-1003.30'}, 'entity_replacement': {'2:2': 'ENTITY', '6:6': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '18:21': 'ENTITYUNRELATED'}}	The new model achieved 89.75 % F-measure , a 13 % relative decrease in F-measure error over the baseline model 's score of 88.2 %.
The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .	sparsity of the feature space	parsing data	model-feature	{'e1': {'word': 'sparsity of the feature space', 'word_index': [(16, 20)], 'id': 'J05-1003.34'}, 'e2': {'word': 'parsing data', 'word_index': [(23, 24)], 'id': 'J05-1003.35'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '16:20': 'ENTITY', '23:24': 'ENTITYOTHER'}}	The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .
We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on feature selection methods within log-linear (maximum-entropy) models .	feature selection methods	log-linear (maximum-entropy) models	part_whole	{'e1': {'word': 'feature selection methods', 'word_index': [(21, 23)], 'id': 'J05-1003.38'}, 'e2': {'word': 'log-linear (maximum-entropy) models', 'word_index': [(25, 30)], 'id': 'J05-1003.39'}, 'entity_replacement': {'21:23': 'ENTITY', '25:30': 'ENTITYOTHER'}}	We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on feature selection methods within log-linear ( maximum -entropy ) models .
Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora .	parallel data	Chinese, Arabic, and English non-parallel newspaper corpora	part_whole	{'e1': {'word': 'parallel data', 'word_index': [(6, 7)], 'id': 'J05-4003.6'}, 'e2': {'word': 'Chinese, Arabic, and English non-parallel newspaper corpora', 'word_index': [(10, 18)], 'id': 'J05-4003.7'}, 'entity_replacement': {'6:7': 'ENTITY', '10:18': 'ENTITYOTHER'}}	Using this approach , we extract parallel data from large Chinese , Arabic , and English non-parallel newspaper corpora .
We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words ) and exploiting a large non-parallel corpus .	parallel corpus	MT system	usage	{'e1': {'word': 'parallel corpus', 'word_index': [(21, 22)], 'id': 'J05-4003.11'}, 'e2': {'word': 'MT system', 'word_index': [(8, 9)], 'id': 'J05-4003.10'}, 'entity_replacement': {'8:9': 'ENTITYOTHER', '21:22': 'ENTITY', '25:25': 'ENTITYUNRELATED', '31:34': 'ENTITYUNRELATED'}}	We also show that a good - quality MT system can be built from scratch by starting with a very small parallel corpus ( 100,000 words ) and exploiting a large non - parallel corpus .
We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure .	phrase translations	suffix array-based data structure	part_whole	{'e1': {'word': 'phrase translations', 'word_index': [(12, 13)], 'id': 'P05-1032.9'}, 'e2': {'word': 'suffix array-based data structure', 'word_index': [(16, 21)], 'id': 'P05-1032.10'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '6:8': 'ENTITYUNRELATED', '12:13': 'ENTITY', '16:21': 'ENTITYOTHER'}}	We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array - based data structure .
 We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation 	syntactic information	source language	model-feature	{'e1': {'word': 'syntactic information', 'word_index': [(11, 12)], 'id': 'P05-1034.2'}, 'e2': {'word': 'source language', 'word_index': [(15, 16)], 'id': 'P05-1034.3'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '11:12': 'ENTITY', '15:16': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED'}}	We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation
We align a parallel corpus , project the source dependency parse onto the target sentence , extract dependency treelet translation pairs , and train a tree-based ordering model .	source dependency parse	sentence	model-feature	{'e1': {'word': 'source dependency parse', 'word_index': [(8, 10)], 'id': 'P05-1034.11'}, 'e2': {'word': 'sentence', 'word_index': [(14, 14)], 'id': 'P05-1034.12'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '8:10': 'ENTITY', '14:14': 'ENTITYOTHER', '17:20': 'ENTITYUNRELATED', '25:29': 'ENTITYUNRELATED'}}	We align a parallel corpus , project the source dependency parse onto the target sentence , extract dependency treelet translation pairs , and train a tree - based ordering model .
Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone.	Chinese word sense disambiguation model	translation candidates	usage	{'e1': {'word': 'Chinese word sense disambiguation model', 'word_index': [(9, 13)], 'id': 'P05-1048.4'}, 'e2': {'word': 'translation candidates', 'word_index': [(16, 17)], 'id': 'P05-1048.5'}, 'entity_replacement': {'9:13': 'ENTITY', '16:17': 'ENTITYOTHER', '21:24': 'ENTITYUNRELATED', '29:31': 'ENTITYUNRELATED', '37:38': 'ENTITYUNRELATED', '41:44': 'ENTITYUNRELATED'}}	Using a state - of - the - art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone .
Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone.	word sense disambiguation	translation quality	result	{'e1': {'word': 'word sense disambiguation', 'word_index': [(29, 31)], 'id': 'P05-1048.7'}, 'e2': {'word': 'translation quality', 'word_index': [(37, 38)], 'id': 'P05-1048.8'}, 'entity_replacement': {'9:13': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '21:24': 'ENTITYUNRELATED', '29:31': 'ENTITY', '37:38': 'ENTITYOTHER', '41:44': 'ENTITYUNRELATED'}}	Using a state - of - the - art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone .
 Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data 	statistical models	structured data	usage	{'e1': {'word': 'statistical models', 'word_index': [(12, 13)], 'id': 'P05-1067.2'}, 'e2': {'word': 'structured data', 'word_index': [(15, 16)], 'id': 'P05-1067.3'}, 'entity_replacement': {'0:8': 'ENTITYUNRELATED', '12:13': 'ENTITY', '15:16': 'ENTITYOTHER'}}	Syntax - based statistical machine translation ( MT ) aims at applying statistical models to structured data
In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar .	probabilistic synchronous dependency insertion grammar	syntax-based statistical machine translation system	usage	{'e1': {'word': 'probabilistic synchronous dependency insertion grammar', 'word_index': [(17, 21)], 'id': 'P05-1067.5'}, 'e2': {'word': 'syntax-based statistical machine translation system', 'word_index': [(7, 13)], 'id': 'P05-1067.4'}, 'entity_replacement': {'7:13': 'ENTITYOTHER', '17:21': 'ENTITY'}}	In this paper , we present a syntax - based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar .
We first introduce our approach to inducing such a grammar from parallel corpora .	grammar	parallel corpora	part_whole	{'e1': {'word': 'grammar', 'word_index': [(9, 9)], 'id': 'P05-1067.9'}, 'e2': {'word': 'parallel corpora', 'word_index': [(11, 12)], 'id': 'P05-1067.10'}, 'entity_replacement': {'9:9': 'ENTITY', '11:12': 'ENTITYOTHER'}}	We first introduce our approach to inducing such a grammar from parallel corpora .
Second, we describe the graphical model for the machine translation task , which can also be viewed as a stochastic tree-to-tree transducer .	graphical model	machine translation task	usage	{'e1': {'word': 'graphical model', 'word_index': [(5, 6)], 'id': 'P05-1067.11'}, 'e2': {'word': 'machine translation task', 'word_index': [(9, 11)], 'id': 'P05-1067.12'}, 'entity_replacement': {'5:6': 'ENTITY', '9:11': 'ENTITYOTHER', '20:26': 'ENTITYUNRELATED'}}	Second , we describe the graphical model for the machine translation task , which can also be viewed as a stochastic tree - to - tree transducer .
The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality .	IBM models	baseline system	usage	{'e1': {'word': 'IBM models', 'word_index': [(13, 14)], 'id': 'P05-1067.19'}, 'e2': {'word': 'baseline system', 'word_index': [(8, 9)], 'id': 'P05-1067.18'}, 'entity_replacement': {'8:9': 'ENTITYOTHER', '13:14': 'ENTITY', '17:20': 'ENTITYUNRELATED'}}	The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality .
 In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT) 	localized phrase-based prediction model	statistical machine translation (SMT)	usage	{'e1': {'word': 'localized phrase-based prediction model', 'word_index': [(12, 17)], 'id': 'P05-1069.2'}, 'e2': {'word': 'statistical machine translation (SMT)', 'word_index': [(19, 24)], 'id': 'P05-1069.3'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '12:17': 'ENTITY', '19:24': 'ENTITYOTHER'}}	In this paper , we present a novel training method for a localized phrase - based prediction model for statistical machine translation ( SMT )
We use a maximum likelihood criterion to train a log-linear block bigram model which uses real-valued features (e.g. a language model score ) as well as binary features based on the block identities themselves, e.g. block bigram features.	real-valued features	log-linear block bigram model	usage	{'e1': {'word': 'real-valued features', 'word_index': [(15, 18)], 'id': 'P05-1069.9'}, 'e2': {'word': 'log-linear block bigram model', 'word_index': [(9, 12)], 'id': 'P05-1069.8'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '9:12': 'ENTITYOTHER', '15:18': 'ENTITY', '22:24': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	We use a maximum likelihood criterion to train a log-linear block bigram model which uses real - valued features ( e.g. a language model score ) as well as binary features based on the block identities themselves , e.g. block bigram features .
 Previous work has used monolingual parallel corpora to extract and generate paraphrases 	paraphrases	monolingual parallel corpora	part_whole	{'e1': {'word': 'paraphrases', 'word_index': [(11, 11)], 'id': 'P05-1074.2'}, 'e2': {'word': 'monolingual parallel corpora', 'word_index': [(4, 6)], 'id': 'P05-1074.1'}, 'entity_replacement': {'4:6': 'ENTITYOTHER', '11:11': 'ENTITY'}}	Previous work has used monolingual parallel corpora to extract and generate paraphrases
We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account.	paraphrases	bilingual parallel corpus	part_whole	{'e1': {'word': 'paraphrases', 'word_index': [(7, 7)], 'id': 'P05-1074.11'}, 'e2': {'word': 'bilingual parallel corpus', 'word_index': [(11, 13)], 'id': 'P05-1074.12'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:7': 'ENTITY', '11:13': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .
We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments .	paraphrases	automatic alignments	part_whole	{'e1': {'word': 'paraphrases', 'word_index': [(21, 21)], 'id': 'P05-1074.18'}, 'e2': {'word': 'automatic alignments', 'word_index': [(24, 25)], 'id': 'P05-1074.19'}, 'entity_replacement': {'3:7': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '21:21': 'ENTITY', '24:25': 'ENTITYOTHER'}}	We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments .
 We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures 	Czech-English statistical machine translation system	tree-to-tree translation	usage	{'e1': {'word': 'Czech-English statistical machine translation system', 'word_index': [(3, 9)], 'id': 'P05-2016.1'}, 'e2': {'word': 'tree-to-tree translation', 'word_index': [(12, 17)], 'id': 'P05-2016.2'}, 'entity_replacement': {'3:9': 'ENTITY', '12:17': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED'}}	We present a Czech - English statistical machine translation system which performs tree - to - tree translation of dependency structures
We also refer to an evaluation method and plan to compare our system&apos;s output with a benchmark system .	system&apos;s output	benchmark system	compare	{'e1': {'word': 'system&apos;s output', 'word_index': [(12, 14)], 'id': 'P05-2016.9'}, 'e2': {'word': 'benchmark system', 'word_index': [(17, 18)], 'id': 'P05-2016.10'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '12:14': 'ENTITY', '17:18': 'ENTITYOTHER'}}	We also refer to an evaluation method and plan to compare our system&apos ;s output with a benchmark system .
The combination with a two-step clustering process using sentence co-occurrences as features allows for accurate results.	sentence co-occurrences	two-step clustering process	usage	{'e1': {'word': 'sentence co-occurrences', 'word_index': [(9, 10)], 'id': 'E06-1018.8'}, 'e2': {'word': 'two-step clustering process', 'word_index': [(4, 7)], 'id': 'E06-1018.7'}, 'entity_replacement': {'4:7': 'ENTITYOTHER', '9:10': 'ENTITY', '12:12': 'ENTITYUNRELATED'}}	The combination with a two -step clustering process using sentence co-occurrences as features allows for accurate results .
 We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers 	addressee identification	four-participants face-to-face meetings	usage	{'e1': {'word': 'addressee identification', 'word_index': [(4, 5)], 'id': 'E06-1022.1'}, 'e2': {'word': 'four-participants face-to-face meetings', 'word_index': [(7, 14)], 'id': 'E06-1022.2'}, 'entity_replacement': {'4:5': 'ENTITY', '7:14': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED'}}	We present results on addressee identification in four- participants face - to - face meetings using Bayesian Network and Naive Bayes classifiers
The classifiers show little gain from information about meeting context .	classifiers	gain	result	{'e1': {'word': 'classifiers', 'word_index': [(1, 1)], 'id': 'E06-1022.17'}, 'e2': {'word': 'gain', 'word_index': [(4, 4)], 'id': 'E06-1022.18'}, 'entity_replacement': {'1:1': 'ENTITY', '4:4': 'ENTITYOTHER', '8:9': 'ENTITYUNRELATED'}}	The classifiers show little gain from information about meeting context .
 Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks	evaluation measures	machine translation	usage	{'e1': {'word': 'evaluation measures', 'word_index': [(7, 8)], 'id': 'E06-1031.1'}, 'e2': {'word': 'machine translation', 'word_index': [(10, 11)], 'id': 'E06-1031.2'}, 'entity_replacement': {'7:8': 'ENTITY', '10:11': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED'}}	Most state - of - the- art evaluation measures for machine translation assign high costs to movements of word blocks
In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation .	evaluation measure	block reordering	usage	{'e1': {'word': 'evaluation measure', 'word_index': [(9, 10)], 'id': 'E06-1031.6'}, 'e2': {'word': 'block reordering', 'word_index': [(14, 15)], 'id': 'E06-1031.7'}, 'entity_replacement': {'9:10': 'ENTITY', '14:15': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	In this paper , we will present a new evaluation measure which explicitly models block reordering as an edit operation .
Furthermore, we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs .	word-dependent substitution costs	evaluation measures	result	{'e1': {'word': 'word-dependent substitution costs', 'word_index': [(16, 20)], 'id': 'E06-1031.12'}, 'e2': {'word': 'evaluation measures', 'word_index': [(7, 8)], 'id': 'E06-1031.11'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '16:20': 'ENTITY'}}	Furthermore , we will show how some evaluation measures can be improved by the introduction of word - dependent substitution costs .
The correlation of the new measure with human judgment has been investigated systematically on two different language pairs .	measure	human judgment	compare	{'e1': {'word': 'measure', 'word_index': [(5, 5)], 'id': 'E06-1031.13'}, 'e2': {'word': 'human judgment', 'word_index': [(7, 8)], 'id': 'E06-1031.14'}, 'entity_replacement': {'5:5': 'ENTITY', '7:8': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED'}}	The correlation of the new measure with human judgment has been investigated systematically on two different language pairs .
Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment .	automatic evaluation measures	human judgment	compare	{'e1': {'word': 'automatic evaluation measures', 'word_index': [(16, 18)], 'id': 'E06-1031.18'}, 'e2': {'word': 'human judgment', 'word_index': [(20, 21)], 'id': 'E06-1031.19'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '16:18': 'ENTITY', '20:21': 'ENTITYOTHER'}}	Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment .
 In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue 	segment boundaries	spoken multiparty dialogue	part_whole	{'e1': {'word': 'segment boundaries', 'word_index': [(11, 12)], 'id': 'E06-1035.1'}, 'e2': {'word': 'spoken multiparty dialogue', 'word_index': [(14, 16)], 'id': 'E06-1035.2'}, 'entity_replacement': {'11:12': 'ENTITY', '14:16': 'ENTITYOTHER'}}	In this paper , we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue
We then explore the impact on performance of using ASR output as opposed to human transcription .	ASR output	human transcription	compare	{'e1': {'word': 'ASR output', 'word_index': [(9, 10)], 'id': 'E06-1035.6'}, 'e2': {'word': 'human transcription', 'word_index': [(14, 15)], 'id': 'E06-1035.7'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:10': 'ENTITY', '14:15': 'ENTITYOTHER'}}	We then explore the impact on performance of using ASR output as opposed to human transcription .
Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task.	machine learning approach	predicting top-level boundaries	usage	{'e1': {'word': 'machine learning approach', 'word_index': [(48, 50)], 'id': 'E06-1035.13'}, 'e2': {'word': 'predicting top-level boundaries', 'word_index': [(43, 45)], 'id': 'E06-1035.12'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:13': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:32': 'ENTITYUNRELATED', '43:45': 'ENTITYOTHER', '48:50': 'ENTITY', '53:58': 'ENTITYUNRELATED', '66:67': 'ENTITYUNRELATED', '71:72': 'ENTITYUNRELATED', '74:75': 'ENTITYUNRELATED'}}	Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : ( 1 ) for predicting subtopic boundaries , the lexical cohesion - based approach alone can achieve competitive results , ( 2 ) for predicting top-level boundaries , the machine learning approach that combines lexical - cohesion and conversational features performs best , and ( 3 ) conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .
We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features , but do not change the general preference of approach for the two tasks.	transcription errors	ASR output	model-feature	{'e1': {'word': 'transcription errors', 'word_index': [(5, 6)], 'id': 'E06-1035.18'}, 'e2': {'word': 'ASR output', 'word_index': [(9, 10)], 'id': 'E06-1035.19'}, 'entity_replacement': {'5:6': 'ENTITY', '9:10': 'ENTITYOTHER', '19:24': 'ENTITYUNRELATED'}}	We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical - cohesion and conversational features , but do not change the general preference of approach for the two tasks .
 Combination methods are an effective way of improving system performance 	Combination methods	system performance	result	{'e1': {'word': 'Combination methods', 'word_index': [(0, 1)], 'id': 'P06-1013.1'}, 'e2': {'word': 'system performance', 'word_index': [(8, 9)], 'id': 'P06-1013.2'}, 'entity_replacement': {'0:1': 'ENTITY', '8:9': 'ENTITYOTHER'}}	Combination methods are an effective way of improving system performance
Our combination methods rely on predominant senses which are derived automatically from raw text .	predominant senses	combination methods	usage	{'e1': {'word': 'predominant senses', 'word_index': [(5, 6)], 'id': 'P06-1013.8'}, 'e2': {'word': 'combination methods', 'word_index': [(1, 2)], 'id': 'P06-1013.7'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '5:6': 'ENTITY', '12:13': 'ENTITYUNRELATED'}}	Our combination methods rely on predominant senses which are derived automatically from raw text .
 We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation (USR) of a scope ambiguity , compute an USR with fewer mutually equivalent readings 	underspecified semantic representation (USR)	scope ambiguity	model-feature	{'e1': {'word': 'underspecified semantic representation (USR)', 'word_index': [(13, 18)], 'id': 'P06-1052.2'}, 'e2': {'word': 'scope ambiguity', 'word_index': [(21, 22)], 'id': 'P06-1052.3'}, 'entity_replacement': {'7:9': 'ENTITYUNRELATED', '13:18': 'ENTITY', '21:22': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation ( USR ) of a scope ambiguity , compute an USR with fewer mutually equivalent readings
 In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque 	machine learning techniques	comma checker	usage	{'e1': {'word': 'machine learning techniques', 'word_index': [(9, 11)], 'id': 'P06-2001.1'}, 'e2': {'word': 'comma checker', 'word_index': [(15, 16)], 'id': 'P06-2001.2'}, 'entity_replacement': {'9:11': 'ENTITY', '15:16': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	In this paper , we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque
 In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque 	grammar checker	Basque	usage	{'e1': {'word': 'grammar checker', 'word_index': [(22, 23)], 'id': 'P06-2001.3'}, 'e2': {'word': 'Basque', 'word_index': [(25, 25)], 'id': 'P06-2001.4'}, 'entity_replacement': {'9:11': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '22:23': 'ENTITY', '25:25': 'ENTITYOTHER'}}	In this paper , we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque
After several experiments, and trained with a little corpus of 100,000 words , the system guesses correctly not placing commas with a precision of 96% and a recall of 98%.	words	corpus	part_whole	{'e1': {'word': 'words', 'word_index': [(12, 12)], 'id': 'P06-2001.6'}, 'e2': {'word': 'corpus', 'word_index': [(9, 9)], 'id': 'P06-2001.5'}, 'entity_replacement': {'9:9': 'ENTITYOTHER', '12:12': 'ENTITY', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	After several experiments , and trained with a little corpus of 100,000 words , the system guesses correctly not placing commas with a precision of 96 % and a recall of 98 %.
Finally, we have shown that these results can be improved using a bigger and a more homogeneous corpus to train, that is, a bigger corpus written by one unique author .	author	corpus	model-feature	{'e1': {'word': 'author', 'word_index': [(32, 32)], 'id': 'P06-2001.15'}, 'e2': {'word': 'corpus', 'word_index': [(27, 27)], 'id': 'P06-2001.14'}, 'entity_replacement': {'18:18': 'ENTITYUNRELATED', '27:27': 'ENTITYOTHER', '32:32': 'ENTITY'}}	Finally , we have shown that these results can be improved using a bigger and a more homogeneous corpus to train , that is , a bigger corpus written by one unique author .
 This paper presents an unsupervised learning approach to disambiguate various relations between named entities by use of various lexical and syntactic features from the contexts 	lexical and syntactic features	unsupervised learning approach	usage	{'e1': {'word': 'lexical and syntactic features', 'word_index': [(18, 21)], 'id': 'P06-2012.3'}, 'e2': {'word': 'unsupervised learning approach', 'word_index': [(4, 6)], 'id': 'P06-2012.1'}, 'entity_replacement': {'4:6': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '18:21': 'ENTITY', '24:24': 'ENTITYUNRELATED'}}	This paper presents an unsupervised learning approach to disambiguate various relations between named entities by use of various lexical and syntactic features from the contexts
Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .	spectral clustering based approach	clustering methods	compare	{'e1': {'word': 'spectral clustering based approach', 'word_index': [(8, 11)], 'id': 'P06-2012.13'}, 'e2': {'word': 'clustering methods', 'word_index': [(15, 16)], 'id': 'P06-2012.14'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '8:11': 'ENTITY', '15:16': 'ENTITYOTHER'}}	Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .
 This paper proposes a novel method of building polarity-tagged corpus from HTML documents 	polarity-tagged corpus	HTML documents	part_whole	{'e1': {'word': 'polarity-tagged corpus', 'word_index': [(8, 11)], 'id': 'P06-2059.1'}, 'e2': {'word': 'HTML documents', 'word_index': [(13, 14)], 'id': 'P06-2059.2'}, 'entity_replacement': {'8:11': 'ENTITY', '13:14': 'ENTITYOTHER'}}	This paper proposes a novel method of building polarity - tagged corpus from HTML documents
In our experiment, the method could construct a corpus consisting of 126,610 sentences .	sentences	corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(13, 13)], 'id': 'P06-2059.8'}, 'e2': {'word': 'corpus', 'word_index': [(9, 9)], 'id': 'P06-2059.7'}, 'entity_replacement': {'9:9': 'ENTITYOTHER', '13:13': 'ENTITY'}}	In our experiment , the method could construct a corpus consisting of 126,610 sentences .
 In this paper we show how two standard outputs from information extraction (IE) systems - named entity annotations and scenario templates - can be used to enhance access to text collections via a standard text browser .	scenario templates	text browser	usage	{'e1': {'word': 'scenario templates', 'word_index': [(21, 22)], 'id': 'H01-1040.3'}, 'e2': {'word': 'text browser', 'word_index': [(36, 37)], 'id': 'H01-1040.5'}, 'entity_replacement': {'10:15': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '21:22': 'ENTITY', '31:32': 'ENTITYUNRELATED', '36:37': 'ENTITYOTHER'}}	In this paper we show how two standard outputs from information extraction ( IE ) systems - named entity annotations and scenario templates - can be used to enhance access to text collections via a standard text browser .
We describe how this information is used in a prototype system designed to support information workers &apos; access to a pharmaceutical news archive as part of their industry watch function.	prototype system	industry watch	usage	{'e1': {'word': 'prototype system', 'word_index': [(9, 10)], 'id': 'H01-1040.6'}, 'e2': {'word': 'industry watch', 'word_index': [(28, 29)], 'id': 'H01-1040.9'}, 'entity_replacement': {'9:10': 'ENTITY', '14:15': 'ENTITYUNRELATED', '21:23': 'ENTITYUNRELATED', '28:29': 'ENTITYOTHER'}}	We describe how this information is used in a prototype system designed to support information workers &apos ; access to a pharmaceutical news archive as part of their industry watch function .
We also report results of a preliminary, qualitative user evaluation of the system, which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of IE-enhanced text browsers .	interface	IE-enhanced text browsers	part_whole	{'e1': {'word': 'interface', 'word_index': [(28, 28)], 'id': 'H01-1040.11'}, 'e2': {'word': 'IE-enhanced text browsers', 'word_index': [(38, 42)], 'id': 'H01-1040.13'}, 'entity_replacement': {'8:10': 'ENTITYUNRELATED', '28:28': 'ENTITY', '31:31': 'ENTITYUNRELATED', '38:42': 'ENTITYOTHER'}}	We also report results of a preliminary , qualitative user evaluation of the system , which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of IE - enhanced text browsers .
 Recent advances in Automatic Speech Recognition technology have put the goal of naturally sounding dialog systems within reach.	Automatic Speech Recognition technology	dialog systems	usage	{'e1': {'word': 'Automatic Speech Recognition technology', 'word_index': [(3, 6)], 'id': 'H01-1055.1'}, 'e2': {'word': 'dialog systems', 'word_index': [(14, 15)], 'id': 'H01-1055.2'}, 'entity_replacement': {'3:6': 'ENTITY', '14:15': 'ENTITYOTHER'}}	Recent advances in Automatic Speech Recognition technology have put the goal of naturally sounding dialog systems within reach .
The issue of system response to users has been extensively studied by the natural language generation community , though rarely in the context of dialog systems .	natural language generation community	system response	topic	{'e1': {'word': 'natural language generation community', 'word_index': [(13, 16)], 'id': 'H01-1055.9'}, 'e2': {'word': 'system response', 'word_index': [(3, 4)], 'id': 'H01-1055.7'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '6:6': 'ENTITYUNRELATED', '13:16': 'ENTITY', '24:25': 'ENTITYUNRELATED'}}	The issue of system response to users has been extensively studied by the natural language generation community , though rarely in the context of dialog systems .
We show how research in generation can be adapted to dialog systems , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques .	generation	dialog systems	usage	{'e1': {'word': 'generation', 'word_index': [(5, 5)], 'id': 'H01-1055.11'}, 'e2': {'word': 'dialog systems', 'word_index': [(10, 11)], 'id': 'H01-1055.12'}, 'entity_replacement': {'5:5': 'ENTITY', '10:11': 'ENTITYOTHER', '22:26': 'ENTITYUNRELATED', '32:34': 'ENTITYUNRELATED'}}	We show how research in generation can be adapted to dialog systems , and how the high cost of hand - crafting knowledge - based generation systems can be overcome by employing machine learning techniques .
We show how research in generation can be adapted to dialog systems , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques .	machine learning techniques	knowledge-based generation systems	usage	{'e1': {'word': 'machine learning techniques', 'word_index': [(32, 34)], 'id': 'H01-1055.14'}, 'e2': {'word': 'knowledge-based generation systems', 'word_index': [(22, 26)], 'id': 'H01-1055.13'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '22:26': 'ENTITYOTHER', '32:34': 'ENTITY'}}	We show how research in generation can be adapted to dialog systems , and how the high cost of hand - crafting knowledge - based generation systems can be overcome by employing machine learning techniques .
 The TAP-XL Automated Analyst&apos;s Assistant is an application designed to help an English -speaking analyst write a topical report , culling information from a large inflow of multilingual, multimedia data .	multilingual, multimedia data	TAP-XL Automated Analyst&apos;s Assistant	usage	{'e1': {'word': 'multilingual, multimedia data', 'word_index': [(30, 33)], 'id': 'N03-4004.4'}, 'e2': {'word': 'TAP-XL Automated Analyst&apos;s Assistant', 'word_index': [(1, 6)], 'id': 'N03-4004.1'}, 'entity_replacement': {'1:6': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '30:33': 'ENTITY'}}	The TAP -XL Automated Analyst&apos ;s Assistant is an application designed to help an English - speaking analyst write a topical report , culling information from a large inflow of multilingual , multimedia data .
 This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation .	computational problems	probabilistic translation models	model-feature	{'e1': {'word': 'computational problems', 'word_index': [(4, 5)], 'id': 'H05-1101.1'}, 'e2': {'word': 'probabilistic translation models', 'word_index': [(8, 10)], 'id': 'H05-1101.2'}, 'entity_replacement': {'4:5': 'ENTITY', '8:10': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation .
These models can be viewed as pairs of probabilistic context-free grammars working in a &apos;synchronous&apos; way.	probabilistic context-free grammars	models	model-feature	{'e1': {'word': 'probabilistic context-free grammars', 'word_index': [(8, 12)], 'id': 'H05-1101.5'}, 'e2': {'word': 'models', 'word_index': [(1, 1)], 'id': 'H05-1101.4'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '8:12': 'ENTITY'}}	These models can be viewed as pairs of probabilistic context - free grammars working in a &apos ; synchronous&apos ; way .
Two hardness results for the class NP are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature.	NP	hardness	model-feature	{'e1': {'word': 'NP', 'word_index': [(6, 6)], 'id': 'H05-1101.7'}, 'e2': {'word': 'hardness', 'word_index': [(1, 1)], 'id': 'H05-1101.6'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '6:6': 'ENTITY', '13:17': 'ENTITYUNRELATED'}}	Two hardness results for the class NP are reported , along with an exponential time lower - bound for certain classes of algorithms that are currently used in the literature .
 Automatic evaluation metrics for Machine Translation (MT) systems , such as BLEU or NIST , are now well established.	evaluation metrics	Machine Translation (MT) systems	usage	{'e1': {'word': 'evaluation metrics', 'word_index': [(1, 2)], 'id': 'I05-2014.1'}, 'e2': {'word': 'Machine Translation (MT) systems', 'word_index': [(4, 9)], 'id': 'I05-2014.2'}, 'entity_replacement': {'1:2': 'ENTITY', '4:9': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED'}}	Automatic evaluation metrics for Machine Translation ( MT ) systems , such as BLEU or NIST , are now well established .
Yet, they are scarcely used for the assessment of language pairs like English-Chinese or English-Japanese , because of the word segmentation problem .	word segmentation problem	English-Japanese	model-feature	{'e1': {'word': 'word segmentation problem', 'word_index': [(24, 26)], 'id': 'I05-2014.8'}, 'e2': {'word': 'English-Japanese', 'word_index': [(17, 19)], 'id': 'I05-2014.7'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '13:15': 'ENTITYUNRELATED', '17:19': 'ENTITYOTHER', '24:26': 'ENTITY'}}	Yet , they are scarcely used for the assessment of language pairs like English - Chinese or English - Japanese , because of the word segmentation problem .
This study establishes the equivalence between the standard use of BLEU in word n-grams and its application at the character level.	BLEU	word n-grams	usage	{'e1': {'word': 'BLEU', 'word_index': [(10, 10)], 'id': 'I05-2014.9'}, 'e2': {'word': 'word n-grams', 'word_index': [(12, 13)], 'id': 'I05-2014.10'}, 'entity_replacement': {'10:10': 'ENTITY', '12:13': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED'}}	This study establishes the equivalence between the standard use of BLEU in word n-grams and its application at the character level .
The use of BLEU at the character level eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with, for instance, statistical MT systems which usually segment their outputs .	BLEU	character	usage	{'e1': {'word': 'BLEU', 'word_index': [(3, 3)], 'id': 'I05-2014.12'}, 'e2': {'word': 'character', 'word_index': [(6, 6)], 'id': 'I05-2014.13'}, 'entity_replacement': {'3:3': 'ENTITY', '6:6': 'ENTITYOTHER', '10:12': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '31:33': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED'}}	The use of BLEU at the character level eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , statistical MT systems which usually segment their outputs .
The method allows a user to explore a model of syntax-based statistical machine translation (MT) , to understand the model &apos;s strengths and weaknesses, and to compare it to other MT systems .	model	MT systems	compare	{'e1': {'word': 'model', 'word_index': [(23, 23)], 'id': 'P05-3025.6'}, 'e2': {'word': 'MT systems', 'word_index': [(36, 37)], 'id': 'P05-3025.7'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '10:18': 'ENTITYUNRELATED', '23:23': 'ENTITY', '36:37': 'ENTITYOTHER'}}	The method allows a user to explore a model of syntax - based statistical machine translation ( MT ) , to understand the model &apos ;s strengths and weaknesses , and to compare it to other MT systems .
Using this visualization method , we can find and address conceptual and practical problems in an MT system .	visualization method	MT system	usage	{'e1': {'word': 'visualization method', 'word_index': [(2, 3)], 'id': 'P05-3025.8'}, 'e2': {'word': 'MT system', 'word_index': [(16, 17)], 'id': 'P05-3025.9'}, 'entity_replacement': {'2:3': 'ENTITY', '16:17': 'ENTITYOTHER'}}	Using this visualization method , we can find and address conceptual and practical problems in an MT system .
 In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation (SMT) but which have not been addressed satisfactorily by the SMT research community .	SMT research community	Statistical Machine Translation (SMT)	topic	{'e1': {'word': 'SMT research community', 'word_index': [(30, 32)], 'id': 'E06-1004.2'}, 'e2': {'word': 'Statistical Machine Translation (SMT)', 'word_index': [(15, 20)], 'id': 'E06-1004.1'}, 'entity_replacement': {'15:20': 'ENTITYOTHER', '30:32': 'ENTITY'}}	In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation ( SMT ) but which have not been addressed satisfactorily by the SMT research community .
Over the last decade, a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT .	computational complexity	SMT	model-feature	{'e1': {'word': 'computational complexity', 'word_index': [(22, 23)], 'id': 'E06-1004.4'}, 'e2': {'word': 'SMT', 'word_index': [(31, 31)], 'id': 'E06-1004.5'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '22:23': 'ENTITY', '31:31': 'ENTITYOTHER'}}	Over the last decade , a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT .
Since it is unlikely that there exists a polynomial time solution for any of these hard problems (unless P = NP and P#P = P ), our results highlight and justify the need for developing polynomial time approximations for these computations.	polynomial time solution	hard problems	usage	{'e1': {'word': 'polynomial time solution', 'word_index': [(8, 10)], 'id': 'E06-1004.10'}, 'e2': {'word': 'hard problems', 'word_index': [(15, 16)], 'id': 'E06-1004.11'}, 'entity_replacement': {'8:10': 'ENTITY', '15:16': 'ENTITYOTHER', '19:21': 'ENTITYUNRELATED', '23:25': 'ENTITYUNRELATED', '37:39': 'ENTITYUNRELATED'}}	Since it is unlikely that there exists a polynomial time solution for any of these hard problems ( unless P = NP and P#P = P ) , our results highlight and justify the need for developing polynomial time approximations for these computations .
We investigate that claim by adopting a simple MT-based paraphrasing technique and evaluating QA system performance on paraphrased questions .	MT-based paraphrasing technique	QA system	usage	{'e1': {'word': 'MT-based paraphrasing technique', 'word_index': [(8, 12)], 'id': 'N06-2009.5'}, 'e2': {'word': 'QA system', 'word_index': [(15, 16)], 'id': 'N06-2009.6'}, 'entity_replacement': {'8:12': 'ENTITY', '15:16': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED'}}	We investigate that claim by adopting a simple MT - based paraphrasing technique and evaluating QA system performance on paraphrased questions .
 There are several approaches that model information extraction as a token classification task , using various tagging strategies to combine multiple tokens .	tagging strategies	token classification task	usage	{'e1': {'word': 'tagging strategies', 'word_index': [(16, 17)], 'id': 'N06-2038.3'}, 'e2': {'word': 'token classification task', 'word_index': [(10, 12)], 'id': 'N06-2038.2'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '10:12': 'ENTITYOTHER', '16:17': 'ENTITY', '21:21': 'ENTITYUNRELATED'}}	There are several approaches that model information extraction as a token classification task , using various tagging strategies to combine multiple tokens .
InfoMagnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining .	InfoMagnets	exploratory corpus analysis	usage	{'e1': {'word': 'InfoMagnets', 'word_index': [(0, 1)], 'id': 'N06-4001.3'}, 'e2': {'word': 'exploratory corpus analysis', 'word_index': [(5, 7)], 'id': 'N06-4001.4'}, 'entity_replacement': {'0:1': 'ENTITY', '5:7': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED'}}	Info Magnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining .
As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains: tutorial dialogue (Kumar et al., submitted) and on-line communities (Arguello et al., 2006).	behavioral patterns	tutorial dialogue 	part_whole	{'e1': {'word': 'behavioral patterns', 'word_index': [(23, 24)], 'id': 'N06-4001.7'}, 'e2': {'word': 'tutorial dialogue ', 'word_index': [(30, 31)], 'id': 'N06-4001.8'}, 'entity_replacement': {'21:21': 'ENTITYUNRELATED', '23:24': 'ENTITY', '30:31': 'ENTITYOTHER', '40:43': 'ENTITYUNRELATED'}}	As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains : tutorial dialogue ( Kumar et al. , submitted ) and on - line communities ( Arguello et al. , 2006 ) .
As an educational tool , it has been used as part of a unit on protocol analysis in an Educational Research Methods course .	educational tool	protocol analysis	usage	{'e1': {'word': 'educational tool', 'word_index': [(2, 3)], 'id': 'N06-4001.10'}, 'e2': {'word': 'protocol analysis', 'word_index': [(15, 16)], 'id': 'N06-4001.11'}, 'entity_replacement': {'2:3': 'ENTITY', '15:16': 'ENTITYOTHER', '19:22': 'ENTITYUNRELATED'}}	As an educational tool , it has been used as part of a unit on protocol analysis in an Educational Research Methods course .
The polarization of the objects of the elementary structures controls the saturation of the final structure .	polarization	elementary structures	usage	{'e1': {'word': 'polarization', 'word_index': [(1, 1)], 'id': 'P06-1018.7'}, 'e2': {'word': 'elementary structures', 'word_index': [(7, 8)], 'id': 'P06-1018.8'}, 'entity_replacement': {'1:1': 'ENTITY', '7:8': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED'}}	The polarization of the objects of the elementary structures controls the saturation of the final structure .
The polarization of the objects of the elementary structures controls the saturation of the final structure .	saturation	structure	model-feature	{'e1': {'word': 'saturation', 'word_index': [(11, 11)], 'id': 'P06-1018.9'}, 'e2': {'word': 'structure', 'word_index': [(15, 15)], 'id': 'P06-1018.10'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '11:11': 'ENTITY', '15:15': 'ENTITYOTHER'}}	The polarization of the objects of the elementary structures controls the saturation of the final structure .
 This paper examines what kind of similarity between words can be represented by what kind of word vectors in the vector space model .	word vectors	similarity	model-feature	{'e1': {'word': 'word vectors', 'word_index': [(16, 17)], 'id': 'P06-2110.3'}, 'e2': {'word': 'similarity', 'word_index': [(6, 6)], 'id': 'P06-2110.1'}, 'entity_replacement': {'6:6': 'ENTITYOTHER', '8:8': 'ENTITYUNRELATED', '16:17': 'ENTITY', '20:22': 'ENTITYUNRELATED'}}	This paper examines what kind of similarity between words can be represented by what kind of word vectors in the vector space model .
Through two experiments, three methods for constructing word vectors , i.e., LSA-based, cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of similarity , i.e., taxonomic similarity and associative similarity .	LSA-based, cooccurrence-based and dictionary-based methods	similarity	usage	{'e1': {'word': 'LSA-based, cooccurrence-based and dictionary-based methods', 'word_index': [(13, 24)], 'id': 'P06-2110.6'}, 'e2': {'word': 'similarity', 'word_index': [(38, 38)], 'id': 'P06-2110.7'}, 'entity_replacement': {'5:9': 'ENTITYUNRELATED', '13:24': 'ENTITY', '38:38': 'ENTITYOTHER', '42:43': 'ENTITYUNRELATED', '45:46': 'ENTITYUNRELATED'}}	Through two experiments , three methods for constructing word vectors , i.e. , LSA - based , cooccurrence - based and dictionary - based methods , were compared in terms of the ability to represent two kinds of similarity , i.e. , taxonomic similarity and associative similarity .
The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the LSA-based and the cooccurrence-based word vectors better reflect associative similarity .	dictionary-based word vectors	taxonomic similarity	usage	{'e1': {'word': 'dictionary-based word vectors', 'word_index': [(8, 12)], 'id': 'P06-2110.10'}, 'e2': {'word': 'taxonomic similarity', 'word_index': [(15, 16)], 'id': 'P06-2110.11'}, 'entity_replacement': {'8:12': 'ENTITY', '15:16': 'ENTITYOTHER', '20:29': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	The result of the comparison was that the dictionary - based word vectors better reflect taxonomic similarity , while the LSA - based and the cooccurrence - based word vectors better reflect associative similarity .
The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the LSA-based and the cooccurrence-based word vectors better reflect associative similarity .	LSA-based and the cooccurrence-based word vectors	associative similarity	usage	{'e1': {'word': 'LSA-based and the cooccurrence-based word vectors', 'word_index': [(20, 29)], 'id': 'P06-2110.12'}, 'e2': {'word': 'associative similarity', 'word_index': [(32, 33)], 'id': 'P06-2110.13'}, 'entity_replacement': {'8:12': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '20:29': 'ENTITY', '32:33': 'ENTITYOTHER'}}	The result of the comparison was that the dictionary - based word vectors better reflect taxonomic similarity , while the LSA - based and the cooccurrence - based word vectors better reflect associative similarity .
In this paper, events are defined as event terms and associated event elements .	event terms	events	model-feature	{'e1': {'word': 'event terms', 'word_index': [(8, 9)], 'id': 'P06-3007.3'}, 'e2': {'word': 'events', 'word_index': [(4, 4)], 'id': 'P06-3007.2'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '8:9': 'ENTITY', '11:13': 'ENTITYUNRELATED'}}	In this paper , events are defined as event terms and associated event elements .
With relevant approach, we identify important contents by PageRank algorithm on the event map constructed from documents .	documents	event map	usage	{'e1': {'word': 'documents', 'word_index': [(17, 17)], 'id': 'P06-3007.9'}, 'e2': {'word': 'event map', 'word_index': [(13, 14)], 'id': 'P06-3007.8'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '17:17': 'ENTITY'}}	With relevant approach , we identify important contents by PageRank algorithm on the event map constructed from documents .
 This paper describes FERRET , an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments.	interactive question-answering (Q/A) system	automatic Q/A	usage	{'e1': {'word': 'interactive question-answering (Q/A) system', 'word_index': [(6, 13)], 'id': 'P06-4007.2'}, 'e2': {'word': 'automatic Q/A', 'word_index': [(21, 22)], 'id': 'P06-4007.3'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:13': 'ENTITY', '21:22': 'ENTITYOTHER'}}	This paper describes FERRET , an interactive question - answering ( Q/A ) system designed to address the challenges of integrating automatic Q/A applications into real - world environments .
FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions (and answers ) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario.	predictive questioning	Q/A	usage	{'e1': {'word': 'predictive questioning', 'word_index': [(9, 10)], 'id': 'P06-4007.6'}, 'e2': {'word': 'Q/A', 'word_index': [(6, 6)], 'id': 'P06-4007.5'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '6:6': 'ENTITYOTHER', '9:10': 'ENTITY', '16:16': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions ( and answers ) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario .
 This paper introduces a method for computational analysis of move structures in abstracts of research articles .	abstracts	research articles	part_whole	{'e1': {'word': 'abstracts', 'word_index': [(12, 12)], 'id': 'P06-4011.2'}, 'e2': {'word': 'research articles', 'word_index': [(14, 15)], 'id': 'P06-4011.3'}, 'entity_replacement': {'6:10': 'ENTITYUNRELATED', '12:12': 'ENTITY', '14:15': 'ENTITYOTHER'}}	This paper introduces a method for computational analysis of move structures in abstracts of research articles .
In our approach, sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions .	sentences	abstract	part_whole	{'e1': {'word': 'sentences', 'word_index': [(4, 4)], 'id': 'P06-4011.4'}, 'e2': {'word': 'abstract', 'word_index': [(8, 8)], 'id': 'P06-4011.5'}, 'entity_replacement': {'4:4': 'ENTITY', '8:8': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED'}}	In our approach , sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions .
The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves .	abstracts	Web	part_whole	{'e1': {'word': 'abstracts', 'word_index': [(9, 9)], 'id': 'P06-4011.8'}, 'e2': {'word': 'Web', 'word_index': [(12, 12)], 'id': 'P06-4011.9'}, 'entity_replacement': {'9:9': 'ENTITY', '12:12': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves .
The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves .	language model	abstract moves	model-feature	{'e1': {'word': 'language model', 'word_index': [(16, 17)], 'id': 'P06-4011.10'}, 'e2': {'word': 'abstract moves', 'word_index': [(19, 20)], 'id': 'P06-4011.11'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '16:17': 'ENTITY', '19:20': 'ENTITYOTHER'}}	The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves .
We also present a prototype concordancer , CARE , which exploits the move-tagged abstracts for digital learning .	move-tagged abstracts	digital learning	usage	{'e1': {'word': 'move-tagged abstracts', 'word_index': [(12, 13)], 'id': 'P06-4011.14'}, 'e2': {'word': 'digital learning', 'word_index': [(15, 16)], 'id': 'P06-4011.15'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '12:13': 'ENTITY', '15:16': 'ENTITYOTHER'}}	We also present a prototype concordancer , CARE , which exploits the move-tagged abstracts for digital learning .
 The LOGON MT demonstrator assembles independently valuable general-purpose NLP components into a machine translation pipeline that capitalizes on output quality .	general-purpose NLP components	machine translation pipeline	part_whole	{'e1': {'word': 'general-purpose NLP components', 'word_index': [(7, 11)], 'id': 'P06-4014.2'}, 'e2': {'word': 'machine translation pipeline', 'word_index': [(14, 16)], 'id': 'P06-4014.3'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '7:11': 'ENTITY', '14:16': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	The LOGON MT demonstrator assembles independently valuable general - purpose NLP components into a machine translation pipeline that capitalizes on output quality .
In this format, developed by the LNR research group at The University of California at San Diego, verbs are represented as interconnected sets of subpredicates .	subpredicates	verbs	model-feature	{'e1': {'word': 'subpredicates', 'word_index': [(26, 26)], 'id': 'T78-1001.3'}, 'e2': {'word': 'verbs', 'word_index': [(19, 19)], 'id': 'T78-1001.2'}, 'entity_replacement': {'19:19': 'ENTITYOTHER', '26:26': 'ENTITY'}}	In this format , developed by the LNR research group at The University of California at San Diego , verbs are represented as interconnected sets of subpredicates .
These subpredicates may be thought of as the almost inevitable inferences that a listener makes when a verb is used in a sentence .	verb	sentence	part_whole	{'e1': {'word': 'verb', 'word_index': [(17, 17)], 'id': 'T78-1001.7'}, 'e2': {'word': 'sentence', 'word_index': [(22, 22)], 'id': 'T78-1001.8'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '17:17': 'ENTITY', '22:22': 'ENTITYOTHER'}}	These subpredicates may be thought of as the almost inevitable inferences that a listener makes when a verb is used in a sentence .
They confer a meaning structure on the sentence in which the verb is used.	verb	sentence	part_whole	{'e1': {'word': 'verb', 'word_index': [(11, 11)], 'id': 'T78-1001.11'}, 'e2': {'word': 'sentence', 'word_index': [(7, 7)], 'id': 'T78-1001.10'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:7': 'ENTITYOTHER', '11:11': 'ENTITY'}}	They confer a meaning structure on the sentence in which the verb is used .
 The paper outlines a computational theory of human plausible reasoning constructed from analysis of people&apos;s answers to everyday questions.	computational theory	human plausible reasoning	model-feature	{'e1': {'word': 'computational theory', 'word_index': [(4, 5)], 'id': 'T78-1028.1'}, 'e2': {'word': 'human plausible reasoning', 'word_index': [(7, 9)], 'id': 'T78-1028.2'}, 'entity_replacement': {'4:5': 'ENTITY', '7:9': 'ENTITYOTHER'}}	The paper outlines a computational theory of human plausible reasoning constructed from analysis of people&apos ;s answers to everyday questions .
Like logic , the theory is expressed in a content-independent formalism .	content-independent formalism	theory	model-feature	{'e1': {'word': 'content-independent formalism', 'word_index': [(9, 12)], 'id': 'T78-1028.5'}, 'e2': {'word': 'theory', 'word_index': [(4, 4)], 'id': 'T78-1028.4'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:4': 'ENTITYOTHER', '9:12': 'ENTITY'}}	Like logic , the theory is expressed in a content - independent formalism .
Unlike logic , the theory specifies how different information in memory affects the certainty of the conclusions drawn.	logic	theory	compare	{'e1': {'word': 'logic', 'word_index': [(1, 1)], 'id': 'T78-1028.6'}, 'e2': {'word': 'theory', 'word_index': [(4, 4)], 'id': 'T78-1028.7'}, 'entity_replacement': {'1:1': 'ENTITY', '4:4': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED'}}	Unlike logic , the theory specifies how different information in memory affects the certainty of the conclusions drawn .
The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person&apos;s knowledge about his own knowledge.	dimensionalized space	theory	part_whole	{'e1': {'word': 'dimensionalized space', 'word_index': [(5, 6)], 'id': 'T78-1028.10'}, 'e2': {'word': 'theory', 'word_index': [(1, 1)], 'id': 'T78-1028.9'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '5:6': 'ENTITY', '9:10': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '20:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta - inference types where the inference depends on the person&apos ;s knowledge about his own knowledge .
The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person&apos;s knowledge about his own knowledge.	certainty conditions	inference types	model-feature	{'e1': {'word': 'certainty conditions', 'word_index': [(13, 14)], 'id': 'T78-1028.12'}, 'e2': {'word': 'inference types', 'word_index': [(9, 10)], 'id': 'T78-1028.11'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '9:10': 'ENTITYOTHER', '13:14': 'ENTITY', '20:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta - inference types where the inference depends on the person&apos ;s knowledge about his own knowledge .
The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person&apos;s knowledge about his own knowledge.	meta-inference types	inference	model-feature	{'e1': {'word': 'meta-inference types', 'word_index': [(20, 21)], 'id': 'T78-1028.13'}, 'e2': {'word': 'inference', 'word_index': [(24, 24)], 'id': 'T78-1028.14'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '20:21': 'ENTITY', '24:24': 'ENTITYOTHER'}}	The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person&apos ;s knowledge about his own knowledge .
Path-based inference rules may be written using a binary relational calculus notation .	binary relational calculus notation	Path-based inference rules	usage	{'e1': {'word': 'binary relational calculus notation', 'word_index': [(8, 11)], 'id': 'T78-1031.10'}, 'e2': {'word': 'Path-based inference rules', 'word_index': [(0, 2)], 'id': 'T78-1031.9'}, 'entity_replacement': {'0:2': 'ENTITYOTHER', '8:11': 'ENTITY'}}	Path-based inference rules may be written using a binary relational calculus notation .
Node-based inference allows a structure of nodes to be inferred from the existence of an instance of a pattern of node structures .	node structures	Node-based inference	usage	{'e1': {'word': 'node structures', 'word_index': [(22, 23)], 'id': 'T78-1031.14'}, 'e2': {'word': 'Node-based inference', 'word_index': [(0, 3)], 'id': 'T78-1031.11'}, 'entity_replacement': {'0:3': 'ENTITYOTHER', '6:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '22:23': 'ENTITY'}}	Node - based inference allows a structure of nodes to be inferred from the existence of an instance of a pattern of node structures .
Node-based inference rules can be constructed in a semantic network using a variant of a predicate calculus notation .	predicate calculus notation	Node-based inference rules	usage	{'e1': {'word': 'predicate calculus notation', 'word_index': [(16, 18)], 'id': 'T78-1031.17'}, 'e2': {'word': 'Node-based inference rules', 'word_index': [(0, 3)], 'id': 'T78-1031.15'}, 'entity_replacement': {'0:3': 'ENTITYOTHER', '9:10': 'ENTITYUNRELATED', '16:18': 'ENTITY'}}	Node- based inference rules can be constructed in a semantic network using a variant of a predicate calculus notation .
Path-based inference is more efficient, while node-based inference is more general.	Path-based inference	node-based inference	compare	{'e1': {'word': 'Path-based inference', 'word_index': [(0, 1)], 'id': 'T78-1031.18'}, 'e2': {'word': 'node-based inference', 'word_index': [(7, 10)], 'id': 'T78-1031.19'}, 'entity_replacement': {'0:1': 'ENTITY', '7:10': 'ENTITYOTHER'}}	Path-based inference is more efficient , while node - based inference is more general .
Applications of path-based inference rules to the representation of the extensional equivalence of intensional concepts , and to the explication of inheritance in hierarchies are sketched.	inheritance	hierarchies	model-feature	{'e1': {'word': 'inheritance', 'word_index': [(23, 23)], 'id': 'T78-1031.24'}, 'e2': {'word': 'hierarchies', 'word_index': [(25, 25)], 'id': 'T78-1031.25'}, 'entity_replacement': {'2:6': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '23:23': 'ENTITY', '25:25': 'ENTITYOTHER'}}	Applications of path - based inference rules to the representation of the extensional equivalence of intensional concepts , and to the explication of inheritance in hierarchies are sketched .
By using commands or rules which are defined to facilitate the construction of format expected or some mathematical expressions , elaborate and pretty documents can be successfully obtained.	rules	mathematical expressions	usage	{'e1': {'word': 'rules', 'word_index': [(4, 4)], 'id': 'C80-1039.7'}, 'e2': {'word': 'mathematical expressions', 'word_index': [(17, 18)], 'id': 'C80-1039.8'}, 'entity_replacement': {'4:4': 'ENTITY', '17:18': 'ENTITYOTHER'}}	By using commands or rules which are defined to facilitate the construction of format expected or some mathematical expressions , elaborate and pretty documents can be successfully obtained .
 An attempt has been made to use an Augmented Transition Network as a procedural dialog model .	Augmented Transition Network	dialog model	usage	{'e1': {'word': 'Augmented Transition Network', 'word_index': [(8, 10)], 'id': 'C80-1073.1'}, 'e2': {'word': 'dialog model', 'word_index': [(14, 15)], 'id': 'C80-1073.2'}, 'entity_replacement': {'8:10': 'ENTITY', '14:15': 'ENTITYOTHER'}}	An attempt has been made to use an Augmented Transition Network as a procedural dialog model .
The development of such a model appears to be important in several respects: as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs .	conversation analysis	dialog schemata	topic	{'e1': {'word': 'conversation analysis', 'word_index': [(28, 29)], 'id': 'C80-1073.5'}, 'e2': {'word': 'dialog schemata', 'word_index': [(23, 24)], 'id': 'C80-1073.4'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER', '28:29': 'ENTITY', '39:42': 'ENTITYUNRELATED', '50:51': 'ENTITYUNRELATED', '54:55': 'ENTITYUNRELATED', '59:65': 'ENTITYUNRELATED'}}	The development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task -oriented and goal - directed dialogs .
A standard ATN should be further developed in order to account for the verbal interactions of task-oriented dialogs .	verbal interactions	task-oriented dialogs	part_whole	{'e1': {'word': 'verbal interactions', 'word_index': [(13, 14)], 'id': 'C80-1073.11'}, 'e2': {'word': 'task-oriented dialogs', 'word_index': [(16, 19)], 'id': 'C80-1073.12'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '13:14': 'ENTITY', '16:19': 'ENTITYOTHER'}}	A standard ATN should be further developed in order to account for the verbal interactions of task - oriented dialogs .
 Interpreting metaphors is an integral and inescapable process in human understanding of natural language .	metaphors	human understanding of natural language	part_whole	{'e1': {'word': 'metaphors', 'word_index': [(1, 1)], 'id': 'P80-1004.1'}, 'e2': {'word': 'human understanding of natural language', 'word_index': [(9, 13)], 'id': 'P80-1004.2'}, 'entity_replacement': {'1:1': 'ENTITY', '9:13': 'ENTITYOTHER'}}	Interpreting metaphors is an integral and inescapable process in human understanding of natural language .
This paper discusses a method of analyzing metaphors based on the existence of a small number of generalized metaphor mappings .	generalized metaphor mappings	method of analyzing metaphors	usage	{'e1': {'word': 'generalized metaphor mappings', 'word_index': [(17, 19)], 'id': 'P80-1004.4'}, 'e2': {'word': 'method of analyzing metaphors', 'word_index': [(4, 7)], 'id': 'P80-1004.3'}, 'entity_replacement': {'4:7': 'ENTITYOTHER', '17:19': 'ENTITY'}}	This paper discusses a method of analyzing metaphors based on the existence of a small number of generalized metaphor mappings .
Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component .	recognition network	generalized metaphor	part_whole	{'e1': {'word': 'recognition network', 'word_index': [(5, 6)], 'id': 'P80-1004.6'}, 'e2': {'word': 'generalized metaphor', 'word_index': [(1, 2)], 'id': 'P80-1004.5'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '5:6': 'ENTITY', '9:10': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '18:20': 'ENTITYUNRELATED'}}	Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component .
 Current natural language interfaces have concentrated largely on determining the literal meaning of input from their users .	meaning	input	model-feature	{'e1': {'word': 'meaning', 'word_index': [(11, 11)], 'id': 'P80-1019.2'}, 'e2': {'word': 'input', 'word_index': [(13, 13)], 'id': 'P80-1019.3'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '11:11': 'ENTITY', '13:13': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	Current natural language interfaces have concentrated largely on determining the literal meaning of input from their users .
While such decoding is an essential underpinning, much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous non-literal aspects of communication , such as robust communication procedures .	non-literal aspects of communication	decoding	usage	{'e1': {'word': 'non-literal aspects of communication', 'word_index': [(27, 30)], 'id': 'P80-1019.7'}, 'e2': {'word': 'decoding', 'word_index': [(2, 2)], 'id': 'P80-1019.5'}, 'entity_replacement': {'2:2': 'ENTITYOTHER', '13:15': 'ENTITYUNRELATED', '27:30': 'ENTITY', '35:36': 'ENTITYUNRELATED'}}	While such decoding is an essential underpinning , much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous non-literal aspects of communication , such as robust communication procedures .
This paper defends that view, but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful personal computers with integral graphics displays offers techniques superior to those of humans for these aspects, while still satisfying human communication needs .	graphics displays	personal computers	part_whole	{'e1': {'word': 'graphics displays', 'word_index': [(39, 40)], 'id': 'P80-1019.11'}, 'e2': {'word': 'personal computers', 'word_index': [(35, 36)], 'id': 'P80-1019.10'}, 'entity_replacement': {'24:27': 'ENTITYUNRELATED', '35:36': 'ENTITYOTHER', '39:40': 'ENTITY', '55:57': 'ENTITYUNRELATED'}}	This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful personal computers with integral graphics displays offers techniques superior to those of humans for these aspects , while still satisfying human communication needs .
The paper proposes interfaces based on a judicious mixture of these techniques and the still valuable methods of more traditional natural language interfaces.	interfaces	natural language interfaces	compare	{'e1': {'word': 'interfaces', 'word_index': [(3, 3)], 'id': 'P80-1019.13'}, 'e2': {'word': 'natural language interfaces', 'word_index': [(20, 22)], 'id': 'P80-1019.14'}, 'entity_replacement': {'3:3': 'ENTITY', '20:22': 'ENTITYOTHER'}}	The paper proposes interfaces based on a judicious mixture of these techniques and the still valuable methods of more traditional natural language interfaces .
We go, on to describe FlexP , a bottom-up pattern-matching parser that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited-domain computer system.	bottom-up pattern-matching parser	restricted natural language	usage	{'e1': {'word': 'bottom-up pattern-matching parser', 'word_index': [(9, 14)], 'id': 'P80-1026.8'}, 'e2': {'word': 'restricted natural language', 'word_index': [(26, 28)], 'id': 'P80-1026.9'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:14': 'ENTITY', '26:28': 'ENTITYOTHER'}}	We go , on to describe FlexP , a bottom - up pattern- matching parser that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited - domain computer system .
 This paper proposes a series of modifications to the left corner parsing algorithm for context-free grammars .	left corner parsing algorithm	context-free grammars	usage	{'e1': {'word': 'left corner parsing algorithm', 'word_index': [(9, 12)], 'id': 'C82-1054.1'}, 'e2': {'word': 'context-free grammars', 'word_index': [(14, 17)], 'id': 'C82-1054.2'}, 'entity_replacement': {'9:12': 'ENTITY', '14:17': 'ENTITYOTHER'}}	This paper proposes a series of modifications to the left corner parsing algorithm for context - free grammars .
It is argued that the resulting algorithm is both efficient and flexible and is, therefore, a good choice for the parser used in a natural language interface .	parser	natural language interface	usage	{'e1': {'word': 'parser', 'word_index': [(22, 22)], 'id': 'C82-1054.3'}, 'e2': {'word': 'natural language interface', 'word_index': [(26, 28)], 'id': 'C82-1054.4'}, 'entity_replacement': {'22:22': 'ENTITY', '26:28': 'ENTITYOTHER'}}	It is argued that the resulting algorithm is both efficient and flexible and is , therefore , a good choice for the parser used in a natural language interface .
The system is implemented entirely in Prolog , a programming language based on logic .	logic	programming language	usage	{'e1': {'word': 'logic', 'word_index': [(13, 13)], 'id': 'J82-3002.6'}, 'e2': {'word': 'programming language', 'word_index': [(9, 10)], 'id': 'J82-3002.5'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:10': 'ENTITYOTHER', '13:13': 'ENTITY'}}	The system is implemented entirely in Prolog , a programming language based on logic .
With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic .	Chat-80	English questions	usage	{'e1': {'word': 'Chat-80', 'word_index': [(13, 14)], 'id': 'J82-3002.9'}, 'e2': {'word': 'English questions', 'word_index': [(16, 17)], 'id': 'J82-3002.10'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '13:14': 'ENTITY', '16:17': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED', '21:23': 'ENTITYUNRELATED'}}	With the aid of a logic- based grammar formalism called extraposition grammars , Chat -80 translates English questions into the Prolog subset of logic .
With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic .	subset of logic	Prolog	part_whole	{'e1': {'word': 'subset of logic', 'word_index': [(21, 23)], 'id': 'J82-3002.12'}, 'e2': {'word': 'Prolog', 'word_index': [(20, 20)], 'id': 'J82-3002.11'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '20:20': 'ENTITYOTHER', '21:23': 'ENTITY'}}	With the aid of a logic- based grammar formalism called extraposition grammars , Chat -80 translates English questions into the Prolog subset of logic .
query optimisation in a relational database .	query optimisation	relational database	usage	{'e1': {'word': 'query optimisation', 'word_index': [(0, 1)], 'id': 'J82-3002.16'}, 'e2': {'word': 'relational database', 'word_index': [(4, 5)], 'id': 'J82-3002.17'}, 'entity_replacement': {'0:1': 'ENTITY', '4:5': 'ENTITYOTHER'}}	query optimisation in a relational database .
However, a great deal of natural language texts e.g., memos , rough drafts , conversation transcripts etc., have features that differ significantly from neat texts , posing special problems for readers, such as misspelled words , missing words , poor syntactic construction , missing periods , etc.	conversation transcripts	neat texts	compare	{'e1': {'word': 'conversation transcripts', 'word_index': [(16, 17)], 'id': 'P82-1035.8'}, 'e2': {'word': 'neat texts', 'word_index': [(26, 27)], 'id': 'P82-1035.9'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITY', '26:27': 'ENTITYOTHER', '37:38': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED', '43:45': 'ENTITYUNRELATED', '47:48': 'ENTITYUNRELATED'}}	However , a great deal of natural language texts e.g. , memos , rough drafts , conversation transcripts etc. , have features that differ significantly from neat texts , posing special problems for readers , such as misspelled words , missing words , poor syntactic construction , missing periods , etc.
Our solution to these problems is to make use of expectations , based both on knowledge of surface English and on world knowledge of the situation being described.	surface English	expectations	usage	{'e1': {'word': 'surface English', 'word_index': [(17, 18)], 'id': 'P82-1035.15'}, 'e2': {'word': 'expectations', 'word_index': [(10, 10)], 'id': 'P82-1035.14'}, 'entity_replacement': {'10:10': 'ENTITYOTHER', '17:18': 'ENTITY', '21:22': 'ENTITYUNRELATED'}}	Our solution to these problems is to make use of expectations , based both on knowledge of surface English and on world knowledge of the situation being described .
These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ).	context	unknown words	model-feature	{'e1': {'word': 'context', 'word_index': [(14, 14)], 'id': 'P82-1035.19'}, 'e2': {'word': 'unknown words', 'word_index': [(11, 12)], 'id': 'P82-1035.18'}, 'entity_replacement': {'1:4': 'ENTITYUNRELATED', '11:12': 'ENTITYOTHER', '14:14': 'ENTITY', '19:21': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '41:41': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED'}}	These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word - senses of words with multiple meanings ( ambiguity ) , fill in missing words ( ellipsis ) , and resolve referents ( anaphora ) .
These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ).	word-senses	words with multiple meanings	model-feature	{'e1': {'word': 'word-senses', 'word_index': [(19, 21)], 'id': 'P82-1035.20'}, 'e2': {'word': 'words with multiple meanings', 'word_index': [(23, 26)], 'id': 'P82-1035.21'}, 'entity_replacement': {'1:4': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '19:21': 'ENTITY', '23:26': 'ENTITYOTHER', '28:28': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '41:41': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED'}}	These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word - senses of words with multiple meanings ( ambiguity ) , fill in missing words ( ellipsis ) , and resolve referents ( anaphora ) .
These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ).	missing words	ellipsis	part_whole	{'e1': {'word': 'missing words', 'word_index': [(33, 34)], 'id': 'P82-1035.23'}, 'e2': {'word': 'ellipsis', 'word_index': [(36, 36)], 'id': 'P82-1035.24'}, 'entity_replacement': {'1:4': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '33:34': 'ENTITY', '36:36': 'ENTITYOTHER', '41:41': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED'}}	These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word - senses of words with multiple meanings ( ambiguity ) , fill in missing words ( ellipsis ) , and resolve referents ( anaphora ) .
These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ).	referents	anaphora	part_whole	{'e1': {'word': 'referents', 'word_index': [(41, 41)], 'id': 'P82-1035.25'}, 'e2': {'word': 'anaphora', 'word_index': [(43, 43)], 'id': 'P82-1035.26'}, 'entity_replacement': {'1:4': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '41:41': 'ENTITY', '43:43': 'ENTITYOTHER'}}	These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word - senses of words with multiple meanings ( ambiguity ) , fill in missing words ( ellipsis ) , and resolve referents ( anaphora ) .
This method of using expectations to aid the understanding of scruffy texts has been incorporated into a working computer program called NOMAD , which understands scruffy texts in the domain of Navy messages.	expectations	scruffy texts	usage	{'e1': {'word': 'expectations', 'word_index': [(4, 4)], 'id': 'P82-1035.27'}, 'e2': {'word': 'scruffy texts', 'word_index': [(10, 11)], 'id': 'P82-1035.28'}, 'entity_replacement': {'4:4': 'ENTITY', '10:11': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED'}}	This method of using expectations to aid the understanding of scruffy texts has been incorporated into a working computer program called NOMAD , which understands scruffy texts in the domain of Navy messages .
 This abstract describes a natural language system which deals usefully with ungrammatical input and describes some actual and potential applications of it in computer aided second language learning .	natural language system	ungrammatical input	usage	{'e1': {'word': 'natural language system', 'word_index': [(4, 6)], 'id': 'P84-1020.1'}, 'e2': {'word': 'ungrammatical input', 'word_index': [(11, 12)], 'id': 'P84-1020.2'}, 'entity_replacement': {'4:6': 'ENTITY', '11:12': 'ENTITYOTHER', '23:27': 'ENTITYUNRELATED'}}	This abstract describes a natural language system which deals usefully with ungrammatical input and describes some actual and potential applications of it in computer aided second language learning .
For English-Japanese machine translation , the syntax directed approach is effective where the Heuristic Parsing Model (HPM) and the Syntactic Role System play important roles.	syntax directed approach	English-Japanese machine translation	usage	{'e1': {'word': 'syntax directed approach', 'word_index': [(8, 10)], 'id': 'P84-1034.5'}, 'e2': {'word': 'English-Japanese machine translation', 'word_index': [(1, 5)], 'id': 'P84-1034.4'}, 'entity_replacement': {'1:5': 'ENTITYOTHER', '8:10': 'ENTITY', '15:20': 'ENTITYUNRELATED', '23:25': 'ENTITYUNRELATED'}}	For English - Japanese machine translation , the syntax directed approach is effective where the Heuristic Parsing Model ( HPM ) and the Syntactic Role System play important roles .
For Japanese-English translation , the semantics directed approach is powerful where the Conceptual Dependency Diagram (CDD) and the Augmented Case Marker System (which is a kind of Semantic Role System ) play essential roles.	Japanese-English translation	semantics directed approach	usage	{'e1': {'word': 'Japanese-English translation', 'word_index': [(1, 2)], 'id': 'P84-1034.8'}, 'e2': {'word': 'semantics directed approach', 'word_index': [(5, 7)], 'id': 'P84-1034.9'}, 'entity_replacement': {'1:2': 'ENTITY', '5:7': 'ENTITYOTHER', '12:17': 'ENTITYUNRELATED', '20:23': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED'}}	For Japanese-English translation , the semantics directed approach is powerful where the Conceptual Dependency Diagram ( CDD ) and the Augmented Case Marker System ( which is a kind of Semantic Role System ) play essential roles .
Some examples of the difference between Japanese sentence structure and English sentence structure , which is vital to machine translation are also discussed together with various interesting ambiguities .	Japanese sentence structure	English sentence structure	compare	{'e1': {'word': 'Japanese sentence structure', 'word_index': [(6, 8)], 'id': 'P84-1034.13'}, 'e2': {'word': 'English sentence structure', 'word_index': [(10, 12)], 'id': 'P84-1034.14'}, 'entity_replacement': {'6:8': 'ENTITY', '10:12': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	Some examples of the difference between Japanese sentence structure and English sentence structure , which is vital to machine translation are also discussed together with various interesting ambiguities .
In this approach, the definitions of the structure and surface representation of domain entities are grouped together.	surface representation	domain entities	model-feature	{'e1': {'word': 'surface representation', 'word_index': [(10, 11)], 'id': 'P84-1047.3'}, 'e2': {'word': 'domain entities', 'word_index': [(13, 14)], 'id': 'P84-1047.4'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '10:11': 'ENTITY', '13:14': 'ENTITYOTHER'}}	In this approach , the definitions of the structure and surface representation of domain entities are grouped together .
In addition, it facilitates fragmentary recognition and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input .	multiple parsing strategies	recognition of extra-grammatical input	usage	{'e1': {'word': 'multiple parsing strategies', 'word_index': [(11, 13)], 'id': 'P84-1047.8'}, 'e2': {'word': 'recognition of extra-grammatical input', 'word_index': [(22, 25)], 'id': 'P84-1047.9'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '11:13': 'ENTITY', '22:25': 'ENTITYOTHER'}}	In addition , it facilitates fragmentary recognition and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input .
Representative samples from an entity-oriented language definition are presented, along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses .	control structure	entity-oriented parser	usage	{'e1': {'word': 'control structure', 'word_index': [(14, 15)], 'id': 'P84-1047.12'}, 'e2': {'word': 'entity-oriented parser', 'word_index': [(18, 21)], 'id': 'P84-1047.13'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '14:15': 'ENTITY', '18:21': 'ENTITYOTHER', '24:25': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED'}}	Representative samples from an entity -oriented language definition are presented , along with a control structure for an entity - oriented parser , some parsing strategies that use the control structure , and worked examples of parses .
Representative samples from an entity-oriented language definition are presented, along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses .	control structure	parsing strategies	usage	{'e1': {'word': 'control structure', 'word_index': [(29, 30)], 'id': 'P84-1047.15'}, 'e2': {'word': 'parsing strategies', 'word_index': [(24, 25)], 'id': 'P84-1047.14'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '18:21': 'ENTITYUNRELATED', '24:25': 'ENTITYOTHER', '29:30': 'ENTITY', '36:36': 'ENTITYUNRELATED'}}	Representative samples from an entity -oriented language definition are presented , along with a control structure for an entity - oriented parser , some parsing strategies that use the control structure , and worked examples of parses .
A parser incorporating the control structure and the parsing strategies is currently under implementation .	control structure	parser	part_whole	{'e1': {'word': 'control structure', 'word_index': [(4, 5)], 'id': 'P84-1047.18'}, 'e2': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'P84-1047.17'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:5': 'ENTITY', '8:9': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED'}}	A parser incorporating the control structure and the parsing strategies is currently under implementation .
An idea which underlies the theory described in this paper is that a disposition may be viewed as a proposition with implicit fuzzy quantifiers which are approximations to all and always, e.g., almost all, almost always, most, frequently, etc.	fuzzy quantifiers	proposition	part_whole	{'e1': {'word': 'fuzzy quantifiers', 'word_index': [(22, 23)], 'id': 'P84-1064.7'}, 'e2': {'word': 'proposition', 'word_index': [(19, 19)], 'id': 'P84-1064.6'}, 'entity_replacement': {'13:13': 'ENTITYUNRELATED', '19:19': 'ENTITYOTHER', '22:23': 'ENTITY'}}	An idea which underlies the theory described in this paper is that a disposition may be viewed as a proposition with implicit fuzzy quantifiers which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc.
For example, birds can fly may be interpreted as the result of suppressing the fuzzy quantifier most in the proposition most birds can fly.	fuzzy quantifier	proposition	part_whole	{'e1': {'word': 'fuzzy quantifier', 'word_index': [(15, 16)], 'id': 'P84-1064.8'}, 'e2': {'word': 'proposition', 'word_index': [(20, 20)], 'id': 'P84-1064.9'}, 'entity_replacement': {'15:16': 'ENTITY', '20:20': 'ENTITYOTHER'}}	For example , birds can fly may be interpreted as the result of suppressing the fuzzy quantifier most in the proposition most birds can fly .
Explicitation sets the stage for representing the meaning of a proposition through the use of test-score semantics (Zadeh, 1978, 1982).	meaning	proposition	model-feature	{'e1': {'word': 'meaning', 'word_index': [(7, 7)], 'id': 'P84-1064.15'}, 'e2': {'word': 'proposition', 'word_index': [(10, 10)], 'id': 'P84-1064.16'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '7:7': 'ENTITY', '10:10': 'ENTITYOTHER', '15:18': 'ENTITYUNRELATED'}}	Explicitation sets the stage for representing the meaning of a proposition through the use of test - score semantics ( Zadeh , 1978 , 1982 ) .
In this approach to semantics , the meaning of a proposition , p, is represented as a procedure which tests, scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to reasoning with dispositions which is based on the concept of a fuzzy syllogism .	meaning	proposition	model-feature	{'e1': {'word': 'meaning', 'word_index': [(7, 7)], 'id': 'P84-1064.19'}, 'e2': {'word': 'proposition', 'word_index': [(10, 10)], 'id': 'P84-1064.20'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '7:7': 'ENTITY', '10:10': 'ENTITYOTHER', '43:45': 'ENTITYUNRELATED', '54:55': 'ENTITYUNRELATED'}}	In this approach to semantics , the meaning of a proposition , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to reasoning with dispositions which is based on the concept of a fuzzy syllogism .
In this approach to semantics , the meaning of a proposition , p, is represented as a procedure which tests, scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to reasoning with dispositions which is based on the concept of a fuzzy syllogism .	fuzzy syllogism	reasoning with dispositions	usage	{'e1': {'word': 'fuzzy syllogism', 'word_index': [(54, 55)], 'id': 'P84-1064.22'}, 'e2': {'word': 'reasoning with dispositions', 'word_index': [(43, 45)], 'id': 'P84-1064.21'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '43:45': 'ENTITYOTHER', '54:55': 'ENTITY'}}	In this approach to semantics , the meaning of a proposition , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to reasoning with dispositions which is based on the concept of a fuzzy syllogism .
Syllogistic reasoning with dispositions has an important bearing on commonsense reasoning as well as on the management of uncertainty in expert systems .	Syllogistic reasoning with dispositions	commonsense reasoning	result	{'e1': {'word': 'Syllogistic reasoning with dispositions', 'word_index': [(0, 3)], 'id': 'P84-1064.23'}, 'e2': {'word': 'commonsense reasoning', 'word_index': [(9, 10)], 'id': 'P84-1064.24'}, 'entity_replacement': {'0:3': 'ENTITY', '9:10': 'ENTITYOTHER', '16:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED'}}	Syllogistic reasoning with dispositions has an important bearing on commonsense reasoning as well as on the management of uncertainty in expert systems .
Syllogistic reasoning with dispositions has an important bearing on commonsense reasoning as well as on the management of uncertainty in expert systems .	management of uncertainty	expert systems	part_whole	{'e1': {'word': 'management of uncertainty', 'word_index': [(16, 18)], 'id': 'P84-1064.25'}, 'e2': {'word': 'expert systems', 'word_index': [(20, 21)], 'id': 'P84-1064.26'}, 'entity_replacement': {'0:3': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '16:18': 'ENTITY', '20:21': 'ENTITYOTHER'}}	Syllogistic reasoning with dispositions has an important bearing on commonsense reasoning as well as on the management of uncertainty in expert systems .
As a simple application of the techniques described in this paper, we formulate a definition of typicality -- a concept which plays an important role in human cognition and is of relevance to default reasoning .	typicality	human cognition	result	{'e1': {'word': 'typicality', 'word_index': [(17, 17)], 'id': 'P84-1064.27'}, 'e2': {'word': 'human cognition', 'word_index': [(27, 28)], 'id': 'P84-1064.28'}, 'entity_replacement': {'17:17': 'ENTITY', '27:28': 'ENTITYOTHER', '34:35': 'ENTITYUNRELATED'}}	As a simple application of the techniques described in this paper , we formulate a definition of typicality -- a concept which plays an important role in human cognition and is of relevance to default reasoning .
 This report describes Paul , a computer text generation system designed to create cohesive text through the use of lexical substitutions .	lexical substitutions	computer text generation system	usage	{'e1': {'word': 'lexical substitutions', 'word_index': [(19, 20)], 'id': 'P84-1078.4'}, 'e2': {'word': 'computer text generation system', 'word_index': [(6, 9)], 'id': 'P84-1078.2'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:9': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED', '19:20': 'ENTITY'}}	This report describes Paul , a computer text generation system designed to create cohesive text through the use of lexical substitutions .
Specifically, this system is designed to deterministically choose between pronominalization , superordinate substitution , and definite noun phrase reiteration .	pronominalization	superordinate substitution	compare	{'e1': {'word': 'pronominalization', 'word_index': [(10, 10)], 'id': 'P84-1078.5'}, 'e2': {'word': 'superordinate substitution', 'word_index': [(12, 13)], 'id': 'P84-1078.6'}, 'entity_replacement': {'10:10': 'ENTITY', '12:13': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED'}}	Specifically , this system is designed to deterministically choose between pronominalization , superordinate substitution , and definite noun phrase reiteration .
The system identifies a strength of antecedence recovery for each of the lexical substitutions , and matches them against the strength of potential antecedence of each element in the text to select the proper substitutions for these elements.	antecedence recovery	lexical substitutions	model-feature	{'e1': {'word': 'antecedence recovery', 'word_index': [(6, 7)], 'id': 'P84-1078.8'}, 'e2': {'word': 'lexical substitutions', 'word_index': [(12, 13)], 'id': 'P84-1078.9'}, 'entity_replacement': {'6:7': 'ENTITY', '12:13': 'ENTITYOTHER', '20:23': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	The system identifies a strength of antecedence recovery for each of the lexical substitutions , and matches them against the strength of potential antecedence of each element in the text to select the proper substitutions for these elements .
The system identifies a strength of antecedence recovery for each of the lexical substitutions , and matches them against the strength of potential antecedence of each element in the text to select the proper substitutions for these elements.	strength of potential antecedence	substitutions	model-feature	{'e1': {'word': 'strength of potential antecedence', 'word_index': [(20, 23)], 'id': 'P84-1078.10'}, 'e2': {'word': 'substitutions', 'word_index': [(34, 34)], 'id': 'P84-1078.12'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '20:23': 'ENTITY', '29:29': 'ENTITYUNRELATED', '34:34': 'ENTITYOTHER'}}	The system identifies a strength of antecedence recovery for each of the lexical substitutions , and matches them against the strength of potential antecedence of each element in the text to select the proper substitutions for these elements .
 Determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the global meaning  of a sentence , even if not in a precise way.	meaning	utterance	model-feature	{'e1': {'word': 'meaning', 'word_index': [(8, 8)], 'id': 'C86-1081.2'}, 'e2': {'word': 'utterance', 'word_index': [(11, 11)], 'id': 'C86-1081.3'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '8:8': 'ENTITY', '11:11': 'ENTITYOTHER', '32:33': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED'}}	Determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the global meaning of a sentence , even if not in a precise way .
 Determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the global meaning  of a sentence , even if not in a precise way.	global meaning 	sentence	model-feature	{'e1': {'word': 'global meaning ', 'word_index': [(32, 33)], 'id': 'C86-1081.4'}, 'e2': {'word': 'sentence', 'word_index': [(36, 36)], 'id': 'C86-1081.5'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '32:33': 'ENTITY', '36:36': 'ENTITYOTHER'}}	Determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the global meaning of a sentence , even if not in a precise way .
Another problem with determiners is their inherent ambiguity .	ambiguity	determiners	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(7, 7)], 'id': 'C86-1081.7'}, 'e2': {'word': 'determiners', 'word_index': [(3, 3)], 'id': 'C86-1081.6'}, 'entity_replacement': {'3:3': 'ENTITYOTHER', '7:7': 'ENTITY'}}	Another problem with determiners is their inherent ambiguity .
In this paper we propose a logical formalism , which, among other things, is suitable for representing determiners without forcing a particular interpretation when their meaning is still not clear.	interpretation	meaning	model-feature	{'e1': {'word': 'interpretation', 'word_index': [(24, 24)], 'id': 'C86-1081.10'}, 'e2': {'word': 'meaning', 'word_index': [(27, 27)], 'id': 'C86-1081.11'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '24:24': 'ENTITY', '27:27': 'ENTITYOTHER'}}	In this paper we propose a logical formalism , which , among other things , is suitable for representing determiners without forcing a particular interpretation when their meaning is still not clear .
 This paper describes a system ( RAREAS ) which synthesizes marine weather forecasts directly from formatted weather data .	RAREAS	formatted weather data	usage	{'e1': {'word': 'RAREAS', 'word_index': [(6, 6)], 'id': 'C86-1132.1'}, 'e2': {'word': 'formatted weather data', 'word_index': [(15, 17)], 'id': 'C86-1132.2'}, 'entity_replacement': {'6:6': 'ENTITY', '15:17': 'ENTITYOTHER'}}	This paper describes a system ( RAREAS ) which synthesizes marine weather forecasts directly from formatted weather data .
Such synthesis appears feasible in certain natural sublanguages with stereotyped text structure .	stereotyped text structure	natural sublanguages	model-feature	{'e1': {'word': 'stereotyped text structure', 'word_index': [(9, 11)], 'id': 'C86-1132.5'}, 'e2': {'word': 'natural sublanguages', 'word_index': [(6, 7)], 'id': 'C86-1132.4'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '6:7': 'ENTITYOTHER', '9:11': 'ENTITY'}}	Such synthesis appears feasible in certain natural sublanguages with stereotyped text structure .
RAREAS draws on several kinds of linguistic and non-linguistic knowledge and mirrors a forecaster&apos;s apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events.	linguistic and non-linguistic knowledge	RAREAS	usage	{'e1': {'word': 'linguistic and non-linguistic knowledge', 'word_index': [(6, 9)], 'id': 'C86-1132.7'}, 'e2': {'word': 'RAREAS', 'word_index': [(0, 0)], 'id': 'C86-1132.6'}, 'entity_replacement': {'0:0': 'ENTITYOTHER', '6:9': 'ENTITY', '21:22': 'ENTITYUNRELATED'}}	RAREAS draws on several kinds of linguistic and non-linguistic knowledge and mirrors a forecaster&apos ;s apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events .
 A method for error correction of ill-formed input is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs.	error correction	ill-formed input	usage	{'e1': {'word': 'error correction', 'word_index': [(3, 4)], 'id': 'J86-1002.1'}, 'e2': {'word': 'ill-formed input', 'word_index': [(6, 9)], 'id': 'J86-1002.2'}, 'entity_replacement': {'3:4': 'ENTITY', '6:9': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED'}}	A method for error correction of ill - formed input is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs .
A dialogue acquisition and tracking algorithm is presented along with a description of its implementation in a voice interactive system .	dialogue acquisition and tracking algorithm	voice interactive system	part_whole	{'e1': {'word': 'dialogue acquisition and tracking algorithm', 'word_index': [(1, 5)], 'id': 'J86-1002.9'}, 'e2': {'word': 'voice interactive system', 'word_index': [(17, 19)], 'id': 'J86-1002.11'}, 'entity_replacement': {'1:5': 'ENTITY', '14:14': 'ENTITYUNRELATED', '17:19': 'ENTITYOTHER'}}	A dialogue acquisition and tracking algorithm is presented along with a description of its implementation in a voice interactive system .
 In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse .	processing	discourse	result	{'e1': {'word': 'processing', 'word_index': [(18, 18)], 'id': 'J86-3001.3'}, 'e2': {'word': 'discourse', 'word_index': [(20, 20)], 'id': 'J86-3001.4'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '18:18': 'ENTITY', '20:20': 'ENTITYOTHER'}}	In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse .
In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of purposes (called the intentional structure ), and the state of focus of attention (called the attentional state ).	linguistic structure	utterances	model-feature	{'e1': {'word': 'linguistic structure', 'word_index': [(25, 26)], 'id': 'J86-3001.7'}, 'e2': {'word': 'utterances', 'word_index': [(21, 21)], 'id': 'J86-3001.6'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER', '25:26': 'ENTITY', '32:32': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED', '44:46': 'ENTITYUNRELATED', '50:51': 'ENTITYUNRELATED'}}	In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances ( called the linguistic structure ) , a structure of purposes ( called the intentional structure ) , and the state of focus of attention ( called the attentional state ) .
In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of purposes (called the intentional structure ), and the state of focus of attention (called the attentional state ).	intentional structure	purposes	model-feature	{'e1': {'word': 'intentional structure', 'word_index': [(36, 37)], 'id': 'J86-3001.9'}, 'e2': {'word': 'purposes', 'word_index': [(32, 32)], 'id': 'J86-3001.8'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '32:32': 'ENTITYOTHER', '36:37': 'ENTITY', '44:46': 'ENTITYUNRELATED', '50:51': 'ENTITYUNRELATED'}}	In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances ( called the linguistic structure ) , a structure of purposes ( called the intentional structure ) , and the state of focus of attention ( called the attentional state ) .
In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of purposes (called the intentional structure ), and the state of focus of attention (called the attentional state ).	attentional state	focus of attention	model-feature	{'e1': {'word': 'attentional state', 'word_index': [(50, 51)], 'id': 'J86-3001.11'}, 'e2': {'word': 'focus of attention', 'word_index': [(44, 46)], 'id': 'J86-3001.10'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED', '44:46': 'ENTITYOTHER', '50:51': 'ENTITY'}}	In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances ( called the linguistic structure ) , a structure of purposes ( called the intentional structure ) , and the state of focus of attention ( called the attentional state ) .
The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate.	utterances	discourse	part_whole	{'e1': {'word': 'utterances', 'word_index': [(12, 12)], 'id': 'J86-3001.14'}, 'e2': {'word': 'discourse', 'word_index': [(8, 8)], 'id': 'J86-3001.13'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '8:8': 'ENTITYOTHER', '12:12': 'ENTITY'}}	The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate .
The intentional structure captures the discourse-relevant purposes , expressed in each of the linguistic segments as well as relationships among them.	intentional structure	discourse-relevant purposes	model-feature	{'e1': {'word': 'intentional structure', 'word_index': [(1, 2)], 'id': 'J86-3001.15'}, 'e2': {'word': 'discourse-relevant purposes', 'word_index': [(5, 8)], 'id': 'J86-3001.16'}, 'entity_replacement': {'1:2': 'ENTITY', '5:8': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	The intentional structure captures the discourse - relevant purposes , expressed in each of the linguistic segments as well as relationships among them .
The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds.	attentional state	focus of attention	model-feature	{'e1': {'word': 'attentional state', 'word_index': [(1, 2)], 'id': 'J86-3001.18'}, 'e2': {'word': 'focus of attention', 'word_index': [(8, 10)], 'id': 'J86-3001.19'}, 'entity_replacement': {'1:2': 'ENTITY', '8:10': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED'}}	The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds .
The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses .	theory of attention, intention, and aggregation of utterances	discourses	model-feature	{'e1': {'word': 'theory of attention, intention, and aggregation of utterances', 'word_index': [(1, 10)], 'id': 'J86-3001.28'}, 'e2': {'word': 'discourses', 'word_index': [(21, 21)], 'id': 'J86-3001.29'}, 'entity_replacement': {'1:10': 'ENTITY', '21:21': 'ENTITYOTHER'}}	The theory of attention , intention , and aggregation of utterances is illustrated in the paper with a number of example discourses .
This theory provides a framework for describing the processing of utterances in a discourse .	utterances	discourse	part_whole	{'e1': {'word': 'utterances', 'word_index': [(10, 10)], 'id': 'J86-3001.35'}, 'e2': {'word': 'discourse', 'word_index': [(13, 13)], 'id': 'J86-3001.36'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '10:10': 'ENTITY', '13:13': 'ENTITYOTHER'}}	This theory provides a framework for describing the processing of utterances in a discourse .
Discourse processing requires recognizing how the utterances of the discourse aggregate into segments , recognizing the intentions expressed in the discourse and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state .	utterances	discourse	part_whole	{'e1': {'word': 'utterances', 'word_index': [(6, 6)], 'id': 'J86-3001.38'}, 'e2': {'word': 'discourse', 'word_index': [(9, 9)], 'id': 'J86-3001.39'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '6:6': 'ENTITY', '9:9': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED'}}	Discourse processing requires recognizing how the utterances of the discourse aggregate into segments , recognizing the intentions expressed in the discourse and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state .
Discourse processing requires recognizing how the utterances of the discourse aggregate into segments , recognizing the intentions expressed in the discourse and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state .	intentions	discourse	part_whole	{'e1': {'word': 'intentions', 'word_index': [(16, 16)], 'id': 'J86-3001.41'}, 'e2': {'word': 'discourse', 'word_index': [(20, 20)], 'id': 'J86-3001.42'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '16:16': 'ENTITY', '20:20': 'ENTITYOTHER', '25:25': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED'}}	Discourse processing requires recognizing how the utterances of the discourse aggregate into segments , recognizing the intentions expressed in the discourse and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state .
 The goal of this work is the enrichment of human-machine interactions in a natural language environment .	natural language environment	human-machine interactions	model-feature	{'e1': {'word': 'natural language environment', 'word_index': [(15, 17)], 'id': 'J86-4002.2'}, 'e2': {'word': 'human-machine interactions', 'word_index': [(9, 12)], 'id': 'J86-4002.1'}, 'entity_replacement': {'9:12': 'ENTITYOTHER', '15:17': 'ENTITY'}}	The goal of this work is the enrichment of human - machine interactions in a natural language environment .
Because a speaker and listener cannot be assured to have the same beliefs , contexts , perceptions , backgrounds , or goals , at each point in a conversation , difficulties and mistakes arise when a listener interprets a speaker&apos;s utterance .	speaker	listener	compare	{'e1': {'word': 'speaker', 'word_index': [(2, 2)], 'id': 'J86-4002.3'}, 'e2': {'word': 'listener', 'word_index': [(4, 4)], 'id': 'J86-4002.4'}, 'entity_replacement': {'2:2': 'ENTITY', '4:4': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '39:41': 'ENTITYUNRELATED'}}	Because a speaker and listener cannot be assured to have the same beliefs , contexts , perceptions , backgrounds , or goals , at each point in a conversation , difficulties and mistakes arise when a listener interprets a speaker&apos ;s utterance .
 We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .	Tree Adjoining Grammars	Head Grammars	compare	{'e1': {'word': 'Tree Adjoining Grammars', 'word_index': [(10, 12)], 'id': 'P86-1011.2'}, 'e2': {'word': 'Head Grammars', 'word_index': [(14, 15)], 'id': 'P86-1011.3'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:12': 'ENTITY', '14:15': 'ENTITYOTHER'}}	We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .
We briefly investigate the weak equivalence of the two formalisms .	equivalence	formalisms	model-feature	{'e1': {'word': 'equivalence', 'word_index': [(5, 5)], 'id': 'P86-1011.4'}, 'e2': {'word': 'formalisms', 'word_index': [(9, 9)], 'id': 'P86-1011.5'}, 'entity_replacement': {'5:5': 'ENTITY', '9:9': 'ENTITYOTHER'}}	We briefly investigate the weak equivalence of the two formalisms .
We then turn to a discussion comparing the linguistic expressiveness of the two formalisms .	linguistic expressiveness	formalisms	model-feature	{'e1': {'word': 'linguistic expressiveness', 'word_index': [(8, 9)], 'id': 'P86-1011.6'}, 'e2': {'word': 'formalisms', 'word_index': [(13, 13)], 'id': 'P86-1011.7'}, 'entity_replacement': {'8:9': 'ENTITY', '13:13': 'ENTITYOTHER'}}	We then turn to a discussion comparing the linguistic expressiveness of the two formalisms .
 Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects .	features	linguistic objects	model-feature	{'e1': {'word': 'features', 'word_index': [(10, 10)], 'id': 'P86-1038.2'}, 'e2': {'word': 'linguistic objects', 'word_index': [(13, 14)], 'id': 'P86-1038.3'}, 'entity_replacement': {'0:4': 'ENTITYUNRELATED', '10:10': 'ENTITY', '13:14': 'ENTITYOTHER'}}	Unification - based grammar formalisms use structures containing sets of features to describe linguistic objects .
We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them.	feature structures	logical formulas	model-feature	{'e1': {'word': 'feature structures', 'word_index': [(9, 10)], 'id': 'P86-1038.7'}, 'e2': {'word': 'logical formulas', 'word_index': [(15, 16)], 'id': 'P86-1038.8'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '9:10': 'ENTITY', '15:16': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED'}}	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .
These graphs are, in fact, transition graphs for a special type of deterministic finite automaton .	transition graphs	deterministic finite automaton	part_whole	{'e1': {'word': 'transition graphs', 'word_index': [(7, 8)], 'id': 'P86-1038.11'}, 'e2': {'word': 'deterministic finite automaton', 'word_index': [(14, 16)], 'id': 'P86-1038.12'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:8': 'ENTITY', '14:16': 'ENTITYOTHER'}}	These graphs are , in fact , transition graphs for a special type of deterministic finite automaton .
This semantics for feature structures extends the ideas of Pereira and Shieber [11], by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	semantics	feature structures	model-feature	{'e1': {'word': 'semantics', 'word_index': [(1, 1)], 'id': 'P86-1038.13'}, 'e2': {'word': 'feature structures', 'word_index': [(3, 4)], 'id': 'P86-1038.14'}, 'entity_replacement': {'1:1': 'ENTITY', '3:4': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	This semantics for feature structures extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .
This semantics for feature structures extends the ideas of Pereira and Shieber [11], by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	path values	disjunctions	part_whole	{'e1': {'word': 'path values', 'word_index': [(28, 29)], 'id': 'P86-1038.16'}, 'e2': {'word': 'disjunctions', 'word_index': [(32, 32)], 'id': 'P86-1038.17'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '28:29': 'ENTITY', '32:32': 'ENTITYOTHER'}}	This semantics for feature structures extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .
Our interpretation differs from that of Pereira and Shieber by using a logical model in place of a denotational semantics .	logical model	denotational semantics	compare	{'e1': {'word': 'logical model', 'word_index': [(12, 13)], 'id': 'P86-1038.18'}, 'e2': {'word': 'denotational semantics', 'word_index': [(18, 19)], 'id': 'P86-1038.19'}, 'entity_replacement': {'12:13': 'ENTITY', '18:19': 'ENTITYOTHER'}}	Our interpretation differs from that of Pereira and Shieber by using a logical model in place of a denotational semantics .
This logical model yields a calculus of equivalences , which can be used to simplify formulas .	logical model	formulas	usage	{'e1': {'word': 'logical model', 'word_index': [(1, 2)], 'id': 'P86-1038.20'}, 'e2': {'word': 'formulas', 'word_index': [(15, 15)], 'id': 'P86-1038.22'}, 'entity_replacement': {'1:2': 'ENTITY', '7:7': 'ENTITYUNRELATED', '15:15': 'ENTITYOTHER'}}	This logical model yields a calculus of equivalences , which can be used to simplify formulas .
Our model allows a careful examination of the computational complexity of unification .	computational complexity	unification	model-feature	{'e1': {'word': 'computational complexity', 'word_index': [(8, 9)], 'id': 'P86-1038.25'}, 'e2': {'word': 'unification', 'word_index': [(11, 11)], 'id': 'P86-1038.26'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '8:9': 'ENTITY', '11:11': 'ENTITYOTHER'}}	Our model allows a careful examination of the computational complexity of unification .
We have shown that the consistency problem for formulas with disjunctive values is NP-complete .	consistency problem	formulas	model-feature	{'e1': {'word': 'consistency problem', 'word_index': [(5, 6)], 'id': 'P86-1038.27'}, 'e2': {'word': 'formulas', 'word_index': [(8, 8)], 'id': 'P86-1038.28'}, 'entity_replacement': {'5:6': 'ENTITY', '8:8': 'ENTITYOTHER', '10:11': 'ENTITYUNRELATED', '13:15': 'ENTITYUNRELATED'}}	We have shown that the consistency problem for formulas with disjunctive values is NP - complete .
Multimedia answers include videodisc images and heuristically-produced complete sentences in text or text-to-speech form .	text	text-to-speech form	compare	{'e1': {'word': 'text', 'word_index': [(12, 12)], 'id': 'A88-1001.7'}, 'e2': {'word': 'text-to-speech form', 'word_index': [(14, 19)], 'id': 'A88-1001.8'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITY', '14:19': 'ENTITYOTHER'}}	Multimedia answers include videodisc images and heuristically - produced complete sentences in text or text - to - speech form .
Deictic reference and feedback about the discourse are enabled.	feedback	discourse	model-feature	{'e1': {'word': 'feedback', 'word_index': [(3, 3)], 'id': 'A88-1001.10'}, 'e2': {'word': 'discourse', 'word_index': [(6, 6)], 'id': 'A88-1001.11'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '3:3': 'ENTITY', '6:6': 'ENTITYOTHER'}}	Deictic reference and feedback about the discourse are enabled .
 In this paper, we describe the pronominal anaphora resolution module of Lucy , a portable English understanding system .	pronominal anaphora resolution module	Lucy	part_whole	{'e1': {'word': 'pronominal anaphora resolution module', 'word_index': [(7, 10)], 'id': 'A88-1003.1'}, 'e2': {'word': 'Lucy', 'word_index': [(12, 12)], 'id': 'A88-1003.2'}, 'entity_replacement': {'7:10': 'ENTITY', '12:12': 'ENTITYOTHER', '16:18': 'ENTITYUNRELATED'}}	In this paper , we describe the pronominal anaphora resolution module of Lucy , a portable English understanding system .
Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose candidate antecedents and to evaluate each other&apos;s proposals.	partial theories	blackboard-like architecture	part_whole	{'e1': {'word': 'partial theories', 'word_index': [(12, 13)], 'id': 'A88-1003.6'}, 'e2': {'word': 'blackboard-like architecture', 'word_index': [(5, 8)], 'id': 'A88-1003.5'}, 'entity_replacement': {'5:8': 'ENTITYOTHER', '12:13': 'ENTITY', '26:26': 'ENTITYUNRELATED'}}	Thus we have implemented a blackboard - like architecture in which individual partial theories can be encoded as separate modules that can interact to propose candidate antecedents and to evaluate each other&apos ;s proposals .
 This paper discusses the application of Unification Categorial Grammar (UCG) to the framework of Isomorphic Grammars for Machine Translation pioneered by Landsbergen.	Unification Categorial Grammar (UCG)	Machine Translation	usage	{'e1': {'word': 'Unification Categorial Grammar (UCG)', 'word_index': [(6, 11)], 'id': 'C88-1007.1'}, 'e2': {'word': 'Machine Translation', 'word_index': [(19, 20)], 'id': 'C88-1007.3'}, 'entity_replacement': {'6:11': 'ENTITY', '16:17': 'ENTITYUNRELATED', '19:20': 'ENTITYOTHER'}}	This paper discusses the application of Unification Categorial Grammar ( UCG ) to the framework of Isomorphic Grammars for Machine Translation pioneered by Landsbergen .
The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel, in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations .	grammars	Source and Target languages	model-feature	{'e1': {'word': 'grammars', 'word_index': [(9, 9)], 'id': 'C88-1007.5'}, 'e2': {'word': 'Source and Target languages', 'word_index': [(12, 15)], 'id': 'C88-1007.6'}, 'entity_replacement': {'1:5': 'ENTITYUNRELATED', '9:9': 'ENTITY', '12:15': 'ENTITYOTHER', '24:24': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED'}}	The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel , in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations .
The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel, in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations .	SL	TL	compare	{'e1': {'word': 'SL', 'word_index': [(24, 24)], 'id': 'C88-1007.7'}, 'e2': {'word': 'TL', 'word_index': [(26, 26)], 'id': 'C88-1007.8'}, 'entity_replacement': {'1:5': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '12:15': 'ENTITYUNRELATED', '24:24': 'ENTITY', '26:26': 'ENTITYOTHER', '32:33': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED'}}	The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel , in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations .
Semantic and other information may still be incorporated, but as constraints on the translation relation , not as levels of textual representation .	translation relation	textual representation	compare	{'e1': {'word': 'translation relation', 'word_index': [(14, 15)], 'id': 'C88-1007.13'}, 'e2': {'word': 'textual representation', 'word_index': [(21, 22)], 'id': 'C88-1007.14'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '14:15': 'ENTITY', '21:22': 'ENTITYOTHER'}}	Semantic and other information may still be incorporated , but as constraints on the translation relation , not as levels of textual representation .
 This paper presents necessary and sufficient conditions for the use of demonstrative expressions in English and discusses implications for current discourse processing algorithms .	demonstrative expressions	English	part_whole	{'e1': {'word': 'demonstrative expressions', 'word_index': [(11, 12)], 'id': 'C88-1044.1'}, 'e2': {'word': 'English', 'word_index': [(14, 14)], 'id': 'C88-1044.2'}, 'entity_replacement': {'11:12': 'ENTITY', '14:14': 'ENTITYOTHER', '20:22': 'ENTITYUNRELATED'}}	This paper presents necessary and sufficient conditions for the use of demonstrative expressions in English and discusses implications for current discourse processing algorithms .
We examine a broad range of texts to show how the distribution of demonstrative forms and functions is genre dependent .	demonstrative forms and functions	texts	part_whole	{'e1': {'word': 'demonstrative forms and functions', 'word_index': [(13, 16)], 'id': 'C88-1044.5'}, 'e2': {'word': 'texts', 'word_index': [(6, 6)], 'id': 'C88-1044.4'}, 'entity_replacement': {'6:6': 'ENTITYOTHER', '13:16': 'ENTITY', '18:19': 'ENTITYUNRELATED'}}	We examine a broad range of texts to show how the distribution of demonstrative forms and functions is genre dependent .
CCRs are Boolean conditions on the cooccurrence of categories in local trees which allow the statement of generalizations which cannot be captured in other current syntax formalisms .	Boolean conditions	categories	model-feature	{'e1': {'word': 'Boolean conditions', 'word_index': [(2, 3)], 'id': 'C88-1066.4'}, 'e2': {'word': 'categories', 'word_index': [(8, 8)], 'id': 'C88-1066.5'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '2:3': 'ENTITY', '8:8': 'ENTITYOTHER', '10:11': 'ENTITYUNRELATED', '15:17': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED'}}	CCRs are Boolean conditions on the cooccurrence of categories in local trees which allow the statement of generalizations which cannot be captured in other current syntax formalisms .
The use of CCRs leads to syntactic descriptions formulated entirely with restrictive statements .	restrictive statements	syntactic descriptions	model-feature	{'e1': {'word': 'restrictive statements', 'word_index': [(11, 12)], 'id': 'C88-1066.11'}, 'e2': {'word': 'syntactic descriptions', 'word_index': [(6, 7)], 'id': 'C88-1066.10'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:7': 'ENTITYOTHER', '11:12': 'ENTITY'}}	The use of CCRs leads to syntactic descriptions formulated entirely with restrictive statements .
The paper shows how conventional algorithms for the analysis of context free languages can be adapted to the CCR formalism .	context free languages	CCR formalism	compare	{'e1': {'word': 'context free languages', 'word_index': [(10, 12)], 'id': 'C88-1066.12'}, 'e2': {'word': 'CCR formalism', 'word_index': [(18, 19)], 'id': 'C88-1066.13'}, 'entity_replacement': {'10:12': 'ENTITY', '18:19': 'ENTITYOTHER'}}	The paper shows how conventional algorithms for the analysis of context free languages can be adapted to the CCR formalism .
Special attention is given to the part of the parser that checks the fulfillment of logical well-formedness conditions on trees .	logical well-formedness conditions	trees	model-feature	{'e1': {'word': 'logical well-formedness conditions', 'word_index': [(15, 19)], 'id': 'C88-1066.15'}, 'e2': {'word': 'trees', 'word_index': [(21, 21)], 'id': 'C88-1066.16'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '15:19': 'ENTITY', '21:21': 'ENTITYOTHER'}}	Special attention is given to the part of the parser that checks the fulfillment of logical well - formedness conditions on trees .
By reappraising these insightful counterexamples, the inferential theory for natural language presuppositions described in Mercer 1987, 1988 gives a simple and straightforward explanation for the presuppositional nature of these sentences .	presuppositional nature	sentences	model-feature	{'e1': {'word': 'presuppositional nature', 'word_index': [(27, 28)], 'id': 'C88-2086.3'}, 'e2': {'word': 'sentences', 'word_index': [(31, 31)], 'id': 'C88-2086.4'}, 'entity_replacement': {'7:12': 'ENTITYUNRELATED', '27:28': 'ENTITY', '31:31': 'ENTITYOTHER'}}	By reappraising these insightful counterexamples , the inferential theory for natural language presuppositions described in Mercer 1987 , 1988 gives a simple and straightforward explanation for the presuppositional nature of these sentences .
 We have developed a computational model of the process of describing the layout of an apartment or house, a much-studied discourse task first characterized linguistically by Linde (1974).	computational model	discourse task	usage	{'e1': {'word': 'computational model', 'word_index': [(4, 5)], 'id': 'C88-2130.1'}, 'e2': {'word': 'discourse task', 'word_index': [(23, 24)], 'id': 'C88-2130.2'}, 'entity_replacement': {'4:5': 'ENTITY', '23:24': 'ENTITYOTHER'}}	We have developed a computational model of the process of describing the layout of an apartment or house , a much - studied discourse task first characterized linguistically by Linde ( 1974 ) .
The model is embodied in a program, APT , that can reproduce segments of actual tape-recorded descriptions, using organizational and discourse strategies derived through analysis of our corpus .	model	APT	part_whole	{'e1': {'word': 'model', 'word_index': [(1, 1)], 'id': 'C88-2130.3'}, 'e2': {'word': 'APT', 'word_index': [(8, 8)], 'id': 'C88-2130.4'}, 'entity_replacement': {'1:1': 'ENTITY', '8:8': 'ENTITYOTHER', '22:25': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	The model is embodied in a program , APT , that can reproduce segments of actual tape - recorded descriptions , using organizational and discourse strategies derived through analysis of our corpus .
So, for any place where the easily identifiable fragments occur in the sentence , the process will extend to both the left and the right of the islands , until possibly completely missing fragments are reached.	fragments	sentence	part_whole	{'e1': {'word': 'fragments', 'word_index': [(9, 9)], 'id': 'C88-2132.7'}, 'e2': {'word': 'sentence', 'word_index': [(13, 13)], 'id': 'C88-2132.8'}, 'entity_replacement': {'9:9': 'ENTITY', '13:13': 'ENTITYOTHER', '28:28': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	So , for any place where the easily identifiable fragments occur in the sentence , the process will extend to both the left and the right of the islands , until possibly completely missing fragments are reached .
This paper presents a new interactive disambiguation scheme based on the paraphrasing of a parser &apos;s multiple output.	parser	paraphrasing	result	{'e1': {'word': 'parser', 'word_index': [(14, 14)], 'id': 'C88-2160.10'}, 'e2': {'word': 'paraphrasing', 'word_index': [(11, 11)], 'id': 'C88-2160.9'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '11:11': 'ENTITYOTHER', '14:14': 'ENTITY'}}	This paper presents a new interactive disambiguation scheme based on the paraphrasing of a parser &apos ;s multiple output .
Some examples of paraphrasing ambiguous sentences are presented.	paraphrasing	sentences	model-feature	{'e1': {'word': 'paraphrasing', 'word_index': [(3, 3)], 'id': 'C88-2160.11'}, 'e2': {'word': 'sentences', 'word_index': [(5, 5)], 'id': 'C88-2160.12'}, 'entity_replacement': {'3:3': 'ENTITY', '5:5': 'ENTITYOTHER'}}	Some examples of paraphrasing ambiguous sentences are presented .
For one thing, learning methodology applicable in general domains does not readily lend itself in the linguistic domain .	general domains	linguistic domain	compare	{'e1': {'word': 'general domains', 'word_index': [(8, 9)], 'id': 'C88-2162.3'}, 'e2': {'word': 'linguistic domain', 'word_index': [(17, 18)], 'id': 'C88-2162.4'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '8:9': 'ENTITY', '17:18': 'ENTITYOTHER'}}	For one thing , learning methodology applicable in general domains does not readily lend itself in the linguistic domain .
For another, linguistic representation used by language processing systems is not geared to learning .	linguistic representation	language processing systems	usage	{'e1': {'word': 'linguistic representation', 'word_index': [(3, 4)], 'id': 'C88-2162.5'}, 'e2': {'word': 'language processing systems', 'word_index': [(7, 9)], 'id': 'C88-2162.6'}, 'entity_replacement': {'3:4': 'ENTITY', '7:9': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED'}}	For another , linguistic representation used by language processing systems is not geared to learning .
We introduced a new linguistic representation , the Dynamic Hierarchical Phrasal Lexicon (DHPL) [Zernik88], to facilitate language acquisition .	Dynamic Hierarchical Phrasal Lexicon (DHPL)	language acquisition	usage	{'e1': {'word': 'Dynamic Hierarchical Phrasal Lexicon (DHPL)', 'word_index': [(8, 14)], 'id': 'C88-2162.9'}, 'e2': {'word': 'language acquisition', 'word_index': [(22, 23)], 'id': 'C88-2162.10'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '8:14': 'ENTITY', '22:23': 'ENTITYOTHER'}}	We introduced a new linguistic representation , the Dynamic Hierarchical Phrasal Lexicon ( DHPL ) [ Zernik 88 ] , to facilitate language acquisition .
From this, a language learning model was implemented in the program RINA , which enhances its own lexical hierarchy by processing examples in context.	lexical hierarchy	language learning model	part_whole	{'e1': {'word': 'lexical hierarchy', 'word_index': [(18, 19)], 'id': 'C88-2162.13'}, 'e2': {'word': 'language learning model', 'word_index': [(4, 6)], 'id': 'C88-2162.11'}, 'entity_replacement': {'4:6': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '18:19': 'ENTITY'}}	From this , a language learning model was implemented in the program RINA , which enhances its own lexical hierarchy by processing examples in context .
We identified two tasks: First, how linguistic concepts are acquired from training examples and organized in a hierarchy ; this task was discussed in previous papers [Zernik87].	linguistic concepts	training examples	model-feature	{'e1': {'word': 'linguistic concepts', 'word_index': [(8, 9)], 'id': 'C88-2162.14'}, 'e2': {'word': 'training examples', 'word_index': [(13, 14)], 'id': 'C88-2162.15'}, 'entity_replacement': {'8:9': 'ENTITY', '13:14': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED'}}	We identified two tasks : First , how linguistic concepts are acquired from training examples and organized in a hierarchy ; this task was discussed in previous papers [ Zernik 87 ] .
Second, we show in this paper how a lexical hierarchy is used in predicting new linguistic concepts .	lexical hierarchy	linguistic concepts	usage	{'e1': {'word': 'lexical hierarchy', 'word_index': [(9, 10)], 'id': 'C88-2162.17'}, 'e2': {'word': 'linguistic concepts', 'word_index': [(16, 17)], 'id': 'C88-2162.18'}, 'entity_replacement': {'9:10': 'ENTITY', '16:17': 'ENTITYOTHER'}}	Second , we show in this paper how a lexical hierarchy is used in predicting new linguistic concepts .
Thus, a program does not stall even in the presence of a lexical unknown , and a hypothesis can be produced for covering that lexical gap .	hypothesis	lexical unknown	model-feature	{'e1': {'word': 'hypothesis', 'word_index': [(18, 18)], 'id': 'C88-2162.21'}, 'e2': {'word': 'lexical unknown', 'word_index': [(13, 14)], 'id': 'C88-2162.20'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '18:18': 'ENTITY', '25:26': 'ENTITYUNRELATED'}}	Thus , a program does not stall even in the presence of a lexical unknown , and a hypothesis can be produced for covering that lexical gap .
 Although every natural language system needs a computational lexicon , each system puts different amounts and types of information into its lexicon according to its individual needs.	computational lexicon	natural language system	usage	{'e1': {'word': 'computational lexicon', 'word_index': [(7, 8)], 'id': 'C88-2166.2'}, 'e2': {'word': 'natural language system', 'word_index': [(2, 4)], 'id': 'C88-2166.1'}, 'entity_replacement': {'2:4': 'ENTITYOTHER', '7:8': 'ENTITY', '21:21': 'ENTITYUNRELATED'}}	Although every natural language system needs a computational lexicon , each system puts different amounts and types of information into its lexicon according to its individual needs .
This paper presents our experience in planning and building COMPLEX , a computational lexicon designed to be a repository of shared lexical information for use by Natural Language Processing (NLP) systems .	shared lexical information	computational lexicon	part_whole	{'e1': {'word': 'shared lexical information', 'word_index': [(20, 22)], 'id': 'C88-2166.6'}, 'e2': {'word': 'computational lexicon', 'word_index': [(12, 13)], 'id': 'C88-2166.5'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '12:13': 'ENTITYOTHER', '20:22': 'ENTITY', '26:32': 'ENTITYUNRELATED'}}	This paper presents our experience in planning and building COMPLEX , a computational lexicon designed to be a repository of shared lexical information for use by Natural Language Processing ( NLP ) systems .
We have drawn primarily on explicit and implicit information from machine-readable dictionaries (MRD&apos;s) to create a broad coverage lexicon .	machine-readable dictionaries (MRD&apos;s)	broad coverage lexicon	usage	{'e1': {'word': 'machine-readable dictionaries (MRD&apos;s)', 'word_index': [(10, 18)], 'id': 'C88-2166.8'}, 'e2': {'word': 'broad coverage lexicon', 'word_index': [(22, 24)], 'id': 'C88-2166.9'}, 'entity_replacement': {'10:18': 'ENTITY', '22:24': 'ENTITYOTHER'}}	We have drawn primarily on explicit and implicit information from machine - readable dictionaries ( MRD&apos ; s ) to create a broad coverage lexicon .
This paper explores the role of user modeling in such systems .	user modeling	systems	result	{'e1': {'word': 'user modeling', 'word_index': [(6, 7)], 'id': 'J88-3002.4'}, 'e2': {'word': 'systems', 'word_index': [(10, 10)], 'id': 'J88-3002.5'}, 'entity_replacement': {'6:7': 'ENTITY', '10:10': 'ENTITYOTHER'}}	This paper explores the role of user modeling in such systems .
The types of information that a user model may be required to keep about a user are then identified and discussed.	user model	user	model-feature	{'e1': {'word': 'user model', 'word_index': [(6, 7)], 'id': 'J88-3002.7'}, 'e2': {'word': 'user', 'word_index': [(15, 15)], 'id': 'J88-3002.8'}, 'entity_replacement': {'6:7': 'ENTITY', '15:15': 'ENTITYOTHER'}}	The types of information that a user model may be required to keep about a user are then identified and discussed .
Since acquiring the knowledge for a user model is a fundamental problem in user modeling , a section is devoted to this topic.	user model	user modeling	part_whole	{'e1': {'word': 'user model', 'word_index': [(6, 7)], 'id': 'J88-3002.10'}, 'e2': {'word': 'user modeling', 'word_index': [(13, 14)], 'id': 'J88-3002.11'}, 'entity_replacement': {'6:7': 'ENTITY', '13:14': 'ENTITYOTHER'}}	Since acquiring the knowledge for a user model is a fundamental problem in user modeling , a section is devoted to this topic .
 This article introduces a bidirectional grammar generation system called feature structure-directed generation , developed for a dialogue translation system .	feature structure-directed generation	dialogue translation system	usage	{'e1': {'word': 'feature structure-directed generation', 'word_index': [(9, 13)], 'id': 'C90-1013.2'}, 'e2': {'word': 'dialogue translation system', 'word_index': [(18, 20)], 'id': 'C90-1013.3'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '9:13': 'ENTITY', '18:20': 'ENTITYOTHER'}}	This article introduces a bidirectional grammar generation system called feature structure - directed generation , developed for a dialogue translation system .
The system utilizes typed feature structures to control the top-down derivation in a declarative way.	typed feature structures	top-down derivation	usage	{'e1': {'word': 'typed feature structures', 'word_index': [(3, 5)], 'id': 'C90-1013.4'}, 'e2': {'word': 'top-down derivation', 'word_index': [(9, 10)], 'id': 'C90-1013.5'}, 'entity_replacement': {'3:5': 'ENTITY', '9:10': 'ENTITYOTHER'}}	The system utilizes typed feature structures to control the top-down derivation in a declarative way .
This generation system also uses disjunctive feature structures to reduce the number of copies of the derivation tree .	disjunctive feature structures	generation system	usage	{'e1': {'word': 'disjunctive feature structures', 'word_index': [(5, 7)], 'id': 'C90-1013.7'}, 'e2': {'word': 'generation system', 'word_index': [(1, 2)], 'id': 'C90-1013.6'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '5:7': 'ENTITY', '16:17': 'ENTITYUNRELATED'}}	This generation system also uses disjunctive feature structures to reduce the number of copies of the derivation tree .
The grammar for this generator is designed to properly generate the speaker&apos;s intention in a telephone dialogue .	grammar	generator	usage	{'e1': {'word': 'grammar', 'word_index': [(1, 1)], 'id': 'C90-1013.9'}, 'e2': {'word': 'generator', 'word_index': [(4, 4)], 'id': 'C90-1013.10'}, 'entity_replacement': {'1:1': 'ENTITY', '4:4': 'ENTITYOTHER', '11:13': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED'}}	The grammar for this generator is designed to properly generate the speaker&apos ;s intention in a telephone dialogue .
 This paper proposes document oriented preference sets(DoPS) for the disambiguation of the dependency structure of sentences .	dependency structure	sentences	model-feature	{'e1': {'word': 'dependency structure', 'word_index': [(15, 16)], 'id': 'C90-2032.2'}, 'e2': {'word': 'sentences', 'word_index': [(18, 18)], 'id': 'C90-2032.3'}, 'entity_replacement': {'3:9': 'ENTITYUNRELATED', '15:16': 'ENTITY', '18:18': 'ENTITYOTHER'}}	This paper proposes document oriented preference sets ( DoPS ) for the disambiguation of the dependency structure of sentences .
The DoPS system extracts preference knowledge from a target document or other documents automatically.	DoPS system	target document	usage	{'e1': {'word': 'DoPS system', 'word_index': [(1, 2)], 'id': 'C90-2032.4'}, 'e2': {'word': 'target document', 'word_index': [(8, 9)], 'id': 'C90-2032.5'}, 'entity_replacement': {'1:2': 'ENTITY', '8:9': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED'}}	The DoPS system extracts preference knowledge from a target document or other documents automatically .
Implementation and empirical results are described for the the analysis of dependency structures of Japanese patent claim sentences .	dependency structures	Japanese patent claim sentences	model-feature	{'e1': {'word': 'dependency structures', 'word_index': [(11, 12)], 'id': 'C90-2032.11'}, 'e2': {'word': 'Japanese patent claim sentences', 'word_index': [(14, 17)], 'id': 'C90-2032.12'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '2:3': 'ENTITYUNRELATED', '11:12': 'ENTITY', '14:17': 'ENTITYOTHER'}}	Implementation and empirical results are described for the the analysis of dependency structures of Japanese patent claim sentences .
 This paper describes the framework of a Korean phonological knowledge base system using the unification-based grammar formalism : Korean Phonology Structure Grammar (KPSG) .	unification-based grammar formalism	Korean phonological knowledge base system	usage	{'e1': {'word': 'unification-based grammar formalism', 'word_index': [(14, 18)], 'id': 'C90-3014.2'}, 'e2': {'word': 'Korean phonological knowledge base system', 'word_index': [(7, 11)], 'id': 'C90-3014.1'}, 'entity_replacement': {'7:11': 'ENTITYOTHER', '14:18': 'ENTITY', '20:26': 'ENTITYUNRELATED'}}	This paper describes the framework of a Korean phonological knowledge base system using the unification - based grammar formalism : Korean Phonology Structure Grammar ( KPSG ) .
The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system .	KPSG	phonological system	usage	{'e1': {'word': 'KPSG', 'word_index': [(3, 3)], 'id': 'C90-3014.4'}, 'e2': {'word': 'phonological system', 'word_index': [(13, 14)], 'id': 'C90-3014.5'}, 'entity_replacement': {'3:3': 'ENTITY', '13:14': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system .
 The unique properties of tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax , for instance, to the task of semantic interpretation or automatic translation of natural language .	syntax	semantic interpretation	compare	{'e1': {'word': 'syntax', 'word_index': [(24, 24)], 'id': 'C90-3045.3'}, 'e2': {'word': 'semantic interpretation', 'word_index': [(33, 34)], 'id': 'C90-3045.4'}, 'entity_replacement': {'4:10': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '24:24': 'ENTITY', '33:34': 'ENTITYOTHER', '36:40': 'ENTITYUNRELATED'}}	The unique properties of tree - adjoining grammars ( TAG ) present a challenge for the application of TAGs beyond the limited confines of syntax , for instance , to the task of semantic interpretation or automatic translation of natural language .
The formalism&apos;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper .	logical form language	semantics	model-feature	{'e1': {'word': 'logical form language', 'word_index': [(19, 21)], 'id': 'C90-3045.11'}, 'e2': {'word': 'semantics', 'word_index': [(15, 15)], 'id': 'C90-3045.10'}, 'entity_replacement': {'8:11': 'ENTITYUNRELATED', '15:15': 'ENTITYOTHER', '19:21': 'ENTITY', '26:26': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '40:40': 'ENTITYUNRELATED', '48:49': 'ENTITYUNRELATED'}}	The formalism&apos ;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary , we intend it to allow TAGs to be used beyond their role in syntax proper .
The formalism&apos;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper .	translates	natural language	model-feature	{'e1': {'word': 'translates', 'word_index': [(26, 26)], 'id': 'C90-3045.12'}, 'e2': {'word': 'natural language', 'word_index': [(29, 30)], 'id': 'C90-3045.13'}, 'entity_replacement': {'8:11': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '26:26': 'ENTITY', '29:30': 'ENTITYOTHER', '40:40': 'ENTITYUNRELATED', '48:49': 'ENTITYUNRELATED'}}	The formalism&apos ;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary , we intend it to allow TAGs to be used beyond their role in syntax proper .
The formalism&apos;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper .	TAGs	syntax proper	usage	{'e1': {'word': 'TAGs', 'word_index': [(40, 40)], 'id': 'C90-3045.14'}, 'e2': {'word': 'syntax proper', 'word_index': [(48, 49)], 'id': 'C90-3045.15'}, 'entity_replacement': {'8:11': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '40:40': 'ENTITY', '48:49': 'ENTITYOTHER'}}	The formalism&apos ;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary , we intend it to allow TAGs to be used beyond their role in syntax proper .
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	sentence analysis	defeasible reasoning	model-feature	{'e1': {'word': 'sentence analysis', 'word_index': [(4, 5)], 'id': 'C90-3046.1'}, 'e2': {'word': 'defeasible reasoning', 'word_index': [(10, 11)], 'id': 'C90-3046.2'}, 'entity_replacement': {'4:5': 'ENTITY', '10:11': 'ENTITYOTHER', '19:21': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED'}}	This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	argumentation system	Japanese sentence analyses	usage	{'e1': {'word': 'argumentation system', 'word_index': [(24, 25)], 'id': 'C90-3046.4'}, 'e2': {'word': 'Japanese sentence analyses', 'word_index': [(19, 21)], 'id': 'C90-3046.3'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '19:21': 'ENTITYOTHER', '24:25': 'ENTITY', '32:32': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED'}}	This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	formalization	defeasible reasoning	model-feature	{'e1': {'word': 'formalization', 'word_index': [(32, 32)], 'id': 'C90-3046.5'}, 'e2': {'word': 'defeasible reasoning', 'word_index': [(34, 35)], 'id': 'C90-3046.6'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '32:32': 'ENTITY', '34:35': 'ENTITYOTHER', '39:39': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED'}}	This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	defeat rules	defeasibility	model-feature	{'e1': {'word': 'defeat rules', 'word_index': [(41, 42)], 'id': 'C90-3046.8'}, 'e2': {'word': 'defeasibility', 'word_index': [(45, 45)], 'id': 'C90-3046.9'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED', '41:42': 'ENTITY', '45:45': 'ENTITYOTHER'}}	This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .
 Spelling-checkers have become an integral part of most text processing software .	Spelling-checkers	text processing software	part_whole	{'e1': {'word': 'Spelling-checkers', 'word_index': [(0, 1)], 'id': 'C90-3072.1'}, 'e2': {'word': 'text processing software', 'word_index': [(9, 11)], 'id': 'C90-3072.2'}, 'entity_replacement': {'0:1': 'ENTITY', '9:11': 'ENTITYOTHER'}}	Spelling -checkers have become an integral part of most text processing software .
From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words .	dictionaries of word forms	words	compare	{'e1': {'word': 'dictionaries of word forms', 'word_index': [(15, 18)], 'id': 'C90-3072.3'}, 'e2': {'word': 'words', 'word_index': [(21, 21)], 'id': 'C90-3072.4'}, 'entity_replacement': {'15:18': 'ENTITY', '21:21': 'ENTITYOTHER'}}	From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words .
This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .	inflection	English	model-feature	{'e1': {'word': 'inflection', 'word_index': [(8, 8)], 'id': 'C90-3072.5'}, 'e2': {'word': 'English', 'word_index': [(11, 11)], 'id': 'C90-3072.6'}, 'entity_replacement': {'8:8': 'ENTITY', '11:11': 'ENTITYOTHER', '16:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED'}}	This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .
This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .	highly inflective languages	Czech	model-feature	{'e1': {'word': 'highly inflective languages', 'word_index': [(16, 18)], 'id': 'C90-3072.7'}, 'e2': {'word': 'Czech', 'word_index': [(21, 21)], 'id': 'C90-3072.8'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '16:18': 'ENTITY', '21:21': 'ENTITYOTHER', '23:23': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED'}}	This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .
The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million (for Czech ).	spelling-checkers	English	usage	{'e1': {'word': 'spelling-checkers', 'word_index': [(16, 18)], 'id': 'C90-3072.14'}, 'e2': {'word': 'English', 'word_index': [(20, 20)], 'id': 'C90-3072.15'}, 'entity_replacement': {'16:18': 'ENTITY', '20:20': 'ENTITYOTHER', '24:24': 'ENTITYUNRELATED', '29:31': 'ENTITYUNRELATED', '38:39': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED'}}	The speed of the resulting program lies somewhere in the middle of the scale of existing spelling - checkers for English and the main dictionary fits into the standard 360 K floppy , whereas the number of recognized word forms exceeds 6 million ( for Czech ) .
The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million (for Czech ).	word forms	Czech	part_whole	{'e1': {'word': 'word forms', 'word_index': [(38, 39)], 'id': 'C90-3072.18'}, 'e2': {'word': 'Czech', 'word_index': [(45, 45)], 'id': 'C90-3072.19'}, 'entity_replacement': {'16:18': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '29:31': 'ENTITYUNRELATED', '38:39': 'ENTITY', '45:45': 'ENTITYOTHER'}}	The speed of the resulting program lies somewhere in the middle of the scale of existing spelling - checkers for English and the main dictionary fits into the standard 360 K floppy , whereas the number of recognized word forms exceeds 6 million ( for Czech ) .
To avoid grammar coverage problems we use a fully-connected first-order statistical class grammar .	fully-connected first-order statistical class grammar	grammar coverage problems	usage	{'e1': {'word': 'fully-connected first-order statistical class grammar', 'word_index': [(8, 14)], 'id': 'H90-1016.4'}, 'e2': {'word': 'grammar coverage problems', 'word_index': [(2, 4)], 'id': 'H90-1016.3'}, 'entity_replacement': {'2:4': 'ENTITYOTHER', '8:14': 'ENTITY'}}	To avoid grammar coverage problems we use a fully - connected first-order statistical class grammar .
The speech-search algorithm is implemented on a board with a single Intel i860 chip , which provides a factor of 5 speed-up over a SUN 4 for straight C code .	Intel i860 chip	board	model-feature	{'e1': {'word': 'Intel i860 chip', 'word_index': [(13, 16)], 'id': 'H90-1016.7'}, 'e2': {'word': 'board', 'word_index': [(9, 9)], 'id': 'H90-1016.6'}, 'entity_replacement': {'1:4': 'ENTITYUNRELATED', '9:9': 'ENTITYOTHER', '13:16': 'ENTITY', '29:30': 'ENTITYUNRELATED', '32:34': 'ENTITYUNRELATED'}}	The speech - search algorithm is implemented on a board with a single Intel i 860 chip , which provides a factor of 5 speed - up over a SUN 4 for straight C code .
The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and application back end .	VME bus	SUN4	part_whole	{'e1': {'word': 'VME bus', 'word_index': [(6, 7)], 'id': 'H90-1016.11'}, 'e2': {'word': 'SUN4', 'word_index': [(10, 10)], 'id': 'H90-1016.12'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '6:7': 'ENTITY', '10:10': 'ENTITYOTHER', '19:21': 'ENTITYUNRELATED', '23:25': 'ENTITYUNRELATED'}}	The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and application back end .
First, we present a new paradigm for speaker-independent (SI) training of hidden Markov models (HMM) , which uses a large amount of speech from a few speakers instead of the traditional practice of using a little speech from many speakers .	speech	speaker-independent (SI) training	usage	{'e1': {'word': 'speech', 'word_index': [(29, 29)], 'id': 'H90-1060.4'}, 'e2': {'word': 'speaker-independent (SI) training', 'word_index': [(8, 14)], 'id': 'H90-1060.2'}, 'entity_replacement': {'8:14': 'ENTITYOTHER', '16:21': 'ENTITYUNRELATED', '29:29': 'ENTITY', '33:33': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED', '46:46': 'ENTITYUNRELATED'}}	First , we present a new paradigm for speaker - independent ( SI ) training of hidden Markov models ( HMM ) , which uses a large amount of speech from a few speakers instead of the traditional practice of using a little speech from many speakers .
In addition, combination of the training speakers is done by averaging the statistics of independently trained models rather than the usual pooling of all the speech data from many speakers prior to training .	statistics	speech data	compare	{'e1': {'word': 'statistics', 'word_index': [(13, 13)], 'id': 'H90-1060.9'}, 'e2': {'word': 'speech data', 'word_index': [(26, 27)], 'id': 'H90-1060.11'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '13:13': 'ENTITY', '15:17': 'ENTITYUNRELATED', '26:27': 'ENTITYOTHER', '30:30': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED'}}	In addition , combination of the training speakers is done by averaging the statistics of independently trained models rather than the usual pooling of all the speech data from many speakers prior to training .
With only 12 training speakers for SI recognition , we achieved a 7.5% word error rate on a standard grammar and test set from the DARPA Resource Management corpus .	training speakers	word error rate	result	{'e1': {'word': 'training speakers', 'word_index': [(3, 4)], 'id': 'H90-1060.14'}, 'e2': {'word': 'word error rate', 'word_index': [(14, 16)], 'id': 'H90-1060.16'}, 'entity_replacement': {'3:4': 'ENTITY', '6:7': 'ENTITYUNRELATED', '14:16': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '26:29': 'ENTITYUNRELATED'}}	With only 12 training speakers for SI recognition , we achieved a 7.5 % word error rate on a standard grammar and test set from the DARPA Resource Management corpus .
With only 12 training speakers for SI recognition , we achieved a 7.5% word error rate on a standard grammar and test set from the DARPA Resource Management corpus .	test set	DARPA Resource Management corpus	part_whole	{'e1': {'word': 'test set', 'word_index': [(22, 23)], 'id': 'H90-1060.18'}, 'e2': {'word': 'DARPA Resource Management corpus', 'word_index': [(26, 29)], 'id': 'H90-1060.19'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '6:7': 'ENTITYUNRELATED', '14:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '22:23': 'ENTITY', '26:29': 'ENTITYOTHER'}}	With only 12 training speakers for SI recognition , we achieved a 7.5 % word error rate on a standard grammar and test set from the DARPA Resource Management corpus .
Second, we show a significant improvement for speaker adaptation (SA) using the new SI corpus and a small amount of speech from the new (target) speaker .	SI corpus	speaker adaptation (SA)	usage	{'e1': {'word': 'SI corpus', 'word_index': [(16, 17)], 'id': 'H90-1060.23'}, 'e2': {'word': 'speaker adaptation (SA)', 'word_index': [(8, 12)], 'id': 'H90-1060.22'}, 'entity_replacement': {'8:12': 'ENTITYOTHER', '16:17': 'ENTITY', '23:23': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED'}}	Second , we show a significant improvement for speaker adaptation ( SA ) using the new SI corpus and a small amount of speech from the new ( target ) speaker .
A probabilistic spectral mapping is estimated independently for each training (reference) speaker and the target speaker .	probabilistic spectral mapping	training (reference) speaker	model-feature	{'e1': {'word': 'probabilistic spectral mapping', 'word_index': [(1, 3)], 'id': 'H90-1060.26'}, 'e2': {'word': 'training (reference) speaker', 'word_index': [(9, 13)], 'id': 'H90-1060.27'}, 'entity_replacement': {'1:3': 'ENTITY', '9:13': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED'}}	A probabilistic spectral mapping is estimated independently for each training ( reference ) speaker and the target speaker .
Each reference model is transformed to the space of the target speaker and combined by averaging .	averaging	reference model	usage	{'e1': {'word': 'averaging', 'word_index': [(15, 15)], 'id': 'H90-1060.32'}, 'e2': {'word': 'reference model', 'word_index': [(1, 2)], 'id': 'H90-1060.29'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '7:7': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '15:15': 'ENTITY'}}	Each reference model is transformed to the space of the target speaker and combined by averaging .
Using only 40 utterances from the target speaker for adaptation , the error rate dropped to 4.1% --- a 45% reduction in error compared to the SI result.	utterances	adaptation	usage	{'e1': {'word': 'utterances', 'word_index': [(3, 3)], 'id': 'H90-1060.33'}, 'e2': {'word': 'adaptation', 'word_index': [(9, 9)], 'id': 'H90-1060.35'}, 'entity_replacement': {'3:3': 'ENTITY', '6:7': 'ENTITYUNRELATED', '9:9': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	Using only 40 utterances from the target speaker for adaptation , the error rate dropped to 4.1 % --- a 45 % reduction in error compared to the SI result .
 This paper presents a specialized editor for a highly structured dictionary .	editor	dictionary	usage	{'e1': {'word': 'editor', 'word_index': [(5, 5)], 'id': 'J90-3002.1'}, 'e2': {'word': 'dictionary', 'word_index': [(10, 10)], 'id': 'J90-3002.2'}, 'entity_replacement': {'5:5': 'ENTITY', '10:10': 'ENTITYOTHER'}}	This paper presents a specialized editor for a highly structured dictionary .
The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent dictionary on the basis of a linguistic theory .	editor	lexicologists	usage	{'e1': {'word': 'editor', 'word_index': [(6, 6)], 'id': 'J90-3002.3'}, 'e2': {'word': 'lexicologists', 'word_index': [(15, 15)], 'id': 'J90-3002.4'}, 'entity_replacement': {'6:6': 'ENTITY', '15:15': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent dictionary on the basis of a linguistic theory .
The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent dictionary on the basis of a linguistic theory .	linguistic theory	dictionary	usage	{'e1': {'word': 'linguistic theory', 'word_index': [(27, 28)], 'id': 'J90-3002.6'}, 'e2': {'word': 'dictionary', 'word_index': [(21, 21)], 'id': 'J90-3002.5'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER', '27:28': 'ENTITY'}}	The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent dictionary on the basis of a linguistic theory .
If we want valuable lexicons and grammars to achieve complex natural language processing , we must provide very powerful tools to help create and ensure the validity of such complex linguistic databases .	grammars	natural language processing	usage	{'e1': {'word': 'grammars', 'word_index': [(6, 6)], 'id': 'J90-3002.8'}, 'e2': {'word': 'natural language processing', 'word_index': [(10, 12)], 'id': 'J90-3002.9'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '6:6': 'ENTITY', '10:12': 'ENTITYOTHER', '30:31': 'ENTITYUNRELATED'}}	If we want valuable lexicons and grammars to achieve complex natural language processing , we must provide very powerful tools to help create and ensure the validity of such complex linguistic databases .
 The principle known as free indexation plays an important role in the determination of the referential properties of noun phrases in the principle-and-parameters language framework .	free indexation	referential properties of noun phrases	result	{'e1': {'word': 'free indexation', 'word_index': [(4, 5)], 'id': 'P90-1014.1'}, 'e2': {'word': 'referential properties of noun phrases', 'word_index': [(15, 19)], 'id': 'P90-1014.2'}, 'entity_replacement': {'4:5': 'ENTITY', '15:19': 'ENTITYOTHER', '22:28': 'ENTITYUNRELATED'}}	The principle known as free indexation plays an important role in the determination of the referential properties of noun phrases in the principle - and - parameters language framework .
We describe three techniques for making syntactic analysis more robust---an agenda-based scheduling parser , a recovery technique for failed parses , and a new technique called terminal substring parsing .	agenda-based scheduling parser	syntactic analysis	usage	{'e1': {'word': 'agenda-based scheduling parser', 'word_index': [(12, 16)], 'id': 'A92-1026.6'}, 'e2': {'word': 'syntactic analysis', 'word_index': [(6, 7)], 'id': 'A92-1026.5'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '12:16': 'ENTITY', '19:23': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED'}}	We describe three techniques for making syntactic analysis more robust --- an agenda - based scheduling parser , a recovery technique for failed parses , and a new technique called terminal substring parsing .
For pragmatics processing , we describe how the method of abductive inference is inherently robust, in that an interpretation is always possible, so that in the absence of the required world knowledge , performance degrades gracefully.	abductive inference	pragmatics processing	usage	{'e1': {'word': 'abductive inference', 'word_index': [(10, 11)], 'id': 'A92-1026.10'}, 'e2': {'word': 'pragmatics processing', 'word_index': [(1, 2)], 'id': 'A92-1026.9'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '10:11': 'ENTITY', '32:33': 'ENTITYUNRELATED'}}	For pragmatics processing , we describe how the method of abductive inference is inherently robust , in that an interpretation is always possible , so that in the absence of the required world knowledge , performance degrades gracefully .
 We present an efficient algorithm for chart-based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task.	chart-based phrase structure parsing	natural language	usage	{'e1': {'word': 'chart-based phrase structure parsing', 'word_index': [(6, 11)], 'id': 'A92-1027.1'}, 'e2': {'word': 'natural language', 'word_index': [(13, 14)], 'id': 'A92-1027.2'}, 'entity_replacement': {'6:11': 'ENTITY', '13:14': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED'}}	We present an efficient algorithm for chart - based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task .
 We present an efficient algorithm for chart-based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task.	words	unrestricted texts	part_whole	{'e1': {'word': 'words', 'word_index': [(32, 32)], 'id': 'A92-1027.4'}, 'e2': {'word': 'unrestricted texts', 'word_index': [(26, 27)], 'id': 'A92-1027.3'}, 'entity_replacement': {'6:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '26:27': 'ENTITYOTHER', '32:32': 'ENTITY', '39:39': 'ENTITYUNRELATED'}}	We present an efficient algorithm for chart - based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task .
As each new edge is added to the chart , the algorithm checks only the topmost of the edges adjacent to it, rather than all such edges as in conventional treatments.	edges	edges	compare	{'e1': {'word': 'edges', 'word_index': [(18, 18)], 'id': 'A92-1027.11'}, 'e2': {'word': 'edges', 'word_index': [(27, 27)], 'id': 'A92-1027.12'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '18:18': 'ENTITY', '27:27': 'ENTITYOTHER'}}	As each new edge is added to the chart , the algorithm checks only the topmost of the edges adjacent to it , rather than all such edges as in conventional treatments .
This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .	function words	phrase boundary heuristics	usage	{'e1': {'word': 'function words', 'word_index': [(15, 16)], 'id': 'A92-1027.18'}, 'e2': {'word': 'phrase boundary heuristics', 'word_index': [(7, 9)], 'id': 'A92-1027.17'}, 'entity_replacement': {'7:9': 'ENTITYOTHER', '15:16': 'ENTITY', '20:21': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED'}}	This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .
This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .	unknown words	phrases	part_whole	{'e1': {'word': 'unknown words', 'word_index': [(35, 36)], 'id': 'A92-1027.21'}, 'e2': {'word': 'phrases', 'word_index': [(27, 27)], 'id': 'A92-1027.20'}, 'entity_replacement': {'7:9': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '27:27': 'ENTITYOTHER', '35:36': 'ENTITY'}}	This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .
A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced.	semantic	syntactic categories	compare	{'e1': {'word': 'semantic', 'word_index': [(11, 11)], 'id': 'A92-1027.23'}, 'e2': {'word': 'syntactic categories', 'word_index': [(14, 15)], 'id': 'A92-1027.24'}, 'entity_replacement': {'2:6': 'ENTITYUNRELATED', '11:11': 'ENTITY', '14:15': 'ENTITYOTHER', '18:21': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED'}}	A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .
A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced.	ambiguity	edges	result	{'e1': {'word': 'ambiguity', 'word_index': [(28, 28)], 'id': 'A92-1027.26'}, 'e2': {'word': 'edges', 'word_index': [(34, 34)], 'id': 'A92-1027.27'}, 'entity_replacement': {'2:6': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '18:21': 'ENTITYUNRELATED', '28:28': 'ENTITY', '34:34': 'ENTITYOTHER', '38:38': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED'}}	A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .
A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced.	semantic	edges	model-feature	{'e1': {'word': 'semantic', 'word_index': [(42, 42)], 'id': 'A92-1027.29'}, 'e2': {'word': 'edges', 'word_index': [(38, 38)], 'id': 'A92-1027.28'}, 'entity_replacement': {'2:6': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '18:21': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '38:38': 'ENTITYOTHER', '42:42': 'ENTITY'}}	A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .
 In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed.	abduction	discourse segmentation	usage	{'e1': {'word': 'abduction', 'word_index': [(16, 16)], 'id': 'C92-1052.3'}, 'e2': {'word': 'discourse segmentation', 'word_index': [(11, 12)], 'id': 'C92-1052.2'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '11:12': 'ENTITYOTHER', '16:16': 'ENTITY', '18:19': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED'}}	In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed .
 In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed.	temporal relations	segments	model-feature	{'e1': {'word': 'temporal relations', 'word_index': [(18, 19)], 'id': 'C92-1052.4'}, 'e2': {'word': 'segments', 'word_index': [(21, 21)], 'id': 'C92-1052.5'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '18:19': 'ENTITY', '21:21': 'ENTITYOTHER'}}	In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed .
 In this paper, a discrimination and robustness oriented adaptive learning procedure is proposed to deal with the task of syntactic ambiguity resolution .	adaptive learning procedure	syntactic ambiguity resolution	usage	{'e1': {'word': 'adaptive learning procedure', 'word_index': [(9, 11)], 'id': 'C92-1055.1'}, 'e2': {'word': 'syntactic ambiguity resolution', 'word_index': [(20, 22)], 'id': 'C92-1055.2'}, 'entity_replacement': {'9:11': 'ENTITY', '20:22': 'ENTITYOTHER'}}	In this paper , a discrimination and robustness oriented adaptive learning procedure is proposed to deal with the task of syntactic ambiguity resolution .
Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications.	language model	approximation error	result	{'e1': {'word': 'language model', 'word_index': [(14, 15)], 'id': 'C92-1055.5'}, 'e2': {'word': 'approximation error', 'word_index': [(9, 10)], 'id': 'C92-1055.4'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '9:10': 'ENTITYOTHER', '14:15': 'ENTITY', '18:19': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '29:31': 'ENTITYUNRELATED', '37:37': 'ENTITYUNRELATED'}}	Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications .
Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications.	maximum likelihood method	statistical approaches	usage	{'e1': {'word': 'maximum likelihood method', 'word_index': [(29, 31)], 'id': 'C92-1055.8'}, 'e2': {'word': 'statistical approaches', 'word_index': [(18, 19)], 'id': 'C92-1055.6'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER', '23:23': 'ENTITYUNRELATED', '29:31': 'ENTITY', '37:37': 'ENTITYUNRELATED'}}	Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications .
The accuracy rate of syntactic disambiguation is raised from 46.0% to 60.62% by using this novel approach.	syntactic disambiguation	accuracy rate	result	{'e1': {'word': 'syntactic disambiguation', 'word_index': [(4, 5)], 'id': 'C92-1055.14'}, 'e2': {'word': 'accuracy rate', 'word_index': [(1, 2)], 'id': 'C92-1055.13'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '4:5': 'ENTITY'}}	The accuracy rate of syntactic disambiguation is raised from 46.0 % to 60.62 % by using this novel approach .
 Graph unification remains the most expensive part of unification-based grammar parsing .	Graph unification	unification-based grammar parsing	part_whole	{'e1': {'word': 'Graph unification', 'word_index': [(0, 1)], 'id': 'C92-2068.1'}, 'e2': {'word': 'unification-based grammar parsing', 'word_index': [(8, 12)], 'id': 'C92-2068.2'}, 'entity_replacement': {'0:1': 'ENTITY', '8:12': 'ENTITYOTHER'}}	Graph unification remains the most expensive part of unification - based grammar parsing .
We propose a method of attaining such a design through a method of structure-sharing which avoids log(d) overheads often associated with structure-sharing of graphs without any use of costly dependency pointers .	log(d) overheads	structure-sharing of graphs	model-feature	{'e1': {'word': 'log(d) overheads', 'word_index': [(18, 22)], 'id': 'C92-2068.7'}, 'e2': {'word': 'structure-sharing of graphs', 'word_index': [(26, 30)], 'id': 'C92-2068.8'}, 'entity_replacement': {'13:15': 'ENTITYUNRELATED', '18:22': 'ENTITY', '26:30': 'ENTITYOTHER', '36:37': 'ENTITYUNRELATED'}}	We propose a method of attaining such a design through a method of structure - sharing which avoids log ( d ) overheads often associated with structure - sharing of graphs without any use of costly dependency pointers .
 The transfer phase in machine translation (MT) systems has been considered to be more complicated than analysis and generation , since it is inherently a conglomeration of individual lexical rules .	transfer phase	analysis	compare	{'e1': {'word': 'transfer phase', 'word_index': [(1, 2)], 'id': 'C92-2115.1'}, 'e2': {'word': 'analysis', 'word_index': [(18, 18)], 'id': 'C92-2115.3'}, 'entity_replacement': {'1:2': 'ENTITY', '4:9': 'ENTITYUNRELATED', '18:18': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	The transfer phase in machine translation ( MT ) systems has been considered to be more complicated than analysis and generation , since it is inherently a conglomeration of individual lexical rules .
Currently some attempts are being made to use case-based reasoning in machine translation , that is, to make decisions on the basis of translation examples at appropriate pints in MT .	case-based reasoning	machine translation	usage	{'e1': {'word': 'case-based reasoning', 'word_index': [(8, 11)], 'id': 'C92-2115.6'}, 'e2': {'word': 'machine translation', 'word_index': [(13, 14)], 'id': 'C92-2115.7'}, 'entity_replacement': {'8:11': 'ENTITY', '13:14': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	Currently some attempts are being made to use case - based reasoning in machine translation , that is , to make decisions on the basis of translation examples at appropriate pints in MT .
This paper proposes a new type of transfer system , called a Similarity-driven Transfer System (SimTran) , for use in such case-based MT (CBMT) .	Similarity-driven Transfer System (SimTran)	case-based MT (CBMT)	usage	{'e1': {'word': 'Similarity-driven Transfer System (SimTran)', 'word_index': [(12, 18)], 'id': 'C92-2115.11'}, 'e2': {'word': 'case-based MT (CBMT)', 'word_index': [(24, 30)], 'id': 'C92-2115.12'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '12:18': 'ENTITY', '24:30': 'ENTITYOTHER'}}	This paper proposes a new type of transfer system , called a Similarity -driven Transfer System ( SimTran ) , for use in such case - based MT ( CBMT ) .
When a very noisy portion is detected, the parser skips that portion using a fake non-terminal symbol .	non-terminal symbol	parser	usage	{'e1': {'word': 'non-terminal symbol', 'word_index': [(16, 17)], 'id': 'C92-3165.7'}, 'e2': {'word': 'parser', 'word_index': [(9, 9)], 'id': 'C92-3165.5'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '9:9': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '16:17': 'ENTITY'}}	When a very noisy portion is detected , the parser skips that portion using a fake non-terminal symbol .
The unidentified portion is resolved by re-utterance of that portion which is parsed very efficiently by using the parse record of the first utterance .	parse record	utterance	model-feature	{'e1': {'word': 'parse record', 'word_index': [(18, 19)], 'id': 'C92-3165.11'}, 'e2': {'word': 'utterance', 'word_index': [(23, 23)], 'id': 'C92-3165.12'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '18:19': 'ENTITY', '23:23': 'ENTITYOTHER'}}	The unidentified portion is resolved by re-utterance of that portion which is parsed very efficiently by using the parse record of the first utterance .
Detected unknown words can be incrementally incorporated into the dictionary after the interaction with the user .	unknown words	dictionary	part_whole	{'e1': {'word': 'unknown words', 'word_index': [(1, 2)], 'id': 'C92-3165.16'}, 'e2': {'word': 'dictionary', 'word_index': [(9, 9)], 'id': 'C92-3165.17'}, 'entity_replacement': {'1:2': 'ENTITY', '9:9': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED'}}	Detected unknown words can be incrementally incorporated into the dictionary after the interaction with the user .
In this paper, a new mechanism, based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in Chinese newspapers .	sublanguage	unknown words	usage	{'e1': {'word': 'sublanguage', 'word_index': [(13, 13)], 'id': 'C92-4199.3'}, 'e2': {'word': 'unknown words', 'word_index': [(19, 20)], 'id': 'C92-4199.4'}, 'entity_replacement': {'13:13': 'ENTITY', '19:20': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	In this paper , a new mechanism , based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in Chinese newspapers .
In this paper, a new mechanism, based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in Chinese newspapers .	personal names	Chinese newspapers	part_whole	{'e1': {'word': 'personal names', 'word_index': [(23, 24)], 'id': 'C92-4199.5'}, 'e2': {'word': 'Chinese newspapers', 'word_index': [(27, 28)], 'id': 'C92-4199.6'}, 'entity_replacement': {'13:13': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '23:24': 'ENTITY', '27:28': 'ENTITYOTHER'}}	In this paper , a new mechanism , based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in Chinese newspapers .
 This paper describes the understanding process of the spatial descriptions in Japanese .	spatial descriptions	Japanese	part_whole	{'e1': {'word': 'spatial descriptions', 'word_index': [(8, 9)], 'id': 'C92-4207.1'}, 'e2': {'word': 'Japanese', 'word_index': [(11, 11)], 'id': 'C92-4207.2'}, 'entity_replacement': {'8:9': 'ENTITY', '11:11': 'ENTITYOTHER'}}	This paper describes the understanding process of the spatial descriptions in Japanese .
It is done by an experimental computer program SPRINT , which takes natural language texts and produces a model of the described world .	model	world	model-feature	{'e1': {'word': 'model', 'word_index': [(18, 18)], 'id': 'C92-4207.7'}, 'e2': {'word': 'world', 'word_index': [(22, 22)], 'id': 'C92-4207.8'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '18:18': 'ENTITY', '22:22': 'ENTITYOTHER'}}	It is done by an experimental computer program SPRINT , which takes natural language texts and produces a model of the described world .
To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the spatial attributes of the entities .	qualitative spatial constraints	text	part_whole	{'e1': {'word': 'qualitative spatial constraints', 'word_index': [(9, 11)], 'id': 'C92-4207.10'}, 'e2': {'word': 'text', 'word_index': [(14, 14)], 'id': 'C92-4207.11'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '9:11': 'ENTITY', '14:14': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the spatial attributes of the entities .
To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the spatial attributes of the entities .	spatial attributes	entities	model-feature	{'e1': {'word': 'spatial attributes', 'word_index': [(25, 26)], 'id': 'C92-4207.13'}, 'e2': {'word': 'entities', 'word_index': [(29, 29)], 'id': 'C92-4207.14'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '9:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '25:26': 'ENTITY', '29:29': 'ENTITYOTHER'}}	To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the spatial attributes of the entities .
The interpretation reflects the temporary belief about the world .	temporary belief	world	model-feature	{'e1': {'word': 'temporary belief', 'word_index': [(4, 5)], 'id': 'C92-4207.16'}, 'e2': {'word': 'world', 'word_index': [(8, 8)], 'id': 'C92-4207.17'}, 'entity_replacement': {'4:5': 'ENTITY', '8:8': 'ENTITYOTHER'}}	The interpretation reflects the temporary belief about the world .
 This paper describes a recently collected spoken language corpus for the ATIS (Air Travel Information System) domain .	ATIS (Air Travel Information System) domain	spoken language corpus	model-feature	{'e1': {'word': 'ATIS (Air Travel Information System) domain', 'word_index': [(11, 18)], 'id': 'H92-1003.2'}, 'e2': {'word': 'spoken language corpus', 'word_index': [(6, 8)], 'id': 'H92-1003.1'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '11:18': 'ENTITY'}}	This paper describes a recently collected spoken language corpus for the ATIS ( Air Travel Information System ) domain .
We summarize the motivation for this effort, the goals, the implementation of a multi-site data collection paradigm , and the accomplishments of MADCOW in monitoring the collection and distribution of 12,000 utterances of spontaneous speech from five sites for use in a multi-site common evaluation of speech, natural language and spoken language .</abstract>	spontaneous speech	utterances	model-feature	{'e1': {'word': 'spontaneous speech', 'word_index': [(35, 36)], 'id': 'H92-1003.8'}, 'e2': {'word': 'utterances', 'word_index': [(33, 33)], 'id': 'H92-1003.7'}, 'entity_replacement': {'15:18': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '33:33': 'ENTITYOTHER', '35:36': 'ENTITY', '44:54': 'ENTITYUNRELATED'}}	We summarize the motivation for this effort , the goals , the implementation of a multi-site data collection paradigm , and the accomplishments of MADCOW in monitoring the collection and distribution of 12,000 utterances of spontaneous speech from five sites for use in a multi-site common evaluation of speech , natural language and spoken language .< / abstract >
 The paper provides an overview of the research conducted at LIMSI in the field of speech processing , but also in the related areas of Human-Machine Communication , including Natural Language Processing , Non Verbal and Multimodal Communication .	speech processing	Human-Machine Communication	compare	{'e1': {'word': 'speech processing', 'word_index': [(15, 16)], 'id': 'H92-1010.2'}, 'e2': {'word': 'Human-Machine Communication', 'word_index': [(25, 28)], 'id': 'H92-1010.3'}, 'entity_replacement': {'10:10': 'ENTITYUNRELATED', '15:16': 'ENTITY', '25:28': 'ENTITYOTHER', '31:33': 'ENTITYUNRELATED', '35:39': 'ENTITYUNRELATED'}}	The paper provides an overview of the research conducted at LIMSI in the field of speech processing , but also in the related areas of Human - Machine Communication , including Natural Language Processing , Non Verbal and Multimodal Communication .
 This paper describes three relatively domain-independent capabilities recently added to the Paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and database query paraphrase .	domain-independent capabilities	Paramax spoken language understanding system	model-feature	{'e1': {'word': 'domain-independent capabilities', 'word_index': [(5, 8)], 'id': 'H92-1017.1'}, 'e2': {'word': 'Paramax spoken language understanding system', 'word_index': [(13, 17)], 'id': 'H92-1017.2'}, 'entity_replacement': {'5:8': 'ENTITY', '13:17': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED', '22:24': 'ENTITYUNRELATED', '27:29': 'ENTITYUNRELATED'}}	This paper describes three relatively domain - independent capabilities recently added to the Paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and database query paraphrase .
Finally, we briefly describe an experiment which we have done in extending the n-best speech/language integration architecture to improving OCR accuracy .	n-best speech/language integration architecture	accuracy	result	{'e1': {'word': 'n-best speech/language integration architecture', 'word_index': [(14, 19)], 'id': 'H92-1017.8'}, 'e2': {'word': 'accuracy', 'word_index': [(23, 23)], 'id': 'H92-1017.10'}, 'entity_replacement': {'14:19': 'ENTITY', '22:22': 'ENTITYUNRELATED', '23:23': 'ENTITYOTHER'}}	Finally , we briefly describe an experiment which we have done in extending the n-best speech / language integration architecture to improving OCR accuracy .
 We describe a generative probabilistic model of natural language , which we call HBG , that takes advantage of detailed linguistic information to resolve ambiguity .	linguistic information	ambiguity	usage	{'e1': {'word': 'linguistic information', 'word_index': [(20, 21)], 'id': 'H92-1026.3'}, 'e2': {'word': 'ambiguity', 'word_index': [(24, 24)], 'id': 'H92-1026.4'}, 'entity_replacement': {'3:8': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '20:21': 'ENTITY', '24:24': 'ENTITYOTHER'}}	We describe a generative probabilistic model of natural language , which we call HBG , that takes advantage of detailed linguistic information to resolve ambiguity .
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.	lexical, syntactic, semantic, and structural information	HBG	usage	{'e1': {'word': 'lexical, syntactic, semantic, and structural information', 'word_index': [(2, 10)], 'id': 'H92-1026.6'}, 'e2': {'word': 'HBG', 'word_index': [(0, 0)], 'id': 'H92-1026.5'}, 'entity_replacement': {'0:0': 'ENTITYOTHER', '2:10': 'ENTITY', '13:14': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	HBG incorporates lexical , syntactic , semantic , and structural information from the parse tree into the disambiguation process in a novel way .
We use a corpus of bracketed sentences , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence .	parse	sentence	model-feature	{'e1': {'word': 'parse', 'word_index': [(33, 33)], 'id': 'H92-1026.13'}, 'e2': {'word': 'sentence', 'word_index': [(36, 36)], 'id': 'H92-1026.14'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '15:17': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '33:33': 'ENTITY', '36:36': 'ENTITYOTHER'}}	We use a corpus of bracketed sentences , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence .
This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse .	linguistic introspection	grammar	usage	{'e1': {'word': 'linguistic introspection', 'word_index': [(15, 16)], 'id': 'H92-1026.16'}, 'e2': {'word': 'grammar', 'word_index': [(10, 10)], 'id': 'H92-1026.15'}, 'entity_replacement': {'10:10': 'ENTITYOTHER', '15:16': 'ENTITY', '24:24': 'ENTITYUNRELATED'}}	This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse .
In head-to-head tests against one of the best existing robust probabilistic parsing models , which we call P-CFG, the HBG model significantly outperforms P-CFG , increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.	HBG model	P-CFG	compare	{'e1': {'word': 'HBG model', 'word_index': [(24, 25)], 'id': 'H92-1026.21'}, 'e2': {'word': 'P-CFG', 'word_index': [(28, 28)], 'id': 'H92-1026.22'}, 'entity_replacement': {'1:6': 'ENTITYUNRELATED', '14:16': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:25': 'ENTITY', '28:28': 'ENTITYOTHER', '32:33': 'ENTITYUNRELATED'}}	In head - to - head tests against one of the best existing robust probabilistic parsing models , which we call P-CFG , the HBG model significantly outperforms P-CFG , increasing the parsing accuracy rate from 60 % to 75 % , a 37 % reduction in error .
The classical MLE reestimation algorithms , namely the forward-backward algorithm and the segmental k-means algorithm , are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities .	reestimation formulas	HMM with Gaussian mixture observation densities	usage	{'e1': {'word': 'reestimation formulas', 'word_index': [(21, 22)], 'id': 'H92-1036.6'}, 'e2': {'word': 'HMM with Gaussian mixture observation densities', 'word_index': [(26, 31)], 'id': 'H92-1036.7'}, 'entity_replacement': {'2:4': 'ENTITYUNRELATED', '8:11': 'ENTITYUNRELATED', '14:16': 'ENTITYUNRELATED', '21:22': 'ENTITY', '26:31': 'ENTITYOTHER'}}	The classical MLE reestimation algorithms , namely the forward - backward algorithm and the segmental k-means algorithm , are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities .
Because of its adaptive nature, Bayesian learning serves as a unified approach for the following four speech recognition applications, namely parameter smoothing , speaker adaptation , speaker group modeling and corrective training .	Bayesian learning	speech recognition	usage	{'e1': {'word': 'Bayesian learning', 'word_index': [(6, 7)], 'id': 'H92-1036.8'}, 'e2': {'word': 'speech recognition', 'word_index': [(17, 18)], 'id': 'H92-1036.9'}, 'entity_replacement': {'6:7': 'ENTITY', '17:18': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '28:30': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four speech recognition applications , namely parameter smoothing , speaker adaptation , speaker group modeling and corrective training .
 It is well-known that there are polysemous words like sentence whose meaning or sense depends on the context of use.	meaning	sentence	model-feature	{'e1': {'word': 'meaning', 'word_index': [(13, 13)], 'id': 'H92-1045.3'}, 'e2': {'word': 'sentence', 'word_index': [(11, 11)], 'id': 'H92-1045.2'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '11:11': 'ENTITYOTHER', '13:13': 'ENTITY', '15:15': 'ENTITYUNRELATED'}}	It is well - known that there are polysemous words like sentence whose meaning or sense depends on the context of use .
We have recently reported on two new word-sense disambiguation systems , one trained on bilingual material (the Canadian Hansards ) and the other trained on monolingual material ( Roget&apos;s Thesaurus and Grolier&apos;s Encyclopedia ).	bilingual material	word-sense disambiguation systems	usage	{'e1': {'word': 'bilingual material', 'word_index': [(16, 17)], 'id': 'H92-1045.6'}, 'e2': {'word': 'word-sense disambiguation systems', 'word_index': [(7, 11)], 'id': 'H92-1045.5'}, 'entity_replacement': {'7:11': 'ENTITYOTHER', '16:17': 'ENTITY', '20:21': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '31:33': 'ENTITYUNRELATED', '35:37': 'ENTITYUNRELATED'}}	We have recently reported on two new word - sense disambiguation systems , one trained on bilingual material ( the Canadian Hansards ) and the other trained on monolingual material ( Roget&apos ;s Thesaurus and Grolier&apos ;s Encyclopedia ) .
That is, if a polysemous word such as sentence appears two or more times in a well-written discourse , it is extremely likely that they will all share the same sense .	sentence	well-written discourse	part_whole	{'e1': {'word': 'sentence', 'word_index': [(9, 9)], 'id': 'H92-1045.13'}, 'e2': {'word': 'well-written discourse', 'word_index': [(17, 20)], 'id': 'H92-1045.14'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '9:9': 'ENTITY', '17:20': 'ENTITYOTHER', '33:33': 'ENTITYUNRELATED'}}	That is , if a polysemous word such as sentence appears two or more times in a well - written discourse , it is extremely likely that they will all share the same sense .
This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98%).	sense	discourse	part_whole	{'e1': {'word': 'sense', 'word_index': [(16, 16)], 'id': 'H92-1045.16'}, 'e2': {'word': 'discourse', 'word_index': [(20, 20)], 'id': 'H92-1045.17'}, 'entity_replacement': {'16:16': 'ENTITY', '20:20': 'ENTITYOTHER'}}	This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong ( 98 % ) .
This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algorithm .	constraint	word-sense disambiguation algorithm	usage	{'e1': {'word': 'constraint', 'word_index': [(10, 10)], 'id': 'H92-1045.18'}, 'e2': {'word': 'word-sense disambiguation algorithm', 'word_index': [(17, 21)], 'id': 'H92-1045.19'}, 'entity_replacement': {'10:10': 'ENTITY', '17:21': 'ENTITYOTHER'}}	This result can be used as an additional source of constraint for improving the performance of the word - sense disambiguation algorithm .
In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint .	discourse constraint	disambiguation algorithms	usage	{'e1': {'word': 'discourse constraint', 'word_index': [(20, 21)], 'id': 'H92-1045.21'}, 'e2': {'word': 'disambiguation algorithms', 'word_index': [(11, 12)], 'id': 'H92-1045.20'}, 'entity_replacement': {'11:12': 'ENTITYOTHER', '20:21': 'ENTITY'}}	In addition , it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint .
In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction.	dependency relation paths	answer extraction	usage	{'e1': {'word': 'dependency relation paths', 'word_index': [(8, 10)], 'id': 'P06-1112.1'}, 'e2': {'word': 'answer extraction', 'word_index': [(16, 17)], 'id': 'P06-1112.2'}, 'entity_replacement': {'8:10': 'ENTITY', '16:17': 'ENTITYOTHER'}}	In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction .
Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question.	dependency relations	question phrases	model-feature	{'e1': {'word': 'dependency relations', 'word_index': [(7, 8)], 'id': 'P06-1112.4'}, 'e2': {'word': 'question phrases', 'word_index': [(15, 16)], 'id': 'P06-1112.5'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '7:8': 'ENTITY', '15:16': 'ENTITYOTHER', '18:18': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED'}}	Using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .
Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure.	mapping score	correlation measure	part_whole	{'e1': {'word': 'mapping score', 'word_index': [(15, 16)], 'id': 'P06-1112.9'}, 'e2': {'word': 'correlation measure', 'word_index': [(19, 20)], 'id': 'P06-1112.10'}, 'entity_replacement': {'8:11': 'ENTITYUNRELATED', '15:16': 'ENTITY', '19:20': 'ENTITYOTHER'}}	Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .
This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus.	cooccurrence patterns	corpus	part_whole	{'e1': {'word': 'cooccurrence patterns', 'word_index': [(10, 11)], 'id': 'C90-3063.3'}, 'e2': {'word': 'corpus', 'word_index': [(15, 15)], 'id': 'C90-3063.4'}, 'entity_replacement': {'10:11': 'ENTITY', '15:15': 'ENTITYOTHER'}}	This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus .
To a large extent, these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities.	semantic constraints	anaphora references	usage	{'e1': {'word': 'semantic constraints', 'word_index': [(8, 9)], 'id': 'C90-3063.5'}, 'e2': {'word': 'anaphora references', 'word_index': [(16, 17)], 'id': 'C90-3063.6'}, 'entity_replacement': {'8:9': 'ENTITY', '16:17': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED'}}	To a large extent , these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities .
"An experiment was performed to resolve references of the pronoun ""it"" in sentences that were randomly selected from the corpus."	references	"pronoun ""it"""	model-feature	"{'e1': {'word': 'references', 'word_index': [(6, 6)], 'id': 'C90-3063.8'}, 'e2': {'word': 'pronoun ""it""', 'word_index': [(9, 12)], 'id': 'C90-3063.9'}, 'entity_replacement': {'6:6': 'ENTITY', '9:12': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED'}}"	"An experiment was performed to resolve references of the pronoun "" it "" in sentences that were randomly selected from the corpus ."
"An experiment was performed to resolve references of the pronoun ""it"" in sentences that were randomly selected from the corpus."	sentences	corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(14, 14)], 'id': 'C90-3063.10'}, 'e2': {'word': 'corpus', 'word_index': [(21, 21)], 'id': 'C90-3063.11'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:12': 'ENTITYUNRELATED', '14:14': 'ENTITY', '21:21': 'ENTITYOTHER'}}	"An experiment was performed to resolve references of the pronoun "" it "" in sentences that were randomly selected from the corpus ."
The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool.	cooccurrence statistics	disambiguation tool	usage	{'e1': {'word': 'cooccurrence statistics', 'word_index': [(13, 14)], 'id': 'C90-3063.12'}, 'e2': {'word': 'disambiguation tool', 'word_index': [(28, 29)], 'id': 'C90-3063.14'}, 'entity_replacement': {'13:14': 'ENTITY', '18:19': 'ENTITYUNRELATED', '28:29': 'ENTITYOTHER'}}	The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool .
We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a probabilistic context-free grammar and a probabilistic finite automaton.	probabilistic context-free grammar	probabilistic finite automaton	compare	{'e1': {'word': 'probabilistic context-free grammar', 'word_index': [(20, 24)], 'id': 'C04-1011.3'}, 'e2': {'word': 'probabilistic finite automaton', 'word_index': [(27, 29)], 'id': 'C04-1011.4'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '20:24': 'ENTITY', '27:29': 'ENTITYOTHER'}}	We consider the problem of computing the Kullback - Leibler distance , also called the relative entropy , between a probabilistic context - free grammar and a probabilistic finite automaton .
We show that there is a closed-form (analytical) solution for one part of the Kullback-Leibler distance, viz. the cross-entropy.	cross-entropy	Kullback-Leibler distance	part_whole	{'e1': {'word': 'cross-entropy', 'word_index': [(26, 28)], 'id': 'C04-1011.7'}, 'e2': {'word': 'Kullback-Leibler distance', 'word_index': [(18, 21)], 'id': 'C04-1011.6'}, 'entity_replacement': {'6:12': 'ENTITYUNRELATED', '18:21': 'ENTITYOTHER', '26:28': 'ENTITY'}}	We show that there is a closed - form ( analytical ) solution for one part of the Kullback - Leibler distance , viz . the cross - entropy .
We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata.	probabilistic finite automata	distributional approximation	usage	{'e1': {'word': 'probabilistic finite automata', 'word_index': [(22, 24)], 'id': 'C04-1011.10'}, 'e2': {'word': 'distributional approximation', 'word_index': [(11, 12)], 'id': 'C04-1011.8'}, 'entity_replacement': {'11:12': 'ENTITYOTHER', '14:18': 'ENTITYUNRELATED', '22:24': 'ENTITY'}}	We discuss several applications of the result to the problem of distributional approximation of probabilistic context - free grammars by means of probabilistic finite automata .
Methods developed for spelling correction for languages like English (see the review by Kukich (Kukich, 1992)) are not readily applicable to agglutinative languages.	spelling correction	languages	usage	{'e1': {'word': 'spelling correction', 'word_index': [(3, 4)], 'id': 'A94-1037.1'}, 'e2': {'word': 'languages', 'word_index': [(6, 6)], 'id': 'A94-1037.2'}, 'entity_replacement': {'3:4': 'ENTITY', '6:6': 'ENTITYOTHER', '8:8': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	Methods developed for spelling correction for languages like English ( see the review by Kukich ( Kukich , 1992 ) ) are not readily applicable to agglutinative languages .
This poster presents an approach to spelling correction in agglutinative languages that is based on two-level morphology and a dynamic-programming based search algorithm.	spelling correction	agglutinative languages	usage	{'e1': {'word': 'spelling correction', 'word_index': [(6, 7)], 'id': 'A94-1037.5'}, 'e2': {'word': 'agglutinative languages', 'word_index': [(9, 10)], 'id': 'A94-1037.6'}, 'entity_replacement': {'6:7': 'ENTITY', '9:10': 'ENTITYOTHER', '15:18': 'ENTITYUNRELATED', '21:25': 'ENTITYUNRELATED'}}	This poster presents an approach to spelling correction in agglutinative languages that is based on two - level morphology and a dynamic- programming based search algorithm .
After an overview of our approach, we present results from experiments with spelling correction in Turkish.	spelling correction	Turkish	usage	{'e1': {'word': 'spelling correction', 'word_index': [(13, 14)], 'id': 'A94-1037.9'}, 'e2': {'word': 'Turkish', 'word_index': [(16, 16)], 'id': 'A94-1037.10'}, 'entity_replacement': {'13:14': 'ENTITY', '16:16': 'ENTITYOTHER'}}	After an overview of our approach , we present results from experiments with spelling correction in Turkish .
The major objective of this program is to develop and demonstrate robust, high performance continuous speech recognition (CSR) techniques focussed on application in Spoken Language Systems (SLS) which will enhance the effectiveness of military and civilian computer-based systems.	continuous speech recognition (CSR) techniques	Spoken Language Systems (SLS)	usage	{'e1': {'word': 'continuous speech recognition (CSR) techniques', 'word_index': [(15, 21)], 'id': 'H94-1102.1'}, 'e2': {'word': 'Spoken Language Systems (SLS)', 'word_index': [(26, 31)], 'id': 'H94-1102.2'}, 'entity_replacement': {'15:21': 'ENTITY', '26:31': 'ENTITYOTHER', '38:44': 'ENTITYUNRELATED'}}	The major objective of this program is to develop and demonstrate robust , high performance continuous speech recognition ( CSR ) techniques focussed on application in Spoken Language Systems ( SLS ) which will enhance the effectiveness of military and civilian computer - based systems .
A key complementary objective is to define and develop applications of robust speech recognition and understanding systems, and to help catalyze the transition of spoken language technology into military and civilian systems, with particular focus on application of robust CSR to mobile military command and control.	spoken language technology	military and civilian systems	usage	{'e1': {'word': 'spoken language technology', 'word_index': [(25, 27)], 'id': 'H94-1102.5'}, 'e2': {'word': 'military and civilian systems', 'word_index': [(29, 32)], 'id': 'H94-1102.6'}, 'entity_replacement': {'12:16': 'ENTITYUNRELATED', '25:27': 'ENTITY', '29:32': 'ENTITYOTHER', '41:41': 'ENTITYUNRELATED', '43:47': 'ENTITYUNRELATED'}}	A key complementary objective is to define and develop applications of robust speech recognition and understanding systems , and to help catalyze the transition of spoken language technology into military and civilian systems , with particular focus on application of robust CSR to mobile military command and control .
A key complementary objective is to define and develop applications of robust speech recognition and understanding systems, and to help catalyze the transition of spoken language technology into military and civilian systems, with particular focus on application of robust CSR to mobile military command and control.	CSR	mobile military command and control	usage	{'e1': {'word': 'CSR', 'word_index': [(41, 41)], 'id': 'H94-1102.7'}, 'e2': {'word': 'mobile military command and control', 'word_index': [(43, 47)], 'id': 'H94-1102.8'}, 'entity_replacement': {'12:16': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED', '29:32': 'ENTITYUNRELATED', '41:41': 'ENTITY', '43:47': 'ENTITYOTHER'}}	A key complementary objective is to define and develop applications of robust speech recognition and understanding systems , and to help catalyze the transition of spoken language technology into military and civilian systems , with particular focus on application of robust CSR to mobile military command and control .
The research effort focusses on developing advanced acoustic modelling, rapid search, and recognition-time adaptation techniques for robust large-vocabulary CSR, and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks.	large-vocabulary CSR	ARPA large-vocabulary CSR corpora	usage	{'e1': {'word': 'large-vocabulary CSR', 'word_index': [(21, 24)], 'id': 'H94-1102.11'}, 'e2': {'word': 'ARPA large-vocabulary CSR corpora', 'word_index': [(34, 39)], 'id': 'H94-1102.12'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '14:18': 'ENTITYUNRELATED', '21:24': 'ENTITY', '34:39': 'ENTITYOTHER'}}	The research effort focusses on developing advanced acoustic modelling , rapid search , and recognition - time adaptation techniques for robust large - vocabulary CSR , and on applying these techniques to the new ARPA large - vocabulary CSR corpora and to military application tasks .
Presentor offers intuitive and powerful declarative languages specifying the presentation at different levels: macro-planning, micro-planning, realization, and formatting.	declarative languages	Presentor	part_whole	{'e1': {'word': 'declarative languages', 'word_index': [(5, 6)], 'id': 'P98-1118.4'}, 'e2': {'word': 'Presentor', 'word_index': [(0, 0)], 'id': 'P98-1118.3'}, 'entity_replacement': {'0:0': 'ENTITYOTHER', '5:6': 'ENTITY'}}	Presentor offers intuitive and powerful declarative languages specifying the presentation at different levels : macro-planning , micro-planning , realization , and formatting .
Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems (SLS) program to agree on a methodology for comparative evaluation of SLS systems, and that methodology has been put into practice several times in comparative tests of several SLS systems.	DARPA Spoken Language Systems (SLS) program	SLS systems	topic	{'e1': {'word': 'DARPA Spoken Language Systems (SLS) program', 'word_index': [(14, 21)], 'id': 'A92-1023.4'}, 'e2': {'word': 'SLS systems', 'word_index': [(31, 32)], 'id': 'A92-1023.5'}, 'entity_replacement': {'14:21': 'ENTITY', '31:32': 'ENTITYOTHER', '49:50': 'ENTITYUNRELATED'}}	Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems ( SLS ) program to agree on a methodology for comparative evaluation of SLS systems , and that methodology has been put into practice several times in comparative tests of several SLS systems .
These evaluations are probably the only NL evaluations other than the series of Message Understanding Conferences (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991).	Message Understanding Conferences	NL evaluations	topic	{'e1': {'word': 'Message Understanding Conferences', 'word_index': [(13, 15)], 'id': 'A92-1023.8'}, 'e2': {'word': 'NL evaluations', 'word_index': [(6, 7)], 'id': 'A92-1023.7'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '13:15': 'ENTITY'}}	These evaluations are probably the only NL evaluations other than the series of Message Understanding Conferences ( Sundheim , 1989 ; Sundheim , 1991 ) to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems ( Palmer et al. , 1989 ; Neal et al. , 1991 ) .
"This paper describes a practical ""black-box"" methodology for automatic evaluation of question-answering NL systems."	"""black-box"" methodology"	question-answering NL systems	usage	"{'e1': {'word': '""black-box"" methodology', 'word_index': [(5, 10)], 'id': 'A92-1023.9'}, 'e2': {'word': 'question-answering NL systems', 'word_index': [(15, 19)], 'id': 'A92-1023.10'}, 'entity_replacement': {'5:10': 'ENTITY', '15:19': 'ENTITYOTHER'}}"	"This paper describes a practical "" black - box "" methodology for automatic evaluation of question - answering NL systems ."
The psycholinguistic literature provides evidence for syntactic priming, i.e., the tendency to repeat structures.	psycholinguistic literature	syntactic priming	topic	{'e1': {'word': 'psycholinguistic literature', 'word_index': [(1, 2)], 'id': 'P06-1053.1'}, 'e2': {'word': 'syntactic priming', 'word_index': [(6, 7)], 'id': 'P06-1053.2'}, 'entity_replacement': {'1:2': 'ENTITY', '6:7': 'ENTITYOTHER'}}	The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency to repeat structures .
This paper describes a method for incorporating priming into an incremental probabilistic parser.	priming	incremental probabilistic parser	usage	{'e1': {'word': 'priming', 'word_index': [(7, 7)], 'id': 'P06-1053.3'}, 'e2': {'word': 'incremental probabilistic parser', 'word_index': [(10, 12)], 'id': 'P06-1053.4'}, 'entity_replacement': {'7:7': 'ENTITY', '10:12': 'ENTITYOTHER'}}	This paper describes a method for incorporating priming into an incremental probabilistic parser .
These models simulate the reading time advantage for parallel structures found in human data, and also yield a small increase in overall parsing accuracy.	parallel structures	human data	part_whole	{'e1': {'word': 'parallel structures', 'word_index': [(8, 9)], 'id': 'P06-1053.10'}, 'e2': {'word': 'human data', 'word_index': [(12, 13)], 'id': 'P06-1053.11'}, 'entity_replacement': {'8:9': 'ENTITY', '12:13': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED'}}	These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .
Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word).	sense priors	word	model-feature	{'e1': {'word': 'sense priors', 'word_index': [(11, 12)], 'id': 'P06-1012.3'}, 'e2': {'word': 'word', 'word_index': [(3, 3)], 'id': 'P06-1012.1'}, 'entity_replacement': {'3:3': 'ENTITYOTHER', '7:7': 'ENTITYUNRELATED', '11:12': 'ENTITY', '19:19': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED'}}	Instances of a word drawn from different domains may have different sense priors ( the proportions of the different senses of a word ) .
Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word).	senses	word	model-feature	{'e1': {'word': 'senses', 'word_index': [(19, 19)], 'id': 'P06-1012.4'}, 'e2': {'word': 'word', 'word_index': [(22, 22)], 'id': 'P06-1012.5'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '19:19': 'ENTITY', '22:22': 'ENTITYOTHER'}}	Instances of a word drawn from different domains may have different sense priors ( the proportions of the different senses of a word ) .
This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains.	word sense disambiguation (WSD) systems	domains	usage	{'e1': {'word': 'word sense disambiguation (WSD) systems', 'word_index': [(7, 13)], 'id': 'P06-1012.6'}, 'e2': {'word': 'domains', 'word_index': [(19, 19)], 'id': 'P06-1012.7'}, 'entity_replacement': {'7:13': 'ENTITY', '19:19': 'ENTITYOTHER'}}	This in turn affects the accuracy of word sense disambiguation ( WSD ) systems trained and applied on different domains .
This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations.	sense priors	words	model-feature	{'e1': {'word': 'sense priors', 'word_index': [(8, 9)], 'id': 'P06-1012.8'}, 'e2': {'word': 'words', 'word_index': [(11, 11)], 'id': 'P06-1012.9'}, 'entity_replacement': {'8:9': 'ENTITY', '11:11': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '24:26': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED'}}	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .
This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations.	well calibrated probabilities	estimations	usage	{'e1': {'word': 'well calibrated probabilities', 'word_index': [(24, 26)], 'id': 'P06-1012.11'}, 'e2': {'word': 'estimations', 'word_index': [(30, 30)], 'id': 'P06-1012.12'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '24:26': 'ENTITY', '30:30': 'ENTITYOTHER'}}	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .
By using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy.	well calibrated probabilities	sense priors	usage	{'e1': {'word': 'well calibrated probabilities', 'word_index': [(2, 4)], 'id': 'P06-1012.13'}, 'e2': {'word': 'sense priors', 'word_index': [(12, 13)], 'id': 'P06-1012.14'}, 'entity_replacement': {'2:4': 'ENTITY', '12:13': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	By using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy .
How to obtain hierarchical relations(e.g. superordinate -hyponym relation, synonym relation) is one of the most important problems for thesaurus construction.	hierarchical relations	thesaurus construction	part_whole	{'e1': {'word': 'hierarchical relations', 'word_index': [(3, 4)], 'id': 'C86-1105.1'}, 'e2': {'word': 'thesaurus construction', 'word_index': [(23, 24)], 'id': 'C86-1105.4'}, 'entity_replacement': {'3:4': 'ENTITY', '7:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER'}}	How to obtain hierarchical relations ( e.g. superordinate - hyponym relation , synonym relation ) is one of the most important problems for thesaurus construction .
A pilot system for extracting these relations automatically from an ordinary Japanese language dictionary (Shinmeikai Kokugojiten, published by Sansei-do, in machine readable form) is given.	relations	Japanese language dictionary	part_whole	{'e1': {'word': 'relations', 'word_index': [(6, 6)], 'id': 'C86-1105.5'}, 'e2': {'word': 'Japanese language dictionary', 'word_index': [(11, 13)], 'id': 'C86-1105.6'}, 'entity_replacement': {'6:6': 'ENTITY', '11:13': 'ENTITYOTHER'}}	A pilot system for extracting these relations automatically from an ordinary Japanese language dictionary ( Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form ) is given .
The features of the definition sentences in the dictionary, the mechanical extraction of the hierarchical relations and the estimation of the results are discussed.	definition sentences	dictionary	part_whole	{'e1': {'word': 'definition sentences', 'word_index': [(4, 5)], 'id': 'C86-1105.7'}, 'e2': {'word': 'dictionary', 'word_index': [(8, 8)], 'id': 'C86-1105.8'}, 'entity_replacement': {'4:5': 'ENTITY', '8:8': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	The features of the definition sentences in the dictionary , the mechanical extraction of the hierarchical relations and the estimation of the results are discussed .
The interlingual approach to MT has been repeatedly advocated by researchers originally interested in natural language understanding who take machine translation to be one possible application.	natural language understanding	machine translation	usage	{'e1': {'word': 'natural language understanding', 'word_index': [(14, 16)], 'id': 'C86-1021.2'}, 'e2': {'word': 'machine translation', 'word_index': [(19, 20)], 'id': 'C86-1021.3'}, 'entity_replacement': {'1:4': 'ENTITYUNRELATED', '14:16': 'ENTITY', '19:20': 'ENTITYOTHER'}}	The interlingual approach to MT has been repeatedly advocated by researchers originally interested in natural language understanding who take machine translation to be one possible application .
However, not only the ambiguity but also the vagueness which every natural language inevitably has leads this approach into essential difficulties.	ambiguity	natural language	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(5, 5)], 'id': 'C86-1021.4'}, 'e2': {'word': 'natural language', 'word_index': [(12, 13)], 'id': 'C86-1021.5'}, 'entity_replacement': {'5:5': 'ENTITY', '12:13': 'ENTITYOTHER'}}	However , not only the ambiguity but also the vagueness which every natural language inevitably has leads this approach into essential difficulties .
In contrast, our project, the Mu-project, adopts the transfer approach as the basic framework of MT.	transfer approach	MT	usage	{'e1': {'word': 'transfer approach', 'word_index': [(11, 12)], 'id': 'C86-1021.7'}, 'e2': {'word': 'MT', 'word_index': [(18, 18)], 'id': 'C86-1021.8'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '11:12': 'ENTITY', '18:18': 'ENTITYOTHER'}}	In contrast , our project , the Mu-project , adopts the transfer approach as the basic framework of MT .
This paper describes the detailed construction of the transfer phase of our system from Japanese to English, and gives some examples of problems which seem difficult to treat in the interlingual approach.	transfer phase	interlingual approach	compare	{'e1': {'word': 'transfer phase', 'word_index': [(8, 9)], 'id': 'C86-1021.9'}, 'e2': {'word': 'interlingual approach', 'word_index': [(31, 32)], 'id': 'C86-1021.12'}, 'entity_replacement': {'8:9': 'ENTITY', '14:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '31:32': 'ENTITYOTHER'}}	This paper describes the detailed construction of the transfer phase of our system from Japanese to English , and gives some examples of problems which seem difficult to treat in the interlingual approach .
Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers, boosting and SVMs are applied to language processing tasks, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point.	SVMs	language processing tasks	usage	{'e1': {'word': 'SVMs', 'word_index': [(21, 21)], 'id': 'C04-1058.4'}, 'e2': {'word': 'language processing tasks', 'word_index': [(25, 27)], 'id': 'C04-1058.5'}, 'entity_replacement': {'12:12': 'ENTITYUNRELATED', '15:17': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '21:21': 'ENTITY', '25:27': 'ENTITYOTHER'}}	Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers , boosting and SVMs are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .
To further improve performance, various error correction mechanisms have been developed, but in practice, most of them cannot be relied on to predictably improve performance on unseen data; indeed, depending upon the test set, they are as likely to degrade accuracy as to improve it.	error correction mechanisms	unseen data	usage	{'e1': {'word': 'error correction mechanisms', 'word_index': [(6, 8)], 'id': 'C04-1058.6'}, 'e2': {'word': 'unseen data', 'word_index': [(29, 30)], 'id': 'C04-1058.7'}, 'entity_replacement': {'6:8': 'ENTITY', '29:30': 'ENTITYOTHER', '37:38': 'ENTITYUNRELATED'}}	To further improve performance , various error correction mechanisms have been developed , but in practice , most of them cannot be relied on to predictably improve performance on unseen data ; indeed , depending upon the test set , they are as likely to degrade accuracy as to improve it .
The study addresses the problem of automatic acquisition of entailment relations between verbs.	entailment relations	verbs	model-feature	{'e1': {'word': 'entailment relations', 'word_index': [(9, 10)], 'id': 'N06-1007.2'}, 'e2': {'word': 'verbs', 'word_index': [(12, 12)], 'id': 'N06-1007.3'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:10': 'ENTITY', '12:12': 'ENTITYOTHER'}}	The study addresses the problem of automatic acquisition of entailment relations between verbs .
While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations.	semantic equivalence	verbs	model-feature	{'e1': {'word': 'semantic equivalence', 'word_index': [(14, 15)], 'id': 'N06-1007.5'}, 'e2': {'word': 'verbs', 'word_index': [(17, 17)], 'id': 'N06-1007.6'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '14:15': 'ENTITY', '17:17': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '28:33': 'ENTITYUNRELATED'}}	While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs , the main challenge of entailment acquisition is to capture asymmetric , or directional , relations .
While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations.	entailment acquisition	asymmetric, or directional, relations	topic	{'e1': {'word': 'entailment acquisition', 'word_index': [(23, 24)], 'id': 'N06-1007.7'}, 'e2': {'word': 'asymmetric, or directional, relations', 'word_index': [(28, 33)], 'id': 'N06-1007.8'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '23:24': 'ENTITY', '28:33': 'ENTITYOTHER'}}	While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs , the main challenge of entailment acquisition is to capture asymmetric , or directional , relations .
Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus.	local structure	coherent text	model-feature	{'e1': {'word': 'local structure', 'word_index': [(9, 10)], 'id': 'N06-1007.9'}, 'e2': {'word': 'coherent text', 'word_index': [(12, 13)], 'id': 'N06-1007.10'}, 'entity_replacement': {'9:10': 'ENTITY', '12:13': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED'}}	Motivated by the intuition that it often underlies the local structure of coherent text , we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus .
Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus.	discourse relations	clauses	model-feature	{'e1': {'word': 'discourse relations', 'word_index': [(26, 27)], 'id': 'N06-1007.12'}, 'e2': {'word': 'clauses', 'word_index': [(29, 29)], 'id': 'N06-1007.13'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '26:27': 'ENTITY', '29:29': 'ENTITYOTHER', '33:34': 'ENTITYUNRELATED'}}	Motivated by the intuition that it often underlies the local structure of coherent text , we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus .
In comparison with earlier work, the proposed method covers a much wider range of verb entailment types and learns the mapping between verbs with highly varied argument structures.	verbs	argument structures	model-feature	{'e1': {'word': 'verbs', 'word_index': [(23, 23)], 'id': 'N06-1007.17'}, 'e2': {'word': 'argument structures', 'word_index': [(27, 28)], 'id': 'N06-1007.18'}, 'entity_replacement': {'15:17': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '23:23': 'ENTITY', '27:28': 'ENTITYOTHER'}}	In comparison with earlier work , the proposed method covers a much wider range of verb entailment types and learns the mapping between verbs with highly varied argument structures .
This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one.	phrases	trees	model-feature	{'e1': {'word': 'phrases', 'word_index': [(13, 13)], 'id': 'A00-2023.2'}, 'e2': {'word': 'trees', 'word_index': [(20, 20)], 'id': 'A00-2023.3'}, 'entity_replacement': {'7:9': 'ENTITYUNRELATED', '13:13': 'ENTITY', '20:20': 'ENTITYOTHER', '23:23': 'ENTITYUNRELATED'}}	This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees , or forests , and then ranked statistically to choose the best one .
An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach.	ranking algorithm	lattice-based approach	compare	{'e1': {'word': 'ranking algorithm', 'word_index': [(2, 3)], 'id': 'A00-2023.8'}, 'e2': {'word': 'lattice-based approach', 'word_index': [(19, 22)], 'id': 'A00-2023.9'}, 'entity_replacement': {'2:3': 'ENTITY', '19:22': 'ENTITYOTHER'}}	An efficient ranking algorithm is described , together with experimental results showing significant improvements over simple enumeration or a lattice - based approach .
This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1.	sentences	summary generation	usage	{'e1': {'word': 'sentences', 'word_index': [(14, 14)], 'id': 'X98-1022.6'}, 'e2': {'word': 'summary generation', 'word_index': [(16, 17)], 'id': 'X98-1022.7'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '14:14': 'ENTITY', '16:17': 'ENTITYOTHER', '23:23': 'ENTITYUNRELATED'}}	This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1 .
For categorization task, positive feature vectors and negative feature vectors are used cooperatively to construct generic, indicative summaries.	positive feature vectors	categorization task	usage	{'e1': {'word': 'positive feature vectors', 'word_index': [(4, 6)], 'id': 'X98-1022.10'}, 'e2': {'word': 'categorization task', 'word_index': [(1, 2)], 'id': 'X98-1022.9'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '4:6': 'ENTITY', '8:10': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED'}}	For categorization task , positive feature vectors and negative feature vectors are used cooperatively to construct generic , indicative summaries .
For adhoc task, a text model based on relationship between nouns and verbs is used to filter out irrelevant discourse segment, to rank relevant sentences, and to generate the user-directed summaries.	text model	user-directed summaries	usage	{'e1': {'word': 'text model', 'word_index': [(5, 6)], 'id': 'X98-1022.13'}, 'e2': {'word': 'user-directed summaries', 'word_index': [(32, 33)], 'id': 'X98-1022.18'}, 'entity_replacement': {'5:6': 'ENTITY', '11:11': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '32:33': 'ENTITYOTHER'}}	For adhoc task , a text model based on relationship between nouns and verbs is used to filter out irrelevant discourse segment , to rank relevant sentences , and to generate the user-directed summaries .
A set of articles is represented by a set of word vectors, and the similarity between the vectors is then calculated.	similarity	vectors	model-feature	{'e1': {'word': 'similarity', 'word_index': [(15, 15)], 'id': 'P98-2213.3'}, 'e2': {'word': 'vectors', 'word_index': [(18, 18)], 'id': 'P98-2213.4'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '15:15': 'ENTITY', '18:18': 'ENTITYOTHER'}}	A set of articles is represented by a set of word vectors , and the similarity between the vectors is then calculated .
By applying some constraints on the chronological ordering of articles, an efficient threading algorithm that runs in 0(n) time (where n is the number of articles) is obtained.	threading algorithm	0(n) time	model-feature	{'e1': {'word': 'threading algorithm', 'word_index': [(13, 14)], 'id': 'P98-2213.8'}, 'e2': {'word': '0(n) time', 'word_index': [(18, 22)], 'id': 'P98-2213.9'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '13:14': 'ENTITY', '18:22': 'ENTITYOTHER'}}	By applying some constraints on the chronological ordering of articles , an efficient threading algorithm that runs in 0 ( n ) time ( where n is the number of articles ) is obtained .
The constructed graph is visualized with words that represent the topics of the threads, and words that represent new information in each article.	topics	threads	model-feature	{'e1': {'word': 'topics', 'word_index': [(10, 10)], 'id': 'P98-2213.12'}, 'e2': {'word': 'threads', 'word_index': [(13, 13)], 'id': 'P98-2213.13'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '10:10': 'ENTITY', '13:13': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED'}}	The constructed graph is visualized with words that represent the topics of the threads , and words that represent new information in each article .
The constructed graph is visualized with words that represent the topics of the threads, and words that represent new information in each article.	words	information	model-feature	{'e1': {'word': 'words', 'word_index': [(16, 16)], 'id': 'P98-2213.14'}, 'e2': {'word': 'information', 'word_index': [(20, 20)], 'id': 'P98-2213.15'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '16:16': 'ENTITY', '20:20': 'ENTITYOTHER'}}	The constructed graph is visualized with words that represent the topics of the threads , and words that represent new information in each article .
In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree.	SSTC	sentence	model-feature	{'e1': {'word': 'SSTC', 'word_index': [(20, 20)], 'id': 'P98-1113.6'}, 'e2': {'word': 'sentence', 'word_index': [(23, 23)], 'id': 'P98-1113.7'}, 'entity_replacement': {'9:17': 'ENTITYUNRELATED', '20:20': 'ENTITY', '23:23': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '37:37': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED', '42:43': 'ENTITYUNRELATED'}}	In our approach , examples are annotated with the Structured String Tree Correspondence ( SSTC ) annotation schema where each SSTC describes a sentence , a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree .
In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree.	substrings	sentence	part_whole	{'e1': {'word': 'substrings', 'word_index': [(34, 34)], 'id': 'P98-1113.9'}, 'e2': {'word': 'sentence', 'word_index': [(37, 37)], 'id': 'P98-1113.10'}, 'entity_replacement': {'9:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '34:34': 'ENTITY', '37:37': 'ENTITYOTHER', '39:39': 'ENTITYUNRELATED', '42:43': 'ENTITYUNRELATED'}}	In our approach , examples are annotated with the Structured String Tree Correspondence ( SSTC ) annotation schema where each SSTC describes a sentence , a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree .
In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree.	subtrees	representation tree	part_whole	{'e1': {'word': 'subtrees', 'word_index': [(39, 39)], 'id': 'P98-1113.11'}, 'e2': {'word': 'representation tree', 'word_index': [(42, 43)], 'id': 'P98-1113.12'}, 'entity_replacement': {'9:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '37:37': 'ENTITYUNRELATED', '39:39': 'ENTITY', '42:43': 'ENTITYOTHER'}}	In our approach , examples are annotated with the Structured String Tree Correspondence ( SSTC ) annotation schema where each SSTC describes a sentence , a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree .
In the process of parsing, we first try to build subtrees for phrases in the input sentence which have been successfully found in the example-base - a bottom up approach.	subtrees	phrases	model-feature	{'e1': {'word': 'subtrees', 'word_index': [(11, 11)], 'id': 'P98-1113.14'}, 'e2': {'word': 'phrases', 'word_index': [(13, 13)], 'id': 'P98-1113.15'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '11:11': 'ENTITY', '13:13': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	In the process of parsing , we first try to build subtrees for phrases in the input sentence which have been successfully found in the example - base - a bottom up approach .
This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.	HMM-based chunk tagger	named entity (NE) recognition (NER) system	usage	{'e1': {'word': 'HMM-based chunk tagger', 'word_index': [(12, 16)], 'id': 'P02-1060.2'}, 'e2': {'word': 'named entity (NE) recognition (NER) system', 'word_index': [(21, 30)], 'id': 'P02-1060.3'}, 'entity_replacement': {'4:9': 'ENTITYUNRELATED', '12:16': 'ENTITY', '21:30': 'ENTITYOTHER', '37:37': 'ENTITYUNRELATED', '39:42': 'ENTITYUNRELATED'}}	This paper proposes a Hidden Markov Model ( HMM ) and an HMM - based chunk tagger , from which a named entity ( NE ) recognition ( NER ) system is built to recognize and classify names , times and numerical quantities .
Through the HMM, our system is able to apply and integrate four types of internal and external evidences : 1) simple deterministic internal feature of the words, such as capitalization and digitalization ; 2) internal semantic feature of important triggers ; 3) internal gazetteer feature; 4) external macro context feature.	capitalization	words	model-feature	{'e1': {'word': 'capitalization', 'word_index': [(32, 32)], 'id': 'P02-1060.8'}, 'e2': {'word': 'words', 'word_index': [(28, 28)], 'id': 'P02-1060.7'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '28:28': 'ENTITYOTHER', '32:32': 'ENTITY', '38:40': 'ENTITYUNRELATED', '47:49': 'ENTITYUNRELATED', '53:56': 'ENTITYUNRELATED'}}	Through the HMM , our system is able to apply and integrate four types of internal and external evidences : 1 ) simple deterministic internal feature of the words , such as capitalization and digitalization ; 2 ) internal semantic feature of important triggers ; 3 ) internal gazetteer feature ; 4 ) external macro context feature .
Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.	system	F-measures	result	{'e1': {'word': 'system', 'word_index': [(3, 3)], 'id': 'P02-1060.13'}, 'e2': {'word': 'F-measures', 'word_index': [(16, 16)], 'id': 'P02-1060.15'}, 'entity_replacement': {'3:3': 'ENTITY', '5:14': 'ENTITYUNRELATED', '16:16': 'ENTITYOTHER'}}	Evaluation of our system on MUC - 6 and MUC - 7 English NE tasks achieves F-measures of 96.6 % and 94.1 % respectively .
Porting a Natural Language Processing (NLP) system to a new domain remains one of the bottlenecks in syntactic parsing, because of the amount of effort required to fix gaps in the lexicon, and to attune the existing grammar to the idiosyncracies of the new sublanguage.	new domain	Natural Language Processing (NLP) system	usage	{'e1': {'word': 'new domain', 'word_index': [(11, 12)], 'id': 'C96-2213.2'}, 'e2': {'word': 'Natural Language Processing (NLP) system', 'word_index': [(2, 8)], 'id': 'C96-2213.1'}, 'entity_replacement': {'2:8': 'ENTITYOTHER', '11:12': 'ENTITY', '19:20': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED', '47:48': 'ENTITYUNRELATED'}}	Porting a Natural Language Processing ( NLP ) system to a new domain remains one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing grammar to the idiosyncracies of the new sublanguage .
Porting a Natural Language Processing (NLP) system to a new domain remains one of the bottlenecks in syntactic parsing, because of the amount of effort required to fix gaps in the lexicon, and to attune the existing grammar to the idiosyncracies of the new sublanguage.	existing grammar	new sublanguage	usage	{'e1': {'word': 'existing grammar', 'word_index': [(40, 41)], 'id': 'C96-2213.5'}, 'e2': {'word': 'new sublanguage', 'word_index': [(47, 48)], 'id': 'C96-2213.6'}, 'entity_replacement': {'2:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '40:41': 'ENTITY', '47:48': 'ENTITYOTHER'}}	Porting a Natural Language Processing ( NLP ) system to a new domain remains one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing grammar to the idiosyncracies of the new sublanguage .
This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach.	lexicalized grammar	domain	usage	{'e1': {'word': 'lexicalized grammar', 'word_index': [(9, 10)], 'id': 'C96-2213.7'}, 'e2': {'word': 'domain', 'word_index': [(13, 13)], 'id': 'C96-2213.8'}, 'entity_replacement': {'9:10': 'ENTITY', '13:13': 'ENTITYOTHER', '24:25': 'ENTITYUNRELATED', '28:32': 'ENTITYUNRELATED', '35:38': 'ENTITYUNRELATED'}}	This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge - based techniques with a corpus - based approach .
This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach.	traditional knowledge-based techniques	corpus-based approach	compare	{'e1': {'word': 'traditional knowledge-based techniques', 'word_index': [(28, 32)], 'id': 'C96-2213.10'}, 'e2': {'word': 'corpus-based approach', 'word_index': [(35, 38)], 'id': 'C96-2213.11'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:32': 'ENTITY', '35:38': 'ENTITYOTHER'}}	This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge - based techniques with a corpus - based approach .
We propose a detection method for orthographic variants caused by transliteration in a large corpus.	transliteration	corpus	part_whole	{'e1': {'word': 'transliteration', 'word_index': [(10, 10)], 'id': 'C04-1102.2'}, 'e2': {'word': 'corpus', 'word_index': [(14, 14)], 'id': 'C04-1102.3'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '10:10': 'ENTITY', '14:14': 'ENTITYOTHER'}}	We propose a detection method for orthographic variants caused by transliteration in a large corpus .
One is string similarity based on edit distance.	edit distance	string similarity	usage	{'e1': {'word': 'edit distance', 'word_index': [(6, 7)], 'id': 'C04-1102.6'}, 'e2': {'word': 'string similarity', 'word_index': [(2, 3)], 'id': 'C04-1102.5'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '6:7': 'ENTITY'}}	One is string similarity based on edit distance .
The other is contextual similarity by a vector space model.	vector space model	contextual similarity	usage	{'e1': {'word': 'vector space model', 'word_index': [(7, 9)], 'id': 'C04-1102.8'}, 'e2': {'word': 'contextual similarity', 'word_index': [(3, 4)], 'id': 'C04-1102.7'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '7:9': 'ENTITY'}}	The other is contextual similarity by a vector space model .
However most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts.	temporal expressions	newswire texts	part_whole	{'e1': {'word': 'temporal expressions', 'word_index': [(15, 16)], 'id': 'N06-1018.2'}, 'e2': {'word': 'newswire texts', 'word_index': [(18, 19)], 'id': 'N06-1018.3'}, 'entity_replacement': {'15:16': 'ENTITY', '18:19': 'ENTITYOTHER'}}	However most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts .
In this paper we report our work on anchoring temporal expressions in a novel genre, emails.	genre	temporal expressions	model-feature	{'e1': {'word': 'genre', 'word_index': [(14, 14)], 'id': 'N06-1018.5'}, 'e2': {'word': 'temporal expressions', 'word_index': [(9, 10)], 'id': 'N06-1018.4'}, 'entity_replacement': {'9:10': 'ENTITYOTHER', '14:14': 'ENTITY'}}	In this paper we report our work on anchoring temporal expressions in a novel genre , emails .
We have developed and evaluated a Temporal Expression Anchoror (TEA), and the result shows that it performs significantly better than the baseline, and compares favorably with some of the closely related work.</abstract>	Temporal Expression Anchoror (TEA)	baseline	compare	{'e1': {'word': 'Temporal Expression Anchoror (TEA)', 'word_index': [(6, 11)], 'id': 'N06-1018.9'}, 'e2': {'word': 'baseline', 'word_index': [(24, 24)], 'id': 'N06-1018.10'}, 'entity_replacement': {'6:11': 'ENTITY', '24:24': 'ENTITYOTHER'}}	We have developed and evaluated a Temporal Expression Anchoror ( TEA ) , and the result shows that it performs significantly better than the baseline , and compares favorably with some of the closely related work . < / abstract >
The goal of this research is to develop a spoken language system that will demonstrate the usefulness of voice input for interactive problem solving.	voice input	interactive problem solving	usage	{'e1': {'word': 'voice input', 'word_index': [(18, 19)], 'id': 'H89-2066.2'}, 'e2': {'word': 'interactive problem solving', 'word_index': [(21, 23)], 'id': 'H89-2066.3'}, 'entity_replacement': {'9:11': 'ENTITYUNRELATED', '18:19': 'ENTITY', '21:23': 'ENTITYOTHER'}}	The goal of this research is to develop a spoken language system that will demonstrate the usefulness of voice input for interactive problem solving .
Combining speech recognition and natural language processing to achieve speech understanding, the system will be demonstrated in an application domain relevant to the DoD.	natural language processing	speech understanding	usage	{'e1': {'word': 'natural language processing', 'word_index': [(4, 6)], 'id': 'H89-2066.8'}, 'e2': {'word': 'speech understanding', 'word_index': [(9, 10)], 'id': 'H89-2066.9'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:6': 'ENTITY', '9:10': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED'}}	Combining speech recognition and natural language processing to achieve speech understanding , the system will be demonstrated in an application domain relevant to the DoD.
The objective of this project is to develop a robust and high-performance speech recognition system using a segment-based approach to phonetic recognition.	segment-based approach	phonetic recognition	usage	{'e1': {'word': 'segment-based approach', 'word_index': [(19, 22)], 'id': 'H89-2066.12'}, 'e2': {'word': 'phonetic recognition', 'word_index': [(24, 25)], 'id': 'H89-2066.13'}, 'entity_replacement': {'9:16': 'ENTITYUNRELATED', '19:22': 'ENTITY', '24:25': 'ENTITYOTHER'}}	The objective of this project is to develop a robust and high - performance speech recognition system using a segment - based approach to phonetic recognition .
The recognition system will eventually be integrated with natural language processing to achieve spoken language understanding.	natural language processing	spoken language understanding	usage	{'e1': {'word': 'natural language processing', 'word_index': [(8, 10)], 'id': 'H89-2066.15'}, 'e2': {'word': 'spoken language understanding', 'word_index': [(13, 15)], 'id': 'H89-2066.16'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '8:10': 'ENTITY', '13:15': 'ENTITYOTHER'}}	The recognition system will eventually be integrated with natural language processing to achieve spoken language understanding .
The knowledge to be expressed in text is first divided into small propositional units, which are then composed into appropriate combinations and converted into text.KDS (Knowledge Delivery System), which embodies this paradigm, has distinct parts devoted to creation of the propositional units, to organization of the text, to prevention of excess redundancy, to creation of combinations of units, to evaluation of these combinations as potential sentences, to selection of the best among competing combinations, and to creation of the final text.	knowledge	text	part_whole	{'e1': {'word': 'knowledge', 'word_index': [(1, 1)], 'id': 'J81-1002.4'}, 'e2': {'word': 'text', 'word_index': [(6, 6)], 'id': 'J81-1002.5'}, 'entity_replacement': {'1:1': 'ENTITY', '6:6': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:32': 'ENTITYUNRELATED', '47:48': 'ENTITYUNRELATED', '54:54': 'ENTITYUNRELATED', '59:60': 'ENTITYUNRELATED', '76:76': 'ENTITYUNRELATED', '92:93': 'ENTITYUNRELATED'}}	The knowledge to be expressed in text is first divided into small propositional units , which are then composed into appropriate combinations and converted into text . KDS ( Knowledge Delivery System ) , which embodies this paradigm , has distinct parts devoted to creation of the propositional units , to organization of the text , to prevention of excess redundancy , to creation of combinations of units , to evaluation of these combinations as potential sentences , to selection of the best among competing combinations , and to creation of the final text .
The Fragment-and-Compose paradigm and the computational methods of KDS are described.	computational methods	KDS	usage	{'e1': {'word': 'computational methods', 'word_index': [(9, 10)], 'id': 'J81-1002.15'}, 'e2': {'word': 'KDS', 'word_index': [(12, 12)], 'id': 'J81-1002.16'}, 'entity_replacement': {'1:6': 'ENTITYUNRELATED', '9:10': 'ENTITY', '12:12': 'ENTITYOTHER'}}	The Fragment - and - Compose paradigm and the computational methods of KDS are described .
A deterministic parser is under development which represents a departure from traditional deterministic parsers in that it combines both symbolic and connectionist components.	A deterministic parser	traditional deterministic parsers	compare	{'e1': {'word': 'A deterministic parser', 'word_index': [(0, 2)], 'id': 'C90-1002.1'}, 'e2': {'word': 'traditional deterministic parsers', 'word_index': [(11, 13)], 'id': 'C90-1002.2'}, 'entity_replacement': {'0:2': 'ENTITY', '11:13': 'ENTITYOTHER', '19:22': 'ENTITYUNRELATED'}}	A deterministic parser is under development which represents a departure from traditional deterministic parsers in that it combines both symbolic and connectionist components .
The connectionist component is trained either from patterns derived from the rules of a deterministic grammar.	rules	deterministic grammar	part_whole	{'e1': {'word': 'rules', 'word_index': [(11, 11)], 'id': 'C90-1002.5'}, 'e2': {'word': 'deterministic grammar', 'word_index': [(14, 15)], 'id': 'C90-1002.6'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '11:11': 'ENTITY', '14:15': 'ENTITYOTHER'}}	The connectionist component is trained either from patterns derived from the rules of a deterministic grammar .
The development and evolution of such a hybrid architecture has lead to a parser which is superior to any known deterministic parser.	parser	known deterministic parser	compare	{'e1': {'word': 'parser', 'word_index': [(13, 13)], 'id': 'C90-1002.8'}, 'e2': {'word': 'known deterministic parser', 'word_index': [(19, 21)], 'id': 'C90-1002.9'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '13:13': 'ENTITY', '19:21': 'ENTITYOTHER'}}	The development and evolution of such a hybrid architecture has lead to a parser which is superior to any known deterministic parser .
Experiments are described and powerful training techniques are demonstrated that permit decision-making by the connectionist component in the parsing process.	training techniques	decision-making	usage	{'e1': {'word': 'training techniques', 'word_index': [(5, 6)], 'id': 'C90-1002.10'}, 'e2': {'word': 'decision-making', 'word_index': [(11, 13)], 'id': 'C90-1002.11'}, 'entity_replacement': {'5:6': 'ENTITY', '11:13': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED'}}	Experiments are described and powerful training techniques are demonstrated that permit decision - making by the connectionist component in the parsing process .
Experiments are described and powerful training techniques are demonstrated that permit decision-making by the connectionist component in the parsing process.	connectionist component	parsing process	part_whole	{'e1': {'word': 'connectionist component', 'word_index': [(16, 17)], 'id': 'C90-1002.12'}, 'e2': {'word': 'parsing process', 'word_index': [(20, 21)], 'id': 'C90-1002.13'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '11:13': 'ENTITYUNRELATED', '16:17': 'ENTITY', '20:21': 'ENTITYOTHER'}}	Experiments are described and powerful training techniques are demonstrated that permit decision - making by the connectionist component in the parsing process .
This approach has permitted some simplifications to the rules of other deterministic parsers, including the elimination of rule packets and priorities.	rules	deterministic parsers	part_whole	{'e1': {'word': 'rules', 'word_index': [(8, 8)], 'id': 'C90-1002.14'}, 'e2': {'word': 'deterministic parsers', 'word_index': [(11, 12)], 'id': 'C90-1002.15'}, 'entity_replacement': {'8:8': 'ENTITY', '11:12': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	This approach has permitted some simplifications to the rules of other deterministic parsers , including the elimination of rule packets and priorities .
Data are presented which show how a connectionist (neural) network trained with linguistic rules can parse both expected (grammatical) sentences as well as some novel (ungrammatical or lexically ambiguous) sentences.	connectionist (neural) network	expected (grammatical) sentences	usage	{'e1': {'word': 'connectionist (neural) network', 'word_index': [(7, 11)], 'id': 'C90-1002.18'}, 'e2': {'word': 'expected (grammatical) sentences', 'word_index': [(19, 23)], 'id': 'C90-1002.20'}, 'entity_replacement': {'7:11': 'ENTITY', '14:15': 'ENTITYUNRELATED', '19:23': 'ENTITYOTHER'}}	Data are presented which show how a connectionist ( neural ) network trained with linguistic rules can parse both expected ( grammatical ) sentences as well as some novel ( ungrammatical or lexically ambiguous ) sentences .
The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization.	supervised learning	summarization	usage	{'e1': {'word': 'supervised learning', 'word_index': [(9, 10)], 'id': 'P02-1059.1'}, 'e2': {'word': 'summarization', 'word_index': [(20, 20)], 'id': 'P02-1059.3'}, 'entity_replacement': {'9:10': 'ENTITY', '12:13': 'ENTITYUNRELATED', '20:20': 'ENTITYOTHER'}}	The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization .
The corpus of human created extracts is created from a newspaper corpus and used as a test set.	newspaper corpus	corpus	part_whole	{'e1': {'word': 'newspaper corpus', 'word_index': [(10, 11)], 'id': 'P02-1059.7'}, 'e2': {'word': 'corpus', 'word_index': [(1, 1)], 'id': 'P02-1059.6'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '10:11': 'ENTITY'}}	The corpus of human created extracts is created from a newspaper corpus and used as a test set .
In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	dynamic programming (DP)	statistical machine translation (MT)	usage	{'e1': {'word': 'dynamic programming (DP)', 'word_index': [(18, 22)], 'id': 'C00-2123.2'}, 'e2': {'word': 'statistical machine translation (MT)', 'word_index': [(10, 15)], 'id': 'C00-2123.1'}, 'entity_replacement': {'10:15': 'ENTITYOTHER', '18:22': 'ENTITY'}}	In this paper , we describe a search procedure for statistical machine translation ( MT ) based on dynamic programming ( DP ) .
The experimental tests are carried out on the Verbmobil task (German-English, 8000-word vocabulary), which is a limited-domain spoken-language task.	limited-domain spoken-language task	Verbmobil task	model-feature	{'e1': {'word': 'limited-domain spoken-language task', 'word_index': [(24, 30)], 'id': 'C00-2123.6'}, 'e2': {'word': 'Verbmobil task', 'word_index': [(8, 9)], 'id': 'C00-2123.5'}, 'entity_replacement': {'8:9': 'ENTITYOTHER', '24:30': 'ENTITY'}}	The experimental tests are carried out on the Verbmobil task ( German - English , 8000 - word vocabulary ) , which is a limited - domain spoken - language task .
This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources.	word-sense ambiguity	machine-readable resources	part_whole	{'e1': {'word': 'word-sense ambiguity', 'word_index': [(6, 9)], 'id': 'C96-1055.1'}, 'e2': {'word': 'machine-readable resources', 'word_index': [(13, 16)], 'id': 'C96-1055.2'}, 'entity_replacement': {'6:9': 'ENTITY', '13:16': 'ENTITYOTHER', '21:25': 'ENTITYUNRELATED'}}	This paper addresses the issue of word - sense ambiguity in extraction from machine - readable resources for the construction of large - scale knowledge sources .
These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses.	verb semantics	syntactic behavior	compare	{'e1': {'word': 'verb semantics', 'word_index': [(27, 28)], 'id': 'C96-1055.8'}, 'e2': {'word': 'syntactic behavior', 'word_index': [(30, 31)], 'id': 'C96-1055.9'}, 'entity_replacement': {'27:28': 'ENTITY', '30:31': 'ENTITYOTHER', '52:53': 'ENTITYUNRELATED', '55:56': 'ENTITYUNRELATED', '62:63': 'ENTITYUNRELATED', '71:72': 'ENTITYUNRELATED'}}	These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that verb semantics and syntactic behavior are predictably related ; ( 2 ) to demonstrate that a 15 - fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses .
These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses.	syntactic cues	semantic information	usage	{'e1': {'word': 'syntactic cues', 'word_index': [(55, 56)], 'id': 'C96-1055.11'}, 'e2': {'word': 'semantic information', 'word_index': [(52, 53)], 'id': 'C96-1055.10'}, 'entity_replacement': {'27:28': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '52:53': 'ENTITYOTHER', '55:56': 'ENTITY', '62:63': 'ENTITYUNRELATED', '71:72': 'ENTITYUNRELATED'}}	These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that verb semantics and syntactic behavior are predictably related ; ( 2 ) to demonstrate that a 15 - fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses .
These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses.	syntactic cues	word senses	compare	{'e1': {'word': 'syntactic cues', 'word_index': [(62, 63)], 'id': 'C96-1055.12'}, 'e2': {'word': 'word senses', 'word_index': [(71, 72)], 'id': 'C96-1055.13'}, 'entity_replacement': {'27:28': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '52:53': 'ENTITYUNRELATED', '55:56': 'ENTITYUNRELATED', '62:63': 'ENTITY', '71:72': 'ENTITYOTHER'}}	These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that verb semantics and syntactic behavior are predictably related ; ( 2 ) to demonstrate that a 15 - fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses .
The objective is a generic system of tools, including a core English lexicon, grammar, and concept representations, for building natural language processing (NLP) systems for text understanding.	natural language processing (NLP) systems	text understanding	usage	{'e1': {'word': 'natural language processing (NLP) systems', 'word_index': [(23, 29)], 'id': 'M91-1029.4'}, 'e2': {'word': 'text understanding', 'word_index': [(31, 32)], 'id': 'M91-1029.5'}, 'entity_replacement': {'11:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '23:29': 'ENTITY', '31:32': 'ENTITYOTHER'}}	The objective is a generic system of tools , including a core English lexicon , grammar , and concept representations , for building natural language processing ( NLP ) systems for text understanding .
PAKTUS supports the adaptation of the generic core to a variety of domains: JINTACCS messages, RAINFORM messages, news reports about a specific type of event, such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words, conceptual mappings, and discourse patterns.	PAKTUS	JINTACCS messages	usage	{'e1': {'word': 'PAKTUS', 'word_index': [(0, 0)], 'id': 'M91-1029.9'}, 'e2': {'word': 'JINTACCS messages', 'word_index': [(14, 15)], 'id': 'M91-1029.10'}, 'entity_replacement': {'0:0': 'ENTITY', '14:15': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '41:45': 'ENTITYUNRELATED', '47:50': 'ENTITYUNRELATED', '53:54': 'ENTITYUNRELATED'}}	PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain -specific grammar , words , conceptual mappings , and discourse patterns .
"The multiplicative fragment of linear logic has found a number of applications in computational linguistics: in the ""glue language"" approach to LFG semantics, and in the formulation and parsing of various categorial grammars."	linear logic	computational linguistics	usage	{'e1': {'word': 'linear logic', 'word_index': [(4, 5)], 'id': 'P98-1088.1'}, 'e2': {'word': 'computational linguistics', 'word_index': [(13, 14)], 'id': 'P98-1088.2'}, 'entity_replacement': {'4:5': 'ENTITY', '13:14': 'ENTITYOTHER', '18:21': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED'}}	"The multiplicative fragment of linear logic has found a number of applications in computational linguistics : in the "" glue language "" approach to LFG semantics , and in the formulation and parsing of various categorial grammars ."
"The multiplicative fragment of linear logic has found a number of applications in computational linguistics: in the ""glue language"" approach to LFG semantics, and in the formulation and parsing of various categorial grammars."	"""glue language"""	LFG semantics	usage	"{'e1': {'word': '""glue language""', 'word_index': [(18, 21)], 'id': 'P98-1088.3'}, 'e2': {'word': 'LFG semantics', 'word_index': [(24, 25)], 'id': 'P98-1088.4'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '18:21': 'ENTITY', '24:25': 'ENTITYOTHER', '32:32': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED'}}"	"The multiplicative fragment of linear logic has found a number of applications in computational linguistics : in the "" glue language "" approach to LFG semantics , and in the formulation and parsing of various categorial grammars ."
We describe an implementation of data-driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system.	embodied conversational agent	dialogue system	part_whole	{'e1': {'word': 'embodied conversational agent', 'word_index': [(13, 15)], 'id': 'E06-1045.1'}, 'e2': {'word': 'dialogue system', 'word_index': [(18, 19)], 'id': 'E06-1045.2'}, 'entity_replacement': {'13:15': 'ENTITY', '18:19': 'ENTITYOTHER'}}	We describe an implementation of data-driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system .
We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for large-vocabulary speech recognition.	parser	large-vocabulary speech recognition	usage	{'e1': {'word': 'parser', 'word_index': [(24, 24)], 'id': 'P04-1030.3'}, 'e2': {'word': 'large-vocabulary speech recognition', 'word_index': [(26, 30)], 'id': 'P04-1030.4'}, 'entity_replacement': {'7:12': 'ENTITYUNRELATED', '20:22': 'ENTITYUNRELATED', '24:24': 'ENTITY', '26:30': 'ENTITYOTHER'}}	We present the first application of the head - driven statistical parsing model of Collins ( 1999 ) as a simultaneous language model and parser for large - vocabulary speech recognition .
The parser uses structural and lexical dependencies not considered by n-gram models, conditioning recognition on more linguistically-grounded relationships.	structural and lexical dependencies	parser	usage	{'e1': {'word': 'structural and lexical dependencies', 'word_index': [(3, 6)], 'id': 'P04-1030.8'}, 'e2': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'P04-1030.7'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '3:6': 'ENTITY', '10:11': 'ENTITYUNRELATED'}}	The parser uses structural and lexical dependencies not considered by n-gram models , conditioning recognition on more linguistically - grounded relationships .
Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding.	structural information	speech understanding	usage	{'e1': {'word': 'structural information', 'word_index': [(24, 25)], 'id': 'P04-1030.13'}, 'e2': {'word': 'speech understanding', 'word_index': [(28, 29)], 'id': 'P04-1030.14'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '11:13': 'ENTITYUNRELATED', '17:20': 'ENTITYUNRELATED', '24:25': 'ENTITY', '28:29': 'ENTITYOTHER'}}	Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding .
They are probability, rank, and entropy.	rank	entropy	compare	{'e1': {'word': 'rank', 'word_index': [(4, 4)], 'id': 'P02-1023.4'}, 'e2': {'word': 'entropy', 'word_index': [(7, 7)], 'id': 'P02-1023.5'}, 'entity_replacement': {'4:4': 'ENTITY', '7:7': 'ENTITYOTHER'}}	They are probability , rank , and entropy .
We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate (CER).	pruning criteria	Chinese text input	usage	{'e1': {'word': 'pruning criteria', 'word_index': [(7, 8)], 'id': 'P02-1023.6'}, 'e2': {'word': 'Chinese text input', 'word_index': [(14, 16)], 'id': 'P02-1023.7'}, 'entity_replacement': {'7:8': 'ENTITY', '14:16': 'ENTITYOTHER', '20:25': 'ENTITYUNRELATED'}}	We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate ( CER ) .
Johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a unification-based grammar that is used by a multidimensional chart parser to compose inputs.	unification-based grammar	multidimensional chart parser	usage	{'e1': {'word': 'unification-based grammar', 'word_index': [(16, 19)], 'id': 'C00-1054.4'}, 'e2': {'word': 'multidimensional chart parser', 'word_index': [(25, 27)], 'id': 'C00-1054.5'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '16:19': 'ENTITY', '25:27': 'ENTITYOTHER'}}	Johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a unification - based grammar that is used by a multidimensional chart parser to compose inputs .
In this paper, we present an alternative approach in which multimodal parsing and understanding are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation.	weighted finite-state device	multimodal parsing and understanding	usage	{'e1': {'word': 'weighted finite-state device', 'word_index': [(19, 23)], 'id': 'C00-1054.8'}, 'e2': {'word': 'multimodal parsing and understanding', 'word_index': [(11, 14)], 'id': 'C00-1054.7'}, 'entity_replacement': {'11:14': 'ENTITYOTHER', '19:23': 'ENTITY', '26:29': 'ENTITYUNRELATED'}}	In this paper , we present an alternative approach in which multimodal parsing and understanding are achieved using a weighted finite - state device which takes speech and gesture streams as inputs and outputs their joint interpretation .
This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.	convolution kernel	syntactic structure information	model-feature	{'e1': {'word': 'convolution kernel', 'word_index': [(6, 7)], 'id': 'N06-1037.1'}, 'e2': {'word': 'syntactic structure information', 'word_index': [(13, 15)], 'id': 'N06-1037.3'}, 'entity_replacement': {'6:7': 'ENTITY', '9:10': 'ENTITYUNRELATED', '13:15': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED'}}	This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction .
Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.	syntactic structure features	parse tree	part_whole	{'e1': {'word': 'syntactic structure features', 'word_index': [(5, 7)], 'id': 'N06-1037.5'}, 'e2': {'word': 'parse tree', 'word_index': [(11, 12)], 'id': 'N06-1037.6'}, 'entity_replacement': {'5:7': 'ENTITY', '11:12': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '28:30': 'ENTITYUNRELATED'}}	Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel .
This paper describes a particular approach to parsing that utilizes recent advances in unification-based parsing and in classification-based knowledge representation.	unification-based parsing	parsing	usage	{'e1': {'word': 'unification-based parsing', 'word_index': [(13, 16)], 'id': 'H90-1011.2'}, 'e2': {'word': 'parsing', 'word_index': [(7, 7)], 'id': 'H90-1011.1'}, 'entity_replacement': {'7:7': 'ENTITYOTHER', '13:16': 'ENTITY', '19:23': 'ENTITYUNRELATED'}}	This paper describes a particular approach to parsing that utilizes recent advances in unification - based parsing and in classification - based knowledge representation .
As unification-based grammatical frameworks are extended to handle richer descriptions of linguistic information, they begin to share many of the properties that have been developed in KL-ONE-like knowledge representation systems.	unification-based grammatical frameworks	KL-ONE-like knowledge representation systems	compare	{'e1': {'word': 'unification-based grammatical frameworks', 'word_index': [(1, 5)], 'id': 'H90-1011.4'}, 'e2': {'word': 'KL-ONE-like knowledge representation systems', 'word_index': [(29, 34)], 'id': 'H90-1011.6'}, 'entity_replacement': {'1:5': 'ENTITY', '13:14': 'ENTITYUNRELATED', '29:34': 'ENTITYOTHER'}}	As unification - based grammatical frameworks are extended to handle richer descriptions of linguistic information , they begin to share many of the properties that have been developed in KL-ONE - like knowledge representation systems .
This commonality suggests that some of the classification-based representation techniques can be applied to unification-based linguistic descriptions.	classification-based representation techniques	unification-based linguistic descriptions	usage	{'e1': {'word': 'classification-based representation techniques', 'word_index': [(7, 11)], 'id': 'H90-1011.7'}, 'e2': {'word': 'unification-based linguistic descriptions', 'word_index': [(16, 20)], 'id': 'H90-1011.8'}, 'entity_replacement': {'7:11': 'ENTITY', '16:20': 'ENTITYOTHER'}}	This commonality suggests that some of the classification - based representation techniques can be applied to unification - based linguistic descriptions .
The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system [2], in which parsing is characterized as an inference process called incremental description refinement.	KL-ONE style representation	parsing	usage	{'e1': {'word': 'KL-ONE style representation', 'word_index': [(4, 6)], 'id': 'H90-1011.11'}, 'e2': {'word': 'parsing', 'word_index': [(8, 8)], 'id': 'H90-1011.12'}, 'entity_replacement': {'4:6': 'ENTITY', '8:8': 'ENTITYOTHER', '10:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '33:35': 'ENTITYUNRELATED'}}	The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system [ 2 ] , in which parsing is characterized as an inference process called incremental description refinement .
The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system [2], in which parsing is characterized as an inference process called incremental description refinement.	incremental description refinement	parsing	usage	{'e1': {'word': 'incremental description refinement', 'word_index': [(33, 35)], 'id': 'H90-1011.16'}, 'e2': {'word': 'parsing', 'word_index': [(25, 25)], 'id': 'H90-1011.15'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '25:25': 'ENTITYOTHER', '33:35': 'ENTITY'}}	The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system [ 2 ] , in which parsing is characterized as an inference process called incremental description refinement .
"""To explain complex phenomena, an explanation system must be able to select information from a formal representation of domain knowledge, organize the selected information into multisentential discourse plans, and realize the discourse plans in text."	domain knowledge	explanation system	usage	{'e1': {'word': 'domain knowledge', 'word_index': [(20, 21)], 'id': 'J97-1004.2'}, 'e2': {'word': 'explanation system', 'word_index': [(7, 8)], 'id': 'J97-1004.1'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '20:21': 'ENTITY', '28:30': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED'}}	""" To explain complex phenomena , an explanation system must be able to select information from a formal representation of domain knowledge , organize the selected information into multisentential discourse plans , and realize the discourse plans in text ."
This paper reports on a seven-year effort to empirically study explanation generation from semantically rich, large-scale knowledge bases.	semantically rich, large-scale knowledge bases	explanation generation	usage	{'e1': {'word': 'semantically rich, large-scale knowledge bases', 'word_index': [(14, 21)], 'id': 'J97-1004.7'}, 'e2': {'word': 'explanation generation', 'word_index': [(11, 12)], 'id': 'J97-1004.6'}, 'entity_replacement': {'11:12': 'ENTITYOTHER', '14:21': 'ENTITY'}}	This paper reports on a seven- year effort to empirically study explanation generation from semantically rich , large - scale knowledge bases .
In particular, it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large-scale knowledge base in the domain of botanical anatomy, physiology, and development.	large-scale knowledge base	robust explanation system	usage	{'e1': {'word': 'large-scale knowledge base', 'word_index': [(18, 22)], 'id': 'J97-1004.10'}, 'e2': {'word': 'robust explanation system', 'word_index': [(6, 8)], 'id': 'J97-1004.8'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '11:14': 'ENTITYUNRELATED', '18:22': 'ENTITY'}}	In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large - scale knowledge base in the domain of botanical anatomy , physiology , and development .
Given a particular concept, or word sense, a topic signature is a set of words that tend to co-occur with it.	words	topic signature	part_whole	{'e1': {'word': 'words', 'word_index': [(16, 16)], 'id': 'P04-2005.5'}, 'e2': {'word': 'topic signature', 'word_index': [(10, 11)], 'id': 'P04-2005.4'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:7': 'ENTITYUNRELATED', '10:11': 'ENTITYOTHER', '16:16': 'ENTITY'}}	Given a particular concept , or word sense , a topic signature is a set of words that tend to co-occur with it .
Topic signatures can be useful in a number of Natural Language Processing (NLP) applications, such as Word Sense Disambiguation (WSD) and Text Summarisation.	Topic signatures	Natural Language Processing (NLP) applications	usage	{'e1': {'word': 'Topic signatures', 'word_index': [(0, 1)], 'id': 'P04-2005.6'}, 'e2': {'word': 'Natural Language Processing (NLP) applications', 'word_index': [(9, 15)], 'id': 'P04-2005.7'}, 'entity_replacement': {'0:1': 'ENTITY', '9:15': 'ENTITYOTHER', '19:24': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	Topic signatures can be useful in a number of Natural Language Processing ( NLP ) applications , such as Word Sense Disambiguation ( WSD ) and Text Summarisation .
Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web.	word senses	English	part_whole	{'e1': {'word': 'word senses', 'word_index': [(10, 11)], 'id': 'P04-2005.10'}, 'e2': {'word': 'English', 'word_index': [(15, 15)], 'id': 'P04-2005.11'}, 'entity_replacement': {'10:11': 'ENTITY', '15:15': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED'}}	Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web .
Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web.	Chinese text	corpora	part_whole	{'e1': {'word': 'Chinese text', 'word_index': [(26, 27)], 'id': 'P04-2005.13'}, 'e2': {'word': 'corpora', 'word_index': [(30, 30)], 'id': 'P04-2005.14'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '26:27': 'ENTITY', '30:30': 'ENTITYOTHER'}}	Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web .
We evaluated the topic signatures on a WSD task, where we trained a second-order vector cooccurrence algorithm on standard WSD datasets, with promising results.	topic signatures	WSD task	usage	{'e1': {'word': 'topic signatures', 'word_index': [(3, 4)], 'id': 'P04-2005.15'}, 'e2': {'word': 'WSD task', 'word_index': [(7, 8)], 'id': 'P04-2005.16'}, 'entity_replacement': {'3:4': 'ENTITY', '7:8': 'ENTITYOTHER', '14:19': 'ENTITYUNRELATED', '21:23': 'ENTITYUNRELATED'}}	We evaluated the topic signatures on a WSD task , where we trained a second - order vector cooccurrence algorithm on standard WSD datasets , with promising results .
This paper presents a novel ensemble learning approach to resolving German pronouns.	ensemble learning approach	German pronouns	usage	{'e1': {'word': 'ensemble learning approach', 'word_index': [(5, 7)], 'id': 'P04-2010.1'}, 'e2': {'word': 'German pronouns', 'word_index': [(10, 11)], 'id': 'P04-2010.2'}, 'entity_replacement': {'5:7': 'ENTITY', '10:11': 'ENTITYOTHER'}}	This paper presents a novel ensemble learning approach to resolving German pronouns .
Furthermore, we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process.	standalone system	unannotated text	usage	{'e1': {'word': 'standalone system', 'word_index': [(5, 6)], 'id': 'P04-2010.7'}, 'e2': {'word': 'unannotated text', 'word_index': [(11, 12)], 'id': 'P04-2010.9'}, 'entity_replacement': {'5:6': 'ENTITY', '9:9': 'ENTITYUNRELATED', '11:12': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	Furthermore , we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .
Although the system performs well within a limited textual domain, further research is needed to make it effective for open-domain question answering and text summarisation.	textual domain	open-domain question answering	compare	{'e1': {'word': 'textual domain', 'word_index': [(8, 9)], 'id': 'P04-2010.12'}, 'e2': {'word': 'open-domain question answering', 'word_index': [(20, 22)], 'id': 'P04-2010.13'}, 'entity_replacement': {'8:9': 'ENTITY', '20:22': 'ENTITYOTHER', '24:25': 'ENTITYUNRELATED'}}	Although the system performs well within a limited textual domain , further research is needed to make it effective for open-domain question answering and text summarisation .
This paper presents a machine learning approach to bare slice disambiguation in dialogue.	machine learning approach	bare slice disambiguation	usage	{'e1': {'word': 'machine learning approach', 'word_index': [(4, 6)], 'id': 'C04-1035.1'}, 'e2': {'word': 'bare slice disambiguation', 'word_index': [(8, 10)], 'id': 'C04-1035.2'}, 'entity_replacement': {'4:6': 'ENTITY', '8:10': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED'}}	This paper presents a machine learning approach to bare slice disambiguation in dialogue .
We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses.	probabilistic Horn clauses	heuristic principles	model-feature	{'e1': {'word': 'probabilistic Horn clauses', 'word_index': [(17, 19)], 'id': 'C04-1035.6'}, 'e2': {'word': 'heuristic principles', 'word_index': [(5, 6)], 'id': 'C04-1035.4'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '9:12': 'ENTITYUNRELATED', '17:19': 'ENTITY'}}	We extract a set of heuristic principles from a corpus - based sample and formulate them as probabilistic Horn clauses .
We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two different machine learning algorithms : SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system.	domain independent features	input dataset	model-feature	{'e1': {'word': 'domain independent features', 'word_index': [(13, 15)], 'id': 'C04-1035.8'}, 'e2': {'word': 'input dataset', 'word_index': [(19, 20)], 'id': 'C04-1035.9'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '13:15': 'ENTITY', '19:20': 'ENTITYOTHER', '26:28': 'ENTITYUNRELATED', '33:37': 'ENTITYUNRELATED', '43:46': 'ENTITYUNRELATED'}}	We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule - based learning algorithm , and TiMBL , a memory - based system .
We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two different machine learning algorithms : SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system.	rule-based learning algorithm	memory-based system	compare	{'e1': {'word': 'rule-based learning algorithm', 'word_index': [(33, 37)], 'id': 'C04-1035.11'}, 'e2': {'word': 'memory-based system', 'word_index': [(43, 46)], 'id': 'C04-1035.12'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '13:15': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '26:28': 'ENTITYUNRELATED', '33:37': 'ENTITY', '43:46': 'ENTITYOTHER'}}	We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule - based learning algorithm , and TiMBL , a memory - based system .
The results show that the features in terms of which we formulate our heuristic principles have significant predictive power, and that rules that closely resemble our Horn clauses can be learnt automatically from these features.	features	heuristic principles	model-feature	{'e1': {'word': 'features', 'word_index': [(5, 5)], 'id': 'C04-1035.14'}, 'e2': {'word': 'heuristic principles', 'word_index': [(13, 14)], 'id': 'C04-1035.15'}, 'entity_replacement': {'5:5': 'ENTITY', '13:14': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED'}}	The results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .
The results show that the features in terms of which we formulate our heuristic principles have significant predictive power, and that rules that closely resemble our Horn clauses can be learnt automatically from these features.	rules	Horn clauses	compare	{'e1': {'word': 'rules', 'word_index': [(22, 22)], 'id': 'C04-1035.16'}, 'e2': {'word': 'Horn clauses', 'word_index': [(27, 28)], 'id': 'C04-1035.17'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '22:22': 'ENTITY', '27:28': 'ENTITYOTHER', '35:35': 'ENTITYUNRELATED'}}	The results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .
The new criterion – meaning-entailing substitutability – fits the needs of semantic-oriented NLP applications and can be evaluated directly (independent of an application) at a good level of human agreement.	meaning-entailing substitutability	semantic-oriented NLP applications	usage	{'e1': {'word': 'meaning-entailing substitutability', 'word_index': [(4, 7)], 'id': 'C04-1036.3'}, 'e2': {'word': 'semantic-oriented NLP applications', 'word_index': [(13, 15)], 'id': 'C04-1036.4'}, 'entity_replacement': {'4:7': 'ENTITY', '13:15': 'ENTITYOTHER', '32:33': 'ENTITYUNRELATED'}}	The new criterion – meaning - entailing substitutability – fits the needs of semantic-oriented NLP applications and can be evaluated directly ( independent of an application ) at a good level of human agreement .
Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality.	semantic criterion	distributional word feature vectors	model-feature	{'e1': {'word': 'semantic criterion', 'word_index': [(3, 4)], 'id': 'C04-1036.6'}, 'e2': {'word': 'distributional word feature vectors', 'word_index': [(11, 14)], 'id': 'C04-1036.7'}, 'entity_replacement': {'3:4': 'ENTITY', '11:14': 'ENTITYOTHER', '19:21': 'ENTITYUNRELATED', '29:31': 'ENTITYUNRELATED'}}	Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results , proposing an objective measure for evaluating feature vector quality .
Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance.</abstract>	feature weighting and selection function	feature vectors	result	{'e1': {'word': 'feature weighting and selection function', 'word_index': [(4, 8)], 'id': 'C04-1036.10'}, 'e2': {'word': 'feature vectors', 'word_index': [(15, 16)], 'id': 'C04-1036.11'}, 'entity_replacement': {'4:8': 'ENTITY', '15:16': 'ENTITYOTHER', '19:21': 'ENTITYUNRELATED'}}	Finally , a novel feature weighting and selection function is presented , which yields superior feature vectors and better word similarity performance .< / abstract >
In this paper, we identify features of electronic discussions that influence the clustering process, and offer a filtering mechanism that removes undesirable influences.	features	electronic discussions	model-feature	{'e1': {'word': 'features', 'word_index': [(6, 6)], 'id': 'C04-1068.4'}, 'e2': {'word': 'electronic discussions', 'word_index': [(8, 9)], 'id': 'C04-1068.5'}, 'entity_replacement': {'6:6': 'ENTITY', '8:9': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	In this paper , we identify features of electronic discussions that influence the clustering process , and offer a filtering mechanism that removes undesirable influences .
We tested the clustering and filtering processes on electronic newsgroup discussions, and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval.	clustering and filtering processes	electronic newsgroup discussions	usage	{'e1': {'word': 'clustering and filtering processes', 'word_index': [(3, 6)], 'id': 'C04-1068.9'}, 'e2': {'word': 'electronic newsgroup discussions', 'word_index': [(8, 10)], 'id': 'C04-1068.10'}, 'entity_replacement': {'3:6': 'ENTITY', '8:10': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '22:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	We tested the clustering and filtering processes on electronic newsgroup discussions , and evaluated their performance by means of two experiments : coarse - level clustering simple information retrieval .
We tested the clustering and filtering processes on electronic newsgroup discussions, and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval.	coarse-level clustering	performance	result	{'e1': {'word': 'coarse-level clustering', 'word_index': [(22, 24)], 'id': 'C04-1068.12'}, 'e2': {'word': 'performance', 'word_index': [(15, 15)], 'id': 'C04-1068.11'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '8:10': 'ENTITYUNRELATED', '15:15': 'ENTITYOTHER', '22:24': 'ENTITY', '26:27': 'ENTITYUNRELATED'}}	We tested the clustering and filtering processes on electronic newsgroup discussions , and evaluated their performance by means of two experiments : coarse- level clustering simple information retrieval .
We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case.	context	word	model-feature	{'e1': {'word': 'context', 'word_index': [(8, 8)], 'id': 'C04-1080.1'}, 'e2': {'word': 'word', 'word_index': [(14, 14)], 'id': 'C04-1080.2'}, 'entity_replacement': {'8:8': 'ENTITY', '14:14': 'ENTITYOTHER', '25:28': 'ENTITYUNRELATED'}}	We present a new HMM tagger that exploits context on both sides of a word to be tagged , and evaluate it in both the unsupervised and supervised case .
Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable.	quality	accuracy	result	{'e1': {'word': 'quality', 'word_index': [(3, 3)], 'id': 'C04-1080.7'}, 'e2': {'word': 'accuracy', 'word_index': [(10, 10)], 'id': 'C04-1080.9'}, 'entity_replacement': {'3:3': 'ENTITY', '6:6': 'ENTITYUNRELATED', '10:10': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms , we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable .
Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable.	HMM training	accuracy	result	{'e1': {'word': 'HMM training', 'word_index': [(24, 25)], 'id': 'C04-1080.11'}, 'e2': {'word': 'accuracy', 'word_index': [(28, 28)], 'id': 'C04-1080.12'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '24:25': 'ENTITY', '28:28': 'ENTITYOTHER', '30:30': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms , we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable .
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects.	referring expressions	objects	usage	{'e1': {'word': 'referring expressions', 'word_index': [(4, 5)], 'id': 'C04-1096.1'}, 'e2': {'word': 'objects', 'word_index': [(10, 10)], 'id': 'C04-1096.2'}, 'entity_replacement': {'4:5': 'ENTITY', '10:10': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED'}}	Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects .
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects.	binary relations	objects	model-feature	{'e1': {'word': 'binary relations', 'word_index': [(12, 13)], 'id': 'C04-1096.3'}, 'e2': {'word': 'objects', 'word_index': [(15, 15)], 'id': 'C04-1096.4'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '12:13': 'ENTITY', '15:15': 'ENTITYOTHER'}}	Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects .
To overcome this limitation, this paper proposes a method utilizing the perceptual groups of objects and n-ary relations among them.	n-ary relations	objects	model-feature	{'e1': {'word': 'n-ary relations', 'word_index': [(17, 18)], 'id': 'C04-1096.7'}, 'e2': {'word': 'objects', 'word_index': [(15, 15)], 'id': 'C04-1096.6'}, 'entity_replacement': {'15:15': 'ENTITYOTHER', '17:18': 'ENTITY'}}	To overcome this limitation , this paper proposes a method utilizing the perceptual groups of objects and n-ary relations among them .
Machine transliteration/back-transliteration plays an important role in many multilingual speech and language applications.	Machine transliteration/back-transliteration	multilingual speech and language applications	part_whole	{'e1': {'word': 'Machine transliteration/back-transliteration', 'word_index': [(0, 3)], 'id': 'C04-1103.1'}, 'e2': {'word': 'multilingual speech and language applications', 'word_index': [(10, 14)], 'id': 'C04-1103.2'}, 'entity_replacement': {'0:3': 'ENTITY', '10:14': 'ENTITYOTHER'}}	Machine transliteration / back-transliteration plays an important role in many multilingual speech and language applications .
In this paper, a novel framework for machine transliteration/backtransliteration that allows us to carry out direct orthographical mapping (DOM) between two different languages is presented.	machine transliteration/backtransliteration	direct orthographical mapping (DOM)	usage	{'e1': {'word': 'machine transliteration/backtransliteration', 'word_index': [(8, 11)], 'id': 'C04-1103.3'}, 'e2': {'word': 'direct orthographical mapping (DOM)', 'word_index': [(18, 23)], 'id': 'C04-1103.4'}, 'entity_replacement': {'8:11': 'ENTITY', '18:23': 'ENTITYOTHER', '27:27': 'ENTITYUNRELATED'}}	In this paper , a novel framework for machine transliteration / backtransliteration that allows us to carry out direct orthographical mapping ( DOM ) between two different languages is presented .
Under this framework, a joint source-channel transliteration model, also called n-gram transliteration model (n-gram TM), is further proposed to model the transliteration process.	n-gram transliteration model (n-gram TM)	transliteration process	model-feature	{'e1': {'word': 'n-gram transliteration model (n-gram TM)', 'word_index': [(14, 21)], 'id': 'C04-1103.7'}, 'e2': {'word': 'transliteration process', 'word_index': [(29, 30)], 'id': 'C04-1103.8'}, 'entity_replacement': {'5:10': 'ENTITYUNRELATED', '14:21': 'ENTITY', '29:30': 'ENTITYOTHER'}}	Under this framework , a joint source - channel transliteration model , also called n-gram transliteration model ( n- gram TM ) , is further proposed to model the transliteration process .
We evaluate the proposed methods through several transliteration/backtransliteration experiments for English/Chinese and English/Japanese language pairs.	transliteration/backtransliteration experiments	English/Chinese and English/Japanese language pairs	usage	{'e1': {'word': 'transliteration/backtransliteration experiments', 'word_index': [(7, 10)], 'id': 'C04-1103.9'}, 'e2': {'word': 'English/Chinese and English/Japanese language pairs', 'word_index': [(12, 20)], 'id': 'C04-1103.10'}, 'entity_replacement': {'7:10': 'ENTITY', '12:20': 'ENTITYOTHER'}}	We evaluate the proposed methods through several transliteration / backtransliteration experiments for English / Chinese and English / Japanese language pairs .
In this paper, we present a corpus-based supervised word sense disambiguation (WSD) system for Dutch which combines statistical classification (maximum entropy) with linguistic information.	corpus-based supervised word sense disambiguation (WSD) system	Dutch	usage	{'e1': {'word': 'corpus-based supervised word sense disambiguation (WSD) system', 'word_index': [(7, 17)], 'id': 'C04-1112.1'}, 'e2': {'word': 'Dutch', 'word_index': [(19, 19)], 'id': 'C04-1112.2'}, 'entity_replacement': {'7:17': 'ENTITY', '19:19': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	In this paper , we present a corpus - based supervised word sense disambiguation ( WSD ) system for Dutch which combines statistical classification ( maximum entropy ) with linguistic information .
Instead of building individual classifiers per ambiguous wordform, we introduce a lemma-based approach.	classifiers	lemma-based approach	compare	{'e1': {'word': 'classifiers', 'word_index': [(4, 4)], 'id': 'C04-1112.6'}, 'e2': {'word': 'lemma-based approach', 'word_index': [(12, 15)], 'id': 'C04-1112.8'}, 'entity_replacement': {'4:4': 'ENTITY', '6:7': 'ENTITYUNRELATED', '12:15': 'ENTITYOTHER'}}	Instead of building individual classifiers per ambiguous wordform , we introduce a lemma - based approach .
The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier, therefore augmenting the training material available to the algorithm.	inflected forms	ambiguous word	model-feature	{'e1': {'word': 'inflected forms', 'word_index': [(11, 12)], 'id': 'C04-1112.9'}, 'e2': {'word': 'ambiguous word', 'word_index': [(15, 16)], 'id': 'C04-1112.10'}, 'entity_replacement': {'11:12': 'ENTITY', '15:16': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier , therefore augmenting the training material available to the algorithm .
The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier, therefore augmenting the training material available to the algorithm.	training material	algorithm	usage	{'e1': {'word': 'training material', 'word_index': [(24, 25)], 'id': 'C04-1112.12'}, 'e2': {'word': 'algorithm', 'word_index': [(29, 29)], 'id': 'C04-1112.13'}, 'entity_replacement': {'11:12': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '24:25': 'ENTITY', '29:29': 'ENTITYOTHER'}}	The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier , therefore augmenting the training material available to the algorithm .
Testing the lemma-based model on the Dutch Senseval-2 test data, we achieve a significant increase in accuracy over the wordform model.	lemma-based model	Dutch Senseval-2 test data	usage	{'e1': {'word': 'lemma-based model', 'word_index': [(2, 4)], 'id': 'C04-1112.14'}, 'e2': {'word': 'Dutch Senseval-2 test data', 'word_index': [(7, 12)], 'id': 'C04-1112.15'}, 'entity_replacement': {'2:4': 'ENTITY', '7:12': 'ENTITYOTHER', '20:20': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED'}}	Testing the lemma- based model on the Dutch Senseval - 2 test data , we achieve a significant increase in accuracy over the wordform model .
We present a text mining method for finding synonymous expressions based on the distributional hypothesis in a set of coherent corpora.	distributional hypothesis	text mining method	usage	{'e1': {'word': 'distributional hypothesis', 'word_index': [(13, 14)], 'id': 'C04-1116.3'}, 'e2': {'word': 'text mining method', 'word_index': [(3, 5)], 'id': 'C04-1116.1'}, 'entity_replacement': {'3:5': 'ENTITYOTHER', '8:9': 'ENTITYUNRELATED', '13:14': 'ENTITY', '20:20': 'ENTITYUNRELATED'}}	We present a text mining method for finding synonymous expressions based on the distributional hypothesis in a set of coherent corpora .
This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author's text as a coherent corpus.	term aggregation system	accuracy	result	{'e1': {'word': 'term aggregation system', 'word_index': [(12, 14)], 'id': 'C04-1116.6'}, 'e2': {'word': 'accuracy', 'word_index': [(9, 9)], 'id': 'C04-1116.5'}, 'entity_replacement': {'9:9': 'ENTITYOTHER', '12:14': 'ENTITY', '19:19': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED'}}	This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author 's text as a coherent corpus .
This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author's text as a coherent corpus.	text	corpus	usage	{'e1': {'word': 'text', 'word_index': [(19, 19)], 'id': 'C04-1116.7'}, 'e2': {'word': 'corpus', 'word_index': [(23, 23)], 'id': 'C04-1116.8'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '19:19': 'ENTITY', '23:23': 'ENTITYOTHER'}}	This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author 's text as a coherent corpus .
Our approach is based on the idea that one person tends to use one expression for one meaning.	meaning	expression	model-feature	{'e1': {'word': 'meaning', 'word_index': [(17, 17)], 'id': 'C04-1116.10'}, 'e2': {'word': 'expression', 'word_index': [(14, 14)], 'id': 'C04-1116.9'}, 'entity_replacement': {'14:14': 'ENTITYOTHER', '17:17': 'ENTITY'}}	Our approach is based on the idea that one person tends to use one expression for one meaning .
According to our assumption, most of the words with similar context features in each author's corpus tend not to be synonymous expressions.	similar context features	words	model-feature	{'e1': {'word': 'similar context features', 'word_index': [(10, 12)], 'id': 'C04-1116.12'}, 'e2': {'word': 'words', 'word_index': [(8, 8)], 'id': 'C04-1116.11'}, 'entity_replacement': {'8:8': 'ENTITYOTHER', '10:12': 'ENTITY', '17:17': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED'}}	According to our assumption , most of the words with similar context features in each author 's corpus tend not to be synonymous expressions .
Our proposed method improves the accuracy of our term aggregation system, showing that our approach is successful.	term aggregation system	accuracy	result	{'e1': {'word': 'term aggregation system', 'word_index': [(8, 10)], 'id': 'C04-1116.16'}, 'e2': {'word': 'accuracy', 'word_index': [(5, 5)], 'id': 'C04-1116.15'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '8:10': 'ENTITY'}}	Our proposed method improves the accuracy of our term aggregation system , showing that our approach is successful .
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent.	sentence extraction	summarization	usage	{'e1': {'word': 'sentence extraction', 'word_index': [(1, 2)], 'id': 'C04-1128.1'}, 'e2': {'word': 'summarization', 'word_index': [(7, 7)], 'id': 'C04-1128.2'}, 'entity_replacement': {'1:2': 'ENTITY', '7:7': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '38:39': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED', '47:47': 'ENTITYUNRELATED', '52:52': 'ENTITYUNRELATED'}}	While sentence extraction as an approach to summarization has been shown to work in documents of certain genres , because of the conversational nature of email communication where utterances are made in relation to one made previously , sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent .
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent.	genres	documents	model-feature	{'e1': {'word': 'genres', 'word_index': [(17, 17)], 'id': 'C04-1128.4'}, 'e2': {'word': 'documents', 'word_index': [(14, 14)], 'id': 'C04-1128.3'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '14:14': 'ENTITYOTHER', '17:17': 'ENTITY', '25:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '38:39': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED', '47:47': 'ENTITYUNRELATED', '52:52': 'ENTITYUNRELATED'}}	While sentence extraction as an approach to summarization has been shown to work in documents of certain genres , because of the conversational nature of email communication where utterances are made in relation to one made previously , sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent .
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent.	email communication	utterances	part_whole	{'e1': {'word': 'email communication', 'word_index': [(25, 26)], 'id': 'C04-1128.5'}, 'e2': {'word': 'utterances', 'word_index': [(28, 28)], 'id': 'C04-1128.6'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '25:26': 'ENTITY', '28:28': 'ENTITYOTHER', '38:39': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED', '47:47': 'ENTITYUNRELATED', '52:52': 'ENTITYUNRELATED'}}	While sentence extraction as an approach to summarization has been shown to work in documents of certain genres , because of the conversational nature of email communication where utterances are made in relation to one made previously , sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent .
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent.	segments	dialogue	part_whole	{'e1': {'word': 'segments', 'word_index': [(45, 45)], 'id': 'C04-1128.8'}, 'e2': {'word': 'dialogue', 'word_index': [(47, 47)], 'id': 'C04-1128.9'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '38:39': 'ENTITYUNRELATED', '45:45': 'ENTITY', '47:47': 'ENTITYOTHER', '52:52': 'ENTITYUNRELATED'}}	While sentence extraction as an approach to summarization has been shown to work in documents of certain genres , because of the conversational nature of email communication where utterances are made in relation to one made previously , sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent .
In this paper, we present our work on the detection of question-answer pairs in an email conversation for the task of email summarization.	email conversation	question-answer pairs	part_whole	{'e1': {'word': 'email conversation', 'word_index': [(18, 19)], 'id': 'C04-1128.12'}, 'e2': {'word': 'question-answer pairs', 'word_index': [(12, 15)], 'id': 'C04-1128.11'}, 'entity_replacement': {'12:15': 'ENTITYOTHER', '18:19': 'ENTITY', '24:25': 'ENTITYUNRELATED'}}	In this paper , we present our work on the detection of question - answer pairs in an email conversation for the task of email summarization .
We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.	features	lexical similarity	usage	{'e1': {'word': 'features', 'word_index': [(4, 4)], 'id': 'C04-1128.14'}, 'e2': {'word': 'lexical similarity', 'word_index': [(19, 20)], 'id': 'C04-1128.15'}, 'entity_replacement': {'4:4': 'ENTITY', '19:20': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '25:28': 'ENTITYUNRELATED'}}	We show that various features based on the structure of email - threads can be used to improve upon lexical similarity of discourse segments for question - answer pairing .
The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms, an independence model, and a parametric affinity model.	co-occurrence distribution	terms	model-feature	{'e1': {'word': 'co-occurrence distribution', 'word_index': [(12, 13)], 'id': 'C04-1147.3'}, 'e2': {'word': 'terms', 'word_index': [(17, 17)], 'id': 'C04-1147.4'}, 'entity_replacement': {'12:13': 'ENTITY', '17:17': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a parametric affinity model .
In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.	similarity	words	model-feature	{'e1': {'word': 'similarity', 'word_index': [(13, 13)], 'id': 'C04-1147.8'}, 'e2': {'word': 'words', 'word_index': [(15, 15)], 'id': 'C04-1147.9'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '13:13': 'ENTITY', '15:15': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED', '44:44': 'ENTITYUNRELATED', '50:50': 'ENTITYUNRELATED'}}	In comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus .
In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.	lexical affinity	sequential models	usage	{'e1': {'word': 'lexical affinity', 'word_index': [(18, 19)], 'id': 'C04-1147.10'}, 'e2': {'word': 'sequential models', 'word_index': [(22, 23)], 'id': 'C04-1147.11'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '18:19': 'ENTITY', '22:23': 'ENTITYOTHER', '31:31': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED', '44:44': 'ENTITYUNRELATED', '50:50': 'ENTITYUNRELATED'}}	In comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus .
In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.	co-occurrence patterns	words	model-feature	{'e1': {'word': 'co-occurrence patterns', 'word_index': [(36, 37)], 'id': 'C04-1147.13'}, 'e2': {'word': 'words', 'word_index': [(42, 42)], 'id': 'C04-1147.14'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '36:37': 'ENTITY', '42:42': 'ENTITYOTHER', '44:44': 'ENTITYUNRELATED', '50:50': 'ENTITYUNRELATED'}}	In comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus .
The paper presents a method for word sense disambiguation based on parallel corpora.	parallel corpora	word sense disambiguation	usage	{'e1': {'word': 'parallel corpora', 'word_index': [(11, 12)], 'id': 'C04-1192.2'}, 'e2': {'word': 'word sense disambiguation', 'word_index': [(6, 8)], 'id': 'C04-1192.1'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '11:12': 'ENTITY'}}	The paper presents a method for word sense disambiguation based on parallel corpora .
The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus.	automatic extraction	word clustering	usage	{'e1': {'word': 'automatic extraction', 'word_index': [(13, 14)], 'id': 'C04-1192.5'}, 'e2': {'word': 'word clustering', 'word_index': [(9, 10)], 'id': 'C04-1192.4'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:10': 'ENTITYOTHER', '13:14': 'ENTITY', '16:17': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED'}}	The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus .
The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet.	alignment errors	multilingually aligned wordnets	part_whole	{'e1': {'word': 'alignment errors', 'word_index': [(16, 17)], 'id': 'C04-1192.14'}, 'e2': {'word': 'multilingually aligned wordnets', 'word_index': [(19, 21)], 'id': 'C04-1192.15'}, 'entity_replacement': {'16:17': 'ENTITY', '19:21': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	The same system used in a validation mode , can be used to check and spot alignment errors in multilingually aligned wordnets as Balka Net and EuroWord Net .
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.	Minimum Bayes-Risk (MBR) decoding	statistical machine translation	usage	{'e1': {'word': 'Minimum Bayes-Risk (MBR) decoding', 'word_index': [(2, 7)], 'id': 'N04-1022.1'}, 'e2': {'word': 'statistical machine translation', 'word_index': [(9, 11)], 'id': 'N04-1022.2'}, 'entity_replacement': {'2:7': 'ENTITY', '9:11': 'ENTITYOTHER'}}	We present Minimum Bayes-Risk ( MBR ) decoding for statistical machine translation .
This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.	loss functions	translation performance	model-feature	{'e1': {'word': 'loss functions', 'word_index': [(12, 13)], 'id': 'N04-1022.5'}, 'e2': {'word': 'translation performance', 'word_index': [(16, 17)], 'id': 'N04-1022.6'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '12:13': 'ENTITY', '16:17': 'ENTITYOTHER'}}	This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.	loss functions	linguistic information	model-feature	{'e1': {'word': 'loss functions', 'word_index': [(5, 6)], 'id': 'N04-1022.7'}, 'e2': {'word': 'linguistic information', 'word_index': [(12, 13)], 'id': 'N04-1022.8'}, 'entity_replacement': {'5:6': 'ENTITY', '12:13': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '18:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '36:40': 'ENTITYUNRELATED'}}	We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word - to - word alignments from an MT system , and syntactic structure from parse- trees of source and target language sentences .
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.	MT system	word-to-word alignments	part_whole	{'e1': {'word': 'MT system', 'word_index': [(26, 27)], 'id': 'N04-1022.11'}, 'e2': {'word': 'word-to-word alignments', 'word_index': [(18, 23)], 'id': 'N04-1022.10'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '18:23': 'ENTITYOTHER', '26:27': 'ENTITY', '30:31': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED', '36:40': 'ENTITYUNRELATED'}}	We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word - to - word alignments from an MT system , and syntactic structure from parse- trees of source and target language sentences .
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.	parse-trees	source and target language sentences	model-feature	{'e1': {'word': 'parse-trees', 'word_index': [(33, 34)], 'id': 'N04-1022.13'}, 'e2': {'word': 'source and target language sentences', 'word_index': [(36, 40)], 'id': 'N04-1022.14'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '18:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '33:34': 'ENTITY', '36:40': 'ENTITYOTHER'}}	We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word - to - word alignments from an MT system , and syntactic structure from parse- trees of source and target language sentences .
We report the performance of the MBR decoders on a Chinese-to-English translation task.	performance	MBR decoders	result	{'e1': {'word': 'performance', 'word_index': [(3, 3)], 'id': 'N04-1022.15'}, 'e2': {'word': 'MBR decoders', 'word_index': [(6, 7)], 'id': 'N04-1022.16'}, 'entity_replacement': {'3:3': 'ENTITY', '6:7': 'ENTITYOTHER', '10:16': 'ENTITYUNRELATED'}}	We report the performance of the MBR decoders on a Chinese - to - English translation task .
Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.	MBR decoding	statistical MT performance	usage	{'e1': {'word': 'MBR decoding', 'word_index': [(4, 5)], 'id': 'N04-1022.18'}, 'e2': {'word': 'statistical MT performance', 'word_index': [(11, 13)], 'id': 'N04-1022.19'}, 'entity_replacement': {'4:5': 'ENTITY', '11:13': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED'}}	Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions .
Information extraction techniques automatically create structured databases from unstructured data sources, such as the Web or newswire documents.	Information extraction techniques	structured databases	result	{'e1': {'word': 'Information extraction techniques', 'word_index': [(0, 2)], 'id': 'N04-4028.1'}, 'e2': {'word': 'structured databases', 'word_index': [(5, 6)], 'id': 'N04-4028.2'}, 'entity_replacement': {'0:2': 'ENTITY', '5:6': 'ENTITYOTHER', '8:10': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents .
For many reasons, it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field.	confidence	extracted field	model-feature	{'e1': {'word': 'confidence', 'word_index': [(12, 12)], 'id': 'N04-4028.6'}, 'e2': {'word': 'extracted field', 'word_index': [(21, 22)], 'id': 'N04-4028.7'}, 'entity_replacement': {'12:12': 'ENTITY', '21:22': 'ENTITYOTHER'}}	For many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field .
The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a  Markov model.	linear-chain conditional random field (CRF)	information extraction system	usage	{'e1': {'word': 'linear-chain conditional random field (CRF)', 'word_index': [(10, 17)], 'id': 'N04-4028.9'}, 'e2': {'word': 'information extraction system', 'word_index': [(1, 3)], 'id': 'N04-4028.8'}, 'entity_replacement': {'1:3': 'ENTITYOTHER', '10:17': 'ENTITY', '20:21': 'ENTITYUNRELATED', '27:29': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED', '45:46': 'ENTITYUNRELATED'}}	The information extraction system we evaluate is based on a linear- chain conditional random field ( CRF ) , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a Markov model .
The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a  Markov model.	probabilistic model	information extraction tasks	usage	{'e1': {'word': 'probabilistic model', 'word_index': [(19, 20)], 'id': 'N04-4028.10'}, 'e2': {'word': 'information extraction tasks', 'word_index': [(26, 28)], 'id': 'N04-4028.11'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '10:16': 'ENTITYUNRELATED', '19:20': 'ENTITY', '26:28': 'ENTITYOTHER', '38:38': 'ENTITYUNRELATED', '41:41': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED'}}	The information extraction system we evaluate is based on a linear-chain conditional random field ( CRF ) , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a Markov model .
The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a  Markov model.	 Markov model	features	model-feature	{'e1': {'word': ' Markov model', 'word_index': [(44, 45)], 'id': 'N04-4028.14'}, 'e2': {'word': 'features', 'word_index': [(38, 38)], 'id': 'N04-4028.12'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '10:16': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '26:28': 'ENTITYUNRELATED', '38:38': 'ENTITYOTHER', '41:41': 'ENTITYUNRELATED', '44:45': 'ENTITY'}}	The information extraction system we evaluate is based on a linear-chain conditional random field ( CRF ) , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a Markov model .
We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records.	confidence	extracted fields	model-feature	{'e1': {'word': 'confidence', 'word_index': [(7, 7)], 'id': 'N04-4028.15'}, 'e2': {'word': 'extracted fields', 'word_index': [(10, 11)], 'id': 'N04-4028.16'}, 'entity_replacement': {'7:7': 'ENTITY', '10:11': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98 % for retrieving correct fields and 87 % for multi-field records .
It then presents an implemented graphic interpretation system that takes into account a variety of communicative signals, and an evaluation study showing that evidence obtained from shallow processing of the graphic's caption has a significant impact on the system's success.	communicative signals	graphic interpretation system	usage	{'e1': {'word': 'communicative signals', 'word_index': [(15, 16)], 'id': 'P05-1028.4'}, 'e2': {'word': 'graphic interpretation system', 'word_index': [(5, 7)], 'id': 'P05-1028.3'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '15:16': 'ENTITY', '27:28': 'ENTITYUNRELATED'}}	It then presents an implemented graphic interpretation system that takes into account a variety of communicative signals , and an evaluation study showing that evidence obtained from shallow processing of the graphic 's caption has a significant impact on the system 's success .
We present a framework for word alignment based on log-linear models.	log-linear models	word alignment	usage	{'e1': {'word': 'log-linear models', 'word_index': [(9, 12)], 'id': 'P05-1057.2'}, 'e2': {'word': 'word alignment', 'word_index': [(5, 6)], 'id': 'P05-1057.1'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '9:12': 'ENTITY'}}	We present a framework for word alignment based on log - linear models .
All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables.	knowledge sources	feature functions	usage	{'e1': {'word': 'knowledge sources', 'word_index': [(1, 2)], 'id': 'P05-1057.3'}, 'e2': {'word': 'feature functions', 'word_index': [(6, 7)], 'id': 'P05-1057.4'}, 'entity_replacement': {'1:2': 'ENTITY', '6:7': 'ENTITYOTHER', '13:15': 'ENTITYUNRELATED', '18:20': 'ENTITYUNRELATED'}}	All knowledge sources are treated as feature functions , which depend on the source langauge sentence , the target language sentence and possible additional variables .
Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.	Log-linear models	statistical alignment models	usage	{'e1': {'word': 'Log-linear models', 'word_index': [(0, 1)], 'id': 'P05-1057.7'}, 'e2': {'word': 'statistical alignment models', 'word_index': [(3, 5)], 'id': 'P05-1057.8'}, 'entity_replacement': {'0:1': 'ENTITY', '3:5': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED'}}	Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information .
In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.	bilingual dictionary coverage	features	usage	{'e1': {'word': 'bilingual dictionary coverage', 'word_index': [(16, 18)], 'id': 'P05-1057.12'}, 'e2': {'word': 'features', 'word_index': [(20, 20)], 'id': 'P05-1057.13'}, 'entity_replacement': {'6:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '16:18': 'ENTITY', '20:20': 'ENTITYOTHER'}}	In this paper , we use IBM Model 3 alignment probabilities , POS correspondence , and bilingual dictionary coverage as features .
Our experiments show that log-linear models significantly outperform IBM translation models.	log-linear models	IBM translation models	compare	{'e1': {'word': 'log-linear models', 'word_index': [(4, 5)], 'id': 'P05-1057.14'}, 'e2': {'word': 'IBM translation models', 'word_index': [(8, 10)], 'id': 'P05-1057.15'}, 'entity_replacement': {'4:5': 'ENTITY', '8:10': 'ENTITYOTHER'}}	Our experiments show that log-linear models significantly outperform IBM translation models .
This paper presents the results of automatically inducing a Combinatory Categorial Grammar (CCG) lexicon from a Turkish dependency treebank.	Turkish dependency treebank	Combinatory Categorial Grammar (CCG) lexicon	usage	{'e1': {'word': 'Turkish dependency treebank', 'word_index': [(18, 20)], 'id': 'P05-2013.2'}, 'e2': {'word': 'Combinatory Categorial Grammar (CCG) lexicon', 'word_index': [(9, 15)], 'id': 'P05-2013.1'}, 'entity_replacement': {'9:15': 'ENTITYOTHER', '18:20': 'ENTITY'}}	This paper presents the results of automatically inducing a Combinatory Categorial Grammar ( CCG ) lexicon from a Turkish dependency treebank .
The fact that Turkish is an agglutinating free word order language presents a challenge for language theories.	agglutinating free word order language	Turkish	model-feature	{'e1': {'word': 'agglutinating free word order language', 'word_index': [(6, 10)], 'id': 'P05-2013.4'}, 'e2': {'word': 'Turkish', 'word_index': [(3, 3)], 'id': 'P05-2013.3'}, 'entity_replacement': {'3:3': 'ENTITYOTHER', '6:10': 'ENTITY', '15:16': 'ENTITYUNRELATED'}}	The fact that Turkish is an agglutinating free word order language presents a challenge for language theories .
We explored possible ways to obtain a compact lexicon, consistent with CCG principles, from a treebank which is an order of magnitude smaller than Penn WSJ.	treebank	Penn WSJ	compare	{'e1': {'word': 'treebank', 'word_index': [(17, 17)], 'id': 'P05-2013.8'}, 'e2': {'word': 'Penn WSJ', 'word_index': [(26, 27)], 'id': 'P05-2013.9'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '17:17': 'ENTITY', '26:27': 'ENTITYOTHER'}}	We explored possible ways to obtain a compact lexicon , consistent with CCG principles , from a treebank which is an order of magnitude smaller than Penn WSJ .
In the Chinese language, a verb may have its dependents on its left, right or on both sides.	verb	Chinese language	part_whole	{'e1': {'word': 'verb', 'word_index': [(6, 6)], 'id': 'I05-2044.2'}, 'e2': {'word': 'Chinese language', 'word_index': [(2, 3)], 'id': 'I05-2044.1'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '6:6': 'ENTITY', '10:10': 'ENTITYUNRELATED'}}	In the Chinese language , a verb may have its dependents on its left , right or on both sides .
The ambiguity resolution of right-side dependencies is essential for dependency parsing of sentences with two or more verbs.	ambiguity resolution	dependency parsing	part_whole	{'e1': {'word': 'ambiguity resolution', 'word_index': [(1, 2)], 'id': 'I05-2044.4'}, 'e2': {'word': 'dependency parsing', 'word_index': [(11, 12)], 'id': 'I05-2044.6'}, 'entity_replacement': {'1:2': 'ENTITY', '4:7': 'ENTITYUNRELATED', '11:12': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED'}}	The ambiguity resolution of right - side dependencies is essential for dependency parsing of sentences with two or more verbs .
The ambiguity resolution of right-side dependencies is essential for dependency parsing of sentences with two or more verbs.	verbs	sentences	part_whole	{'e1': {'word': 'verbs', 'word_index': [(19, 19)], 'id': 'I05-2044.8'}, 'e2': {'word': 'sentences', 'word_index': [(14, 14)], 'id': 'I05-2044.7'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:7': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '14:14': 'ENTITYOTHER', '19:19': 'ENTITY'}}	The ambiguity resolution of right - side dependencies is essential for dependency parsing of sentences with two or more verbs .
Previous works on shift-reduce dependency parsers may not guarantee the connectivity of a dependency tree due to their weakness at resolving the right-side dependencies.	connectivity	dependency tree	model-feature	{'e1': {'word': 'connectivity', 'word_index': [(12, 12)], 'id': 'I05-2044.10'}, 'e2': {'word': 'dependency tree', 'word_index': [(15, 16)], 'id': 'I05-2044.11'}, 'entity_replacement': {'3:7': 'ENTITYUNRELATED', '12:12': 'ENTITY', '15:16': 'ENTITYOTHER', '24:27': 'ENTITYUNRELATED'}}	Previous works on shift - reduce dependency parsers may not guarantee the connectivity of a dependency tree due to their weakness at resolving the right - side dependencies .
This paper proposes a two-phase shift-reduce dependency parser based on SVM learning.	SVM learning	two-phase shift-reduce dependency parser	usage	{'e1': {'word': 'SVM learning', 'word_index': [(13, 14)], 'id': 'I05-2044.14'}, 'e2': {'word': 'two-phase shift-reduce dependency parser', 'word_index': [(4, 10)], 'id': 'I05-2044.13'}, 'entity_replacement': {'4:10': 'ENTITYOTHER', '13:14': 'ENTITY'}}	This paper proposes a two - phase shift- reduce dependency parser based on SVM learning .
In experimental evaluation, our proposed method outperforms previous shift-reduce dependency parsers for the Chine language, showing improvement of dependency accuracy by 10.08%.	shift-reduce dependency parsers	Chine language	usage	{'e1': {'word': 'shift-reduce dependency parsers', 'word_index': [(9, 13)], 'id': 'I05-2044.18'}, 'e2': {'word': 'Chine language', 'word_index': [(16, 17)], 'id': 'I05-2044.19'}, 'entity_replacement': {'9:13': 'ENTITY', '16:17': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED'}}	In experimental evaluation , our proposed method outperforms previous shift - reduce dependency parsers for the Chine language , showing improvement of dependency accuracy by 10.08 %.
We present an operable definition of focus which is argued to be of a cognito-pragmatic nature and explore how it is determined in discourse in a formalized manner.	focus	discourse	part_whole	{'e1': {'word': 'focus', 'word_index': [(6, 6)], 'id': 'E99-1038.1'}, 'e2': {'word': 'discourse', 'word_index': [(25, 25)], 'id': 'E99-1038.2'}, 'entity_replacement': {'6:6': 'ENTITY', '25:25': 'ENTITYOTHER'}}	We present an operable definition of focus which is argued to be of a cognito - pragmatic nature and explore how it is determined in discourse in a formalized manner .
Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via FDA as a discourse-level construct into speech synthesis systems, in particular, concept-to-speech systems, is also briefly discussed.	focus	speech synthesis systems	usage	{'e1': {'word': 'focus', 'word_index': [(16, 16)], 'id': 'E99-1038.9'}, 'e2': {'word': 'speech synthesis systems', 'word_index': [(26, 28)], 'id': 'E99-1038.12'}, 'entity_replacement': {'16:16': 'ENTITY', '18:18': 'ENTITYUNRELATED', '21:24': 'ENTITYUNRELATED', '26:28': 'ENTITYOTHER', '33:35': 'ENTITYUNRELATED'}}	Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via FDA as a discourse - level construct into speech synthesis systems , in particular , concept-to -speech systems , is also briefly discussed .
Currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone, e.g. LFG and PATR-II.	context-free phrase-structure grammar	grammatical formalisms	usage	{'e1': {'word': 'context-free phrase-structure grammar', 'word_index': [(11, 17)], 'id': 'E87-1037.2'}, 'e2': {'word': 'grammatical formalisms', 'word_index': [(2, 3)], 'id': 'E87-1037.1'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '11:17': 'ENTITY', '23:23': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	Currently several grammatical formalisms converge towards being declarative and towards utilizing context - free phrase - structure grammar as a backbone , e.g. LFG and PATR - II .
The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing.	rule-invocation strategies	context-free chart parsing	part_whole	{'e1': {'word': 'rule-invocation strategies', 'word_index': [(16, 19)], 'id': 'E87-1037.11'}, 'e2': {'word': 'context-free chart parsing', 'word_index': [(21, 25)], 'id': 'E87-1037.12'}, 'entity_replacement': {'16:19': 'ENTITY', '21:25': 'ENTITYOTHER'}}	The aim of this paper is to provide a survey and a practical comparison of fundamental rule - invocation strategies within context - free chart parsing .
In this paper I will argue for a model of grammatical processing that is based on uniform processing and knowledge sources.	uniform processing	model of grammatical processing	usage	{'e1': {'word': 'uniform processing', 'word_index': [(16, 17)], 'id': 'E91-1043.2'}, 'e2': {'word': 'model of grammatical processing', 'word_index': [(8, 11)], 'id': 'E91-1043.1'}, 'entity_replacement': {'8:11': 'ENTITYOTHER', '16:17': 'ENTITY', '19:20': 'ENTITYUNRELATED'}}	In this paper I will argue for a model of grammatical processing that is based on uniform processing and knowledge sources .
The main feature of this model is to view parsing and generation as two strongly interleaved tasks performed by a single parametrized deduction process.	parsing	generation	compare	{'e1': {'word': 'parsing', 'word_index': [(9, 9)], 'id': 'E91-1043.5'}, 'e2': {'word': 'generation', 'word_index': [(11, 11)], 'id': 'E91-1043.6'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '9:9': 'ENTITY', '11:11': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED'}}	The main feature of this model is to view parsing and generation as two strongly interleaved tasks performed by a single parametrized deduction process .
One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity: the generation of multiple analyses for one input word, many of which are implausible.	constituent parts	words	part_whole	{'e1': {'word': 'constituent parts', 'word_index': [(14, 15)], 'id': 'E93-1023.2'}, 'e2': {'word': 'words', 'word_index': [(11, 11)], 'id': 'E93-1023.1'}, 'entity_replacement': {'11:11': 'ENTITYOTHER', '14:15': 'ENTITY', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity : the generation of multiple analyses for one input word , many of which are implausible .
One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity: the generation of multiple analyses for one input word, many of which are implausible.	analyses	input word	model-feature	{'e1': {'word': 'analyses', 'word_index': [(23, 23)], 'id': 'E93-1023.5'}, 'e2': {'word': 'input word', 'word_index': [(26, 27)], 'id': 'E93-1023.6'}, 'entity_replacement': {'11:11': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITY', '26:27': 'ENTITYOTHER'}}	One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity : the generation of multiple analyses for one input word , many of which are implausible .
In order to deal with ambiguity, the MORphological PArser MORPA is provided with a probabilistic context-free grammar (PCFG), i.e.	probabilistic context-free grammar (PCFG)	MORphological PArser MORPA	part_whole	{'e1': {'word': 'probabilistic context-free grammar (PCFG)', 'word_index': [(15, 22)], 'id': 'E93-1023.9'}, 'e2': {'word': 'MORphological PArser MORPA', 'word_index': [(8, 10)], 'id': 'E93-1023.8'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:10': 'ENTITYOTHER', '15:22': 'ENTITY'}}	In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context - free grammar ( PCFG ) , i.e.
"it combines a ""conventional"" context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse."	probability-based scoring function	"""conventional"" context-free morphological grammar"	usage	"{'e1': {'word': 'probability-based scoring function', 'word_index': [(18, 22)], 'id': 'E93-1023.12'}, 'e2': {'word': '""conventional"" context-free morphological grammar', 'word_index': [(3, 10)], 'id': 'E93-1023.10'}, 'entity_replacement': {'3:10': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '18:22': 'ENTITY', '30:30': 'ENTITYUNRELATED'}}"	"it combines a "" conventional "" context - free morphological grammar to filter out ungrammatical segmentations with a probability - based scoring function which determines the likelihood of each successful parse ."
MORPA is a fully implemented parser developed for use in a text-to-speech conversion system.	parser	text-to-speech conversion system	usage	{'e1': {'word': 'parser', 'word_index': [(5, 5)], 'id': 'E93-1023.18'}, 'e2': {'word': 'text-to-speech conversion system', 'word_index': [(11, 16)], 'id': 'E93-1023.19'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '5:5': 'ENTITY', '11:16': 'ENTITYOTHER'}}	MORPA is a fully implemented parser developed for use in a text - to -speech conversion system .
The output can be customized to meet different segmentation standards through the application of an ordered list of transformation.	segmentation standards	output	model-feature	{'e1': {'word': 'segmentation standards', 'word_index': [(8, 9)], 'id': 'I05-3022.5'}, 'e2': {'word': 'output', 'word_index': [(1, 1)], 'id': 'I05-3022.4'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '8:9': 'ENTITY'}}	The output can be customized to meet different segmentation standards through the application of an ordered list of transformation .
The system participated in all the tracks of the segmentation bakeoff -- PK-open, PK-closed, AS-open, AS-closed, HK-open, HK-closed, MSR-open and MSR- closed -- and achieved the state-of-the-art performance in MSR-open, MSR-close and PK-open tracks.	system	state-of-the-art performance	result	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'I05-3022.6'}, 'e2': {'word': 'state-of-the-art performance', 'word_index': [(47, 54)], 'id': 'I05-3022.16'}, 'entity_replacement': {'1:1': 'ENTITY', '9:10': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '16:18': 'ENTITYUNRELATED', '20:22': 'ENTITYUNRELATED', '24:26': 'ENTITYUNRELATED', '28:30': 'ENTITYUNRELATED', '32:34': 'ENTITYUNRELATED', '36:38': 'ENTITYUNRELATED', '40:42': 'ENTITYUNRELATED', '47:54': 'ENTITYOTHER', '56:58': 'ENTITYUNRELATED', '60:62': 'ENTITYUNRELATED', '64:66': 'ENTITYUNRELATED'}}	The system participated in all the tracks of the segmentation bakeoff -- PK - open , PK - closed , AS - open , AS - closed , HK - open , HK - closed , MSR - open and MSR - closed -- and achieved the state - of - the - art performance in MSR - open , MSR - close and PK - open tracks .
In this paper a morphological component with a limited capability to automatically interpret (and generate) derived words is presented.	morphological component	derived words	usage	{'e1': {'word': 'morphological component', 'word_index': [(4, 5)], 'id': 'E93-1043.1'}, 'e2': {'word': 'derived words', 'word_index': [(17, 18)], 'id': 'E93-1043.2'}, 'entity_replacement': {'4:5': 'ENTITY', '17:18': 'ENTITYOTHER'}}	In this paper a morphological component with a limited capability to automatically interpret ( and generate ) derived words is presented .
The system combines an extended two-level morphology [Trost, 1991a; Trost, 1991b] with a feature-based word grammar building on a hierarchical lexicon.	hierarchical lexicon	feature-based word grammar	usage	{'e1': {'word': 'hierarchical lexicon', 'word_index': [(30, 31)], 'id': 'E93-1043.5'}, 'e2': {'word': 'feature-based word grammar', 'word_index': [(22, 26)], 'id': 'E93-1043.4'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '22:26': 'ENTITYOTHER', '30:31': 'ENTITY'}}	The system combines an extended two - level morphology [ Trost , 1991 a ; Trost , 1991 b ] with a feature - based word grammar building on a hierarchical lexicon .
Polymorphemic stems not explicitly stored in the lexicon are given a compositional interpretation.	compositional interpretation	Polymorphemic stems	model-feature	{'e1': {'word': 'compositional interpretation', 'word_index': [(11, 12)], 'id': 'E93-1043.8'}, 'e2': {'word': 'Polymorphemic stems', 'word_index': [(0, 1)], 'id': 'E93-1043.6'}, 'entity_replacement': {'0:1': 'ENTITYOTHER', '7:7': 'ENTITYUNRELATED', '11:12': 'ENTITY'}}	Polymorphemic stems not explicitly stored in the lexicon are given a compositional interpretation .
This paper proposes an approach to full parsing suitable for Information Extraction from texts.	full parsing	Information Extraction	usage	{'e1': {'word': 'full parsing', 'word_index': [(6, 7)], 'id': 'E99-1014.1'}, 'e2': {'word': 'Information Extraction', 'word_index': [(10, 11)], 'id': 'E99-1014.2'}, 'entity_replacement': {'6:7': 'ENTITY', '10:11': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED'}}	This paper proposes an approach to full parsing suitable for Information Extraction from texts .
Sequences of cascades of rules deterministically analyze the text, building unambiguous structures.	unambiguous structures	text	model-feature	{'e1': {'word': 'unambiguous structures', 'word_index': [(11, 12)], 'id': 'E99-1014.6'}, 'e2': {'word': 'text', 'word_index': [(8, 8)], 'id': 'E99-1014.5'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '8:8': 'ENTITYOTHER', '11:12': 'ENTITY'}}	Sequences of cascades of rules deterministically analyze the text , building unambiguous structures .
It was implemented in the IE module of FACILE, a EU project for multilingual text classification and IE.	IE module	FACILE, a EU project for multilingual text classification and IE	part_whole	{'e1': {'word': 'IE module', 'word_index': [(5, 6)], 'id': 'E99-1014.13'}, 'e2': {'word': 'FACILE, a EU project for multilingual text classification and IE', 'word_index': [(8, 18)], 'id': 'E99-1014.14'}, 'entity_replacement': {'5:6': 'ENTITY', '8:18': 'ENTITYOTHER'}}	It was implemented in the IE module of FACILE , a EU project for multilingual text classification and IE .
A very simple improved duration model has reduced the error rate by about 10% in both triphone and semiphone systems.	duration model	error rate	result	{'e1': {'word': 'duration model', 'word_index': [(4, 5)], 'id': 'H91-1010.3'}, 'e2': {'word': 'error rate', 'word_index': [(9, 10)], 'id': 'H91-1010.4'}, 'entity_replacement': {'4:5': 'ENTITY', '9:10': 'ENTITYOTHER', '17:20': 'ENTITYUNRELATED'}}	A very simple improved duration model has reduced the error rate by about 10 % in both triphone and semiphone systems .
Finally, the recognizer has been modified to use bigram back-off language models.	bigram back-off language models	recognizer	usage	{'e1': {'word': 'bigram back-off language models', 'word_index': [(9, 14)], 'id': 'H91-1010.8'}, 'e2': {'word': 'recognizer', 'word_index': [(3, 3)], 'id': 'H91-1010.7'}, 'entity_replacement': {'3:3': 'ENTITYOTHER', '9:14': 'ENTITY'}}	Finally , the recognizer has been modified to use bigram back - off language models .
There are four language pairs currently supported by GLOSSER: English-Bulgarian, English-Estonian, English-Hungarian and French-Dutch.	language pairs	GLOSSER	model-feature	{'e1': {'word': 'language pairs', 'word_index': [(3, 4)], 'id': 'A97-1020.3'}, 'e2': {'word': 'GLOSSER', 'word_index': [(8, 8)], 'id': 'A97-1020.4'}, 'entity_replacement': {'3:4': 'ENTITY', '8:8': 'ENTITYOTHER', '10:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:18': 'ENTITYUNRELATED'}}	There are four language pairs currently supported by GLOSSER : English-Bulgarian , English-Estonian , English-Hungarian and French - Dutch .
A demonstration (in UNIX) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis (ICALL), including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples.	word examples	aligned bilingual corpus	part_whole	{'e1': {'word': 'word examples', 'word_index': [(42, 43)], 'id': 'A97-1020.14'}, 'e2': {'word': 'aligned bilingual corpus', 'word_index': [(38, 40)], 'id': 'A97-1020.13'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '19:27': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED', '38:40': 'ENTITYOTHER', '42:43': 'ENTITY'}}	A demonstration ( in UNIX ) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer - assisted morphological analysis ( ICALL ) , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .
This paper addresses the problem of identifying likely topics of texts by their position in the text.	topics	texts	model-feature	{'e1': {'word': 'topics', 'word_index': [(8, 8)], 'id': 'A97-1042.1'}, 'e2': {'word': 'texts', 'word_index': [(10, 10)], 'id': 'A97-1042.2'}, 'entity_replacement': {'8:8': 'ENTITY', '10:10': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	This paper addresses the problem of identifying likely topics of texts by their position in the text .
It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure.	genre-specific regularities	discourse structure	model-feature	{'e1': {'word': 'genre-specific regularities', 'word_index': [(27, 28)], 'id': 'A97-1042.7'}, 'e2': {'word': 'discourse structure', 'word_index': [(30, 31)], 'id': 'A97-1042.8'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '9:11': 'ENTITYUNRELATED', '21:24': 'ENTITYUNRELATED', '27:28': 'ENTITY', '30:31': 'ENTITYOTHER'}}	It describes the automated training and evaluation of an Optimal Position Policy , a method of locating the likely positions of topic - bearing sentences based on genre-specific regularities of discourse structure .
We make use of a conditional log-linear model, with hidden variables representing the assignment of lexical items to word clusters or word senses.	hidden variables	conditional log-linear model	model-feature	{'e1': {'word': 'hidden variables', 'word_index': [(10, 11)], 'id': 'H05-1064.4'}, 'e2': {'word': 'conditional log-linear model', 'word_index': [(5, 7)], 'id': 'H05-1064.3'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '10:11': 'ENTITY', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED'}}	We make use of a conditional log-linear model , with hidden variables representing the assignment of lexical items to word clusters or word senses .
We make use of a conditional log-linear model, with hidden variables representing the assignment of lexical items to word clusters or word senses.	word clusters	lexical items	model-feature	{'e1': {'word': 'word clusters', 'word_index': [(20, 21)], 'id': 'H05-1064.7'}, 'e2': {'word': 'lexical items', 'word_index': [(17, 18)], 'id': 'H05-1064.6'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '17:18': 'ENTITYOTHER', '20:21': 'ENTITY', '23:24': 'ENTITYUNRELATED'}}	We make use of a conditional log- linear model , with hidden variables representing the assignment of lexical items to word clusters or word senses .
The model learns to automatically make these assignments based on a discriminative training criterion.	discriminative training criterion	assignments	usage	{'e1': {'word': 'discriminative training criterion', 'word_index': [(11, 13)], 'id': 'H05-1064.10'}, 'e2': {'word': 'assignments', 'word_index': [(7, 7)], 'id': 'H05-1064.9'}, 'entity_replacement': {'7:7': 'ENTITYOTHER', '11:13': 'ENTITY'}}	The model learns to automatically make these assignments based on a discriminative training criterion .
Training and decoding with the model requires summing over an exponential number of hidden-variable assignments: the required summations can be computed efficiently and exactly using dynamic programming.	dynamic programming	decoding	usage	{'e1': {'word': 'dynamic programming', 'word_index': [(28, 29)], 'id': 'H05-1064.14'}, 'e2': {'word': 'decoding', 'word_index': [(2, 2)], 'id': 'H05-1064.12'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '2:2': 'ENTITYOTHER', '13:16': 'ENTITYUNRELATED', '28:29': 'ENTITY'}}	Training and decoding with the model requires summing over an exponential number of hidden - variable assignments : the required summations can be computed efficiently and exactly using dynamic programming .
The model gives an F-measure improvement of ~1.25% beyond the base parser, and an ~0.25% improvement beyond Collins (2000) reranker.	base parser	Collins (2000) reranker	compare	{'e1': {'word': 'base parser', 'word_index': [(12, 13)], 'id': 'H05-1064.17'}, 'e2': {'word': 'Collins (2000) reranker', 'word_index': [(22, 26)], 'id': 'H05-1064.18'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '12:13': 'ENTITY', '22:26': 'ENTITYOTHER'}}	The model gives an F-measure improvement of ~ 1.25 % beyond the base parser , and an ~ 0.25 % improvement beyond Collins ( 2000 ) reranker .
Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan.	scripts	Taiwan Child Language Corpus	part_whole	{'e1': {'word': 'scripts', 'word_index': [(5, 5)], 'id': 'I05-4008.2'}, 'e2': {'word': 'Taiwan Child Language Corpus', 'word_index': [(0, 3)], 'id': 'I05-4008.1'}, 'entity_replacement': {'0:3': 'ENTITYOTHER', '5:5': 'ENTITY', '12:12': 'ENTITYUNRELATED', '18:20': 'ENTITYUNRELATED'}}	Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan .
Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan.	Southern Min Chinese	recordings	model-feature	{'e1': {'word': 'Southern Min Chinese', 'word_index': [(18, 20)], 'id': 'I05-4008.4'}, 'e2': {'word': 'recordings', 'word_index': [(12, 12)], 'id': 'I05-4008.3'}, 'entity_replacement': {'0:3': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '12:12': 'ENTITYOTHER', '18:20': 'ENTITY'}}	Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan .
The format of the corpus adopts the Child Language Data Exchange System (CHILDES).	Child Language Data Exchange System (CHILDES)	corpus	model-feature	{'e1': {'word': 'Child Language Data Exchange System (CHILDES)', 'word_index': [(7, 14)], 'id': 'I05-4008.6'}, 'e2': {'word': 'corpus', 'word_index': [(4, 4)], 'id': 'I05-4008.5'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '7:14': 'ENTITY'}}	The format of the corpus adopts the Child Language Data Exchange System ( CHILDES ) .
The size of the corpus is about 1.6 million words.	words	corpus	part_whole	{'e1': {'word': 'words', 'word_index': [(9, 9)], 'id': 'I05-4008.8'}, 'e2': {'word': 'corpus', 'word_index': [(4, 4)], 'id': 'I05-4008.7'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '9:9': 'ENTITY'}}	The size of the corpus is about 1.6 million words .
In this paper, we describe data collection, transcription, word segmentation, and part-of-speech annotation of this corpus.	part-of-speech annotation	corpus	usage	{'e1': {'word': 'part-of-speech annotation', 'word_index': [(15, 19)], 'id': 'I05-4008.12'}, 'e2': {'word': 'corpus', 'word_index': [(22, 22)], 'id': 'I05-4008.13'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '15:19': 'ENTITY', '22:22': 'ENTITYOTHER'}}	In this paper , we describe data collection , transcription , word segmentation , and part -of - speech annotation of this corpus .
Robust natural language interpretation requires strong semantic domain models, fail-soft recovery heuristics, and very flexible control structures.	semantic domain models	natural language interpretation	usage	{'e1': {'word': 'semantic domain models', 'word_index': [(6, 8)], 'id': 'P81-1032.2'}, 'e2': {'word': 'natural language interpretation', 'word_index': [(1, 3)], 'id': 'P81-1032.1'}, 'entity_replacement': {'1:3': 'ENTITYOTHER', '6:8': 'ENTITY', '10:14': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED'}}	Robust natural language interpretation requires strong semantic domain models , fail - soft recovery heuristics , and very flexible control structures .
Although single-strategy parsers have met with a measure of success, a multi-strategy approach is shown to provide a much higher degree of flexibility, redundancy, and ability to bring task-specific domain knowledge (in addition to general linguistic knowledge) to bear on both grammatical and ungrammatical input.	single-strategy parsers	multi-strategy approach	compare	{'e1': {'word': 'single-strategy parsers', 'word_index': [(1, 2)], 'id': 'P81-1032.5'}, 'e2': {'word': 'multi-strategy approach', 'word_index': [(12, 13)], 'id': 'P81-1032.6'}, 'entity_replacement': {'1:2': 'ENTITY', '12:13': 'ENTITYOTHER', '31:35': 'ENTITYUNRELATED', '40:42': 'ENTITYUNRELATED', '48:51': 'ENTITYUNRELATED'}}	Although single-strategy parsers have met with a measure of success , a multi-strategy approach is shown to provide a much higher degree of flexibility , redundancy , and ability to bring task - specific domain knowledge ( in addition to general linguistic knowledge ) to bear on both grammatical and ungrammatical input .
Although single-strategy parsers have met with a measure of success, a multi-strategy approach is shown to provide a much higher degree of flexibility, redundancy, and ability to bring task-specific domain knowledge (in addition to general linguistic knowledge) to bear on both grammatical and ungrammatical input.	task-specific domain knowledge	general linguistic knowledge	compare	{'e1': {'word': 'task-specific domain knowledge', 'word_index': [(31, 35)], 'id': 'P81-1032.7'}, 'e2': {'word': 'general linguistic knowledge', 'word_index': [(40, 42)], 'id': 'P81-1032.8'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '31:35': 'ENTITY', '40:42': 'ENTITYOTHER', '48:51': 'ENTITYUNRELATED'}}	Although single-strategy parsers have met with a measure of success , a multi-strategy approach is shown to provide a much higher degree of flexibility , redundancy , and ability to bring task - specific domain knowledge ( in addition to general linguistic knowledge ) to bear on both grammatical and ungrammatical input .
A parsing algorithm is presented that integrates several different parsing strategies, with case-frame instantiation dominating.	parsing strategies	parsing algorithm	usage	{'e1': {'word': 'parsing strategies', 'word_index': [(9, 10)], 'id': 'P81-1032.11'}, 'e2': {'word': 'parsing algorithm', 'word_index': [(1, 2)], 'id': 'P81-1032.10'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '9:10': 'ENTITY', '13:16': 'ENTITYUNRELATED'}}	A parsing algorithm is presented that integrates several different parsing strategies , with case - frame instantiation dominating .
Each of these parsing strategies exploits different types of knowledge; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and ungrammatical structures, as well as less exotic, grammatically correct input.	types of knowledge	parsing strategies	usage	{'e1': {'word': 'types of knowledge', 'word_index': [(7, 9)], 'id': 'P81-1032.14'}, 'e2': {'word': 'parsing strategies', 'word_index': [(3, 4)], 'id': 'P81-1032.13'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '7:9': 'ENTITY', '22:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '37:39': 'ENTITYUNRELATED'}}	Each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and ungrammatical structures , as well as less exotic , grammatically correct input .
Each of these parsing strategies exploits different types of knowledge; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and ungrammatical structures, as well as less exotic, grammatically correct input.	ungrammatical structures	grammatically correct input	compare	{'e1': {'word': 'ungrammatical structures', 'word_index': [(28, 29)], 'id': 'P81-1032.17'}, 'e2': {'word': 'grammatically correct input', 'word_index': [(37, 39)], 'id': 'P81-1032.18'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:9': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '28:29': 'ENTITY', '37:39': 'ENTITYOTHER'}}	Each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and ungrammatical structures , as well as less exotic , grammatically correct input .
Several specific heuristics for handling ungrammatical input are presented within this multi-strategy framework.	specific heuristics	multi-strategy framework	part_whole	{'e1': {'word': 'specific heuristics', 'word_index': [(1, 2)], 'id': 'P81-1032.19'}, 'e2': {'word': 'multi-strategy framework', 'word_index': [(11, 12)], 'id': 'P81-1032.21'}, 'entity_replacement': {'1:2': 'ENTITY', '5:6': 'ENTITYUNRELATED', '11:12': 'ENTITYOTHER'}}	Several specific heuristics for handling ungrammatical input are presented within this multi-strategy framework .
By generalizing the notion of location of a constituent to allow discontinuous locations, one can describe the discontinuous constituents of non-configurational languages.	discontinuous constituents	non-configurational languages	part_whole	{'e1': {'word': 'discontinuous constituents', 'word_index': [(18, 19)], 'id': 'P85-1015.3'}, 'e2': {'word': 'non-configurational languages', 'word_index': [(21, 23)], 'id': 'P85-1015.4'}, 'entity_replacement': {'5:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '18:19': 'ENTITY', '21:23': 'ENTITYOTHER'}}	By generalizing the notion of location of a constituent to allow discontinuous locations , one can describe the discontinuous constituents of non- configurational languages .
These discontinuous constituents can be described by a variant of definite clause grammars, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages.	definite clause grammars	discontinuous constituents	model-feature	{'e1': {'word': 'definite clause grammars', 'word_index': [(10, 12)], 'id': 'P85-1015.6'}, 'e2': {'word': 'discontinuous constituents', 'word_index': [(1, 2)], 'id': 'P85-1015.5'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '10:12': 'ENTITY', '16:16': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '29:32': 'ENTITYUNRELATED'}}	These discontinuous constituents can be described by a variant of definite clause grammars , and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages .
These discontinuous constituents can be described by a variant of definite clause grammars, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages.	grammars	parser for non-configurational languages	usage	{'e1': {'word': 'grammars', 'word_index': [(16, 16)], 'id': 'P85-1015.7'}, 'e2': {'word': 'parser for non-configurational languages', 'word_index': [(29, 32)], 'id': 'P85-1015.9'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '16:16': 'ENTITY', '24:25': 'ENTITYUNRELATED', '29:32': 'ENTITYOTHER'}}	These discontinuous constituents can be described by a variant of definite clause grammars , and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages .
 A system is described for acquiring a context-sensitive, phrase structure grammar which is applied by a best-path, bottom-up, deterministic parser.	context-sensitive, phrase structure grammar	best-path, bottom-up, deterministic parser	usage	{'e1': {'word': 'context-sensitive, phrase structure grammar', 'word_index': [(7, 12)], 'id': 'P91-1016.1'}, 'e2': {'word': 'best-path, bottom-up, deterministic parser', 'word_index': [(18, 27)], 'id': 'P91-1016.2'}, 'entity_replacement': {'7:12': 'ENTITY', '18:27': 'ENTITYOTHER'}}	A system is described for acquiring a context -sensitive , phrase structure grammar which is applied by a best - path , bottom - up , deterministic parser .
Overall, this research concludes that CSG is a computationally and conceptually tractable approach to the construction of phrase structure grammar for news story text.	CSG	phrase structure grammar	usage	{'e1': {'word': 'CSG', 'word_index': [(6, 6)], 'id': 'P91-1016.6'}, 'e2': {'word': 'phrase structure grammar', 'word_index': [(18, 20)], 'id': 'P91-1016.7'}, 'entity_replacement': {'6:6': 'ENTITY', '18:20': 'ENTITYOTHER', '22:24': 'ENTITYUNRELATED'}}	Overall , this research concludes that CSG is a computationally and conceptually tractable approach to the construction of phrase structure grammar for news story text .
We present a corpus-based study of methods that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjectives.	semantically unmarked term	antonymous adjectives	part_whole	{'e1': {'word': 'semantically unmarked term', 'word_index': [(20, 22)], 'id': 'P95-1027.3'}, 'e2': {'word': 'antonymous adjectives', 'word_index': [(28, 29)], 'id': 'P95-1027.4'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '20:22': 'ENTITY', '28:29': 'ENTITYOTHER'}}	We present a corpus - based study of methods that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjectives .
In the paper we propose a black-box method for comparing the lexical coverage of MT systems.	lexical coverage	MT systems	model-feature	{'e1': {'word': 'lexical coverage', 'word_index': [(13, 14)], 'id': 'P97-1015.4'}, 'e2': {'word': 'MT systems', 'word_index': [(16, 17)], 'id': 'P97-1015.5'}, 'entity_replacement': {'6:9': 'ENTITYUNRELATED', '13:14': 'ENTITY', '16:17': 'ENTITYOTHER'}}	In the paper we propose a black - box method for comparing the lexical coverage of MT systems .
The method is based on lists of words from different frequency classes.	frequency classes	words	model-feature	{'e1': {'word': 'frequency classes', 'word_index': [(10, 11)], 'id': 'P97-1015.7'}, 'e2': {'word': 'words', 'word_index': [(7, 7)], 'id': 'P97-1015.6'}, 'entity_replacement': {'7:7': 'ENTITYOTHER', '10:11': 'ENTITY'}}	The method is based on lists of words from different frequency classes .
We describe a method for interpreting abstract flat syntactic representations, LFG f-structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs).	underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs)	abstract flat syntactic representations, LFG f-structures	model-feature	{'e1': {'word': 'underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs)', 'word_index': [(15, 27)], 'id': 'P97-1052.2'}, 'e2': {'word': 'abstract flat syntactic representations, LFG f-structures', 'word_index': [(6, 12)], 'id': 'P97-1052.1'}, 'entity_replacement': {'6:12': 'ENTITYOTHER', '15:27': 'ENTITY'}}	We describe a method for interpreting abstract flat syntactic representations , LFG f-structures , as underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRS s ) .
It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f-structures through the translation images of f-structures as UDRSs.	model theoretic interpretation	f-structures	model-feature	{'e1': {'word': 'model theoretic interpretation', 'word_index': [(3, 5)], 'id': 'P97-1052.6'}, 'e2': {'word': 'f-structures', 'word_index': [(17, 17)], 'id': 'P97-1052.9'}, 'entity_replacement': {'3:5': 'ENTITY', '8:9': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '17:17': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f-structures through the translation images of f-structures as UDRSs .
It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f-structures through the translation images of f-structures as UDRSs.	translation images	f-structures	model-feature	{'e1': {'word': 'translation images', 'word_index': [(20, 21)], 'id': 'P97-1052.10'}, 'e2': {'word': 'f-structures', 'word_index': [(23, 23)], 'id': 'P97-1052.11'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:21': 'ENTITY', '23:23': 'ENTITYOTHER', '25:25': 'ENTITYUNRELATED'}}	It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f-structures through the translation images of f-structures as UDRSs .
In this paper we describe a systematic approach for creating a dialog management system based on a Construct Algebra, a collection of relations and operations on a task representation.	Construct Algebra	dialog management system	usage	{'e1': {'word': 'Construct Algebra', 'word_index': [(17, 18)], 'id': 'P99-1025.2'}, 'e2': {'word': 'dialog management system', 'word_index': [(11, 13)], 'id': 'P99-1025.1'}, 'entity_replacement': {'11:13': 'ENTITYOTHER', '17:18': 'ENTITY', '21:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED'}}	In this paper we describe a systematic approach for creating a dialog management system based on a Construct Algebra , a collection of relations and operations on a task representation .
In this paper we describe a systematic approach for creating a dialog management system based on a Construct Algebra, a collection of relations and operations on a task representation.	collection of relations and operations	task representation	model-feature	{'e1': {'word': 'collection of relations and operations', 'word_index': [(21, 25)], 'id': 'P99-1025.3'}, 'e2': {'word': 'task representation', 'word_index': [(28, 29)], 'id': 'P99-1025.4'}, 'entity_replacement': {'11:13': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '21:25': 'ENTITY', '28:29': 'ENTITYOTHER'}}	In this paper we describe a systematic approach for creating a dialog management system based on a Construct Algebra , a collection of relations and operations on a task representation .
These relations and operations are analytical components for building higher level abstractions called dialog motivators.	analytical components	dialog motivators	part_whole	{'e1': {'word': 'analytical components', 'word_index': [(5, 6)], 'id': 'P99-1025.6'}, 'e2': {'word': 'dialog motivators', 'word_index': [(13, 14)], 'id': 'P99-1025.7'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '5:6': 'ENTITY', '13:14': 'ENTITYOTHER'}}	These relations and operations are analytical components for building higher level abstractions called dialog motivators .
The dialog manager, consisting of a collection of dialog motivators, is entirely built using the Construct Algebra.	collection of dialog motivators	dialog manager	part_whole	{'e1': {'word': 'collection of dialog motivators', 'word_index': [(7, 10)], 'id': 'P99-1025.9'}, 'e2': {'word': 'dialog manager', 'word_index': [(1, 2)], 'id': 'P99-1025.8'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '7:10': 'ENTITY', '17:18': 'ENTITYUNRELATED'}}	The dialog manager , consisting of a collection of dialog motivators , is entirely built using the Construct Algebra .
STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World Wide Web.	language-independent system	automatic discovery of text	usage	{'e1': {'word': 'language-independent system', 'word_index': [(8, 11)], 'id': 'P99-1068.2'}, 'e2': {'word': 'automatic discovery of text', 'word_index': [(13, 16)], 'id': 'P99-1068.3'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '8:11': 'ENTITY', '13:16': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	STRAND ( Resnik , 1998 ) is a language - independent system for automatic discovery of text in parallel translation on the World Wide Web .
This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance.	automatic language identification	STRAND	part_whole	{'e1': {'word': 'automatic language identification', 'word_index': [(9, 11)], 'id': 'P99-1068.6'}, 'e2': {'word': 'STRAND', 'word_index': [(5, 5)], 'id': 'P99-1068.5'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '9:11': 'ENTITY'}}	This paper extends the preliminary STRAND results by adding automatic language identification , scaling up by orders of magnitude , and formally evaluating performance .
The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language.	English-French document pairs	automatically acquired parallel corpus	part_whole	{'e1': {'word': 'English-French document pairs', 'word_index': [(14, 18)], 'id': 'P99-1068.8'}, 'e2': {'word': 'automatically acquired parallel corpus', 'word_index': [(8, 11)], 'id': 'P99-1068.7'}, 'entity_replacement': {'8:11': 'ENTITYOTHER', '14:18': 'ENTITY', '23:23': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED'}}	The most recent end - product is an automatically acquired parallel corpus comprising 2491 English - French document pairs , approximately 1.5 million words per language .
The main of this project is computer-assisted acquisition and morpho-syntactic description of verb-noun collocations in Polish.	computer-assisted acquisition and morpho-syntactic description of verb-noun collocations	Polish	usage	{'e1': {'word': 'computer-assisted acquisition and morpho-syntactic description of verb-noun collocations', 'word_index': [(6, 15)], 'id': 'L08-1260.2'}, 'e2': {'word': 'Polish', 'word_index': [(17, 17)], 'id': 'L08-1260.3'}, 'entity_replacement': {'6:15': 'ENTITY', '17:17': 'ENTITYOTHER'}}	The main of this project is computer - assisted acquisition and morpho-syntactic description of verb-noun collocations in Polish .
The presented here corpus-based approach permitted us to triple the size the verb-noun collocation dictionary for Polish.	corpus-based approach	verb-noun collocation dictionary for Polish	usage	{'e1': {'word': 'corpus-based approach', 'word_index': [(3, 6)], 'id': 'L08-1260.9'}, 'e2': {'word': 'verb-noun collocation dictionary for Polish', 'word_index': [(14, 18)], 'id': 'L08-1260.10'}, 'entity_replacement': {'3:6': 'ENTITY', '14:18': 'ENTITYOTHER'}}	The presented here corpus - based approach permitted us to triple the size the verb-noun collocation dictionary for Polish .
In this paper we deal with a recently developed large Czech MWE database containing at the moment 160 000 MWEs (treated as lexical units).	MWEs	large Czech MWE database	part_whole	{'e1': {'word': 'MWEs', 'word_index': [(19, 19)], 'id': 'L08-1540.2'}, 'e2': {'word': 'large Czech MWE database', 'word_index': [(9, 12)], 'id': 'L08-1540.1'}, 'entity_replacement': {'9:12': 'ENTITYOTHER', '19:19': 'ENTITY', '23:24': 'ENTITYUNRELATED'}}	In this paper we deal with a recently developed large Czech MWE database containing at the moment 160 000 MWEs ( treated as lexical units ) .
It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others.	proper names	databases	part_whole	{'e1': {'word': 'proper names', 'word_index': [(15, 16)], 'id': 'L08-1540.7'}, 'e2': {'word': 'databases', 'word_index': [(13, 13)], 'id': 'L08-1540.6'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:13': 'ENTITYOTHER', '15:16': 'ENTITY', '18:18': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '28:31': 'ENTITYUNRELATED'}}	It was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , collocations obtained from Czech WordNet , lists of botanical and zoological terms and others .
It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others.	collocations	Czech WordNet	part_whole	{'e1': {'word': 'collocations', 'word_index': [(20, 20)], 'id': 'L08-1540.9'}, 'e2': {'word': 'Czech WordNet', 'word_index': [(23, 24)], 'id': 'L08-1540.10'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '20:20': 'ENTITY', '23:24': 'ENTITYOTHER', '28:31': 'ENTITYUNRELATED'}}	It was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , collocations obtained from Czech WordNet , lists of botanical and zoological terms and others .
We compare the built MWEs database with the corpus data from Czech National Corpus (approx.	MWEs database	corpus data	compare	{'e1': {'word': 'MWEs database', 'word_index': [(4, 5)], 'id': 'L08-1540.14'}, 'e2': {'word': 'corpus data', 'word_index': [(8, 9)], 'id': 'L08-1540.15'}, 'entity_replacement': {'4:5': 'ENTITY', '8:9': 'ENTITYOTHER', '11:13': 'ENTITYUNRELATED'}}	We compare the built MWEs database with the corpus data from Czech National Corpus ( approx .
To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs.	statistical parameters	Word Sketch Engine	model-feature	{'e1': {'word': 'statistical parameters', 'word_index': [(26, 27)], 'id': 'L08-1540.21'}, 'e2': {'word': 'Word Sketch Engine', 'word_index': [(16, 18)], 'id': 'L08-1540.20'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '16:18': 'ENTITYOTHER', '26:27': 'ENTITY', '32:32': 'ENTITYUNRELATED', '41:41': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED'}}	To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine , which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs .
To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs.	salience	MWEs	model-feature	{'e1': {'word': 'salience', 'word_index': [(41, 41)], 'id': 'L08-1540.23'}, 'e2': {'word': 'MWEs', 'word_index': [(45, 45)], 'id': 'L08-1540.24'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '16:18': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '41:41': 'ENTITY', '45:45': 'ENTITYOTHER'}}	To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine , which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs .
We also discuss exploitation of the database for working out a more adequate tagging and lemmatization.	database	tagging	usage	{'e1': {'word': 'database', 'word_index': [(6, 6)], 'id': 'L08-1540.25'}, 'e2': {'word': 'tagging', 'word_index': [(13, 13)], 'id': 'L08-1540.26'}, 'entity_replacement': {'6:6': 'ENTITY', '13:13': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED'}}	We also discuss exploitation of the database for working out a more adequate tagging and lemmatization .
The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units, i. e. to make tagging and lemmatization more adequate.	MWEs	corpus text	part_whole	{'e1': {'word': 'MWEs', 'word_index': [(9, 9)], 'id': 'L08-1540.28'}, 'e2': {'word': 'corpus text', 'word_index': [(11, 12)], 'id': 'L08-1540.29'}, 'entity_replacement': {'9:9': 'ENTITY', '11:12': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units , i. e. to make tagging and lemmatization more adequate .
We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses.	statistical techniques	translations	usage	{'e1': {'word': 'statistical techniques', 'word_index': [(8, 9)], 'id': 'L08-1110.1'}, 'e2': {'word': 'translations', 'word_index': [(16, 16)], 'id': 'L08-1110.2'}, 'entity_replacement': {'8:9': 'ENTITY', '16:16': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED'}}	We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses .
We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses.	translation hypotheses	graph	part_whole	{'e1': {'word': 'translation hypotheses', 'word_index': [(21, 22)], 'id': 'L08-1110.4'}, 'e2': {'word': 'graph', 'word_index': [(19, 19)], 'id': 'L08-1110.3'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '19:19': 'ENTITYOTHER', '21:22': 'ENTITY'}}	We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses .
In a previous paper (Carl, 2007) we have described how the hypotheses graph is generated through shallow mapping and permutation rules.	shallow mapping	hypotheses graph	usage	{'e1': {'word': 'shallow mapping', 'word_index': [(19, 20)], 'id': 'L08-1110.6'}, 'e2': {'word': 'hypotheses graph', 'word_index': [(14, 15)], 'id': 'L08-1110.5'}, 'entity_replacement': {'14:15': 'ENTITYOTHER', '19:20': 'ENTITY', '22:23': 'ENTITYUNRELATED'}}	In a previous paper ( Carl , 2007 ) we have described how the hypotheses graph is generated through shallow mapping and permutation rules .
We have given examples of its nodes consisting of vectors representing morpho-syntactic properties of words and phrases.	vectors representing morpho-syntactic properties	nodes	part_whole	{'e1': {'word': 'vectors representing morpho-syntactic properties', 'word_index': [(9, 12)], 'id': 'L08-1110.9'}, 'e2': {'word': 'nodes', 'word_index': [(6, 6)], 'id': 'L08-1110.8'}, 'entity_replacement': {'6:6': 'ENTITYOTHER', '9:12': 'ENTITY', '14:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED'}}	We have given examples of its nodes consisting of vectors representing morpho-syntactic properties of words and phrases .
The feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best M translation paths in the graph.	log-linear combination	translation paths	usage	{'e1': {'word': 'log-linear combination', 'word_index': [(13, 14)], 'id': 'L08-1110.16'}, 'e2': {'word': 'translation paths', 'word_index': [(23, 24)], 'id': 'L08-1110.17'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:14': 'ENTITY', '23:24': 'ENTITYOTHER', '27:27': 'ENTITYUNRELATED'}}	The feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best M translation paths in the graph .
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	CMU	SRI toolkit	compare	{'e1': {'word': 'CMU', 'word_index': [(8, 8)], 'id': 'L08-1110.20'}, 'e2': {'word': 'SRI toolkit', 'word_index': [(11, 12)], 'id': 'L08-1110.21'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '8:8': 'ENTITY', '11:12': 'ENTITYOTHER', '21:27': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED', '41:45': 'ENTITYUNRELATED', '48:51': 'ENTITYUNRELATED', '58:58': 'ENTITYUNRELATED', '60:61': 'ENTITYUNRELATED', '66:67': 'ENTITYUNRELATED', '72:72': 'ENTITYUNRELATED'}}	We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 ) word - lemma based feature function models produce better results than token - based models , 2 ) adding a PoS - tag feature function to the word - lemma model improves the output and 3 ) weights for lexical translations are suitable if the training material is similar to the texts to be translated . < / abstract >
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	word-lemma based feature function models	token-based models	compare	{'e1': {'word': 'word-lemma based feature function models', 'word_index': [(21, 27)], 'id': 'L08-1110.22'}, 'e2': {'word': 'token-based models', 'word_index': [(32, 35)], 'id': 'L08-1110.23'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '21:27': 'ENTITY', '32:35': 'ENTITYOTHER', '41:45': 'ENTITYUNRELATED', '48:51': 'ENTITYUNRELATED', '58:58': 'ENTITYUNRELATED', '60:61': 'ENTITYUNRELATED', '66:67': 'ENTITYUNRELATED', '72:72': 'ENTITYUNRELATED'}}	We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 ) word - lemma based feature function models produce better results than token - based models , 2 ) adding a PoS - tag feature function to the word - lemma model improves the output and 3 ) weights for lexical translations are suitable if the training material is similar to the texts to be translated . < / abstract >
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	PoS-tag feature function	word-lemma model	part_whole	{'e1': {'word': 'PoS-tag feature function', 'word_index': [(41, 44)], 'id': 'L08-1110.24'}, 'e2': {'word': 'word-lemma model', 'word_index': [(47, 50)], 'id': 'L08-1110.25'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '21:27': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED', '41:44': 'ENTITY', '47:50': 'ENTITYOTHER', '57:57': 'ENTITYUNRELATED', '59:60': 'ENTITYUNRELATED', '65:66': 'ENTITYUNRELATED', '71:71': 'ENTITYUNRELATED'}}	We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 ) word - lemma based feature function models produce better results than token - based models , 2 ) adding a PoS -tag feature function to the word - lemma model improves the output and 3 ) weights for lexical translations are suitable if the training material is similar to the texts to be translated . < / abstract >
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	weights	lexical translations	model-feature	{'e1': {'word': 'weights', 'word_index': [(58, 58)], 'id': 'L08-1110.26'}, 'e2': {'word': 'lexical translations', 'word_index': [(60, 61)], 'id': 'L08-1110.27'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '21:27': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED', '41:45': 'ENTITYUNRELATED', '48:51': 'ENTITYUNRELATED', '58:58': 'ENTITY', '60:61': 'ENTITYOTHER', '66:67': 'ENTITYUNRELATED', '72:72': 'ENTITYUNRELATED'}}	We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 ) word - lemma based feature function models produce better results than token - based models , 2 ) adding a PoS - tag feature function to the word - lemma model improves the output and 3 ) weights for lexical translations are suitable if the training material is similar to the texts to be translated . < / abstract >
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	training material	texts	compare	{'e1': {'word': 'training material', 'word_index': [(66, 67)], 'id': 'L08-1110.28'}, 'e2': {'word': 'texts', 'word_index': [(72, 72)], 'id': 'L08-1110.29'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '21:27': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED', '41:45': 'ENTITYUNRELATED', '48:51': 'ENTITYUNRELATED', '58:58': 'ENTITYUNRELATED', '60:61': 'ENTITYUNRELATED', '66:67': 'ENTITY', '72:72': 'ENTITYOTHER'}}	We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 ) word - lemma based feature function models produce better results than token - based models , 2 ) adding a PoS - tag feature function to the word - lemma model improves the output and 3 ) weights for lexical translations are suitable if the training material is similar to the texts to be translated . < / abstract >
Existing techniques extract term candidates by looking for internal and contextual information associated with domain specific terms.	internal and contextual information	domain specific terms	model-feature	{'e1': {'word': 'internal and contextual information', 'word_index': [(8, 11)], 'id': 'L08-1154.2'}, 'e2': {'word': 'domain specific terms', 'word_index': [(14, 16)], 'id': 'L08-1154.3'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '8:11': 'ENTITY', '14:16': 'ENTITYOTHER'}}	Existing techniques extract term candidates by looking for internal and contextual information associated with domain specific terms .
The algorithms always face the dilemma that fewer features are not enough to distinguish terms from non-terms whereas more features lead to more conflicts among selected features.	features	terms	model-feature	{'e1': {'word': 'features', 'word_index': [(8, 8)], 'id': 'L08-1154.4'}, 'e2': {'word': 'terms', 'word_index': [(14, 14)], 'id': 'L08-1154.5'}, 'entity_replacement': {'8:8': 'ENTITY', '14:14': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	The algorithms always face the dilemma that fewer features are not enough to distinguish terms from non- terms whereas more features lead to more conflicts among selected features .
This paper presents a novel approach for term extraction based on delimiters which are much more stable and domain independent.	delimiters	term extraction	usage	{'e1': {'word': 'delimiters', 'word_index': [(11, 11)], 'id': 'L08-1154.10'}, 'e2': {'word': 'term extraction', 'word_index': [(7, 8)], 'id': 'L08-1154.9'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '11:11': 'ENTITY'}}	This paper presents a novel approach for term extraction based on delimiters which are much more stable and domain independent .
The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the discourse relations in Czech.	discourse relations	Czech	part_whole	{'e1': {'word': 'discourse relations', 'word_index': [(17, 18)], 'id': 'L08-1050.2'}, 'e2': {'word': 'Czech', 'word_index': [(20, 20)], 'id': 'L08-1050.3'}, 'entity_replacement': {'11:14': 'ENTITYUNRELATED', '17:18': 'ENTITY', '20:20': 'ENTITYOTHER'}}	The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the discourse relations in Czech .
We primarily focus on the description of the syntactically motivated relations in discourse, basing our findings on the theoretical background of the Prague Dependency Treebank 2.0 and the Penn Discourse Treebank 2.	syntactically motivated relations	discourse	part_whole	{'e1': {'word': 'syntactically motivated relations', 'word_index': [(8, 10)], 'id': 'L08-1050.4'}, 'e2': {'word': 'discourse', 'word_index': [(12, 12)], 'id': 'L08-1050.5'}, 'entity_replacement': {'8:10': 'ENTITY', '12:12': 'ENTITYOTHER', '23:26': 'ENTITYUNRELATED', '29:32': 'ENTITYUNRELATED'}}	We primarily focus on the description of the syntactically motivated relations in discourse , basing our findings on the theoretical background of the Prague Dependency Treebank 2.0 and the Penn Discourse Treebank 2 .
Our aim is to revisit the present-day syntactico-semantic (tectogrammatical) annotation in the Prague Dependency Treebank, extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new, discourse level of annotation.	syntactico-semantic (tectogrammatical) annotation	Prague Dependency Treebank	part_whole	{'e1': {'word': 'syntactico-semantic (tectogrammatical) annotation', 'word_index': [(9, 13)], 'id': 'L08-1050.8'}, 'e2': {'word': 'Prague Dependency Treebank', 'word_index': [(16, 18)], 'id': 'L08-1050.9'}, 'entity_replacement': {'9:13': 'ENTITY', '16:18': 'ENTITYOTHER', '27:32': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED'}}	Our aim is to revisit the present - day syntactico-semantic ( tectogrammatical ) annotation in the Prague Dependency Treebank , extend it for the purposes of a sentence - boundary - crossing representation and eventually to design a new , discourse level of annotation .
Our aim is to revisit the present-day syntactico-semantic (tectogrammatical) annotation in the Prague Dependency Treebank, extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new, discourse level of annotation.	discourse level	annotation	model-feature	{'e1': {'word': 'discourse level', 'word_index': [(40, 41)], 'id': 'L08-1050.11'}, 'e2': {'word': 'annotation', 'word_index': [(43, 43)], 'id': 'L08-1050.12'}, 'entity_replacement': {'9:13': 'ENTITYUNRELATED', '16:18': 'ENTITYUNRELATED', '27:32': 'ENTITYUNRELATED', '40:41': 'ENTITY', '43:43': 'ENTITYOTHER'}}	Our aim is to revisit the present - day syntactico-semantic ( tectogrammatical ) annotation in the Prague Dependency Treebank , extend it for the purposes of a sentence - boundary - crossing representation and eventually to design a new , discourse level of annotation .
In this paper, we propose a feasible process of such a transfer, comparing the possibilities the Praguian dependency-based approach offers with the Penn discourse annotation based primarily on the analysis and classification of discourse connectives.	Praguian dependency-based approach	Penn discourse annotation	compare	{'e1': {'word': 'Praguian dependency-based approach', 'word_index': [(18, 22)], 'id': 'L08-1050.13'}, 'e2': {'word': 'Penn discourse annotation', 'word_index': [(26, 28)], 'id': 'L08-1050.14'}, 'entity_replacement': {'18:22': 'ENTITY', '26:28': 'ENTITYOTHER', '37:38': 'ENTITYUNRELATED'}}	In this paper , we propose a feasible process of such a transfer , comparing the possibilities the Praguian dependency - based approach offers with the Penn discourse annotation based primarily on the analysis and classification of discourse connectives .
In this paper, we reported experiments of unsupervised automatic acquisition of Italian and English verb subcategorization frames (SCFs) from general and domain corpora.	general and domain corpora	unsupervised automatic acquisition	usage	{'e1': {'word': 'general and domain corpora', 'word_index': [(22, 25)], 'id': 'L08-1097.3'}, 'e2': {'word': 'unsupervised automatic acquisition', 'word_index': [(8, 10)], 'id': 'L08-1097.1'}, 'entity_replacement': {'8:10': 'ENTITYOTHER', '12:20': 'ENTITYUNRELATED', '22:25': 'ENTITY'}}	In this paper , we reported experiments of unsupervised automatic acquisition of Italian and English verb subcategorization frames ( SCFs ) from general and domain corpora .
The proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about SCFs.	lexico-syntactic knowledge	SCFs	model-feature	{'e1': {'word': 'lexico-syntactic knowledge', 'word_index': [(25, 26)], 'id': 'L08-1097.6'}, 'e2': {'word': 'SCFs', 'word_index': [(28, 28)], 'id': 'L08-1097.7'}, 'entity_replacement': {'5:9': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '25:26': 'ENTITY', '28:28': 'ENTITYOTHER'}}	The proposed technique operates on syntactically shallow - parsed corpora on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about SCFs .
The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle (MDL).	SCFs distributions	verbs	model-feature	{'e1': {'word': 'SCFs distributions', 'word_index': [(7, 8)], 'id': 'L08-1097.10'}, 'e2': {'word': 'verbs', 'word_index': [(4, 4)], 'id': 'L08-1097.9'}, 'entity_replacement': {'4:4': 'ENTITYOTHER', '7:8': 'ENTITY', '12:14': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '32:38': 'ENTITYUNRELATED'}}	The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle ( MDL ) .
The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle (MDL).	frames	verbs	model-feature	{'e1': {'word': 'frames', 'word_index': [(25, 25)], 'id': 'L08-1097.13'}, 'e2': {'word': 'verbs', 'word_index': [(22, 22)], 'id': 'L08-1097.12'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '12:14': 'ENTITYUNRELATED', '22:22': 'ENTITYOTHER', '25:25': 'ENTITY', '29:29': 'ENTITYUNRELATED', '32:38': 'ENTITYUNRELATED'}}	The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle ( MDL ) .
The translation of English text into American Sign Language (ASL) animation tests the limits of traditional MT architectural designs.	traditional MT architectural designs	translation	usage	{'e1': {'word': 'traditional MT architectural designs', 'word_index': [(17, 20)], 'id': 'N04-2005.4'}, 'e2': {'word': 'translation', 'word_index': [(1, 1)], 'id': 'N04-2005.1'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '3:4': 'ENTITYUNRELATED', '6:12': 'ENTITYUNRELATED', '17:20': 'ENTITY'}}	The translation of English text into American Sign Language ( ASL ) animation tests the limits of traditional MT architectural designs .
"A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called ""classifier predicates."""	semantic representation	virtual reality 3D scene modeling software	usage	{'e1': {'word': 'semantic representation', 'word_index': [(2, 3)], 'id': 'N04-2005.5'}, 'e2': {'word': 'virtual reality 3D scene modeling software', 'word_index': [(8, 13)], 'id': 'N04-2005.6'}, 'entity_replacement': {'2:3': 'ENTITY', '8:13': 'ENTITYOTHER', '16:19': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED'}}	"A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called "" classifier predicates . """
The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system.	interlingua	multi-pathway MT architecture design	part_whole	{'e1': {'word': 'interlingua', 'word_index': [(5, 5)], 'id': 'N04-2005.9'}, 'e2': {'word': 'multi-pathway MT architecture design', 'word_index': [(9, 12)], 'id': 'N04-2005.10'}, 'entity_replacement': {'5:5': 'ENTITY', '9:12': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED'}}	The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system .
In our current research into the design of cognitively well-motivated interfaces relying primarily on the display of graphical information, we have observed that graphical information alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users' expectations.	display of graphical information	cognitively well-motivated interfaces	usage	{'e1': {'word': 'display of graphical information', 'word_index': [(17, 20)], 'id': 'A92-1010.2'}, 'e2': {'word': 'cognitively well-motivated interfaces', 'word_index': [(8, 12)], 'id': 'A92-1010.1'}, 'entity_replacement': {'8:12': 'ENTITYOTHER', '17:20': 'ENTITY', '26:27': 'ENTITYUNRELATED'}}	In our current research into the design of cognitively well - motivated interfaces relying primarily on the display of graphical information , we have observed that graphical information alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users ' expectations .
To solve this problem, we are working towards the integration of natural language generation to augment the interaction</abstract>	natural language generation	interaction	usage	{'e1': {'word': 'natural language generation', 'word_index': [(12, 14)], 'id': 'A92-1010.6'}, 'e2': {'word': 'interaction', 'word_index': [(18, 18)], 'id': 'A92-1010.7'}, 'entity_replacement': {'12:14': 'ENTITY', '18:18': 'ENTITYOTHER'}}	To solve this problem , we are working towards the integration of natural language generation to augment the interaction < / abstract >
For both corpora word recognition experiments were carried out with vocabularies containing up to 20k words.	words	vocabularies	part_whole	{'e1': {'word': 'words', 'word_index': [(15, 15)], 'id': 'H94-1064.8'}, 'e2': {'word': 'vocabularies', 'word_index': [(10, 10)], 'id': 'H94-1064.7'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '3:5': 'ENTITYUNRELATED', '10:10': 'ENTITYOTHER', '15:15': 'ENTITY'}}	For both corpora word recognition experiments were carried out with vocabularies containing up to 20k words .
The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling.	Gaussian mixture	continuous density HMM	model-feature	{'e1': {'word': 'Gaussian mixture', 'word_index': [(9, 10)], 'id': 'H94-1064.10'}, 'e2': {'word': 'continuous density HMM', 'word_index': [(5, 7)], 'id': 'H94-1064.9'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '9:10': 'ENTITY', '12:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED'}}	The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling .
The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling.	n-gram statistics	language modeling	usage	{'e1': {'word': 'n-gram statistics', 'word_index': [(15, 16)], 'id': 'H94-1064.12'}, 'e2': {'word': 'language modeling', 'word_index': [(23, 24)], 'id': 'H94-1064.14'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '15:16': 'ENTITY', '20:21': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER'}}	The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling .
A second forward pass, which makes use of a word graph generated with the bigram, incorporates a trigram language model.	word graph	forward pass	usage	{'e1': {'word': 'word graph', 'word_index': [(10, 11)], 'id': 'H94-1064.18'}, 'e2': {'word': 'forward pass', 'word_index': [(2, 3)], 'id': 'H94-1064.17'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '10:11': 'ENTITY', '15:15': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED'}}	A second forward pass , which makes use of a word graph generated with the bigram , incorporates a trigram language model .
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex-dependent models.	cepstrum-based features	Acoustic modeling	usage	{'e1': {'word': 'cepstrum-based features', 'word_index': [(3, 6)], 'id': 'H94-1064.22'}, 'e2': {'word': 'Acoustic modeling', 'word_index': [(0, 1)], 'id': 'H94-1064.21'}, 'entity_replacement': {'0:1': 'ENTITYOTHER', '3:6': 'ENTITY', '8:16': 'ENTITYUNRELATED', '18:20': 'ENTITYUNRELATED', '23:25': 'ENTITYUNRELATED'}}	Acoustic modeling uses cepstrum - based features , context -dependent phone models ( intra and interword ) , phone duration models , and sex -dependent models .
The system is based on a multi-component architecture where each component is responsible for identifying one class of unknown words.	multi-component architecture	system	usage	{'e1': {'word': 'multi-component architecture', 'word_index': [(6, 7)], 'id': 'A00-1024.3'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'A00-1024.2'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '6:7': 'ENTITY', '10:10': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED'}}	The system is based on a multi-component architecture where each component is responsible for identifying one class of unknown words .
The focus of this paper is the components that identify names and spelling errors.	components	names	usage	{'e1': {'word': 'components', 'word_index': [(7, 7)], 'id': 'A00-1024.6'}, 'e2': {'word': 'names', 'word_index': [(10, 10)], 'id': 'A00-1024.7'}, 'entity_replacement': {'7:7': 'ENTITY', '10:10': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED'}}	The focus of this paper is the components that identify names and spelling errors .
Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word.	decision tree architecture	component	usage	{'e1': {'word': 'decision tree architecture', 'word_index': [(4, 6)], 'id': 'A00-1024.10'}, 'e2': {'word': 'component', 'word_index': [(1, 1)], 'id': 'A00-1024.9'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:6': 'ENTITY', '12:12': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED'}}	Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word .
Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word.	evidence	unknown word	model-feature	{'e1': {'word': 'evidence', 'word_index': [(12, 12)], 'id': 'A00-1024.11'}, 'e2': {'word': 'unknown word', 'word_index': [(15, 16)], 'id': 'A00-1024.12'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:6': 'ENTITYUNRELATED', '12:12': 'ENTITY', '15:16': 'ENTITYOTHER'}}	Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word .
The system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words.	unknown words	live closed captions	part_whole	{'e1': {'word': 'unknown words', 'word_index': [(19, 20)], 'id': 'A00-1024.15'}, 'e2': {'word': 'live closed captions', 'word_index': [(7, 9)], 'id': 'A00-1024.14'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:9': 'ENTITYOTHER', '19:20': 'ENTITY'}}	The system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words .
Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2]).	Recognition of proper nouns	Japanese text	usage	{'e1': {'word': 'Recognition of proper nouns', 'word_index': [(0, 3)], 'id': 'X96-1059.1'}, 'e2': {'word': 'Japanese text', 'word_index': [(5, 6)], 'id': 'X96-1059.2'}, 'entity_replacement': {'0:3': 'ENTITY', '5:6': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED', '22:24': 'ENTITYUNRELATED'}}	Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ( [ 1 ] [ 2 ] ) .
Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2]).	morphological analysis	Japanese text processing	part_whole	{'e1': {'word': 'morphological analysis', 'word_index': [(19, 20)], 'id': 'X96-1059.3'}, 'e2': {'word': 'Japanese text processing', 'word_index': [(22, 24)], 'id': 'X96-1059.4'}, 'entity_replacement': {'0:3': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '19:20': 'ENTITY', '22:24': 'ENTITYOTHER'}}	Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ( [ 1 ] [ 2 ] ) .
Our approach to the Multi-lingual Evaluation Task (MET) for Japanese text is to consider the given task as a morphological analysis problem in Japanese.	morphological analysis problem	Japanese	usage	{'e1': {'word': 'morphological analysis problem', 'word_index': [(21, 23)], 'id': 'X96-1059.7'}, 'e2': {'word': 'Japanese', 'word_index': [(25, 25)], 'id': 'X96-1059.8'}, 'entity_replacement': {'11:12': 'ENTITYUNRELATED', '21:23': 'ENTITY', '25:25': 'ENTITYOTHER'}}	Our approach to the Multi-lingual Evaluation Task ( MET ) for Japanese text is to consider the given task as a morphological analysis problem in Japanese .
Our morphological analyzer has done all the necessary work for the recognition and classification of proper names, numerical and temporal expressions, i.ehological analyzer has done all the necessary work for the recognition and classification of proper names, numerical and temporal expressions, i.e.	morphological analyzer	recognition and classification of proper names, numerical and temporal expressions, i.e	usage	{'e1': {'word': 'morphological analyzer', 'word_index': [(1, 2)], 'id': 'X96-1059.9'}, 'e2': {'word': 'recognition and classification of proper names, numerical and temporal expressions, i.e', 'word_index': [(11, 23)], 'id': 'X96-1059.10'}, 'entity_replacement': {'1:2': 'ENTITY', '11:23': 'ENTITYOTHER'}}	Our morphological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i.e hological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i.e.
First, it uses several kinds of dictionaries to segment and tag Japanese character strings.	dictionaries	Japanese character strings	usage	{'e1': {'word': 'dictionaries', 'word_index': [(7, 7)], 'id': 'X96-1059.16'}, 'e2': {'word': 'Japanese character strings', 'word_index': [(12, 14)], 'id': 'X96-1059.17'}, 'entity_replacement': {'7:7': 'ENTITY', '12:14': 'ENTITYOTHER'}}	First , it uses several kinds of dictionaries to segment and tag Japanese character strings .
Second, based on the information resulting from the dictionary lookup stage, a set of rules is applied to the segmented strings in order to identify NE items.	rules	segmented strings	usage	{'e1': {'word': 'rules', 'word_index': [(16, 16)], 'id': 'X96-1059.19'}, 'e2': {'word': 'segmented strings', 'word_index': [(21, 22)], 'id': 'X96-1059.20'}, 'entity_replacement': {'9:11': 'ENTITYUNRELATED', '16:16': 'ENTITY', '21:22': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED'}}	Second , based on the information resulting from the dictionary lookup stage , a set of rules is applied to the segmented strings in order to identify NE items .
We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines.	practically unsupervised learning method	single-snippet answers	usage	{'e1': {'word': 'practically unsupervised learning method', 'word_index': [(3, 6)], 'id': 'H05-1041.1'}, 'e2': {'word': 'single-snippet answers', 'word_index': [(9, 12)], 'id': 'H05-1041.2'}, 'entity_replacement': {'3:6': 'ENTITY', '9:12': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '22:24': 'ENTITYUNRELATED'}}	We present a practically unsupervised learning method to produce single - snippet answers to definition questions in question answering systems that supplement Web search engines .
The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples, which are then used to train an svm to separate the two classes.	on-line encyclopedias and dictionaries	positive and negative definition examples	usage	{'e1': {'word': 'on-line encyclopedias and dictionaries', 'word_index': [(3, 7)], 'id': 'H05-1041.6'}, 'e2': {'word': 'positive and negative definition examples', 'word_index': [(16, 20)], 'id': 'H05-1041.7'}, 'entity_replacement': {'3:7': 'ENTITY', '16:20': 'ENTITYOTHER', '29:29': 'ENTITYUNRELATED'}}	The method exploits on- line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples , which are then used to train an svm to separate the two classes .
Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms.	terms	corpora	part_whole	{'e1': {'word': 'terms', 'word_index': [(13, 13)], 'id': 'W02-1403.2'}, 'e2': {'word': 'corpora', 'word_index': [(16, 16)], 'id': 'W02-1403.3'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '13:13': 'ENTITY', '16:16': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '36:43': 'ENTITYUNRELATED', '46:46': 'ENTITYUNRELATED'}}	Terminology structuring has been the subject of much work in the context of terms extracted from corpora : given a set of terms , obtained from an existing resource or extracted from a corpus , identifying hierarchical ( or other types of ) relations between these terms .
Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms.	terms	corpus	part_whole	{'e1': {'word': 'terms', 'word_index': [(22, 22)], 'id': 'W02-1403.4'}, 'e2': {'word': 'corpus', 'word_index': [(33, 33)], 'id': 'W02-1403.5'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '22:22': 'ENTITY', '33:33': 'ENTITYOTHER', '36:43': 'ENTITYUNRELATED', '46:46': 'ENTITYUNRELATED'}}	Terminology structuring has been the subject of much work in the context of terms extracted from corpora : given a set of terms , obtained from an existing resource or extracted from a corpus , identifying hierarchical ( or other types of ) relations between these terms .
Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms.	hierarchical (or other types of) relations	terms	model-feature	{'e1': {'word': 'hierarchical (or other types of) relations', 'word_index': [(36, 43)], 'id': 'W02-1403.6'}, 'e2': {'word': 'terms', 'word_index': [(46, 46)], 'id': 'W02-1403.7'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '36:43': 'ENTITY', '46:46': 'ENTITYOTHER'}}	Terminology structuring has been the subject of much work in the context of terms extracted from corpora : given a set of terms , obtained from an existing resource or extracted from a corpus , identifying hierarchical ( or other types of ) relations between these terms .
The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account.	lexical methods	terminology structuring	usage	{'e1': {'word': 'lexical methods', 'word_index': [(8, 9)], 'id': 'W02-1403.9'}, 'e2': {'word': 'terminology structuring', 'word_index': [(5, 6)], 'id': 'W02-1403.8'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '8:9': 'ENTITY', '13:13': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED'}}	The present paper focusses on terminology structuring by lexical methods , which match terms on the basis on their content words , taking morphological variants into account .
The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account.	content words	terms	part_whole	{'e1': {'word': 'content words', 'word_index': [(19, 20)], 'id': 'W02-1403.11'}, 'e2': {'word': 'terms', 'word_index': [(13, 13)], 'id': 'W02-1403.10'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '13:13': 'ENTITYOTHER', '19:20': 'ENTITY', '23:24': 'ENTITYUNRELATED'}}	The present paper focusses on terminology structuring by lexical methods , which match terms on the basis on their content words , taking morphological variants into account .
Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus.	terms	hierarchically-structured terminology	part_whole	{'e1': {'word': 'terms', 'word_index': [(10, 10)], 'id': 'W02-1403.13'}, 'e2': {'word': 'hierarchically-structured terminology', 'word_index': [(15, 18)], 'id': 'W02-1403.14'}, 'entity_replacement': {'10:10': 'ENTITY', '15:18': 'ENTITYOTHER', '25:31': 'ENTITYUNRELATED'}}	Experiments are done on a ' flat ' list of terms obtained from an originally hierarchically - structured terminology : the French version of the US National Library of Medicine MeSH thesaurus .
We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis ofthe 'new' relations not present in the MeSH.	lexically-induced relations	MeSH relations	compare	{'e1': {'word': 'lexically-induced relations', 'word_index': [(3, 6)], 'id': 'W02-1403.16'}, 'e2': {'word': 'MeSH relations', 'word_index': [(10, 11)], 'id': 'W02-1403.17'}, 'entity_replacement': {'3:6': 'ENTITY', '10:11': 'ENTITYOTHER', '21:24': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED'}}	We compare the lexically - induced relations with the original MeSH relations : after a quantitative evaluation of their congruence through recall and precision metrics , we perform a qualitative , human analysis of the ' new ' relations not present in the MeSH .
We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis ofthe 'new' relations not present in the MeSH.	relations	MeSH	part_whole	{'e1': {'word': 'relations', 'word_index': [(37, 37)], 'id': 'W02-1403.19'}, 'e2': {'word': 'MeSH', 'word_index': [(42, 42)], 'id': 'W02-1403.20'}, 'entity_replacement': {'3:6': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '21:24': 'ENTITYUNRELATED', '37:37': 'ENTITY', '42:42': 'ENTITYOTHER'}}	We compare the lexically - induced relations with the original MeSH relations : after a quantitative evaluation of their congruence through recall and precision metrics , we perform a qualitative , human analysis ofthe ' new ' relations not present in the MeSH .
On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.	naming conventions	MeSH	model-feature	{'e1': {'word': 'naming conventions', 'word_index': [(13, 14)], 'id': 'W02-1403.22'}, 'e2': {'word': 'MeSH', 'word_index': [(18, 18)], 'id': 'W02-1403.23'}, 'entity_replacement': {'13:14': 'ENTITY', '18:18': 'ENTITYOTHER', '30:31': 'ENTITYUNRELATED'}}	On the other hand , it also reveals some specific structuring choices and naming conventions made by the MeSH designers , and emphasizes ontological commitments that cannot be left to automatic structuring .
In this study, we propose a knowledge-independent method for aligning terms and thus extracting translations from a small, domain-specific corpus consisting of parallel English and Chinese court judgments from Hong Kong.	translations	small, domain-specific corpus	part_whole	{'e1': {'word': 'translations', 'word_index': [(17, 17)], 'id': 'W02-1404.3'}, 'e2': {'word': 'small, domain-specific corpus', 'word_index': [(20, 25)], 'id': 'W02-1404.4'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '17:17': 'ENTITY', '20:25': 'ENTITYOTHER', '28:33': 'ENTITYUNRELATED'}}	In this study , we propose a knowledge - independent method for aligning terms and thus extracting translations from a small , domain - specific corpus consisting of parallel English and Chinese court judgments from Hong Kong .
With a sentence-aligned corpus, translation equivalences are suggested by analysing the frequency profiles of parallel concordances.	frequency profiles	parallel concordances	model-feature	{'e1': {'word': 'frequency profiles', 'word_index': [(14, 15)], 'id': 'W02-1404.8'}, 'e2': {'word': 'parallel concordances', 'word_index': [(17, 18)], 'id': 'W02-1404.9'}, 'entity_replacement': {'2:5': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '14:15': 'ENTITY', '17:18': 'ENTITYOTHER'}}	With a sentence - aligned corpus , translation equivalences are suggested by analysing the frequency profiles of parallel concordances .
The method overcomes the limitations of conventional statistical methods which require large corpora to be effective, and lexical approaches which depend on existing bilingual dictionaries.	large corpora	conventional statistical methods	usage	{'e1': {'word': 'large corpora', 'word_index': [(11, 12)], 'id': 'W02-1404.11'}, 'e2': {'word': 'conventional statistical methods', 'word_index': [(6, 8)], 'id': 'W02-1404.10'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '11:12': 'ENTITY', '18:19': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	The method overcomes the limitations of conventional statistical methods which require large corpora to be effective , and lexical approaches which depend on existing bilingual dictionaries .
The method overcomes the limitations of conventional statistical methods which require large corpora to be effective, and lexical approaches which depend on existing bilingual dictionaries.	bilingual dictionaries	lexical approaches	usage	{'e1': {'word': 'bilingual dictionaries', 'word_index': [(24, 25)], 'id': 'W02-1404.13'}, 'e2': {'word': 'lexical approaches', 'word_index': [(18, 19)], 'id': 'W02-1404.12'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER', '24:25': 'ENTITY'}}	The method overcomes the limitations of conventional statistical methods which require large corpora to be effective , and lexical approaches which depend on existing bilingual dictionaries .
Pilot testing on a parallel corpus of about 113K Chinese words and 120K English words gives an encouraging 85% precision and 45% recall.	Chinese words	parallel corpus	part_whole	{'e1': {'word': 'Chinese words', 'word_index': [(9, 10)], 'id': 'W02-1404.15'}, 'e2': {'word': 'parallel corpus', 'word_index': [(4, 5)], 'id': 'W02-1404.14'}, 'entity_replacement': {'4:5': 'ENTITYOTHER', '9:10': 'ENTITY', '13:14': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	Pilot testing on a parallel corpus of about 113K Chinese words and 120K English words gives an encouraging 85 % precision and 45 % recall .
Future work includes fine-tuning the algorithm upon the analysis of the errors, and acquiring a translation lexicon for legal terminology by filtering out general terms.	legal terminology	translation lexicon	part_whole	{'e1': {'word': 'legal terminology', 'word_index': [(21, 22)], 'id': 'W02-1404.21'}, 'e2': {'word': 'translation lexicon', 'word_index': [(18, 19)], 'id': 'W02-1404.20'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER', '21:22': 'ENTITY', '26:27': 'ENTITYUNRELATED'}}	Future work includes fine - tuning the algorithm upon the analysis of the errors , and acquiring a translation lexicon for legal terminology by filtering out general terms .
Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to share text revision across languages.	Coedition	natural language text	usage	{'e1': {'word': 'Coedition', 'word_index': [(0, 0)], 'id': 'W02-1602.1'}, 'e2': {'word': 'natural language text', 'word_index': [(3, 5)], 'id': 'W02-1602.2'}, 'entity_replacement': {'0:0': 'ENTITY', '3:5': 'ENTITYOTHER', '11:12': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to share text revision across languages .
We are developing a prototype where, in the simplest sharing scenario, naive users interact directly with the text in their language (L0), and indirectly with the associated graph.	language (L0)	text	model-feature	{'e1': {'word': 'language (L0)', 'word_index': [(22, 25)], 'id': 'W02-1602.10'}, 'e2': {'word': 'text', 'word_index': [(19, 19)], 'id': 'W02-1602.9'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '19:19': 'ENTITYOTHER', '22:25': 'ENTITY', '32:32': 'ENTITYUNRELATED'}}	We are developing a prototype where , in the simplest sharing scenario , naive users interact directly with the text in their language ( L0 ) , and indirectly with the associated graph .
"Establishing a ""best"" correspondence between the ""UNL-tree+L0"" and the ""MS-L0 structure"", a lattice, may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible."	crossing liaisons	tree	model-feature	{'e1': {'word': 'crossing liaisons', 'word_index': [(46, 47)], 'id': 'W02-1602.35'}, 'e2': {'word': 'tree', 'word_index': [(38, 38)], 'id': 'W02-1602.33'}, 'entity_replacement': {'9:13': 'ENTITYUNRELATED', '18:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '38:38': 'ENTITYOTHER', '42:42': 'ENTITYUNRELATED', '46:47': 'ENTITY'}}	"Establishing a "" best "" correspondence between the "" UNL - tree + L0 "" and the "" MS - L0 structure "" , a lattice , may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible ."
In this paper, we improve an unsupervised learning method using the Expectation-Maximization (EM) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation (WSD) problems.	Expectation-Maximization (EM) algorithm	text classification problems	usage	{'e1': {'word': 'Expectation-Maximization (EM) algorithm', 'word_index': [(12, 18)], 'id': 'W03-0406.2'}, 'e2': {'word': 'text classification problems', 'word_index': [(25, 27)], 'id': 'W03-0406.3'}, 'entity_replacement': {'7:9': 'ENTITYUNRELATED', '12:18': 'ENTITY', '25:27': 'ENTITYOTHER', '34:40': 'ENTITYUNRELATED'}}	In this paper , we improve an unsupervised learning method using the Expectation - Maximization ( EM ) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation ( WSD ) problems .
The improved method stops the EM algorithm at the optimum iteration number.	optimum iteration number	EM algorithm	model-feature	{'e1': {'word': 'optimum iteration number', 'word_index': [(9, 11)], 'id': 'W03-0406.6'}, 'e2': {'word': 'EM algorithm', 'word_index': [(5, 6)], 'id': 'W03-0406.5'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '9:11': 'ENTITY'}}	The improved method stops the EM algorithm at the optimum iteration number .
In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2.	noun WSD problems	Japanese Dictionary Task in SENSEVAL2	part_whole	{'e1': {'word': 'noun WSD problems', 'word_index': [(6, 8)], 'id': 'W03-0406.7'}, 'e2': {'word': 'Japanese Dictionary Task in SENSEVAL2', 'word_index': [(11, 15)], 'id': 'W03-0406.8'}, 'entity_replacement': {'6:8': 'ENTITY', '11:15': 'ENTITYOTHER'}}	In experiments , we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2 .
In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system, a language tutoring application for deaf learners of written English.	user knowledge modeling architecture	ICICLE system	part_whole	{'e1': {'word': 'user knowledge modeling architecture', 'word_index': [(7, 10)], 'id': 'W99-0408.1'}, 'e2': {'word': 'ICICLE system', 'word_index': [(13, 14)], 'id': 'W99-0408.2'}, 'entity_replacement': {'7:10': 'ENTITY', '13:14': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system , a language tutoring application for deaf learners of written English .
In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system, a language tutoring application for deaf learners of written English.	language tutoring application	written English	usage	{'e1': {'word': 'language tutoring application', 'word_index': [(17, 19)], 'id': 'W99-0408.3'}, 'e2': {'word': 'written English', 'word_index': [(24, 25)], 'id': 'W99-0408.4'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '17:19': 'ENTITY', '24:25': 'ENTITYOTHER'}}	In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system , a language tutoring application for deaf learners of written English .
We motivate our model design by citing relevant research on second language and cognitive skill acquisition, and briefly discuss preliminary empirical evidence supporting the design.	second language and cognitive skill acquisition	model design	usage	{'e1': {'word': 'second language and cognitive skill acquisition', 'word_index': [(10, 15)], 'id': 'W99-0408.9'}, 'e2': {'word': 'model design', 'word_index': [(3, 4)], 'id': 'W99-0408.8'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '10:15': 'ENTITY', '25:25': 'ENTITYUNRELATED'}}	We motivate our model design by citing relevant research on second language and cognitive skill acquisition , and briefly discuss preliminary empirical evidence supporting the design .
This paper describes novel and practical Japanese parsers that uses decision trees.	decision trees	Japanese parsers	usage	{'e1': {'word': 'decision trees', 'word_index': [(10, 11)], 'id': 'P98-1083.2'}, 'e2': {'word': 'Japanese parsers', 'word_index': [(6, 7)], 'id': 'P98-1083.1'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '10:11': 'ENTITY'}}	This paper describes novel and practical Japanese parsers that uses decision trees .
First, we construct a single decision tree to estimate modification probabilities; how one phrase tends to modify another.	decision tree	modification probabilities	model-feature	{'e1': {'word': 'decision tree', 'word_index': [(6, 7)], 'id': 'P98-1083.3'}, 'e2': {'word': 'modification probabilities', 'word_index': [(10, 11)], 'id': 'P98-1083.4'}, 'entity_replacement': {'6:7': 'ENTITY', '10:11': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED'}}	First , we construct a single decision tree to estimate modification probabilities ; how one phrase tends to modify another .
Next, we introduce a boosting algorithm in which several decision trees are constructed and then combined for probability estimation.	decision trees	probability estimation	usage	{'e1': {'word': 'decision trees', 'word_index': [(10, 11)], 'id': 'P98-1083.7'}, 'e2': {'word': 'probability estimation', 'word_index': [(18, 19)], 'id': 'P98-1083.8'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '10:11': 'ENTITY', '18:19': 'ENTITYOTHER'}}	Next , we introduce a boosting algorithm in which several decision trees are constructed and then combined for probability estimation .
Automatic estimation of word significance oriented for speech-based Information Retrieval (IR) is addressed.	word significance	speech-based Information Retrieval (IR)	usage	{'e1': {'word': 'word significance', 'word_index': [(3, 4)], 'id': 'I08-1027.2'}, 'e2': {'word': 'speech-based Information Retrieval (IR)', 'word_index': [(7, 14)], 'id': 'I08-1027.3'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '3:4': 'ENTITY', '7:14': 'ENTITYOTHER'}}	Automatic estimation of word significance oriented for speech - based Information Retrieval ( IR ) is addressed .
Since the significance of words differs in IR, automatic speech recognition (ASR) performance has been evaluated based on weighted word error rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly.	significance	words	model-feature	{'e1': {'word': 'significance', 'word_index': [(2, 2)], 'id': 'I08-1027.4'}, 'e2': {'word': 'words', 'word_index': [(4, 4)], 'id': 'I08-1027.5'}, 'entity_replacement': {'2:2': 'ENTITY', '4:4': 'ENTITYOTHER', '7:7': 'ENTITYUNRELATED', '9:15': 'ENTITYUNRELATED', '21:27': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED', '43:48': 'ENTITYUNRELATED', '53:53': 'ENTITYUNRELATED'}}	Since the significance of words differs in IR , automatic speech recognition ( ASR ) performance has been evaluated based on weighted word error rate ( WWER ) , which gives a weight on errors from the viewpoint of IR , instead of word error rate ( WER ) , which treats all words uniformly .
Since the significance of words differs in IR, automatic speech recognition (ASR) performance has been evaluated based on weighted word error rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly.	weight	word error rate (WER)	compare	{'e1': {'word': 'weight', 'word_index': [(32, 32)], 'id': 'I08-1027.9'}, 'e2': {'word': 'word error rate (WER)', 'word_index': [(43, 48)], 'id': 'I08-1027.11'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '9:15': 'ENTITYUNRELATED', '21:27': 'ENTITYUNRELATED', '32:32': 'ENTITY', '39:39': 'ENTITYUNRELATED', '43:48': 'ENTITYOTHER', '53:53': 'ENTITYUNRELATED'}}	Since the significance of words differs in IR , automatic speech recognition ( ASR ) performance has been evaluated based on weighted word error rate ( WWER ) , which gives a weight on errors from the viewpoint of IR , instead of word error rate ( WER ) , which treats all words uniformly .
A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown, and the reduction of errors on both ASR and IR has been reported.	Minimum Bayes-Risk framework	decoding strategy	usage	{'e1': {'word': 'Minimum Bayes-Risk framework', 'word_index': [(9, 13)], 'id': 'I08-1027.15'}, 'e2': {'word': 'decoding strategy', 'word_index': [(1, 2)], 'id': 'I08-1027.13'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '5:5': 'ENTITYUNRELATED', '9:13': 'ENTITY', '25:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	A decoding strategy that minimizes WWER based on a Minimum Bayes - Risk framework has been shown , and the reduction of errors on both ASR and IR has been reported .
A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown, and the reduction of errors on both ASR and IR has been reported.	ASR	IR	compare	{'e1': {'word': 'ASR', 'word_index': [(25, 25)], 'id': 'I08-1027.16'}, 'e2': {'word': 'IR', 'word_index': [(27, 27)], 'id': 'I08-1027.17'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '9:13': 'ENTITYUNRELATED', '25:25': 'ENTITY', '27:27': 'ENTITYOTHER'}}	A decoding strategy that minimizes WWER based on a Minimum Bayes - Risk framework has been shown , and the reduction of errors on both ASR and IR has been reported .
In this paper, we propose an automatic estimation method for word significance (weights) based on its influence on IR.	automatic estimation method	word significance (weights)	usage	{'e1': {'word': 'automatic estimation method', 'word_index': [(7, 9)], 'id': 'I08-1027.18'}, 'e2': {'word': 'word significance (weights)', 'word_index': [(11, 15)], 'id': 'I08-1027.19'}, 'entity_replacement': {'7:9': 'ENTITY', '11:15': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED'}}	In this paper , we propose an automatic estimation method for word significance ( weights ) based on its influence on IR .
This study presents a method to automatically acquire paraphrases using bilingual corpora, which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques.	bilingual corpora	method to automatically acquire paraphrases	usage	{'e1': {'word': 'bilingual corpora', 'word_index': [(10, 11)], 'id': 'I08-1043.2'}, 'e2': {'word': 'method to automatically acquire paraphrases', 'word_index': [(4, 8)], 'id': 'I08-1043.1'}, 'entity_replacement': {'4:8': 'ENTITYOTHER', '10:11': 'ENTITY', '16:18': 'ENTITYUNRELATED', '23:25': 'ENTITYUNRELATED', '33:35': 'ENTITYUNRELATED'}}	This study presents a method to automatically acquire paraphrases using bilingual corpora , which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques .
This study presents a method to automatically acquire paraphrases using bilingual corpora, which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques.	statistical alignment techniques	bilingual dependency relations	usage	{'e1': {'word': 'statistical alignment techniques', 'word_index': [(33, 35)], 'id': 'I08-1043.5'}, 'e2': {'word': 'bilingual dependency relations', 'word_index': [(16, 18)], 'id': 'I08-1043.3'}, 'entity_replacement': {'4:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '16:18': 'ENTITYOTHER', '23:25': 'ENTITYUNRELATED', '33:35': 'ENTITY'}}	This study presents a method to automatically acquire paraphrases using bilingual corpora , which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques .
Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation, it would be possible to obtain interchangeable paraphrases under a given context.	sense	phrase	model-feature	{'e1': {'word': 'sense', 'word_index': [(10, 10)], 'id': 'I08-1043.7'}, 'e2': {'word': 'phrase', 'word_index': [(14, 14)], 'id': 'I08-1043.8'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '10:10': 'ENTITY', '14:14': 'ENTITYOTHER', '17:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation , it would be possible to obtain interchangeable paraphrases under a given context .
Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation, it would be possible to obtain interchangeable paraphrases under a given context.	context	paraphrases	model-feature	{'e1': {'word': 'context', 'word_index': [(34, 34)], 'id': 'I08-1043.12'}, 'e2': {'word': 'paraphrases', 'word_index': [(30, 30)], 'id': 'I08-1043.11'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '30:30': 'ENTITYOTHER', '34:34': 'ENTITY'}}	Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation , it would be possible to obtain interchangeable paraphrases under a given context .
Also, we provide an advanced method to acquire generalized translation knowledge using the extracted paraphrases.	paraphrases	generalized translation knowledge	usage	{'e1': {'word': 'paraphrases', 'word_index': [(15, 15)], 'id': 'I08-1043.14'}, 'e2': {'word': 'generalized translation knowledge', 'word_index': [(9, 11)], 'id': 'I08-1043.13'}, 'entity_replacement': {'9:11': 'ENTITYOTHER', '15:15': 'ENTITY'}}	Also , we provide an advanced method to acquire generalized translation knowledge using the extracted paraphrases .
We applied the method to acquire the generalized translation knowledge for Korean-English translation.	generalized translation knowledge	Korean-English translation	model-feature	{'e1': {'word': 'generalized translation knowledge', 'word_index': [(7, 9)], 'id': 'I08-1043.15'}, 'e2': {'word': 'Korean-English translation', 'word_index': [(11, 14)], 'id': 'I08-1043.16'}, 'entity_replacement': {'7:9': 'ENTITY', '11:14': 'ENTITYOTHER'}}	We applied the method to acquire the generalized translation knowledge for Korean - English translation .
Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.	Korean and English language pairs	parallel corpora	part_whole	{'e1': {'word': 'Korean and English language pairs', 'word_index': [(7, 11)], 'id': 'I08-1043.18'}, 'e2': {'word': 'parallel corpora', 'word_index': [(3, 4)], 'id': 'I08-1043.17'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '7:11': 'ENTITY', '17:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED', '52:52': 'ENTITYUNRELATED', '57:58': 'ENTITYUNRELATED'}}	Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .
Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.	paraphrasing method	precision	result	{'e1': {'word': 'paraphrasing method', 'word_index': [(17, 18)], 'id': 'I08-1043.19'}, 'e2': {'word': 'precision', 'word_index': [(24, 24)], 'id': 'I08-1043.21'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:11': 'ENTITYUNRELATED', '17:18': 'ENTITY', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYOTHER', '33:33': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED', '52:52': 'ENTITYUNRELATED', '57:58': 'ENTITYUNRELATED'}}	Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .
Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.	translation knowledge	bilingual corpora	part_whole	{'e1': {'word': 'translation knowledge', 'word_index': [(39, 40)], 'id': 'I08-1043.24'}, 'e2': {'word': 'bilingual corpora', 'word_index': [(44, 45)], 'id': 'I08-1043.25'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '39:40': 'ENTITY', '44:45': 'ENTITYOTHER', '52:52': 'ENTITYUNRELATED', '57:58': 'ENTITYUNRELATED'}}	Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .
Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.	compression ratio	paraphrases	model-feature	{'e1': {'word': 'compression ratio', 'word_index': [(57, 58)], 'id': 'I08-1043.27'}, 'e2': {'word': 'paraphrases', 'word_index': [(52, 52)], 'id': 'I08-1043.26'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '7:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED', '52:52': 'ENTITYOTHER', '57:58': 'ENTITY'}}	Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .
This paper describes a computational model of word segmentation and presents simulation results on realistic acquisition.	computational model	word segmentation	model-feature	{'e1': {'word': 'computational model', 'word_index': [(4, 5)], 'id': 'W04-1307.1'}, 'e2': {'word': 'word segmentation', 'word_index': [(7, 8)], 'id': 'W04-1307.2'}, 'entity_replacement': {'4:5': 'ENTITY', '7:8': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED'}}	This paper describes a computational model of word segmentation and presents simulation results on realistic acquisition .
In particular, we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics.	statistical learning mechanisms	cognitive psychology	usage	{'e1': {'word': 'statistical learning mechanisms', 'word_index': [(10, 12)], 'id': 'W04-1307.4'}, 'e2': {'word': 'cognitive psychology', 'word_index': [(19, 20)], 'id': 'W04-1307.5'}, 'entity_replacement': {'10:12': 'ENTITY', '19:20': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED'}}	In particular , we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics .
Dictionary construction, one of the most difficult tasks in developing a machine translation system, is expensive.	Dictionary construction	machine translation system	part_whole	{'e1': {'word': 'Dictionary construction', 'word_index': [(0, 1)], 'id': 'W04-2204.2'}, 'e2': {'word': 'machine translation system', 'word_index': [(12, 14)], 'id': 'W04-2204.3'}, 'entity_replacement': {'0:1': 'ENTITY', '12:14': 'ENTITYOTHER'}}	Dictionary construction , one of the most difficult tasks in developing a machine translation system , is expensive .
To avoid this problem, we investigate how we build a dictionary using existing linguistic resources.	linguistic resources	dictionary	usage	{'e1': {'word': 'linguistic resources', 'word_index': [(14, 15)], 'id': 'W04-2204.5'}, 'e2': {'word': 'dictionary', 'word_index': [(11, 11)], 'id': 'W04-2204.4'}, 'entity_replacement': {'11:11': 'ENTITYOTHER', '14:15': 'ENTITY'}}	To avoid this problem , we investigate how we build a dictionary using existing linguistic resources .
Our algorithm can be applied to any language pairs, but for the present we focus on building a Korean-to-Japanese dictionary using English as a pivot.	algorithm	language pairs	usage	{'e1': {'word': 'algorithm', 'word_index': [(1, 1)], 'id': 'W04-2204.6'}, 'e2': {'word': 'language pairs', 'word_index': [(7, 8)], 'id': 'W04-2204.7'}, 'entity_replacement': {'1:1': 'ENTITY', '7:8': 'ENTITYOTHER', '19:24': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED'}}	Our algorithm can be applied to any language pairs , but for the present we focus on building a Korean - to - Japanese dictionary using English as a pivot .
Our algorithm can be applied to any language pairs, but for the present we focus on building a Korean-to-Japanese dictionary using English as a pivot.	pivot	Korean-to-Japanese dictionary	usage	{'e1': {'word': 'pivot', 'word_index': [(29, 29)], 'id': 'W04-2204.10'}, 'e2': {'word': 'Korean-to-Japanese dictionary', 'word_index': [(19, 24)], 'id': 'W04-2204.8'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '19:24': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '29:29': 'ENTITY'}}	Our algorithm can be applied to any language pairs , but for the present we focus on building a Korean - to - Japanese dictionary using English as a pivot .
We attempt three ways of automatic construction to corroborate the effect of the directionality of dictionaries.	directionality	dictionaries	model-feature	{'e1': {'word': 'directionality', 'word_index': [(13, 13)], 'id': 'W04-2204.12'}, 'e2': {'word': 'dictionaries', 'word_index': [(15, 15)], 'id': 'W04-2204.13'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '13:13': 'ENTITY', '15:15': 'ENTITYOTHER'}}	We attempt three ways of automatic construction to corroborate the effect of the directionality of dictionaries .
"First, we introduce ""one-time look up"" method using a Korean-to-English and a Japanese-to-English dictionary."	Korean-to-English and a Japanese-to-English dictionary	"""one-time look up"" method"	usage	"{'e1': {'word': 'Korean-to-English and a Japanese-to-English dictionary', 'word_index': [(14, 26)], 'id': 'W04-2204.15'}, 'e2': {'word': '""one-time look up"" method', 'word_index': [(4, 11)], 'id': 'W04-2204.14'}, 'entity_replacement': {'4:11': 'ENTITYOTHER', '14:26': 'ENTITY'}}"	"First , we introduce "" one - time look up "" method using a Korean - to - English and a Japanese - to - English dictionary ."
We present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments.	discourse connectives	discourse structure	usage	{'e1': {'word': 'discourse connectives', 'word_index': [(16, 17)], 'id': 'W04-2703.4'}, 'e2': {'word': 'discourse structure', 'word_index': [(9, 10)], 'id': 'W04-2703.3'}, 'entity_replacement': {'9:10': 'ENTITYOTHER', '16:17': 'ENTITY', '20:20': 'ENTITYUNRELATED'}}	We present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments .
In this paper, we present a fully automated extraction system, named IntEx, to identify gene and protein interactions in biomedical text.	gene and protein interactions	biomedical text	part_whole	{'e1': {'word': 'gene and protein interactions', 'word_index': [(17, 20)], 'id': 'W05-1308.3'}, 'e2': {'word': 'biomedical text', 'word_index': [(22, 23)], 'id': 'W05-1308.4'}, 'entity_replacement': {'7:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '17:20': 'ENTITY', '22:23': 'ENTITYOTHER'}}	In this paper , we present a fully automated extraction system , named IntEx , to identify gene and protein interactions in biomedical text .
Our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles.	simple clausal structures	complex sentences	part_whole	{'e1': {'word': 'simple clausal structures', 'word_index': [(10, 12)], 'id': 'W05-1308.6'}, 'e2': {'word': 'complex sentences', 'word_index': [(7, 8)], 'id': 'W05-1308.5'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '10:12': 'ENTITY', '16:17': 'ENTITYUNRELATED'}}	Our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles .
Then, tagging biological entities with the help of biomedical and linguistic ontologies.	biomedical and linguistic ontologies	biological entities	model-feature	{'e1': {'word': 'biomedical and linguistic ontologies', 'word_index': [(9, 12)], 'id': 'W05-1308.9'}, 'e2': {'word': 'biological entities', 'word_index': [(3, 4)], 'id': 'W05-1308.8'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '9:12': 'ENTITY'}}	Then , tagging biological entities with the help of biomedical and linguistic ontologies .
Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence.	extraction system	complex sentences	usage	{'e1': {'word': 'extraction system', 'word_index': [(1, 2)], 'id': 'W05-1308.12'}, 'e2': {'word': 'complex sentences', 'word_index': [(4, 5)], 'id': 'W05-1308.13'}, 'entity_replacement': {'1:2': 'ENTITY', '4:5': 'ENTITYOTHER', '8:11': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED'}}	Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence .
Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence.	multiple and nested interactions	sentence	part_whole	{'e1': {'word': 'multiple and nested interactions', 'word_index': [(8, 11)], 'id': 'W05-1308.14'}, 'e2': {'word': 'sentence', 'word_index': [(15, 15)], 'id': 'W05-1308.15'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:5': 'ENTITYUNRELATED', '8:11': 'ENTITY', '15:15': 'ENTITYOTHER'}}	Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence .
Experimental evaluations with two other state of the art extraction systems indicate that the IntEx system achieves better performance without the labor intensive pattern engineering requirement.	IntEx system	performance	result	{'e1': {'word': 'IntEx system', 'word_index': [(14, 15)], 'id': 'W05-1308.17'}, 'e2': {'word': 'performance', 'word_index': [(18, 18)], 'id': 'W05-1308.18'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '14:15': 'ENTITY', '18:18': 'ENTITYOTHER', '23:25': 'ENTITYUNRELATED'}}	Experimental evaluations with two other state of the art extraction systems indicate that the IntEx system achieves better performance without the labor intensive pattern engineering requirement .
We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences.	distance	concepts	model-feature	{'e1': {'word': 'distance', 'word_index': [(7, 7)], 'id': 'W06-1605.1'}, 'e2': {'word': 'concepts', 'word_index': [(9, 9)], 'id': 'W06-1605.2'}, 'entity_replacement': {'7:7': 'ENTITY', '9:9': 'ENTITYOTHER', '11:15': 'ENTITYUNRELATED'}}	We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences .
We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept-concept matrix roughly.01% the size of that created by existing measures.	categories	thesaurus	part_whole	{'e1': {'word': 'categories', 'word_index': [(3, 3)], 'id': 'W06-1605.4'}, 'e2': {'word': 'thesaurus', 'word_index': [(7, 7)], 'id': 'W06-1605.5'}, 'entity_replacement': {'3:3': 'ENTITY', '7:7': 'ENTITYOTHER', '9:10': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '22:25': 'ENTITYUNRELATED'}}	We use the categories in a published thesaurus as coarse-grained concepts , allowing all possible distance values to be stored in a concept - concept matrix roughly . 01 % the size of that created by existing measures .
We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept-concept matrix roughly.01% the size of that created by existing measures.	coarse-grained concepts	concept-concept matrix	part_whole	{'e1': {'word': 'coarse-grained concepts', 'word_index': [(9, 10)], 'id': 'W06-1605.6'}, 'e2': {'word': 'concept-concept matrix', 'word_index': [(22, 25)], 'id': 'W06-1605.8'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '9:10': 'ENTITY', '15:16': 'ENTITYUNRELATED', '22:25': 'ENTITYOTHER'}}	We use the categories in a published thesaurus as coarse-grained concepts , allowing all possible distance values to be stored in a concept - concept matrix roughly . 01 % the size of that created by existing measures .
We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors.	concept-distance measures	traditional distributional word-distance measures	compare	{'e1': {'word': 'concept-distance measures', 'word_index': [(6, 7)], 'id': 'W06-1605.9'}, 'e2': {'word': 'traditional distributional word-distance measures', 'word_index': [(9, 14)], 'id': 'W06-1605.10'}, 'entity_replacement': {'6:7': 'ENTITY', '9:14': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '36:40': 'ENTITYUNRELATED'}}	We show that the newly proposed concept-distance measures outperform traditional distributional word - distance measures in the tasks of ( 1 ) ranking word pairs in order of semantic distance , and ( 2 ) correcting real - word spelling errors .
We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors.	semantic distance	word pairs	model-feature	{'e1': {'word': 'semantic distance', 'word_index': [(28, 29)], 'id': 'W06-1605.12'}, 'e2': {'word': 'word pairs', 'word_index': [(23, 24)], 'id': 'W06-1605.11'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:14': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER', '28:29': 'ENTITY', '36:40': 'ENTITYUNRELATED'}}	We show that the newly proposed concept-distance measures outperform traditional distributional word - distance measures in the tasks of ( 1 ) ranking word pairs in order of semantic distance , and ( 2 ) correcting real - word spelling errors .
In the latter task, of all the WordNet-based measures, only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures.	WordNet-based measures	distributional concept-distance measures	compare	{'e1': {'word': 'WordNet-based measures', 'word_index': [(8, 11)], 'id': 'W06-1605.14'}, 'e2': {'word': 'distributional concept-distance measures', 'word_index': [(23, 25)], 'id': 'W06-1605.15'}, 'entity_replacement': {'8:11': 'ENTITY', '23:25': 'ENTITYOTHER'}}	In the latter task , of all the WordNet - based measures , only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures .
We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures, and illustrate how this allows one to view NLP tasks as graph transformations.	labeled directed graph	linguistic structures	model-feature	{'e1': {'word': 'labeled directed graph', 'word_index': [(9, 11)], 'id': 'W07-0208.1'}, 'e2': {'word': 'linguistic structures', 'word_index': [(17, 18)], 'id': 'W07-0208.2'}, 'entity_replacement': {'9:11': 'ENTITY', '17:18': 'ENTITYOTHER', '28:29': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED'}}	We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures , and illustrate how this allows one to view NLP tasks as graph transformations .
We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures, and illustrate how this allows one to view NLP tasks as graph transformations.	graph transformations	NLP tasks	model-feature	{'e1': {'word': 'graph transformations', 'word_index': [(31, 32)], 'id': 'W07-0208.4'}, 'e2': {'word': 'NLP tasks', 'word_index': [(28, 29)], 'id': 'W07-0208.3'}, 'entity_replacement': {'9:11': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '28:29': 'ENTITYOTHER', '31:32': 'ENTITY'}}	We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures , and illustrate how this allows one to view NLP tasks as graph transformations .
We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using Penn Treebank data) and semantic role labeling (using Proposition Bank data).	identification of non-local depenencies	Penn Treebank data	model-feature	{'e1': {'word': 'identification of non-local depenencies', 'word_index': [(23, 26)], 'id': 'W07-0208.7'}, 'e2': {'word': 'Penn Treebank data', 'word_index': [(29, 31)], 'id': 'W07-0208.8'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '23:26': 'ENTITY', '29:31': 'ENTITYOTHER', '34:36': 'ENTITYUNRELATED', '39:41': 'ENTITYUNRELATED'}}	We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method : identification of non-local depenencies ( using Penn Treebank data ) and semantic role labeling ( using Proposition Bank data ) .
We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using Penn Treebank data) and semantic role labeling (using Proposition Bank data).	semantic role labeling	Proposition Bank data	model-feature	{'e1': {'word': 'semantic role labeling', 'word_index': [(34, 36)], 'id': 'W07-0208.9'}, 'e2': {'word': 'Proposition Bank data', 'word_index': [(39, 41)], 'id': 'W07-0208.10'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '29:31': 'ENTITYUNRELATED', '34:36': 'ENTITY', '39:41': 'ENTITYOTHER'}}	We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method : identification of non-local depenencies ( using Penn Treebank data ) and semantic role labeling ( using Proposition Bank data ) .
Topical blog post retrieval is the task of ranking blog posts with respect to their relevance for a given topic.	relevance	blog posts	model-feature	{'e1': {'word': 'relevance', 'word_index': [(15, 15)], 'id': 'P08-1105.3'}, 'e2': {'word': 'blog posts', 'word_index': [(9, 10)], 'id': 'P08-1105.2'}, 'entity_replacement': {'0:3': 'ENTITYUNRELATED', '9:10': 'ENTITYOTHER', '15:15': 'ENTITY', '19:19': 'ENTITYUNRELATED'}}	Topical blog post retrieval is the task of ranking blog posts with respect to their relevance for a given topic .
To improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process.	textual credibility indicators	topical blog post retrieval	usage	{'e1': {'word': 'textual credibility indicators', 'word_index': [(8, 10)], 'id': 'P08-1105.6'}, 'e2': {'word': 'topical blog post retrieval', 'word_index': [(2, 5)], 'id': 'P08-1105.5'}, 'entity_replacement': {'2:5': 'ENTITYOTHER', '8:10': 'ENTITY', '13:14': 'ENTITYUNRELATED'}}	To improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process .
We describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models.	indicators	retrieval approach	usage	{'e1': {'word': 'indicators', 'word_index': [(6, 6)], 'id': 'P08-1105.11'}, 'e2': {'word': 'retrieval approach', 'word_index': [(14, 15)], 'id': 'P08-1105.12'}, 'entity_replacement': {'6:6': 'ENTITY', '14:15': 'ENTITYOTHER', '18:19': 'ENTITYUNRELATED'}}	We describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models .
Experiments on the TREC Blog track test set show that both groups of credibility indicators significantly improve retrieval effectiveness; the best performance is achieved when combining them.	credibility indicators	retrieval effectiveness	result	{'e1': {'word': 'credibility indicators', 'word_index': [(13, 14)], 'id': 'P08-1105.15'}, 'e2': {'word': 'retrieval effectiveness', 'word_index': [(17, 18)], 'id': 'P08-1105.16'}, 'entity_replacement': {'3:7': 'ENTITYUNRELATED', '13:14': 'ENTITY', '17:18': 'ENTITYOTHER'}}	Experiments on the TREC Blog track test set show that both groups of credibility indicators significantly improve retrieval effectiveness ; the best performance is achieved when combining them .
Four problems render vector space model (VSM)-based text classification approach ineffective: 1) Many words within song lyrics actually contribute little to sentiment; 2) Nouns and verbs used to express sentiment are ambiguous; 3) Negations and modifiers around the sentiment keywords make particular contributions to sentiment; 4) Song lyric is usually very short.	words	song lyrics	part_whole	{'e1': {'word': 'words', 'word_index': [(19, 19)], 'id': 'P08-2034.4'}, 'e2': {'word': 'song lyrics', 'word_index': [(21, 22)], 'id': 'P08-2034.5'}, 'entity_replacement': {'3:13': 'ENTITYUNRELATED', '19:19': 'ENTITY', '21:22': 'ENTITYOTHER', '27:27': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '37:37': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED', '45:45': 'ENTITYUNRELATED', '48:49': 'ENTITYUNRELATED', '54:54': 'ENTITYUNRELATED', '58:59': 'ENTITYUNRELATED'}}	Four problems render vector space model ( VSM ) - based text classification approach ineffective : 1 ) Many words within song lyrics actually contribute little to sentiment ; 2 ) Nouns and verbs used to express sentiment are ambiguous ; 3 ) Negations and modifiers around the sentiment keywords make particular contributions to sentiment ; 4 ) Song lyric is usually very short .
To address these problems, the sentiment vector space model (s-VSM) is proposed to represent song lyric document.	sentiment vector space model (s-VSM)	song lyric document	model-feature	{'e1': {'word': 'sentiment vector space model (s-VSM)', 'word_index': [(6, 12)], 'id': 'P08-2034.15'}, 'e2': {'word': 'song lyric document', 'word_index': [(17, 19)], 'id': 'P08-2034.16'}, 'entity_replacement': {'6:12': 'ENTITY', '17:19': 'ENTITYOTHER'}}	To address these problems , the sentiment vector space model ( s-VSM ) is proposed to represent song lyric document .
The preliminary experiments prove that the s-VSM model outperforms the VSM model in the lyric-based song sentiment classification task.	s-VSM model	VSM model	compare	{'e1': {'word': 's-VSM model', 'word_index': [(6, 7)], 'id': 'P08-2034.17'}, 'e2': {'word': 'VSM model', 'word_index': [(10, 11)], 'id': 'P08-2034.18'}, 'entity_replacement': {'6:7': 'ENTITY', '10:11': 'ENTITYOTHER', '14:20': 'ENTITYUNRELATED'}}	The preliminary experiments prove that the s-VSM model outperforms the VSM model in the lyric - based song sentiment classification task .
This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%-96.7% accuracy depending on classification method).	source language	EUROPARL corpus	model-feature	{'e1': {'word': 'source language', 'word_index': [(12, 13)], 'id': 'C08-1118.1'}, 'e2': {'word': 'EUROPARL corpus', 'word_index': [(21, 22)], 'id': 'C08-1118.2'}, 'entity_replacement': {'12:13': 'ENTITY', '21:22': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED'}}	This paper shows that it is very often possible to identify the source language of medium - length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams ( 87.2 %-96.7 % accuracy depending on classification method ) .
This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%-96.7% accuracy depending on classification method).	frequency counts	word n-grams	model-feature	{'e1': {'word': 'frequency counts', 'word_index': [(27, 28)], 'id': 'C08-1118.3'}, 'e2': {'word': 'word n-grams', 'word_index': [(30, 31)], 'id': 'C08-1118.4'}, 'entity_replacement': {'12:13': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '27:28': 'ENTITY', '30:31': 'ENTITYOTHER', '36:36': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED'}}	This paper shows that it is very often possible to identify the source language of medium - length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams ( 87.2 %-96.7 % accuracy depending on classification method ) .
This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%-96.7% accuracy depending on classification method).	classification method	accuracy	result	{'e1': {'word': 'classification method', 'word_index': [(39, 40)], 'id': 'C08-1118.6'}, 'e2': {'word': 'accuracy', 'word_index': [(36, 36)], 'id': 'C08-1118.5'}, 'entity_replacement': {'12:13': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '36:36': 'ENTITYOTHER', '39:40': 'ENTITY'}}	This paper shows that it is very often possible to identify the source language of medium - length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams ( 87.2 %-96.7 % accuracy depending on classification method ) .
Words in Chinese text are not naturally separated by delimiters, which poses a challenge to standard machine translation (MT) systems.	Words	Chinese text	part_whole	{'e1': {'word': 'Words', 'word_index': [(0, 0)], 'id': 'C08-1128.1'}, 'e2': {'word': 'Chinese text', 'word_index': [(2, 3)], 'id': 'C08-1128.2'}, 'entity_replacement': {'0:0': 'ENTITY', '2:3': 'ENTITYOTHER', '9:9': 'ENTITYUNRELATED', '16:22': 'ENTITYUNRELATED'}}	Words in Chinese text are not naturally separated by delimiters , which poses a challenge to standard machine translation ( MT ) systems .
In MT, the widely used approach is to apply a Chinese word segmenter trained from manually annotated data, using a fixed lexicon.	Chinese word segmenter	MT	usage	{'e1': {'word': 'Chinese word segmenter', 'word_index': [(11, 13)], 'id': 'C08-1128.6'}, 'e2': {'word': 'MT', 'word_index': [(1, 1)], 'id': 'C08-1128.5'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '11:13': 'ENTITY', '16:18': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED'}}	In MT , the widely used approach is to apply a Chinese word segmenter trained from manually annotated data , using a fixed lexicon .
We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT.	monolingual and bilingual information	Bayesian semi-supervised Chinese word segmentation model	usage	{'e1': {'word': 'monolingual and bilingual information', 'word_index': [(12, 15)], 'id': 'C08-1128.12'}, 'e2': {'word': 'Bayesian semi-supervised Chinese word segmentation model', 'word_index': [(3, 8)], 'id': 'C08-1128.11'}, 'entity_replacement': {'3:8': 'ENTITYOTHER', '12:15': 'ENTITY', '19:19': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED'}}	We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT .
We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT.	segmentation	MT	usage	{'e1': {'word': 'segmentation', 'word_index': [(19, 19)], 'id': 'C08-1128.13'}, 'e2': {'word': 'MT', 'word_index': [(22, 22)], 'id': 'C08-1128.14'}, 'entity_replacement': {'3:8': 'ENTITYUNRELATED', '12:15': 'ENTITYUNRELATED', '19:19': 'ENTITY', '22:22': 'ENTITYOTHER'}}	We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT .
Indeed, automatic evaluations need high-quality data that allow the comparison of both automatic and human translations.	high-quality data	automatic evaluations	usage	{'e1': {'word': 'high-quality data', 'word_index': [(5, 8)], 'id': 'C08-2010.7'}, 'e2': {'word': 'automatic evaluations', 'word_index': [(2, 3)], 'id': 'C08-2010.6'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '5:8': 'ENTITY', '15:18': 'ENTITYUNRELATED'}}	Indeed , automatic evaluations need high - quality data that allow the comparison of both automatic and human translations .
This paper describes the impact of using different-quality references on evaluation.	different-quality references	evaluation	usage	{'e1': {'word': 'different-quality references', 'word_index': [(7, 10)], 'id': 'C08-2010.9'}, 'e2': {'word': 'evaluation', 'word_index': [(12, 12)], 'id': 'C08-2010.10'}, 'entity_replacement': {'7:10': 'ENTITY', '12:12': 'ENTITYOTHER'}}	This paper describes the impact of using different - quality references on evaluation .
Thus, the limitations of the automatic metrics used within MT are also discussed in this regard.	automatic metrics	MT	usage	{'e1': {'word': 'automatic metrics', 'word_index': [(6, 7)], 'id': 'C08-2010.11'}, 'e2': {'word': 'MT', 'word_index': [(10, 10)], 'id': 'C08-2010.12'}, 'entity_replacement': {'6:7': 'ENTITY', '10:10': 'ENTITYOTHER'}}	Thus , the limitations of the automatic metrics used within MT are also discussed in this regard .
The tool supports queries with an arbitrary number of wildcards.	wildcards	queries	model-feature	{'e1': {'word': 'wildcards', 'word_index': [(9, 9)], 'id': 'C08-3010.4'}, 'e2': {'word': 'queries', 'word_index': [(3, 3)], 'id': 'C08-3010.3'}, 'entity_replacement': {'3:3': 'ENTITYOTHER', '9:9': 'ENTITY'}}	The tool supports queries with an arbitrary number of wildcards .
This paper presents an approach to the unsupervised learning of parts of speech which uses both morphological and syntactic information.	morphological and syntactic information	unsupervised learning	usage	{'e1': {'word': 'morphological and syntactic information', 'word_index': [(16, 19)], 'id': 'W03-2907.3'}, 'e2': {'word': 'unsupervised learning', 'word_index': [(7, 8)], 'id': 'W03-2907.1'}, 'entity_replacement': {'7:8': 'ENTITYOTHER', '10:12': 'ENTITYUNRELATED', '16:19': 'ENTITY'}}	This paper presents an approach to the unsupervised learning of parts of speech which uses both morphological and syntactic information .
While the model is more complex than those which have been employed for unsupervised learning of POS tags in English, which use only syntactic information, the variety of languages in the world requires that we consider morphology as well.	syntactic information	unsupervised learning	usage	{'e1': {'word': 'syntactic information', 'word_index': [(24, 25)], 'id': 'W03-2907.7'}, 'e2': {'word': 'unsupervised learning', 'word_index': [(13, 14)], 'id': 'W03-2907.5'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '16:19': 'ENTITYUNRELATED', '24:25': 'ENTITY', '30:30': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED'}}	While the model is more complex than those which have been employed for unsupervised learning of POS tags in English , which use only syntactic information , the variety of languages in the world requires that we consider morphology as well .
While the model is more complex than those which have been employed for unsupervised learning of POS tags in English, which use only syntactic information, the variety of languages in the world requires that we consider morphology as well.	morphology	languages	part_whole	{'e1': {'word': 'morphology', 'word_index': [(38, 38)], 'id': 'W03-2907.9'}, 'e2': {'word': 'languages', 'word_index': [(30, 30)], 'id': 'W03-2907.8'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '16:19': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '30:30': 'ENTITYOTHER', '38:38': 'ENTITY'}}	While the model is more complex than those which have been employed for unsupervised learning of POS tags in English , which use only syntactic information , the variety of languages in the world requires that we consider morphology as well .
In many languages, morphology provides better clues to a word's category than word order.	morphology	word order	compare	{'e1': {'word': 'morphology', 'word_index': [(4, 4)], 'id': 'W03-2907.11'}, 'e2': {'word': 'word order', 'word_index': [(14, 15)], 'id': 'W03-2907.12'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '4:4': 'ENTITY', '14:15': 'ENTITYOTHER'}}	In many languages , morphology provides better clues to a word 's category than word order .
We present the computational model for POS learning, and present results for applying it to Bulgarian, a Slavic language with relatively free word order and rich morphology.	computational model	POS learning	usage	{'e1': {'word': 'computational model', 'word_index': [(3, 4)], 'id': 'W03-2907.13'}, 'e2': {'word': 'POS learning', 'word_index': [(6, 7)], 'id': 'W03-2907.14'}, 'entity_replacement': {'3:4': 'ENTITY', '6:7': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '19:20': 'ENTITYUNRELATED', '23:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	We present the computational model for POS learning , and present results for applying it to Bulgarian , a Slavic language with relatively free word order and rich morphology .
We present the computational model for POS learning, and present results for applying it to Bulgarian, a Slavic language with relatively free word order and rich morphology.	Bulgarian	free word order	model-feature	{'e1': {'word': 'Bulgarian', 'word_index': [(16, 16)], 'id': 'W03-2907.15'}, 'e2': {'word': 'free word order', 'word_index': [(23, 25)], 'id': 'W03-2907.17'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '6:7': 'ENTITYUNRELATED', '16:16': 'ENTITY', '19:20': 'ENTITYUNRELATED', '23:25': 'ENTITYOTHER', '27:28': 'ENTITYUNRELATED'}}	We present the computational model for POS learning , and present results for applying it to Bulgarian , a Slavic language with relatively free word order and rich morphology .
We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies.	generative history-based latent variable model	CoNLL 2008 shared task	usage	{'e1': {'word': 'generative history-based latent variable model', 'word_index': [(16, 22)], 'id': 'W08-2122.2'}, 'e2': {'word': 'CoNLL 2008 shared task', 'word_index': [(9, 12)], 'id': 'W08-2122.1'}, 'entity_replacement': {'9:12': 'ENTITYOTHER', '16:22': 'ENTITY', '28:28': 'ENTITYUNRELATED', '31:33': 'ENTITYUNRELATED', '36:39': 'ENTITYUNRELATED'}}	We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history - based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies .
We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies.	derivation	syntactic and semantic dependencies	model-feature	{'e1': {'word': 'derivation', 'word_index': [(28, 28)], 'id': 'W08-2122.3'}, 'e2': {'word': 'syntactic and semantic dependencies', 'word_index': [(36, 39)], 'id': 'W08-2122.5'}, 'entity_replacement': {'9:12': 'ENTITYUNRELATED', '16:22': 'ENTITYUNRELATED', '28:28': 'ENTITY', '31:33': 'ENTITYUNRELATED', '36:39': 'ENTITYOTHER'}}	We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history - based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies .
The submitted model yields 79.1% macro-average F1 performance, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1.	model	macro-average F1 performance	result	{'e1': {'word': 'model', 'word_index': [(2, 2)], 'id': 'W08-2122.6'}, 'e2': {'word': 'macro-average F1 performance', 'word_index': [(6, 8)], 'id': 'W08-2122.7'}, 'entity_replacement': {'2:2': 'ENTITY', '6:8': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED', '23:25': 'ENTITYUNRELATED'}}	The submitted model yields 79.1 % macro-average F1 performance , for the joint task , 86.9 % syntactic dependencies LAS and 71.0 % semantic dependencies F1 .
A larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1.	model	macro-average F1	result	{'e1': {'word': 'model', 'word_index': [(2, 2)], 'id': 'W08-2122.10'}, 'e2': {'word': 'macro-average F1', 'word_index': [(10, 11)], 'id': 'W08-2122.11'}, 'entity_replacement': {'2:2': 'ENTITY', '10:11': 'ENTITYOTHER', '15:17': 'ENTITYUNRELATED', '22:24': 'ENTITYUNRELATED'}}	A larger model trained after the deadline achieves 80.5 % macro-average F1 , 87.6 % syntactic dependencies LAS , and 73.1 % semantic dependencies F1 .
Pipelined Natural Language Generation (NLG) systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions, lexical choice, and revision.	architectural modules	language functionalities	usage	{'e1': {'word': 'architectural modules', 'word_index': [(13, 14)], 'id': 'P03-1034.2'}, 'e2': {'word': 'language functionalities', 'word_index': [(19, 20)], 'id': 'P03-1034.3'}, 'entity_replacement': {'0:7': 'ENTITYUNRELATED', '13:14': 'ENTITY', '19:20': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED'}}	Pipelined Natural Language Generation ( NLG ) systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions , lexical choice , and revision .
This has given rise to discussions about the relative placement of these new modules in the overall architecture.	modules	architecture	part_whole	{'e1': {'word': 'modules', 'word_index': [(13, 13)], 'id': 'P03-1034.7'}, 'e2': {'word': 'architecture', 'word_index': [(17, 17)], 'id': 'P03-1034.8'}, 'entity_replacement': {'13:13': 'ENTITY', '17:17': 'ENTITYOTHER'}}	This has given rise to discussions about the relative placement of these new modules in the overall architecture .
We present examples which suggest that in a pipelined NLG architecture, the best approach is to strongly tie it to a revision component.	revision component	pipelined NLG architecture	part_whole	{'e1': {'word': 'revision component', 'word_index': [(22, 23)], 'id': 'P03-1034.13'}, 'e2': {'word': 'pipelined NLG architecture', 'word_index': [(8, 10)], 'id': 'P03-1034.12'}, 'entity_replacement': {'8:10': 'ENTITYOTHER', '22:23': 'ENTITY'}}	We present examples which suggest that in a pipelined NLG architecture , the best approach is to strongly tie it to a revision component .
With performance above 97% accuracy for newspaper text, part of speech (pos) tagging might be considered a solved problem.	part of speech (pos) tagging	accuracy	result	{'e1': {'word': 'part of speech (pos) tagging', 'word_index': [(10, 16)], 'id': 'P06-1088.3'}, 'e2': {'word': 'accuracy', 'word_index': [(5, 5)], 'id': 'P06-1088.1'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '7:8': 'ENTITYUNRELATED', '10:16': 'ENTITY'}}	With performance above 97 % accuracy for newspaper text , part of speech ( pos ) tagging might be considered a solved problem .
Previous studies have shown that allowing the parser to resolve pos tag ambiguity does not improve performance.	parser	pos tag ambiguity	usage	{'e1': {'word': 'parser', 'word_index': [(7, 7)], 'id': 'P06-1088.4'}, 'e2': {'word': 'pos tag ambiguity', 'word_index': [(10, 12)], 'id': 'P06-1088.5'}, 'entity_replacement': {'7:7': 'ENTITY', '10:12': 'ENTITYOTHER'}}	Previous studies have shown that allowing the parser to resolve pos tag ambiguity does not improve performance .
However, for grammar formalisms which use more fine-grained grammatical categories, for example tag and ccg, tagging accuracy is much lower.	fine-grained grammatical categories	grammar formalisms	usage	{'e1': {'word': 'fine-grained grammatical categories', 'word_index': [(8, 12)], 'id': 'P06-1088.7'}, 'e2': {'word': 'grammar formalisms', 'word_index': [(3, 4)], 'id': 'P06-1088.6'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '8:12': 'ENTITY', '16:16': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED'}}	However , for grammar formalisms which use more fine - grained grammatical categories , for example tag and ccg , tagging accuracy is much lower .
We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient ccg parsing.	multi-tagging approach	ccg parsing	usage	{'e1': {'word': 'multi-tagging approach', 'word_index': [(3, 4)], 'id': 'P06-1088.14'}, 'e2': {'word': 'ccg parsing', 'word_index': [(18, 19)], 'id': 'P06-1088.16'}, 'entity_replacement': {'3:4': 'ENTITY', '11:13': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER'}}	We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient ccg parsing .
Although pos tagging accuracy seems high, maintaining some pos tag ambiguity in the language processing pipeline results in more accurate ccg supertagging.	pos tag ambiguity	ccg supertagging	result	{'e1': {'word': 'pos tag ambiguity', 'word_index': [(9, 11)], 'id': 'P06-1088.21'}, 'e2': {'word': 'ccg supertagging', 'word_index': [(21, 22)], 'id': 'P06-1088.23'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '9:11': 'ENTITY', '14:16': 'ENTITYUNRELATED', '21:22': 'ENTITYOTHER'}}	Although pos tagging accuracy seems high , maintaining some pos tag ambiguity in the language processing pipeline results in more accurate ccg supertagging .
Both rhetorical structure and punctuation have been helpful in discourse processing.	punctuation	discourse processing	usage	{'e1': {'word': 'punctuation', 'word_index': [(4, 4)], 'id': 'P06-3008.2'}, 'e2': {'word': 'discourse processing', 'word_index': [(9, 10)], 'id': 'P06-3008.3'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '4:4': 'ENTITY', '9:10': 'ENTITYOTHER'}}	Both rhetorical structure and punctuation have been helpful in discourse processing .
Based on a corpus annotation project, this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts: Colon, Dash, Ellipsis, Exclamation Mark, Question Mark, and Semicolon.	discursive usage	Chinese punctuation marks	model-feature	{'e1': {'word': 'discursive usage', 'word_index': [(11, 12)], 'id': 'P06-3008.5'}, 'e2': {'word': 'Chinese punctuation marks', 'word_index': [(15, 17)], 'id': 'P06-3008.6'}, 'entity_replacement': {'3:5': 'ENTITYUNRELATED', '11:12': 'ENTITY', '15:17': 'ENTITYOTHER', '19:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED'}}	Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : Colon , Dash , Ellipsis , Exclamation Mark , Question Mark , and Semicolon .
The rhetorical patterns of these marks are compared against patterns around cue phrases in general.	rhetorical patterns	patterns	compare	{'e1': {'word': 'rhetorical patterns', 'word_index': [(1, 2)], 'id': 'P06-3008.14'}, 'e2': {'word': 'patterns', 'word_index': [(9, 9)], 'id': 'P06-3008.15'}, 'entity_replacement': {'1:2': 'ENTITY', '9:9': 'ENTITYOTHER', '11:12': 'ENTITYUNRELATED'}}	The rhetorical patterns of these marks are compared against patterns around cue phrases in general .
Results show that these Chinese punctuation marks, though fewer in number than cue phrases, are easy to identify, have strong correlation with certain relations, and can be used as distinctive indicators of nuclearity in Chinese texts.	Chinese punctuation marks	cue phrases	compare	{'e1': {'word': 'Chinese punctuation marks', 'word_index': [(4, 6)], 'id': 'P06-3008.17'}, 'e2': {'word': 'cue phrases', 'word_index': [(13, 14)], 'id': 'P06-3008.18'}, 'entity_replacement': {'4:6': 'ENTITY', '13:14': 'ENTITYOTHER', '38:39': 'ENTITYUNRELATED'}}	Results show that these Chinese punctuation marks , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in Chinese texts .
We show that the crucial operation of consistency checking for such descriptions is NP-complete, and therefore probably intractable, but proceed to develop algorithms which can sometimes alleviate the unpleasant consequences of this intractability.	algorithms	intractability	usage	{'e1': {'word': 'algorithms', 'word_index': [(26, 26)], 'id': 'C90-3007.4'}, 'e2': {'word': 'intractability', 'word_index': [(36, 36)], 'id': 'C90-3007.5'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '26:26': 'ENTITY', '36:36': 'ENTITYOTHER'}}	We show that the crucial operation of consistency checking for such descriptions is NP - complete , and therefore probably intractable , but proceed to develop algorithms which can sometimes alleviate the unpleasant consequences of this intractability .
This paper presents an algorithm for selecting an appropriate classifier word for a noun.	classifier word	noun	model-feature	{'e1': {'word': 'classifier word', 'word_index': [(9, 10)], 'id': 'C94-1091.1'}, 'e2': {'word': 'noun', 'word_index': [(13, 13)], 'id': 'C94-1091.2'}, 'entity_replacement': {'9:10': 'ENTITY', '13:13': 'ENTITYOTHER'}}	This paper presents an algorithm for selecting an appropriate classifier word for a noun .
In Thai language, it frequently happens that there is fluctuation in the choice of classifier for a given concrete noun, both from the point of view of the whole speech community and individual speakers.	classifier	concrete noun	model-feature	{'e1': {'word': 'classifier', 'word_index': [(15, 15)], 'id': 'C94-1091.4'}, 'e2': {'word': 'concrete noun', 'word_index': [(19, 20)], 'id': 'C94-1091.5'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '15:15': 'ENTITY', '19:20': 'ENTITYOTHER', '31:32': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED'}}	In Thai language , it frequently happens that there is fluctuation in the choice of classifier for a given concrete noun , both from the point of view of the whole speech community and individual speakers .
As far as we can do in the rule-based approach is to give a default rule to pick up a corresponding classifier of each noun.	classifier	noun	model-feature	{'e1': {'word': 'classifier', 'word_index': [(23, 23)], 'id': 'C94-1091.11'}, 'e2': {'word': 'noun', 'word_index': [(26, 26)], 'id': 'C94-1091.12'}, 'entity_replacement': {'8:11': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '23:23': 'ENTITY', '26:26': 'ENTITYOTHER'}}	As far as we can do in the rule - based approach is to give a default rule to pick up a corresponding classifier of each noun .
Registration of classifier for each noun is limited to the type of unit classifier because other types are open due to the meaning of representation.	classifier	noun	model-feature	{'e1': {'word': 'classifier', 'word_index': [(2, 2)], 'id': 'C94-1091.13'}, 'e2': {'word': 'noun', 'word_index': [(5, 5)], 'id': 'C94-1091.14'}, 'entity_replacement': {'2:2': 'ENTITY', '5:5': 'ENTITYOTHER', '10:13': 'ENTITYUNRELATED'}}	Registration of classifier for each noun is limited to the type of unit classifier because other types are open due to the meaning of representation .
We propose a corpus-based method (Biber,1993; Nagao,1993; Smadja,1993) which generates Noun Classifier Associations (NCA) to overcome the problems in classifier assignment and semantic construction of noun phrase.	corpus-based method	classifier assignment	usage	{'e1': {'word': 'corpus-based method', 'word_index': [(3, 6)], 'id': 'C94-1091.16'}, 'e2': {'word': 'classifier assignment', 'word_index': [(33, 34)], 'id': 'C94-1091.18'}, 'entity_replacement': {'3:6': 'ENTITY', '22:27': 'ENTITYUNRELATED', '33:34': 'ENTITYOTHER', '36:40': 'ENTITYUNRELATED'}}	We propose a corpus - based method ( Biber , 1993 ; Nagao , 1993 ; Smadja , 1993 ) which generates Noun Classifier Associations ( NCA ) to overcome the problems in classifier assignment and semantic construction of noun phrase .
This paper describes an unsupervised learning method for associative relationships between verb phrases, which is important in developing reliable Q&amp;A systems.	unsupervised learning method	associative relationships between verb phrases	usage	{'e1': {'word': 'unsupervised learning method', 'word_index': [(4, 6)], 'id': 'C02-1120.1'}, 'e2': {'word': 'associative relationships between verb phrases', 'word_index': [(8, 12)], 'id': 'C02-1120.2'}, 'entity_replacement': {'4:6': 'ENTITY', '8:12': 'ENTITYOTHER', '20:23': 'ENTITYUNRELATED'}}	This paper describes an unsupervised learning method for associative relationships between verb phrases , which is important in developing reliable Q&amp ; A systems .
Our aim is to develop an unsupervised learning method that can obtain such an associative relationship, which we call scenario consistency.	unsupervised learning method	associative relationship	usage	{'e1': {'word': 'unsupervised learning method', 'word_index': [(6, 8)], 'id': 'C02-1120.12'}, 'e2': {'word': 'associative relationship', 'word_index': [(14, 15)], 'id': 'C02-1120.13'}, 'entity_replacement': {'6:8': 'ENTITY', '14:15': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	Our aim is to develop an unsupervised learning method that can obtain such an associative relationship , which we call scenario consistency .
The method we are currently working on uses an expectation-maximization (EM) based word-clustering algorithm, and we have evaluated the effectiveness of this method using Japanese verb phrases.	expectation-maximization (EM) based word-clustering algorithm	Japanese verb phrases	usage	{'e1': {'word': 'expectation-maximization (EM) based word-clustering algorithm', 'word_index': [(9, 19)], 'id': 'C02-1120.15'}, 'e2': {'word': 'Japanese verb phrases', 'word_index': [(31, 33)], 'id': 'C02-1120.16'}, 'entity_replacement': {'9:19': 'ENTITY', '31:33': 'ENTITYOTHER'}}	The method we are currently working on uses an expectation - maximization ( EM ) based word - clustering algorithm , and we have evaluated the effectiveness of this method using Japanese verb phrases .
These models provide principled ways of including additional conditioning variables other than the preceding words, such as morphological or syntactic features.	models	preceding words	compare	{'e1': {'word': 'models', 'word_index': [(1, 1)], 'id': 'C04-1022.4'}, 'e2': {'word': 'preceding words', 'word_index': [(13, 14)], 'id': 'C04-1022.6'}, 'entity_replacement': {'1:1': 'ENTITY', '8:9': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '18:21': 'ENTITYUNRELATED'}}	These models provide principled ways of including additional conditioning variables other than the preceding words , such as morphological or syntactic features .
This paper presents an entirely data-driven model selection procedure based on genetic search, which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks (Arabic and Turkish).	genetic search	entirely data-driven model selection procedure	usage	{'e1': {'word': 'genetic search', 'word_index': [(11, 12)], 'id': 'C04-1022.11'}, 'e2': {'word': 'entirely data-driven model selection procedure', 'word_index': [(4, 8)], 'id': 'C04-1022.10'}, 'entity_replacement': {'4:8': 'ENTITYOTHER', '11:12': 'ENTITY', '20:26': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED'}}	This paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge - based and random selection procedures on two different language modeling tasks ( Arabic and Turkish ) .
This paper presents an entirely data-driven model selection procedure based on genetic search, which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks (Arabic and Turkish).	knowledge-based and random selection procedures	language modeling tasks	usage	{'e1': {'word': 'knowledge-based and random selection procedures', 'word_index': [(20, 26)], 'id': 'C04-1022.12'}, 'e2': {'word': 'language modeling tasks', 'word_index': [(30, 32)], 'id': 'C04-1022.13'}, 'entity_replacement': {'4:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '20:26': 'ENTITY', '30:32': 'ENTITYOTHER', '34:34': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED'}}	This paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge - based and random selection procedures on two different language modeling tasks ( Arabic and Turkish ) .
Landsbergen's advocacy of analytical inverses for compositional syntax rules encourages the application of Definite Clause Grammar techniques to the construction of a parser returning  Montague analysis trees.	analytical inverses	compositional syntax rules	model-feature	{'e1': {'word': 'analytical inverses', 'word_index': [(4, 5)], 'id': 'E85-1004.1'}, 'e2': {'word': 'compositional syntax rules', 'word_index': [(7, 9)], 'id': 'E85-1004.2'}, 'entity_replacement': {'4:5': 'ENTITY', '7:9': 'ENTITYOTHER', '14:17': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '25:27': 'ENTITYUNRELATED'}}	Landsbergen 's advocacy of analytical inverses for compositional syntax rules encourages the application of Definite Clause Grammar techniques to the construction of a parser returning Montague analysis trees .
Landsbergen's advocacy of analytical inverses for compositional syntax rules encourages the application of Definite Clause Grammar techniques to the construction of a parser returning  Montague analysis trees.	Definite Clause Grammar techniques	parser	usage	{'e1': {'word': 'Definite Clause Grammar techniques', 'word_index': [(14, 17)], 'id': 'E85-1004.3'}, 'e2': {'word': 'parser', 'word_index': [(23, 23)], 'id': 'E85-1004.4'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '7:9': 'ENTITYUNRELATED', '14:17': 'ENTITY', '23:23': 'ENTITYOTHER', '25:27': 'ENTITYUNRELATED'}}	Landsbergen 's advocacy of analytical inverses for compositional syntax rules encourages the application of Definite Clause Grammar techniques to the construction of a parser returning Montague analysis trees .
A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing* and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae.	augmented Friedman - Warren algorithm	parser MDCC	usage	{'e1': {'word': 'augmented Friedman - Warren algorithm', 'word_index': [(8, 12)], 'id': 'E85-1004.7'}, 'e2': {'word': 'parser MDCC', 'word_index': [(1, 2)], 'id': 'E85-1004.6'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '8:12': 'ENTITY', '14:15': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED', '36:38': 'ENTITYUNRELATED'}}	A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing * and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae .
A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing* and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae.	derivational history	reduced IL formulae	model-feature	{'e1': {'word': 'derivational history', 'word_index': [(32, 33)], 'id': 'E85-1004.10'}, 'e2': {'word': 'reduced IL formulae', 'word_index': [(36, 38)], 'id': 'E85-1004.11'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '8:12': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '23:26': 'ENTITYUNRELATED', '32:33': 'ENTITY', '36:38': 'ENTITYOTHER'}}	A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing * and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae .
Theoretical research in the area of machine translation usually involves the search for and creation of an appropriate formalism.	formalism	machine translation	usage	{'e1': {'word': 'formalism', 'word_index': [(18, 18)], 'id': 'E89-1040.2'}, 'e2': {'word': 'machine translation', 'word_index': [(6, 7)], 'id': 'E89-1040.1'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '18:18': 'ENTITY'}}	Theoretical research in the area of machine translation usually involves the search for and creation of an appropriate formalism .
An important issue in this respect is the way in which the compositionality of translation is to be defined.	compositionality	translation	model-feature	{'e1': {'word': 'compositionality', 'word_index': [(12, 12)], 'id': 'E89-1040.3'}, 'e2': {'word': 'translation', 'word_index': [(14, 14)], 'id': 'E89-1040.4'}, 'entity_replacement': {'12:12': 'ENTITY', '14:14': 'ENTITYOTHER'}}	An important issue in this respect is the way in which the compositionality of translation is to be defined .
In this paper, we will introduce the anaphoric component of the Mimo formalism.	anaphoric component	Mimo formalism	part_whole	{'e1': {'word': 'anaphoric component', 'word_index': [(8, 9)], 'id': 'E89-1040.5'}, 'e2': {'word': 'Mimo formalism', 'word_index': [(12, 13)], 'id': 'E89-1040.6'}, 'entity_replacement': {'8:9': 'ENTITY', '12:13': 'ENTITYOTHER'}}	In this paper , we will introduce the anaphoric component of the Mimo formalism .
In Mimo, the translation of anaphoric relations is compositional.	Mimo	translation	usage	{'e1': {'word': 'Mimo', 'word_index': [(1, 1)], 'id': 'E89-1040.11'}, 'e2': {'word': 'translation', 'word_index': [(4, 4)], 'id': 'E89-1040.12'}, 'entity_replacement': {'1:1': 'ENTITY', '4:4': 'ENTITYOTHER', '6:7': 'ENTITYUNRELATED'}}	In Mimo , the translation of anaphoric relations is compositional .
The anaphoric component is used to define linguistic phenomena such as wh-movement, the passive and the binding of reflexives and pronouns mono-lingually.	anaphoric component	linguistic phenomena	usage	{'e1': {'word': 'anaphoric component', 'word_index': [(1, 2)], 'id': 'E89-1040.14'}, 'e2': {'word': 'linguistic phenomena', 'word_index': [(7, 8)], 'id': 'E89-1040.15'}, 'entity_replacement': {'1:2': 'ENTITY', '7:8': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '17:21': 'ENTITYUNRELATED'}}	The anaphoric component is used to define linguistic phenomena such as wh-movement , the passive and the binding of reflexives and pronouns mono-lingually .
A domain independent model is proposed for the automated interpretation of nominal compounds in English.	domain independent model	automated interpretation	usage	{'e1': {'word': 'domain independent model', 'word_index': [(1, 3)], 'id': 'C96-1062.1'}, 'e2': {'word': 'automated interpretation', 'word_index': [(8, 9)], 'id': 'C96-1062.2'}, 'entity_replacement': {'1:3': 'ENTITY', '8:9': 'ENTITYOTHER', '11:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED'}}	A domain independent model is proposed for the automated interpretation of nominal compounds in English .
A domain independent model is proposed for the automated interpretation of nominal compounds in English.	nominal compounds	English	part_whole	{'e1': {'word': 'nominal compounds', 'word_index': [(11, 12)], 'id': 'C96-1062.3'}, 'e2': {'word': 'English', 'word_index': [(14, 14)], 'id': 'C96-1062.4'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '11:12': 'ENTITY', '14:14': 'ENTITYOTHER'}}	A domain independent model is proposed for the automated interpretation of nominal compounds in English .
This model is meant to account for productive rules of interpretation which are inferred from the morpho-syntactic and semantic characteristics of the nominal constituents.	morpho-syntactic and semantic characteristics	nominal constituents	model-feature	{'e1': {'word': 'morpho-syntactic and semantic characteristics', 'word_index': [(16, 19)], 'id': 'C96-1062.7'}, 'e2': {'word': 'nominal constituents', 'word_index': [(22, 23)], 'id': 'C96-1062.8'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:10': 'ENTITYUNRELATED', '16:19': 'ENTITY', '22:23': 'ENTITYOTHER'}}	This model is meant to account for productive rules of interpretation which are inferred from the morpho-syntactic and semantic characteristics of the nominal constituents .
In particular, we make extensive use of Pustejovsky's principles concerning the predicative information associated with nominals.	predicative information	nominals	model-feature	{'e1': {'word': 'predicative information', 'word_index': [(13, 14)], 'id': 'C96-1062.9'}, 'e2': {'word': 'nominals', 'word_index': [(17, 17)], 'id': 'C96-1062.10'}, 'entity_replacement': {'13:14': 'ENTITY', '17:17': 'ENTITYOTHER'}}	In particular , we make extensive use of Pustejovsky 's principles concerning the predicative information associated with nominals .
We argue that it is necessary to draw a line between generalizable semantic principles and domain-specific semantic information.	generalizable semantic principles	domain-specific semantic information	compare	{'e1': {'word': 'generalizable semantic principles', 'word_index': [(11, 13)], 'id': 'C96-1062.11'}, 'e2': {'word': 'domain-specific semantic information', 'word_index': [(15, 17)], 'id': 'C96-1062.12'}, 'entity_replacement': {'11:13': 'ENTITY', '15:17': 'ENTITYOTHER'}}	We argue that it is necessary to draw a line between generalizable semantic principles and domain-specific semantic information .
We explain this distinction and we show how this model may be applied to the interpretation of compounds in real texts, provided that complementary semantic information are retrieved.	interpretation	compounds	model-feature	{'e1': {'word': 'interpretation', 'word_index': [(15, 15)], 'id': 'C96-1062.13'}, 'e2': {'word': 'compounds', 'word_index': [(17, 17)], 'id': 'C96-1062.14'}, 'entity_replacement': {'15:15': 'ENTITY', '17:17': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED'}}	We explain this distinction and we show how this model may be applied to the interpretation of compounds in real texts , provided that complementary semantic information are retrieved .
We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text.	entity-based representation	discourse	model-feature	{'e1': {'word': 'entity-based representation', 'word_index': [(4, 7)], 'id': 'P05-1018.2'}, 'e2': {'word': 'discourse', 'word_index': [(9, 9)], 'id': 'P05-1018.3'}, 'entity_replacement': {'4:7': 'ENTITY', '9:9': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED'}}	We present a novel entity - based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text .
We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function.	ranking learning problem	coherence assessment	model-feature	{'e1': {'word': 'ranking learning problem', 'word_index': [(6, 8)], 'id': 'P05-1018.7'}, 'e2': {'word': 'coherence assessment', 'word_index': [(2, 3)], 'id': 'P05-1018.6'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '6:8': 'ENTITY', '14:15': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED'}}	We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function .
Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model.	induced model	state-of-the-art coherence model	compare	{'e1': {'word': 'induced model', 'word_index': [(5, 6)], 'id': 'P05-1018.10'}, 'e2': {'word': 'state-of-the-art coherence model', 'word_index': [(13, 21)], 'id': 'P05-1018.12'}, 'entity_replacement': {'5:6': 'ENTITY', '10:10': 'ENTITYUNRELATED', '13:21': 'ENTITYOTHER'}}	Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state - of - the - art coherence model .
Sentence boundary detection in speech is important for enriching speech recognition output, making it easier for humans to read and downstream modules to process.	Sentence boundary detection	speech recognition	usage	{'e1': {'word': 'Sentence boundary detection', 'word_index': [(0, 2)], 'id': 'P05-1056.1'}, 'e2': {'word': 'speech recognition', 'word_index': [(9, 10)], 'id': 'P05-1056.3'}, 'entity_replacement': {'0:2': 'ENTITY', '4:4': 'ENTITYUNRELATED', '9:10': 'ENTITYOTHER'}}	Sentence boundary detection in speech is important for enriching speech recognition output , making it easier for humans to read and downstream modules to process .
In previous work, we have developed hidden Markov model (HMM) and maximum entropy (Maxent) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries.	knowledge sources	hidden Markov model (HMM) and maximum entropy (Maxent) classifiers	usage	{'e1': {'word': 'knowledge sources', 'word_index': [(25, 26)], 'id': 'P05-1056.5'}, 'e2': {'word': 'hidden Markov model (HMM) and maximum entropy (Maxent) classifiers', 'word_index': [(7, 19)], 'id': 'P05-1056.4'}, 'entity_replacement': {'7:19': 'ENTITYOTHER', '25:26': 'ENTITY', '29:30': 'ENTITYUNRELATED'}}	In previous work , we have developed hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries .
We evaluate across two corpora (conversational telephone speech and broadcast news speech) on both human transcriptions and speech recognition output.	human transcriptions	speech recognition	compare	{'e1': {'word': 'human transcriptions', 'word_index': [(16, 17)], 'id': 'P05-1056.10'}, 'e2': {'word': 'speech recognition', 'word_index': [(19, 20)], 'id': 'P05-1056.11'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '16:17': 'ENTITY', '19:20': 'ENTITYOTHER'}}	We evaluate across two corpora ( conversational telephone speech and broadcast news speech ) on both human transcriptions and speech recognition output .
In general, our CRF model yields a lower error rate than the HMM and Max-ent models on the NIST sentence boundary detection task in speech, although it is interesting to note that the best results are achieved by three-way voting among the classifiers.	CRF	HMM and Max-ent models	compare	{'e1': {'word': 'CRF', 'word_index': [(4, 4)], 'id': 'P05-1056.12'}, 'e2': {'word': 'HMM and Max-ent models', 'word_index': [(13, 18)], 'id': 'P05-1056.13'}, 'entity_replacement': {'4:4': 'ENTITY', '13:18': 'ENTITYOTHER', '21:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '42:45': 'ENTITYUNRELATED', '48:48': 'ENTITYUNRELATED'}}	In general , our CRF model yields a lower error rate than the HMM and Max - ent models on the NIST sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three - way voting among the classifiers .
Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic.	topic	training and test data	model-feature	{'e1': {'word': 'topic', 'word_index': [(38, 38)], 'id': 'P05-2008.6'}, 'e2': {'word': 'training and test data', 'word_index': [(31, 34)], 'id': 'P05-2008.5'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '31:34': 'ENTITYOTHER', '38:38': 'ENTITY'}}	Traditional machine learning techniques have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the training and test data with respect to topic .
This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.	emoticons	training data	model-feature	{'e1': {'word': 'emoticons', 'word_index': [(24, 24)], 'id': 'P05-2008.8'}, 'e2': {'word': 'training data', 'word_index': [(20, 21)], 'id': 'P05-2008.7'}, 'entity_replacement': {'20:21': 'ENTITYOTHER', '24:24': 'ENTITY', '34:34': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED'}}	This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with training data labeled with emoticons , which has the potential of being independent of domain , topic and time .
Using natural language processing, we carried out a trend survey on Japanese natural language processing studies that have been done over the last ten years.	natural language processing	Japanese natural language processing studies	usage	{'e1': {'word': 'natural language processing', 'word_index': [(1, 3)], 'id': 'I05-2043.2'}, 'e2': {'word': 'Japanese natural language processing studies', 'word_index': [(12, 16)], 'id': 'I05-2043.3'}, 'entity_replacement': {'1:3': 'ENTITY', '12:16': 'ENTITYOTHER'}}	Using natural language processing , we carried out a trend survey on Japanese natural language processing studies that have been done over the last ten years .
This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful.	co-occurrence similarities	terms	model-feature	{'e1': {'word': 'co-occurrence similarities', 'word_index': [(8, 9)], 'id': 'E99-1034.1'}, 'e2': {'word': 'terms', 'word_index': [(11, 11)], 'id': 'E99-1034.2'}, 'entity_replacement': {'8:9': 'ENTITY', '11:11': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED'}}	This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful .
This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful.	query terms	retrieval	usage	{'e1': {'word': 'query terms', 'word_index': [(14, 15)], 'id': 'E99-1034.3'}, 'e2': {'word': 'retrieval', 'word_index': [(20, 20)], 'id': 'E99-1034.4'}, 'entity_replacement': {'8:9': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:15': 'ENTITY', '20:20': 'ENTITYOTHER'}}	This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful .
The hypothesis under examination is that useful terms tend to be more similar to each other than to other query terms.	useful terms	query terms	compare	{'e1': {'word': 'useful terms', 'word_index': [(6, 7)], 'id': 'E99-1034.5'}, 'e2': {'word': 'query terms', 'word_index': [(19, 20)], 'id': 'E99-1034.6'}, 'entity_replacement': {'6:7': 'ENTITY', '19:20': 'ENTITYOTHER'}}	The hypothesis under examination is that useful terms tend to be more similar to each other than to other query terms .
Term similarities could then be used for determining which query terms are useful and best reflect the user's information need.	Term similarities	query terms	model-feature	{'e1': {'word': 'Term similarities', 'word_index': [(0, 1)], 'id': 'E99-1034.8'}, 'e2': {'word': 'query terms', 'word_index': [(9, 10)], 'id': 'E99-1034.9'}, 'entity_replacement': {'0:1': 'ENTITY', '9:10': 'ENTITYOTHER'}}	Term similarities could then be used for determining which query terms are useful and best reflect the user 's information need .
A possible application would be to use this source of evidence for tuning the weights of the query terms.	weights	query terms	model-feature	{'e1': {'word': 'weights', 'word_index': [(14, 14)], 'id': 'E99-1034.10'}, 'e2': {'word': 'query terms', 'word_index': [(17, 18)], 'id': 'E99-1034.11'}, 'entity_replacement': {'14:14': 'ENTITY', '17:18': 'ENTITYOTHER'}}	A possible application would be to use this source of evidence for tuning the weights of the query terms .
We propose a draft scheme of the model formalizing the structure of communicative context in dialogue interaction.	structure of communicative context	dialogue interaction	model-feature	{'e1': {'word': 'structure of communicative context', 'word_index': [(10, 13)], 'id': 'E85-1041.2'}, 'e2': {'word': 'dialogue interaction', 'word_index': [(15, 16)], 'id': 'E85-1041.3'}, 'entity_replacement': {'7:7': 'ENTITYUNRELATED', '10:13': 'ENTITY', '15:16': 'ENTITYOTHER'}}	We propose a draft scheme of the model formalizing the structure of communicative context in dialogue interaction .
Memo-functions also facilitate a simple way to construct a very compact representation of the parse forest.	Memo-functions	parse forest	usage	{'e1': {'word': 'Memo-functions', 'word_index': [(0, 0)], 'id': 'E91-1012.8'}, 'e2': {'word': 'parse forest', 'word_index': [(14, 15)], 'id': 'E91-1012.9'}, 'entity_replacement': {'0:0': 'ENTITY', '14:15': 'ENTITYOTHER'}}	Memo-functions also facilitate a simple way to construct a very compact representation of the parse forest .
Extended CF grammars (grammars with regular expressions at the right hand side) can be parsed with a simple modification of the LR-parser for normal CF grammars.	regular expressions	grammars	part_whole	{'e1': {'word': 'regular expressions', 'word_index': [(6, 7)], 'id': 'E91-1012.14'}, 'e2': {'word': 'grammars', 'word_index': [(4, 4)], 'id': 'E91-1012.13'}, 'entity_replacement': {'0:2': 'ENTITYUNRELATED', '4:4': 'ENTITYOTHER', '6:7': 'ENTITY', '23:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED'}}	Extended CF grammars ( grammars with regular expressions at the right hand side ) can be parsed with a simple modification of the LR - parser for normal CF grammars .
Extended CF grammars (grammars with regular expressions at the right hand side) can be parsed with a simple modification of the LR-parser for normal CF grammars.	LR-parser	CF grammars	usage	{'e1': {'word': 'LR-parser', 'word_index': [(23, 25)], 'id': 'E91-1012.15'}, 'e2': {'word': 'CF grammars', 'word_index': [(28, 29)], 'id': 'E91-1012.16'}, 'entity_replacement': {'0:2': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '6:7': 'ENTITYUNRELATED', '23:25': 'ENTITY', '28:29': 'ENTITYOTHER'}}	Extended CF grammars ( grammars with regular expressions at the right hand side ) can be parsed with a simple modification of the LR - parser for normal CF grammars .
This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA.	generative probabilistic model	parse trees	model-feature	{'e1': {'word': 'generative probabilistic model', 'word_index': [(4, 6)], 'id': 'P05-1010.1'}, 'e2': {'word': 'parse trees', 'word_index': [(8, 9)], 'id': 'P05-1010.2'}, 'entity_replacement': {'4:6': 'ENTITY', '8:9': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED'}}	This paper defines a generative probabilistic model of parse trees , which we call PCFG-LA .
Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.	parsed corpus	CFG rules	usage	{'e1': {'word': 'parsed corpus', 'word_index': [(8, 9)], 'id': 'P05-1010.9'}, 'e2': {'word': 'CFG rules', 'word_index': [(1, 2)], 'id': 'P05-1010.8'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '8:9': 'ENTITY', '11:11': 'ENTITYUNRELATED', '13:16': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED'}}	Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG - LA model using an EM - algorithm .
Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.	EM-algorithm	training	usage	{'e1': {'word': 'EM-algorithm', 'word_index': [(19, 21)], 'id': 'P05-1010.12'}, 'e2': {'word': 'training', 'word_index': [(11, 11)], 'id': 'P05-1010.10'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '11:11': 'ENTITYOTHER', '13:16': 'ENTITYUNRELATED', '19:21': 'ENTITY'}}	Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG - LA model using an EM - algorithm .
Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.	PCFG-LA	parsing	usage	{'e1': {'word': 'PCFG-LA', 'word_index': [(5, 5)], 'id': 'P05-1010.14'}, 'e2': {'word': 'parsing', 'word_index': [(2, 2)], 'id': 'P05-1010.13'}, 'entity_replacement': {'2:2': 'ENTITYOTHER', '5:5': 'ENTITY', '7:9': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED'}}	Because exact parsing with a PCFG-LA is NP - hard , several approximations are described and empirically compared .
In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.	model	performance	result	{'e1': {'word': 'model', 'word_index': [(11, 11)], 'id': 'P05-1010.18'}, 'e2': {'word': 'performance', 'word_index': [(14, 14)], 'id': 'P05-1010.19'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '11:11': 'ENTITY', '14:14': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '35:37': 'ENTITYUNRELATED', '41:43': 'ENTITYUNRELATED'}}	In experiments using the Penn WSJ corpus , our automatically trained model gave a performance of 86.6 % ( F1 , sentences &lt ; 40 words ) , which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection .
In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.	words	sentences	part_whole	{'e1': {'word': 'words', 'word_index': [(25, 25)], 'id': 'P05-1010.21'}, 'e2': {'word': 'sentences', 'word_index': [(21, 21)], 'id': 'P05-1010.20'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER', '25:25': 'ENTITY', '35:37': 'ENTITYUNRELATED', '41:43': 'ENTITYUNRELATED'}}	In experiments using the Penn WSJ corpus , our automatically trained model gave a performance of 86.6 % ( F1 , sentences &lt ; 40 words ) , which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection .
In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.	manual feature selection	unlexicalized PCFG parser	usage	{'e1': {'word': 'manual feature selection', 'word_index': [(41, 43)], 'id': 'P05-1010.23'}, 'e2': {'word': 'unlexicalized PCFG parser', 'word_index': [(35, 37)], 'id': 'P05-1010.22'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '35:37': 'ENTITYOTHER', '41:43': 'ENTITY'}}	In experiments using the Penn WSJ corpus , our automatically trained model gave a performance of 86.6 % ( F1 , sentences &lt ; 40 words ) , which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection .
This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	SVM	feature-based relation extraction	usage	{'e1': {'word': 'SVM', 'word_index': [(20, 20)], 'id': 'P05-1053.4'}, 'e2': {'word': 'feature-based relation extraction', 'word_index': [(14, 18)], 'id': 'P05-1053.3'}, 'entity_replacement': {'7:12': 'ENTITYUNRELATED', '14:18': 'ENTITYOTHER', '20:20': 'ENTITY'}}	This paper investigates the incorporation of diverse lexical , syntactic and semantic knowledge in feature - based relation extraction using SVM .
Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	phrase chunking	performance improvement	result	{'e1': {'word': 'phrase chunking', 'word_index': [(6, 7)], 'id': 'P05-1053.5'}, 'e2': {'word': 'performance improvement', 'word_index': [(21, 22)], 'id': 'P05-1053.7'}, 'entity_replacement': {'6:7': 'ENTITY', '13:14': 'ENTITYUNRELATED', '21:22': 'ENTITYOTHER', '24:24': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement .
We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	semantic information	performance	result	{'e1': {'word': 'semantic information', 'word_index': [(4, 5)], 'id': 'P05-1053.13'}, 'e2': {'word': 'performance', 'word_index': [(26, 26)], 'id': 'P05-1053.17'}, 'entity_replacement': {'4:5': 'ENTITY', '8:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '17:21': 'ENTITYUNRELATED', '26:26': 'ENTITYOTHER'}}	We also demonstrate how semantic information such as WordNet and Name List , can be used in feature - based relation extraction to further improve the performance .
Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	system	systems	compare	{'e1': {'word': 'system', 'word_index': [(14, 14)], 'id': 'P05-1053.21'}, 'e2': {'word': 'systems', 'word_index': [(18, 18)], 'id': 'P05-1053.22'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:14': 'ENTITY', '18:18': 'ENTITYOTHER', '21:24': 'ENTITYUNRELATED', '28:32': 'ENTITYUNRELATED', '37:37': 'ENTITYUNRELATED', '40:43': 'ENTITYUNRELATED'}}	Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel - based systems by over 20 in F-measure on the 5 ACE relation types .
This paper describes a novel system for acquiring adjectival subcategorization frames (scfs) and associated frequency information from English corpus data.	system	acquiring adjectival subcategorization frames	usage	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'P05-1076.1'}, 'e2': {'word': 'acquiring adjectival subcategorization frames', 'word_index': [(7, 10)], 'id': 'P05-1076.2'}, 'entity_replacement': {'5:5': 'ENTITY', '7:10': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED'}}	This paper describes a novel system for acquiring adjectival subcategorization frames ( scfs ) and associated frequency information from English corpus data .
This paper describes a novel system for acquiring adjectival subcategorization frames (scfs) and associated frequency information from English corpus data.	English	corpus data	model-feature	{'e1': {'word': 'English', 'word_index': [(19, 19)], 'id': 'P05-1076.4'}, 'e2': {'word': 'corpus data', 'word_index': [(20, 21)], 'id': 'P05-1076.5'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '7:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '19:19': 'ENTITY', '20:21': 'ENTITYOTHER'}}	This paper describes a novel system for acquiring adjectival subcategorization frames ( scfs ) and associated frequency information from English corpus data .
The system incorporates a decision-tree classifier for 30 scf types which tests for the presence of grammatical relations (grs) in the output of a robust statistical parser.	decision-tree classifier	system	part_whole	{'e1': {'word': 'decision-tree classifier', 'word_index': [(4, 7)], 'id': 'P05-1076.7'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'P05-1076.6'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:7': 'ENTITY', '10:11': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	The system incorporates a decision - tree classifier for 30 scf types which tests for the presence of grammatical relations ( grs ) in the output of a robust statistical parser .
The system incorporates a decision-tree classifier for 30 scf types which tests for the presence of grammatical relations (grs) in the output of a robust statistical parser.	grammatical relations	output	part_whole	{'e1': {'word': 'grammatical relations', 'word_index': [(18, 19)], 'id': 'P05-1076.9'}, 'e2': {'word': 'output', 'word_index': [(25, 25)], 'id': 'P05-1076.11'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:7': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '18:19': 'ENTITY', '21:21': 'ENTITYUNRELATED', '25:25': 'ENTITYOTHER', '29:30': 'ENTITYUNRELATED'}}	The system incorporates a decision - tree classifier for 30 scf types which tests for the presence of grammatical relations ( grs ) in the output of a robust statistical parser .
It uses a powerful pattern-matching language to classify grs into frames hierarchically in a way that mirrors inheritance-based lexica.	frames	grs	model-feature	{'e1': {'word': 'frames', 'word_index': [(12, 12)], 'id': 'P05-1076.15'}, 'e2': {'word': 'grs', 'word_index': [(10, 10)], 'id': 'P05-1076.14'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '10:10': 'ENTITYOTHER', '12:12': 'ENTITY', '19:22': 'ENTITYUNRELATED'}}	It uses a powerful pattern - matching language to classify grs into frames hierarchically in a way that mirrors inheritance - based lexica .
The experiments show that the system is able to detect scf types with 70% precision and 66% recall rate.	system	70% precision	result	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'P05-1076.18'}, 'e2': {'word': '70% precision', 'word_index': [(13, 15)], 'id': 'P05-1076.20'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:5': 'ENTITY', '10:11': 'ENTITYUNRELATED', '13:15': 'ENTITYOTHER', '17:20': 'ENTITYUNRELATED'}}	The experiments show that the system is able to detect scf types with 70 % precision and 66 % recall rate .
A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.	tool	linguistic annotation	usage	{'e1': {'word': 'tool', 'word_index': [(2, 2)], 'id': 'P05-1076.22'}, 'e2': {'word': 'linguistic annotation', 'word_index': [(4, 5)], 'id': 'P05-1076.23'}, 'entity_replacement': {'2:2': 'ENTITY', '4:5': 'ENTITYOTHER', '7:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '22:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition .
A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.	scfs	corpus data	part_whole	{'e1': {'word': 'scfs', 'word_index': [(7, 7)], 'id': 'P05-1076.24'}, 'e2': {'word': 'corpus data', 'word_index': [(9, 10)], 'id': 'P05-1076.25'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '4:5': 'ENTITYUNRELATED', '7:7': 'ENTITY', '9:10': 'ENTITYOTHER', '22:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition .
A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.	training and test data	subcategorization acquisition	usage	{'e1': {'word': 'training and test data', 'word_index': [(22, 25)], 'id': 'P05-1076.26'}, 'e2': {'word': 'subcategorization acquisition', 'word_index': [(27, 28)], 'id': 'P05-1076.27'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '4:5': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '22:25': 'ENTITY', '27:28': 'ENTITYOTHER'}}	A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition .
We present a tool, called ILIMP, which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ANA] for anaphoric or [IMP] for impersonal or expletive.	French	raw text	model-feature	{'e1': {'word': 'French', 'word_index': [(16, 16)], 'id': 'I05-2013.4'}, 'e2': {'word': 'raw text', 'word_index': [(13, 14)], 'id': 'I05-2013.3'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '16:16': 'ENTITY', '23:23': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '37:39': 'ENTITYUNRELATED', '41:41': 'ENTITYUNRELATED', '43:45': 'ENTITYUNRELATED', '47:47': 'ENTITYUNRELATED', '49:49': 'ENTITYUNRELATED'}}	We present a tool , called ILIMP , which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ ANA ] for anaphoric or [ IMP ] for impersonal or expletive .
We present a tool, called ILIMP, which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ANA] for anaphoric or [IMP] for impersonal or expletive.	[ANA]	pronoun il	model-feature	{'e1': {'word': '[ANA]', 'word_index': [(37, 39)], 'id': 'I05-2013.7'}, 'e2': {'word': 'pronoun il', 'word_index': [(30, 31)], 'id': 'I05-2013.6'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '30:31': 'ENTITYOTHER', '37:39': 'ENTITY', '41:41': 'ENTITYUNRELATED', '43:45': 'ENTITYUNRELATED', '47:47': 'ENTITYUNRELATED', '49:49': 'ENTITYUNRELATED'}}	We present a tool , called ILIMP , which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ ANA ] for anaphoric or [ IMP ] for impersonal or expletive .
This tool is therefore designed to distinguish between the anaphoric occurrences of il, for which an anaphora resolution system has to look for an antecedent, and the expletive occurrences of this pronoun, for which it does not make sense to look for an antecedent.	expletive occurrences	pronoun	model-feature	{'e1': {'word': 'expletive occurrences', 'word_index': [(29, 30)], 'id': 'I05-2013.15'}, 'e2': {'word': 'pronoun', 'word_index': [(33, 33)], 'id': 'I05-2013.16'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '9:12': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '29:30': 'ENTITY', '33:33': 'ENTITYOTHER'}}	This tool is therefore designed to distinguish between the anaphoric occurrences of il , for which an anaphora resolution system has to look for an antecedent , and the expletive occurrences of this pronoun , for which it does not make sense to look for an antecedent .
The precision rate for ILIMP is 97,5%.	ILIMP	precision rate	result	{'e1': {'word': 'ILIMP', 'word_index': [(4, 4)], 'id': 'I05-2013.18'}, 'e2': {'word': 'precision rate', 'word_index': [(1, 2)], 'id': 'I05-2013.17'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '4:4': 'ENTITY'}}	The precision rate for ILIMP is 97,5 %.
Other tasks using the method developed for ILIMP are described briefly, as well as the use of ILIMP in a modular syntactic analysis system.	method	tasks	usage	{'e1': {'word': 'method', 'word_index': [(4, 4)], 'id': 'I05-2013.21'}, 'e2': {'word': 'tasks', 'word_index': [(1, 1)], 'id': 'I05-2013.20'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '4:4': 'ENTITY', '7:7': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '22:24': 'ENTITYUNRELATED'}}	Other tasks using the method developed for ILIMP are described briefly , as well as the use of ILIMP in a modular syntactic analysis system .
Other tasks using the method developed for ILIMP are described briefly, as well as the use of ILIMP in a modular syntactic analysis system.	ILIMP	syntactic analysis system	usage	{'e1': {'word': 'ILIMP', 'word_index': [(18, 18)], 'id': 'I05-2013.23'}, 'e2': {'word': 'syntactic analysis system', 'word_index': [(22, 24)], 'id': 'I05-2013.24'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:4': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '18:18': 'ENTITY', '22:24': 'ENTITYOTHER'}}	Other tasks using the method developed for ILIMP are described briefly , as well as the use of ILIMP in a modular syntactic analysis system .
Systemic grammar has been used for AI text generation work in the past, but the implementations have tended be ad hoc or inefficient.	Systemic grammar	AI text generation	usage	{'e1': {'word': 'Systemic grammar', 'word_index': [(0, 1)], 'id': 'E85-1037.1'}, 'e2': {'word': 'AI text generation', 'word_index': [(6, 8)], 'id': 'E85-1037.2'}, 'entity_replacement': {'0:1': 'ENTITY', '6:8': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	Systemic grammar has been used for AI text generation work in the past , but the implementations have tended be ad hoc or inefficient .
This paper presents an approach to systemic text generation where AI problem solving techniques are applied directly to an unadulterated systemic grammar.	AI problem solving techniques	systemic grammar	usage	{'e1': {'word': 'AI problem solving techniques', 'word_index': [(10, 13)], 'id': 'E85-1037.5'}, 'e2': {'word': 'systemic grammar', 'word_index': [(20, 21)], 'id': 'E85-1037.6'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:13': 'ENTITY', '20:21': 'ENTITYOTHER'}}	This paper presents an approach to systemic text generation where AI problem solving techniques are applied directly to an unadulterated systemic grammar .
The result is simple, efficient text generation firmly based in a linguistic theory.	linguistic theory	text generation	usage	{'e1': {'word': 'linguistic theory', 'word_index': [(12, 13)], 'id': 'E85-1037.11'}, 'e2': {'word': 'text generation', 'word_index': [(6, 7)], 'id': 'E85-1037.10'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '12:13': 'ENTITY'}}	The result is simple , efficient text generation firmly based in a linguistic theory .
This paper presents a critical discussion of the various approaches that have been used in the evaluation of Natural Language systems.	critical discussion	evaluation of Natural Language systems	topic	{'e1': {'word': 'critical discussion', 'word_index': [(4, 5)], 'id': 'E89-1016.1'}, 'e2': {'word': 'evaluation of Natural Language systems', 'word_index': [(16, 20)], 'id': 'E89-1016.3'}, 'entity_replacement': {'4:5': 'ENTITY', '9:9': 'ENTITYUNRELATED', '16:20': 'ENTITYOTHER'}}	This paper presents a critical discussion of the various approaches that have been used in the evaluation of Natural Language systems .
We conclude that previous approaches have neglected to evaluate systems in the context of their use, e.g. solving a task requiring data retrieval.	data retrieval	task	usage	{'e1': {'word': 'data retrieval', 'word_index': [(22, 23)], 'id': 'E89-1016.7'}, 'e2': {'word': 'task', 'word_index': [(20, 20)], 'id': 'E89-1016.6'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '9:9': 'ENTITYUNRELATED', '20:20': 'ENTITYOTHER', '22:23': 'ENTITY'}}	We conclude that previous approaches have neglected to evaluate systems in the context of their use , e.g. solving a task requiring data retrieval .
In the second half of the paper, we report a laboratory study using the Wizard of Oz technique to identify NL requirements for carrying out this task.	Wizard of Oz technique	laboratory study	usage	{'e1': {'word': 'Wizard of Oz technique', 'word_index': [(15, 18)], 'id': 'E89-1016.10'}, 'e2': {'word': 'laboratory study', 'word_index': [(11, 12)], 'id': 'E89-1016.9'}, 'entity_replacement': {'11:12': 'ENTITYOTHER', '15:18': 'ENTITY', '21:22': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED'}}	In the second half of the paper , we report a laboratory study using the Wizard of Oz technique to identify NL requirements for carrying out this task .
We identify three important requirements which arose from the task that we gave our subjects: operators specific to the task of database access, complex contextual reference and reference to the structure of the information source.	structure	information source	model-feature	{'e1': {'word': 'structure', 'word_index': [(32, 32)], 'id': 'E89-1016.19'}, 'e2': {'word': 'information source', 'word_index': [(35, 36)], 'id': 'E89-1016.20'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '32:32': 'ENTITY', '35:36': 'ENTITYOTHER'}}	We identify three important requirements which arose from the task that we gave our subjects : operators specific to the task of database access , complex contextual reference and reference to the structure of the information source .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts.	Semantic theories	natural language	topic	{'e1': {'word': 'Semantic theories', 'word_index': [(0, 1)], 'id': 'E93-1013.1'}, 'e2': {'word': 'natural language', 'word_index': [(3, 4)], 'id': 'E93-1013.2'}, 'entity_replacement': {'0:1': 'ENTITY', '3:4': 'ENTITYOTHER', '6:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts.	meanings	utterances	model-feature	{'e1': {'word': 'meanings', 'word_index': [(6, 6)], 'id': 'E93-1013.3'}, 'e2': {'word': 'utterances', 'word_index': [(8, 8)], 'id': 'E93-1013.4'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '6:6': 'ENTITY', '8:8': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts.	meanings	lexical items	model-feature	{'e1': {'word': 'meanings', 'word_index': [(11, 11)], 'id': 'E93-1013.5'}, 'e2': {'word': 'lexical items', 'word_index': [(13, 14)], 'id': 'E93-1013.6'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:11': 'ENTITY', '13:14': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts.	meaning	units	model-feature	{'e1': {'word': 'meaning', 'word_index': [(20, 20)], 'id': 'E93-1013.8'}, 'e2': {'word': 'units', 'word_index': [(23, 23)], 'id': 'E93-1013.9'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '3:4': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITY', '23:23': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED'}}	Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts .
Traditionally, meanings are combined via function composition, which works well when constituent structure trees are used to guide semantic composition.	constituent structure trees	semantic composition	usage	{'e1': {'word': 'constituent structure trees', 'word_index': [(13, 15)], 'id': 'E93-1013.13'}, 'e2': {'word': 'semantic composition', 'word_index': [(20, 21)], 'id': 'E93-1013.14'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '6:7': 'ENTITYUNRELATED', '13:15': 'ENTITY', '20:21': 'ENTITYOTHER'}}	Traditionally , meanings are combined via function composition , which works well when constituent structure trees are used to guide semantic composition .
More recently, the functional structure of LFG has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format.	functional structure	LFG	part_whole	{'e1': {'word': 'functional structure', 'word_index': [(4, 5)], 'id': 'E93-1013.15'}, 'e2': {'word': 'LFG', 'word_index': [(7, 7)], 'id': 'E93-1013.16'}, 'entity_replacement': {'4:5': 'ENTITY', '7:7': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:26': 'ENTITYUNRELATED'}}	More recently , the functional structure of LFG has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format .
More recently, the functional structure of LFG has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format.	syntactic information	derivations	usage	{'e1': {'word': 'syntactic information', 'word_index': [(14, 15)], 'id': 'E93-1013.17'}, 'e2': {'word': 'derivations', 'word_index': [(19, 19)], 'id': 'E93-1013.18'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '14:15': 'ENTITY', '19:19': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '24:26': 'ENTITYUNRELATED'}}	More recently , the functional structure of LFG has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format .
In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the functional structure.	compositional approaches	deductive approach	compare	{'e1': {'word': 'compositional approaches', 'word_index': [(3, 4)], 'id': 'E93-1013.24'}, 'e2': {'word': 'deductive approach', 'word_index': [(9, 10)], 'id': 'E93-1013.25'}, 'entity_replacement': {'3:4': 'ENTITY', '9:10': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	In contrast to compositional approaches , we present a deductive approach to assembling meanings , based on reasoning with constraints , which meshes well with the unordered nature of information in the functional structure .
In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the functional structure.	information	functional structure	part_whole	{'e1': {'word': 'information', 'word_index': [(29, 29)], 'id': 'E93-1013.28'}, 'e2': {'word': 'functional structure', 'word_index': [(32, 33)], 'id': 'E93-1013.29'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '29:29': 'ENTITY', '32:33': 'ENTITYOTHER'}}	In contrast to compositional approaches , we present a deductive approach to assembling meanings , based on reasoning with constraints , which meshes well with the unordered nature of information in the functional structure .
Our use of linear logic as a 'glue' for assembling meanings also allows for a coherent treatment of modification as well as of the LFG requirements of completeness and coherence.	completeness	LFG	model-feature	{'e1': {'word': 'completeness', 'word_index': [(29, 29)], 'id': 'E93-1013.34'}, 'e2': {'word': 'LFG', 'word_index': [(26, 26)], 'id': 'E93-1013.33'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '26:26': 'ENTITYOTHER', '29:29': 'ENTITY', '31:31': 'ENTITYUNRELATED'}}	Our use of linear logic as a ' glue ' for assembling meanings also allows for a coherent treatment of modification as well as of the LFG requirements of completeness and coherence .
This paper presents an analysis of temporal anaphora in sentences which contain quantification over events, within the framework of Discourse Representation Theory.	quantification over events	sentences	part_whole	{'e1': {'word': 'quantification over events', 'word_index': [(12, 14)], 'id': 'E95-1036.3'}, 'e2': {'word': 'sentences', 'word_index': [(9, 9)], 'id': 'E95-1036.2'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:9': 'ENTITYOTHER', '12:14': 'ENTITY', '20:22': 'ENTITYUNRELATED'}}	This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .
The analysis in (Partee, 1984) of quantified sentences, introduced by a temporal connective, gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after.	temporal connective	quantified sentences	part_whole	{'e1': {'word': 'temporal connective', 'word_index': [(15, 16)], 'id': 'E95-1036.6'}, 'e2': {'word': 'quantified sentences', 'word_index': [(9, 10)], 'id': 'E95-1036.5'}, 'entity_replacement': {'9:10': 'ENTITYOTHER', '15:16': 'ENTITY', '21:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	The analysis in ( Partee , 1984 ) of quantified sentences , introduced by a temporal connective , gives the wrong truth - conditions when the temporal connective in the subordinate clause is before or after .
The analysis in (Partee, 1984) of quantified sentences, introduced by a temporal connective, gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after.	temporal connective	subordinate clause	part_whole	{'e1': {'word': 'temporal connective', 'word_index': [(26, 27)], 'id': 'E95-1036.8'}, 'e2': {'word': 'subordinate clause', 'word_index': [(30, 31)], 'id': 'E95-1036.9'}, 'entity_replacement': {'9:10': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '21:23': 'ENTITYUNRELATED', '26:27': 'ENTITY', '30:31': 'ENTITYOTHER'}}	The analysis in ( Partee , 1984 ) of quantified sentences , introduced by a temporal connective , gives the wrong truth - conditions when the temporal connective in the subordinate clause is before or after .
This problem has been previously analyzed in (de Swart, 1991) as an instance of the proportion problem and given a solution from a Generalized Quantifier approach.	Generalized Quantifier approach	proportion problem	topic	{'e1': {'word': 'Generalized Quantifier approach', 'word_index': [(26, 28)], 'id': 'E95-1036.12'}, 'e2': {'word': 'proportion problem', 'word_index': [(18, 19)], 'id': 'E95-1036.11'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '18:19': 'ENTITYOTHER', '26:28': 'ENTITY'}}	This problem has been previously analyzed in ( de Swart , 1991 ) as an instance of the proportion problem and given a solution from a Generalized Quantifier approach .
By using a careful distinction between the different notions of reference time based on (Kamp and Reyle, 1993), we propose a solution to this problem, within the framework of DRT.	DRT	problem	usage	{'e1': {'word': 'DRT', 'word_index': [(34, 34)], 'id': 'E95-1036.15'}, 'e2': {'word': 'problem', 'word_index': [(28, 28)], 'id': 'E95-1036.14'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '28:28': 'ENTITYOTHER', '34:34': 'ENTITY'}}	By using a careful distinction between the different notions of reference time based on ( Kamp and Reyle , 1993 ) , we propose a solution to this problem , within the framework of DRT .
We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences.	solution	temporal anaphora phenomena	usage	{'e1': {'word': 'solution', 'word_index': [(6, 6)], 'id': 'E95-1036.16'}, 'e2': {'word': 'temporal anaphora phenomena', 'word_index': [(9, 11)], 'id': 'E95-1036.17'}, 'entity_replacement': {'6:6': 'ENTITY', '9:11': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED'}}	We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences .
"This paper proposes an automatic, essentially domain-independent means of evaluating Spoken Language Systems (SLS) which combines software we have developed for that purpose (the ""Comparator"") and a set of specifications for answer expressions (the ""Common Answer Specification"", or CAS)."	software	domain-independent means of evaluating Spoken Language Systems (SLS)	usage	{'e1': {'word': 'software', 'word_index': [(21, 21)], 'id': 'H89-2019.2'}, 'e2': {'word': 'domain-independent means of evaluating Spoken Language Systems (SLS)', 'word_index': [(7, 18)], 'id': 'H89-2019.1'}, 'entity_replacement': {'7:18': 'ENTITYOTHER', '21:21': 'ENTITY', '31:31': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED', '45:47': 'ENTITYUNRELATED', '51:51': 'ENTITYUNRELATED'}}	"This paper proposes an automatic , essentially domain - independent means of evaluating Spoken Language Systems ( SLS ) which combines software we have developed for that purpose ( the "" Comparator "" ) and a set of specifications for answer expressions ( the "" Common Answer Specification "" , or CAS ) ."
"This paper proposes an automatic, essentially domain-independent means of evaluating Spoken Language Systems (SLS) which combines software we have developed for that purpose (the ""Comparator"") and a set of specifications for answer expressions (the ""Common Answer Specification"", or CAS)."	specifications	answer expressions	model-feature	{'e1': {'word': 'specifications', 'word_index': [(38, 38)], 'id': 'H89-2019.4'}, 'e2': {'word': 'answer expressions', 'word_index': [(40, 41)], 'id': 'H89-2019.5'}, 'entity_replacement': {'7:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '38:38': 'ENTITY', '40:41': 'ENTITYOTHER', '45:47': 'ENTITYUNRELATED', '51:51': 'ENTITYUNRELATED'}}	"This paper proposes an automatic , essentially domain - independent means of evaluating Spoken Language Systems ( SLS ) which combines software we have developed for that purpose ( the "" Comparator "" ) and a set of specifications for answer expressions ( the "" Common Answer Specification "" , or CAS ) ."
The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the procedures used by the Comparator.	syntax	answer expressions	model-feature	{'e1': {'word': 'syntax', 'word_index': [(6, 6)], 'id': 'H89-2019.12'}, 'e2': {'word': 'answer expressions', 'word_index': [(8, 9)], 'id': 'H89-2019.13'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '6:6': 'ENTITY', '8:9': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED'}}	The Common Answer Specification determines the syntax of answer expressions , the minimal content that must be included in them , the data to be included in and excluded from test corpora , and the procedures used by the Comparator .
The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the procedures used by the Comparator.	data	test corpora	part_whole	{'e1': {'word': 'data', 'word_index': [(22, 22)], 'id': 'H89-2019.15'}, 'e2': {'word': 'test corpora', 'word_index': [(30, 31)], 'id': 'H89-2019.16'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '22:22': 'ENTITY', '30:31': 'ENTITYOTHER', '35:35': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED'}}	The Common Answer Specification determines the syntax of answer expressions , the minimal content that must be included in them , the data to be included in and excluded from test corpora , and the procedures used by the Comparator .
The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the procedures used by the Comparator.	procedures	Comparator	usage	{'e1': {'word': 'procedures', 'word_index': [(35, 35)], 'id': 'H89-2019.17'}, 'e2': {'word': 'Comparator', 'word_index': [(39, 39)], 'id': 'H89-2019.18'}, 'entity_replacement': {'1:3': 'ENTITYUNRELATED', '6:6': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '35:35': 'ENTITY', '39:39': 'ENTITYOTHER'}}	The Common Answer Specification determines the syntax of answer expressions , the minimal content that must be included in them , the data to be included in and excluded from test corpora , and the procedures used by the Comparator .
Though some details of the CAS are particular to individual domains, the Comparator software is domain-independent, as is the CAS approach.	domains	CAS	model-feature	{'e1': {'word': 'domains', 'word_index': [(10, 10)], 'id': 'H89-2019.20'}, 'e2': {'word': 'CAS', 'word_index': [(5, 5)], 'id': 'H89-2019.19'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '10:10': 'ENTITY', '13:14': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED'}}	Though some details of the CAS are particular to individual domains , the Comparator software is domain-independent , as is the CAS approach .
Though some details of the CAS are particular to individual domains, the Comparator software is domain-independent, as is the CAS approach.	domain-independent	Comparator software	model-feature	{'e1': {'word': 'domain-independent', 'word_index': [(16, 18)], 'id': 'H89-2019.22'}, 'e2': {'word': 'Comparator software', 'word_index': [(13, 14)], 'id': 'H89-2019.21'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '16:18': 'ENTITY', '23:24': 'ENTITYUNRELATED'}}	Though some details of the CAS are particular to individual domains , the Comparator software is domain - independent , as is the CAS approach .
Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications.	Xerox PARC	speech and text image processing	topic	{'e1': {'word': 'Xerox PARC', 'word_index': [(12, 13)], 'id': 'H93-1076.4'}, 'e2': {'word': 'speech and text image processing', 'word_index': [(5, 9)], 'id': 'H93-1076.3'}, 'entity_replacement': {'5:9': 'ENTITYOTHER', '12:13': 'ENTITY', '21:22': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED'}}	Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications .
Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications.	recognition technology	document-oriented applications	part_whole	{'e1': {'word': 'recognition technology', 'word_index': [(21, 22)], 'id': 'H93-1076.5'}, 'e2': {'word': 'document-oriented applications', 'word_index': [(24, 25)], 'id': 'H93-1076.6'}, 'entity_replacement': {'5:9': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '21:22': 'ENTITY', '24:25': 'ENTITYOTHER'}}	Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications .
One is the development of systems that provide functionality similar to that of text processors but operate directly on audio and scanned image data.	text processors	audio and scanned image data	compare	{'e1': {'word': 'text processors', 'word_index': [(13, 14)], 'id': 'H93-1076.7'}, 'e2': {'word': 'audio and scanned image data', 'word_index': [(19, 23)], 'id': 'H93-1076.8'}, 'entity_replacement': {'13:14': 'ENTITY', '19:23': 'ENTITYOTHER'}}	One is the development of systems that provide functionality similar to that of text processors but operate directly on audio and scanned image data .
A second, related theme is the use of speech and text-image recognition to retrieve arbitrary, user-specified information from documents with signal content.	speech and text-image recognition	documents with signal content	usage	{'e1': {'word': 'speech and text-image recognition', 'word_index': [(9, 14)], 'id': 'H93-1076.9'}, 'e2': {'word': 'documents with signal content', 'word_index': [(22, 25)], 'id': 'H93-1076.10'}, 'entity_replacement': {'9:14': 'ENTITY', '22:25': 'ENTITYOTHER'}}	A second , related theme is the use of speech and text - image recognition to retrieve arbitrary , user-specified information from documents with signal content .
In this paper we present a statistical profile of the Named Entity task, a specific information extraction task for which corpora in several languages are available.	statistical profile	Named Entity task	model-feature	{'e1': {'word': 'statistical profile', 'word_index': [(6, 7)], 'id': 'A97-1028.1'}, 'e2': {'word': 'Named Entity task', 'word_index': [(10, 12)], 'id': 'A97-1028.2'}, 'entity_replacement': {'6:7': 'ENTITY', '10:12': 'ENTITYOTHER', '16:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED'}}	In this paper we present a statistical profile of the Named Entity task , a specific information extraction task for which corpora in several languages are available .
In this paper we present a statistical profile of the Named Entity task, a specific information extraction task for which corpora in several languages are available.	languages	corpora	model-feature	{'e1': {'word': 'languages', 'word_index': [(24, 24)], 'id': 'A97-1028.5'}, 'e2': {'word': 'corpora', 'word_index': [(21, 21)], 'id': 'A97-1028.4'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '10:12': 'ENTITYUNRELATED', '16:18': 'ENTITYUNRELATED', '21:21': 'ENTITYOTHER', '24:24': 'ENTITY'}}	In this paper we present a statistical profile of the Named Entity task , a specific information extraction task for which corpora in several languages are available .
Using the results of the statistical analysis, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.	statistical analysis	results	result	{'e1': {'word': 'statistical analysis', 'word_index': [(5, 6)], 'id': 'A97-1028.7'}, 'e2': {'word': 'results', 'word_index': [(2, 2)], 'id': 'A97-1028.6'}, 'entity_replacement': {'2:2': 'ENTITYOTHER', '5:6': 'ENTITY', '11:11': 'ENTITYUNRELATED', '13:15': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	Using the results of the statistical analysis , we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis .
Using the results of the statistical analysis, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.	algorithm	lower bound estimation	usage	{'e1': {'word': 'algorithm', 'word_index': [(11, 11)], 'id': 'A97-1028.8'}, 'e2': {'word': 'lower bound estimation', 'word_index': [(13, 15)], 'id': 'A97-1028.9'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '11:11': 'ENTITY', '13:15': 'ENTITYOTHER', '17:19': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	Using the results of the statistical analysis , we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis .
Using the results of the statistical analysis, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.	analysis	cross-lingual comparisons	topic	{'e1': {'word': 'analysis', 'word_index': [(31, 31)], 'id': 'A97-1028.12'}, 'e2': {'word': 'cross-lingual comparisons', 'word_index': [(26, 27)], 'id': 'A97-1028.11'}, 'entity_replacement': {'2:2': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '13:15': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '26:27': 'ENTITYOTHER', '31:31': 'ENTITY'}}	Using the results of the statistical analysis , we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis .
We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time.	question-focused sentence retrieval	news articles	usage	{'e1': {'word': 'question-focused sentence retrieval', 'word_index': [(5, 9)], 'id': 'H05-1115.1'}, 'e2': {'word': 'news articles', 'word_index': [(12, 13)], 'id': 'H05-1115.2'}, 'entity_replacement': {'5:9': 'ENTITY', '12:13': 'ENTITYOTHER', '15:19': 'ENTITYUNRELATED'}}	We consider the problem of question - focused sentence retrieval from complex news articles describing multi-event stories published over time .
Annotators generated a list of questions central to understanding each story in our corpus.	story	corpus	part_whole	{'e1': {'word': 'story', 'word_index': [(10, 10)], 'id': 'H05-1115.6'}, 'e2': {'word': 'corpus', 'word_index': [(13, 13)], 'id': 'H05-1115.7'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '10:10': 'ENTITY', '13:13': 'ENTITYOTHER'}}	Annotators generated a list of questions central to understanding each story in our corpus .
Judges found sentences providing an answer to each question.	answer	sentences	part_whole	{'e1': {'word': 'answer', 'word_index': [(5, 5)], 'id': 'H05-1115.12'}, 'e2': {'word': 'sentences', 'word_index': [(2, 2)], 'id': 'H05-1115.11'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '2:2': 'ENTITYOTHER', '5:5': 'ENTITY', '8:8': 'ENTITYUNRELATED'}}	Judges found sentences providing an answer to each question .
To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization.	stochastic, graph-based method	sentence retrieval problem	usage	{'e1': {'word': 'stochastic, graph-based method', 'word_index': [(10, 15)], 'id': 'H05-1115.15'}, 'e2': {'word': 'sentence retrieval problem', 'word_index': [(3, 5)], 'id': 'H05-1115.14'}, 'entity_replacement': {'3:5': 'ENTITYOTHER', '10:15': 'ENTITY', '23:24': 'ENTITYUNRELATED', '32:33': 'ENTITYUNRELATED'}}	To address the sentence retrieval problem , we apply a stochastic , graph - based method for comparing the relative importance of the textual units , which was previously used successfully for generic summarization .
Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap.	method	baseline	compare	{'e1': {'word': 'method', 'word_index': [(9, 9)], 'id': 'H05-1115.18'}, 'e2': {'word': 'baseline', 'word_index': [(18, 18)], 'id': 'H05-1115.19'}, 'entity_replacement': {'9:9': 'ENTITY', '18:18': 'ENTITYOTHER', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '32:36': 'ENTITYUNRELATED'}}	Currently , we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline , which compares the similarity of each sentence to the input question via IDF - weighted word overlap .
Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap.	similarity	sentence	model-feature	{'e1': {'word': 'similarity', 'word_index': [(23, 23)], 'id': 'H05-1115.20'}, 'e2': {'word': 'sentence', 'word_index': [(26, 26)], 'id': 'H05-1115.21'}, 'entity_replacement': {'9:9': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '23:23': 'ENTITY', '26:26': 'ENTITYOTHER', '30:30': 'ENTITYUNRELATED', '32:36': 'ENTITYUNRELATED'}}	Currently , we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline , which compares the similarity of each sentence to the input question via IDF - weighted word overlap .
In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline.	method	TRDR score	result	{'e1': {'word': 'method', 'word_index': [(5, 5)], 'id': 'H05-1115.24'}, 'e2': {'word': 'TRDR score', 'word_index': [(8, 9)], 'id': 'H05-1115.25'}, 'entity_replacement': {'5:5': 'ENTITY', '8:9': 'ENTITYOTHER', '18:18': 'ENTITYUNRELATED'}}	In our experiments , the method achieves a TRDR score that is significantly higher than that of the baseline .
A model is presented to characterize the class of languages obtained by adding reduplication to context-free languages.	model	class of languages	model-feature	{'e1': {'word': 'model', 'word_index': [(1, 1)], 'id': 'J89-4003.1'}, 'e2': {'word': 'class of languages', 'word_index': [(7, 9)], 'id': 'J89-4003.2'}, 'entity_replacement': {'1:1': 'ENTITY', '7:9': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '15:18': 'ENTITYUNRELATED'}}	A model is presented to characterize the class of languages obtained by adding reduplication to context - free languages .
A model is presented to characterize the class of languages obtained by adding reduplication to context-free languages.	reduplication	context-free languages	part_whole	{'e1': {'word': 'reduplication', 'word_index': [(13, 13)], 'id': 'J89-4003.3'}, 'e2': {'word': 'context-free languages', 'word_index': [(15, 18)], 'id': 'J89-4003.4'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:9': 'ENTITYUNRELATED', '13:13': 'ENTITY', '15:18': 'ENTITYOTHER'}}	A model is presented to characterize the class of languages obtained by adding reduplication to context - free languages .
The model is a pushdown automaton augmented with the ability to check reduplication by using the stack in a new way.	stack	pushdown automaton	usage	{'e1': {'word': 'stack', 'word_index': [(16, 16)], 'id': 'J89-4003.8'}, 'e2': {'word': 'pushdown automaton', 'word_index': [(4, 5)], 'id': 'J89-4003.6'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:5': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '16:16': 'ENTITY'}}	The model is a pushdown automaton augmented with the ability to check reduplication by using the stack in a new way .
The model appears capable of accommodating the sort of reduplications that have been observed to occur in natural languages, but it excludes many of the unnatural constructions that other formal models have permitted.	reduplications	natural languages	part_whole	{'e1': {'word': 'reduplications', 'word_index': [(9, 9)], 'id': 'J89-4003.13'}, 'e2': {'word': 'natural languages', 'word_index': [(17, 18)], 'id': 'J89-4003.14'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '9:9': 'ENTITY', '17:18': 'ENTITYOTHER', '27:27': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED'}}	The model appears capable of accommodating the sort of reduplications that have been observed to occur in natural languages , but it excludes many of the unnatural constructions that other formal models have permitted .
This article is devoted to the problem of quantifying noun groups in German.	quantifying noun groups	German	part_whole	{'e1': {'word': 'quantifying noun groups', 'word_index': [(8, 10)], 'id': 'I05-6010.1'}, 'e2': {'word': 'German', 'word_index': [(12, 12)], 'id': 'I05-6010.2'}, 'entity_replacement': {'8:10': 'ENTITY', '12:12': 'ENTITYOTHER'}}	This article is devoted to the problem of quantifying noun groups in German .
Moreover, some examples are given that underline the necessity of integrating some kind of information other than grammar sensu stricto into the treebank.	treebank	grammar sensu stricto	model-feature	{'e1': {'word': 'treebank', 'word_index': [(23, 23)], 'id': 'I05-6010.5'}, 'e2': {'word': 'grammar sensu stricto', 'word_index': [(18, 20)], 'id': 'I05-6010.4'}, 'entity_replacement': {'18:20': 'ENTITYOTHER', '23:23': 'ENTITY'}}	Moreover , some examples are given that underline the necessity of integrating some kind of information other than grammar sensu stricto into the treebank .
We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source of data for theoretical linguistic investigations.	annotation	tree-bank	part_whole	{'e1': {'word': 'annotation', 'word_index': [(10, 10)], 'id': 'I05-6010.6'}, 'e2': {'word': 'tree-bank', 'word_index': [(13, 15)], 'id': 'I05-6010.7'}, 'entity_replacement': {'10:10': 'ENTITY', '13:15': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED', '27:29': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '43:43': 'ENTITYUNRELATED', '48:50': 'ENTITYUNRELATED', '52:54': 'ENTITYUNRELATED'}}	We argue that a more sophisticated and fine - grained annotation in the tree - bank would have very positve effects on stochastic parsers trained on the tree - bank and on grammars induced from the treebank , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .
We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source of data for theoretical linguistic investigations.	treebank	theoretical linguistic investigations	usage	{'e1': {'word': 'treebank', 'word_index': [(43, 43)], 'id': 'I05-6010.12'}, 'e2': {'word': 'theoretical linguistic investigations', 'word_index': [(52, 54)], 'id': 'I05-6010.14'}, 'entity_replacement': {'10:10': 'ENTITYUNRELATED', '13:15': 'ENTITYUNRELATED', '22:23': 'ENTITYUNRELATED', '27:29': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '43:43': 'ENTITY', '48:50': 'ENTITYUNRELATED', '52:54': 'ENTITYOTHER'}}	We argue that a more sophisticated and fine - grained annotation in the tree - bank would have very positve effects on stochastic parsers trained on the tree - bank and on grammars induced from the treebank , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .
The information gained from corpus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and extraction tool for German text corpora.	extraction tool	German text corpora	usage	{'e1': {'word': 'extraction tool', 'word_index': [(23, 24)], 'id': 'I05-6010.18'}, 'e2': {'word': 'German text corpora', 'word_index': [(26, 28)], 'id': 'I05-6010.19'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '18:18': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '23:24': 'ENTITY', '26:28': 'ENTITYOTHER'}}	The information gained from corpus research and the analyses that are proposed are realized in the framework of SILVA , a parsing and extraction tool for German text corpora .
Metagrammatical formalisms that combine context-free phrase structure rules and metarules (MPS grammars) allow concise statement of generalizations about the syntax of natural languages.	context-free phrase structure rules	Metagrammatical formalisms	part_whole	{'e1': {'word': 'context-free phrase structure rules', 'word_index': [(4, 9)], 'id': 'P83-1004.2'}, 'e2': {'word': 'Metagrammatical formalisms', 'word_index': [(0, 1)], 'id': 'P83-1004.1'}, 'entity_replacement': {'0:1': 'ENTITYOTHER', '4:9': 'ENTITY', '11:15': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED'}}	Metagrammatical formalisms that combine context - free phrase structure rules and metarules ( MPS grammars ) allow concise statement of generalizations about the syntax of natural languages .
Metagrammatical formalisms that combine context-free phrase structure rules and metarules (MPS grammars) allow concise statement of generalizations about the syntax of natural languages.	syntax	natural languages	part_whole	{'e1': {'word': 'syntax', 'word_index': [(23, 23)], 'id': 'P83-1004.4'}, 'e2': {'word': 'natural languages', 'word_index': [(25, 26)], 'id': 'P83-1004.5'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '4:9': 'ENTITYUNRELATED', '11:15': 'ENTITYUNRELATED', '23:23': 'ENTITY', '25:26': 'ENTITYOTHER'}}	Metagrammatical formalisms that combine context - free phrase structure rules and metarules ( MPS grammars ) allow concise statement of generalizations about the syntax of natural languages .
In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.	formalization	attentional structure in discourse	model-feature	{'e1': {'word': 'formalization', 'word_index': [(6, 6)], 'id': 'P87-1022.1'}, 'e2': {'word': 'attentional structure in discourse', 'word_index': [(13, 16)], 'id': 'P87-1022.3'}, 'entity_replacement': {'6:6': 'ENTITY', '9:10': 'ENTITYUNRELATED', '13:16': 'ENTITYOTHER', '25:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns .
In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.	algorithm	discourse context	usage	{'e1': {'word': 'algorithm', 'word_index': [(25, 25)], 'id': 'P87-1022.4'}, 'e2': {'word': 'discourse context', 'word_index': [(28, 29)], 'id': 'P87-1022.5'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '13:16': 'ENTITYUNRELATED', '25:25': 'ENTITY', '28:29': 'ENTITYOTHER', '32:32': 'ENTITYUNRELATED'}}	In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns .
The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.	HPSG natural language system	database query application	usage	{'e1': {'word': 'HPSG natural language system', 'word_index': [(7, 10)], 'id': 'P87-1022.12'}, 'e2': {'word': 'database query application', 'word_index': [(18, 20)], 'id': 'P87-1022.13'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:10': 'ENTITY', '18:20': 'ENTITYOTHER'}}	The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application .
While HPSG has a more elaborated principle-based theory of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly.	principle-based theory	HPSG	part_whole	{'e1': {'word': 'principle-based theory', 'word_index': [(6, 9)], 'id': 'P95-1013.6'}, 'e2': {'word': 'HPSG', 'word_index': [(1, 1)], 'id': 'P95-1013.5'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '6:9': 'ENTITY', '12:13': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED'}}	While HPSG has a more elaborated principle - based theory of possible phrase structures , TAG provides the means to represent lexicalized structures more explicitly .
While HPSG has a more elaborated principle-based theory of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly.	TAG	lexicalized structures	usage	{'e1': {'word': 'TAG', 'word_index': [(15, 15)], 'id': 'P95-1013.8'}, 'e2': {'word': 'lexicalized structures', 'word_index': [(21, 22)], 'id': 'P95-1013.9'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '6:9': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '15:15': 'ENTITY', '21:22': 'ENTITYOTHER'}}	While HPSG has a more elaborated principle - based theory of possible phrase structures , TAG provides the means to represent lexicalized structures more explicitly .
Our objectives are met by giving clear definitions that determine the projection of structures from the lexicon, and identify maximal projections, auxiliary trees and foot nodes.	projection of structures	lexicon	part_whole	{'e1': {'word': 'projection of structures', 'word_index': [(11, 13)], 'id': 'P95-1013.10'}, 'e2': {'word': 'lexicon', 'word_index': [(16, 16)], 'id': 'P95-1013.11'}, 'entity_replacement': {'11:13': 'ENTITY', '16:16': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	Our objectives are met by giving clear definitions that determine the projection of structures from the lexicon , and identify maximal projections , auxiliary trees and foot nodes .
Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing.	Boolean matrix multiplication (BMM)	CFG parsing	usage	{'e1': {'word': 'Boolean matrix multiplication (BMM)', 'word_index': [(3, 8)], 'id': 'P97-1002.1'}, 'e2': {'word': 'CFG parsing', 'word_index': [(13, 14)], 'id': 'P97-1002.2'}, 'entity_replacement': {'3:8': 'ENTITY', '13:14': 'ENTITYOTHER'}}	Valiant showed that Boolean matrix multiplication ( BMM ) can be used for CFG parsing .
We prove a dual result: CFG parsers running in time O(|G||w|3-e) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time O(m3-e/3).	time O(|G||w|3-e)	CFG parsers	model-feature	{'e1': {'word': 'time O(|G||w|3-e)', 'word_index': [(10, 14)], 'id': 'P97-1002.4'}, 'e2': {'word': 'CFG parsers', 'word_index': [(6, 7)], 'id': 'P97-1002.3'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '10:14': 'ENTITY', '17:18': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '28:32': 'ENTITYUNRELATED', '34:40': 'ENTITYUNRELATED'}}	We prove a dual result : CFG parsers running in time O ( |G||w|3-e ) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time O ( m3 - e/3 ) .
In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang.	formal definition	parsing	model-feature	{'e1': {'word': 'formal definition', 'word_index': [(7, 8)], 'id': 'P97-1002.9'}, 'e2': {'word': 'parsing', 'word_index': [(10, 10)], 'id': 'P97-1002.10'}, 'entity_replacement': {'7:8': 'ENTITY', '10:10': 'ENTITYOTHER'}}	In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang .
Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist.	CFG parser	BMM algorithm	result	{'e1': {'word': 'CFG parser', 'word_index': [(17, 18)], 'id': 'P97-1002.12'}, 'e2': {'word': 'BMM algorithm', 'word_index': [(25, 26)], 'id': 'P97-1002.13'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '17:18': 'ENTITY', '25:26': 'ENTITYOTHER'}}	Our result establishes one of the first limitations on general CFG parsing : a fast , practical CFG parser would yield a fast , practical BMM algorithm , which is not believed to exist .
This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT.	primitive Optimality Theory (OTP)	OT	model-feature	{'e1': {'word': 'primitive Optimality Theory (OTP)', 'word_index': [(3, 8)], 'id': 'P97-1040.1'}, 'e2': {'word': 'OT', 'word_index': [(15, 15)], 'id': 'P97-1040.2'}, 'entity_replacement': {'3:8': 'ENTITY', '15:15': 'ENTITYOTHER'}}	This paper introduces primitive Optimality Theory ( OTP ) , a linguistically motivated formalization of OT .
In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994).	Generalized Alignment	theories	usage	{'e1': {'word': 'Generalized Alignment', 'word_index': [(7, 8)], 'id': 'P97-1040.8'}, 'e2': {'word': 'theories', 'word_index': [(5, 5)], 'id': 'P97-1040.7'}, 'entity_replacement': {'5:5': 'ENTITYOTHER', '7:8': 'ENTITY', '10:10': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '19:22': 'ENTITYUNRELATED'}}	In contrast to less restricted theories using Generalized Alignment , OTP 's optimal surface forms can be generated with finite - state methods adapted from ( Ellison , 1994 ) .
In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994).	finite-state methods	surface forms	usage	{'e1': {'word': 'finite-state methods', 'word_index': [(19, 22)], 'id': 'P97-1040.11'}, 'e2': {'word': 'surface forms', 'word_index': [(13, 14)], 'id': 'P97-1040.10'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:14': 'ENTITYOTHER', '19:22': 'ENTITY'}}	In contrast to less restricted theories using Generalized Alignment , OTP 's optimal surface forms can be generated with finite - state methods adapted from ( Ellison , 1994 ) .
Unfortunately these methods take time exponential on the size of the grammar.	time exponential on the size of the grammar	methods	model-feature	{'e1': {'word': 'time exponential on the size of the grammar', 'word_index': [(4, 11)], 'id': 'P97-1040.13'}, 'e2': {'word': 'methods', 'word_index': [(2, 2)], 'id': 'P97-1040.12'}, 'entity_replacement': {'2:2': 'ENTITYOTHER', '4:11': 'ENTITY'}}	Unfortunately these methods take time exponential on the size of the grammar .
Indeed the generation problem is shown NP-complete in this sense.	NP-complete	generation problem	model-feature	{'e1': {'word': 'NP-complete', 'word_index': [(6, 8)], 'id': 'P97-1040.15'}, 'e2': {'word': 'generation problem', 'word_index': [(2, 3)], 'id': 'P97-1040.14'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '6:8': 'ENTITY'}}	Indeed the generation problem is shown NP - complete in this sense .
One avenue for future improvements is a new finite-state notion, factored automata, where regular languages are represented compactly via formal intersections of FSAs.	formal intersections of FSAs	regular languages	model-feature	{'e1': {'word': 'formal intersections of FSAs', 'word_index': [(23, 26)], 'id': 'P97-1040.21'}, 'e2': {'word': 'regular languages', 'word_index': [(17, 18)], 'id': 'P97-1040.20'}, 'entity_replacement': {'8:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '17:18': 'ENTITYOTHER', '23:26': 'ENTITY'}}	One avenue for future improvements is a new finite - state notion , factored automata , where regular languages are represented compactly via formal intersections of FSAs .
We report our analysis of a collection of 20 Wall Street Journal articles from the Penn Treebank Corpus and our experiments with WordNet to identify relations between bridging descriptions and their antecedents.	Wall Street Journal articles	Penn Treebank Corpus	part_whole	{'e1': {'word': 'Wall Street Journal articles', 'word_index': [(9, 12)], 'id': 'P97-1072.2'}, 'e2': {'word': 'Penn Treebank Corpus', 'word_index': [(15, 17)], 'id': 'P97-1072.3'}, 'entity_replacement': {'9:12': 'ENTITY', '15:17': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED'}}	We report our analysis of a collection of 20 Wall Street Journal articles from the Penn Treebank Corpus and our experiments with WordNet to identify relations between bridging descriptions and their antecedents .
To verify hardware designs by model checking, circuit specifications are commonly expressed in the temporal logic CTL.	model checking	hardware designs	usage	{'e1': {'word': 'model checking', 'word_index': [(5, 6)], 'id': 'P99-1058.2'}, 'e2': {'word': 'hardware designs', 'word_index': [(2, 3)], 'id': 'P99-1058.1'}, 'entity_replacement': {'2:3': 'ENTITYOTHER', '5:6': 'ENTITY', '8:9': 'ENTITYUNRELATED', '15:17': 'ENTITYUNRELATED'}}	To verify hardware designs by model checking , circuit specifications are commonly expressed in the temporal logic CTL .
To verify hardware designs by model checking, circuit specifications are commonly expressed in the temporal logic CTL.	temporal logic CTL	circuit specifications	model-feature	{'e1': {'word': 'temporal logic CTL', 'word_index': [(15, 17)], 'id': 'P99-1058.4'}, 'e2': {'word': 'circuit specifications', 'word_index': [(8, 9)], 'id': 'P99-1058.3'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '8:9': 'ENTITYOTHER', '15:17': 'ENTITY'}}	To verify hardware designs by model checking , circuit specifications are commonly expressed in the temporal logic CTL .
Automatic conversion of English to CTL requires the definition of an appropriately restricted subset of English.	restricted subset	English	part_whole	{'e1': {'word': 'restricted subset', 'word_index': [(12, 13)], 'id': 'P99-1058.6'}, 'e2': {'word': 'English', 'word_index': [(15, 15)], 'id': 'P99-1058.7'}, 'entity_replacement': {'0:5': 'ENTITYUNRELATED', '12:13': 'ENTITY', '15:15': 'ENTITYOTHER'}}	Automatic conversion of English to CTL requires the definition of an appropriately restricted subset of English .
We show how the limited semantic expressibility of CTL can be exploited to derive a hierarchy of subsets.	semantic expressibility	CTL	model-feature	{'e1': {'word': 'semantic expressibility', 'word_index': [(5, 6)], 'id': 'P99-1058.8'}, 'e2': {'word': 'CTL', 'word_index': [(8, 8)], 'id': 'P99-1058.9'}, 'entity_replacement': {'5:6': 'ENTITY', '8:8': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED'}}	We show how the limited semantic expressibility of CTL can be exploited to derive a hierarchy of subsets .
Our strategy avoids potential difficulties with approaches that take existing computational semantic analyses of English as their starting point--such as the need to ensure that all sentences in the subset possess a CTL translation.	computational semantic analyses	English	topic	{'e1': {'word': 'computational semantic analyses', 'word_index': [(10, 12)], 'id': 'P99-1058.11'}, 'e2': {'word': 'English', 'word_index': [(14, 14)], 'id': 'P99-1058.12'}, 'entity_replacement': {'10:12': 'ENTITY', '14:14': 'ENTITYOTHER', '28:28': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '34:35': 'ENTITYUNRELATED'}}	Our strategy avoids potential difficulties with approaches that take existing computational semantic analyses of English as their starting point -- such as the need to ensure that all sentences in the subset possess a CTL translation .
Our strategy avoids potential difficulties with approaches that take existing computational semantic analyses of English as their starting point--such as the need to ensure that all sentences in the subset possess a CTL translation.	sentences	subset	part_whole	{'e1': {'word': 'sentences', 'word_index': [(28, 28)], 'id': 'P99-1058.13'}, 'e2': {'word': 'subset', 'word_index': [(31, 31)], 'id': 'P99-1058.14'}, 'entity_replacement': {'10:12': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '28:28': 'ENTITY', '31:31': 'ENTITYOTHER', '34:35': 'ENTITYUNRELATED'}}	Our strategy avoids potential difficulties with approaches that take existing computational semantic analyses of English as their starting point -- such as the need to ensure that all sentences in the subset possess a CTL translation .
In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus.	unlexicalized parser	German	usage	{'e1': {'word': 'unlexicalized parser', 'word_index': [(7, 8)], 'id': 'P05-1039.1'}, 'e2': {'word': 'German', 'word_index': [(10, 10)], 'id': 'P05-1039.2'}, 'entity_replacement': {'7:8': 'ENTITY', '10:10': 'ENTITYOTHER', '13:13': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '20:22': 'ENTITYUNRELATED', '33:34': 'ENTITYUNRELATED'}}	In this paper , we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .
In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus.	suffix analysis	labelled bracket F-score	result	{'e1': {'word': 'suffix analysis', 'word_index': [(15, 16)], 'id': 'P05-1039.4'}, 'e2': {'word': 'labelled bracket F-score', 'word_index': [(20, 22)], 'id': 'P05-1039.5'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:16': 'ENTITY', '20:22': 'ENTITYOTHER', '33:34': 'ENTITYUNRELATED'}}	In this paper , we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .
In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results.	smoothing	unlexicalized parser	usage	{'e1': {'word': 'smoothing', 'word_index': [(13, 13)], 'id': 'P05-1039.8'}, 'e2': {'word': 'unlexicalized parser', 'word_index': [(16, 17)], 'id': 'P05-1039.9'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '13:13': 'ENTITY', '16:17': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED'}}	In addition to the high accuracy of the model , the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results .
This paper proposes an alignment adaptation approach to improve domain-specific (in-domain) word alignment.	alignment adaptation approach	domain-specific (in-domain) word alignment	usage	{'e1': {'word': 'alignment adaptation approach', 'word_index': [(4, 6)], 'id': 'P05-1058.1'}, 'e2': {'word': 'domain-specific (in-domain) word alignment', 'word_index': [(9, 14)], 'id': 'P05-1058.2'}, 'entity_replacement': {'4:6': 'ENTITY', '9:14': 'ENTITYOTHER'}}	This paper proposes an alignment adaptation approach to improve domain-specific ( in-domain ) word alignment .
The basic idea of alignment adaptation is to use out-of-domain corpus to improve in-domain word alignment results.	out-of-domain corpus	in-domain word alignment	usage	{'e1': {'word': 'out-of-domain corpus', 'word_index': [(9, 14)], 'id': 'P05-1058.4'}, 'e2': {'word': 'in-domain word alignment', 'word_index': [(17, 21)], 'id': 'P05-1058.5'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '9:14': 'ENTITY', '17:21': 'ENTITYOTHER'}}	The basic idea of alignment adaptation is to use out - of - domain corpus to improve in - domain word alignment results .
In this paper, we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively, and then interpolate these two models to improve the domain-specific word alignment.	out-of-domain corpus	in-domain corpus	compare	{'e1': {'word': 'out-of-domain corpus', 'word_index': [(17, 22)], 'id': 'P05-1058.7'}, 'e2': {'word': 'in-domain corpus', 'word_index': [(28, 31)], 'id': 'P05-1058.8'}, 'entity_replacement': {'8:11': 'ENTITYUNRELATED', '17:22': 'ENTITY', '28:31': 'ENTITYOTHER', '43:46': 'ENTITYUNRELATED'}}	In this paper , we first train two statistical word alignment models with the large - scale out - of - domain corpus and the small - scale in - domain corpus respectively , and then interpolate these two models to improve the domain -specific word alignment .
Experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall, achieving a relative error rate reduction of 6.56% as compared with the state-of-the-art technologies.	domain-specific word alignment	relative error rate reduction	result	{'e1': {'word': 'domain-specific word alignment', 'word_index': [(7, 10)], 'id': 'P05-1058.10'}, 'e2': {'word': 'relative error rate reduction', 'word_index': [(21, 24)], 'id': 'P05-1058.13'}, 'entity_replacement': {'7:10': 'ENTITY', '15:15': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '21:24': 'ENTITYOTHER'}}	Experimental results show that our approach improves domain -specific word alignment in terms of both precision and recall , achieving a relative error rate reduction of 6.56 % as compared with the state - of - the - art technologies .
Our contributions include a concise, modular architecture with reversible processes of understanding and generation, an information-state model of reference, and flexible links between semantics and collaborative problem solving.	understanding	modular architecture	part_whole	{'e1': {'word': 'understanding', 'word_index': [(12, 12)], 'id': 'P05-3001.3'}, 'e2': {'word': 'modular architecture', 'word_index': [(6, 7)], 'id': 'P05-3001.2'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '12:12': 'ENTITY', '14:14': 'ENTITYUNRELATED', '17:22': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '30:32': 'ENTITYUNRELATED'}}	Our contributions include a concise , modular architecture with reversible processes of understanding and generation , an information - state model of reference , and flexible links between semantics and collaborative problem solving .
This article deals with the interpretation of conceptual operations underlying the communicative use of natural language (NL) within the Structured Inheritance Network (SI-Nets) paradigm.	interpretation	conceptual operations	model-feature	{'e1': {'word': 'interpretation', 'word_index': [(5, 5)], 'id': 'E83-1021.1'}, 'e2': {'word': 'conceptual operations', 'word_index': [(7, 8)], 'id': 'E83-1021.2'}, 'entity_replacement': {'5:5': 'ENTITY', '7:8': 'ENTITYOTHER', '14:18': 'ENTITYUNRELATED', '21:29': 'ENTITYUNRELATED'}}	This article deals with the interpretation of conceptual operations underlying the communicative use of natural language ( NL ) within the Structured Inheritance Network ( SI - Nets ) paradigm .
The operations are reduced to functions of a formal language, thus changing the level of abstraction of the operations to be performed on SI-Nets.	functions	formal language	part_whole	{'e1': {'word': 'functions', 'word_index': [(5, 5)], 'id': 'E83-1021.5'}, 'e2': {'word': 'formal language', 'word_index': [(8, 9)], 'id': 'E83-1021.6'}, 'entity_replacement': {'5:5': 'ENTITY', '8:9': 'ENTITYOTHER', '24:26': 'ENTITYUNRELATED'}}	The operations are reduced to functions of a formal language , thus changing the level of abstraction of the operations to be performed on SI - Nets .
In this sense, operations on SI-Nets are not merely isomorphic to single epistemological objects, but can be viewed as a simulation of processes on a different level, that pertaining to the conceptual system of NL.	conceptual system	NL	model-feature	{'e1': {'word': 'conceptual system', 'word_index': [(36, 37)], 'id': 'E83-1021.9'}, 'e2': {'word': 'NL', 'word_index': [(39, 39)], 'id': 'E83-1021.10'}, 'entity_replacement': {'6:8': 'ENTITYUNRELATED', '36:37': 'ENTITY', '39:39': 'ENTITYOTHER'}}	In this sense , operations on SI - Nets are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the conceptual system of NL .
For this purpose, we have designed a version of KL-ONE which represents the epistemological level, while the new experimental language, KL-Conc, represents the conceptual level.	KL-ONE	epistemological level	model-feature	{'e1': {'word': 'KL-ONE', 'word_index': [(10, 10)], 'id': 'E83-1021.11'}, 'e2': {'word': 'epistemological level', 'word_index': [(14, 15)], 'id': 'E83-1021.12'}, 'entity_replacement': {'10:10': 'ENTITY', '14:15': 'ENTITYOTHER', '23:25': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED'}}	For this purpose , we have designed a version of KL-ONE which represents the epistemological level , while the new experimental language , KL - Conc , represents the conceptual level .
For this purpose, we have designed a version of KL-ONE which represents the epistemological level, while the new experimental language, KL-Conc, represents the conceptual level.	KL-Conc	conceptual level	model-feature	{'e1': {'word': 'KL-Conc', 'word_index': [(23, 25)], 'id': 'E83-1021.13'}, 'e2': {'word': 'conceptual level', 'word_index': [(29, 30)], 'id': 'E83-1021.14'}, 'entity_replacement': {'10:10': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '23:25': 'ENTITY', '29:30': 'ENTITYOTHER'}}	For this purpose , we have designed a version of KL-ONE which represents the epistemological level , while the new experimental language , KL - Conc , represents the conceptual level .
The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present, past or future (= deictic information) 2. whether the event described in a sentence is presented as completed, going on, just starting or being finished (= aspectual information).	information	verb forms	model-feature	{'e1': {'word': 'information', 'word_index': [(11, 11)], 'id': 'E87-1043.2'}, 'e2': {'word': 'verb forms', 'word_index': [(1, 2)], 'id': 'E87-1043.1'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '11:11': 'ENTITY', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '53:54': 'ENTITYUNRELATED'}}	The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present , past or future (= deictic information ) 2. whether the event described in a sentence is presented as completed , going on , just starting or being finished (= aspectual information ) .
The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present, past or future (= deictic information) 2. whether the event described in a sentence is presented as completed, going on, just starting or being finished (= aspectual information).	event	sentence	part_whole	{'e1': {'word': 'event', 'word_index': [(16, 16)], 'id': 'E87-1043.3'}, 'e2': {'word': 'sentence', 'word_index': [(20, 20)], 'id': 'E87-1043.4'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '16:16': 'ENTITY', '20:20': 'ENTITYOTHER', '22:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED', '53:54': 'ENTITYUNRELATED'}}	The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present , past or future (= deictic information ) 2. whether the event described in a sentence is presented as completed , going on , just starting or being finished (= aspectual information ) .
The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present, past or future (= deictic information) 2. whether the event described in a sentence is presented as completed, going on, just starting or being finished (= aspectual information).	event	sentence	part_whole	{'e1': {'word': 'event', 'word_index': [(34, 34)], 'id': 'E87-1043.9'}, 'e2': {'word': 'sentence', 'word_index': [(38, 38)], 'id': 'E87-1043.10'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '20:20': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED', '34:34': 'ENTITY', '38:38': 'ENTITYOTHER', '53:54': 'ENTITYUNRELATED'}}	The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present , past or future (= deictic information ) 2. whether the event described in a sentence is presented as completed , going on , just starting or being finished (= aspectual information ) .
It will be demonstrated in this paper that one has to add a third component to the analysis of verb form meanings, namely whether or not they express habituality.	habituality	verb form meanings	model-feature	{'e1': {'word': 'habituality', 'word_index': [(29, 29)], 'id': 'E87-1043.13'}, 'e2': {'word': 'verb form meanings', 'word_index': [(19, 21)], 'id': 'E87-1043.12'}, 'entity_replacement': {'19:21': 'ENTITYOTHER', '29:29': 'ENTITY'}}	It will be demonstrated in this paper that one has to add a third component to the analysis of verb form meanings , namely whether or not they express habituality .
Unification is often the appropriate method for expressing relations between representations in the form of feature structures; however, there are circumstances in which a different approach is desirable.	relations	representations	model-feature	{'e1': {'word': 'relations', 'word_index': [(8, 8)], 'id': 'E91-1050.2'}, 'e2': {'word': 'representations', 'word_index': [(10, 10)], 'id': 'E91-1050.3'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '8:8': 'ENTITY', '10:10': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	Unification is often the appropriate method for expressing relations between representations in the form of feature structures ; however , there are circumstances in which a different approach is desirable .
A declarative formalism is presented which permits direct mappings of one feature structure into another, and illustrative examples are given of its application to areas of current interest.	declarative formalism	mappings	usage	{'e1': {'word': 'declarative formalism', 'word_index': [(1, 2)], 'id': 'E91-1050.5'}, 'e2': {'word': 'mappings', 'word_index': [(8, 8)], 'id': 'E91-1050.6'}, 'entity_replacement': {'1:2': 'ENTITY', '8:8': 'ENTITYOTHER', '11:12': 'ENTITYUNRELATED'}}	A declarative formalism is presented which permits direct mappings of one feature structure into another , and illustrative examples are given of its application to areas of current interest .
 We give an analysis of ellipsis resolution in terms of a straightforward discourse copying algorithm that correctly predicts a wide range of phenomena.	discourse copying algorithm	ellipsis resolution	model-feature	{'e1': {'word': 'discourse copying algorithm', 'word_index': [(12, 14)], 'id': 'E93-1025.2'}, 'e2': {'word': 'ellipsis resolution', 'word_index': [(5, 6)], 'id': 'E93-1025.1'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '12:14': 'ENTITY'}}	We give an analysis of ellipsis resolution in terms of a straightforward discourse copying algorithm that correctly predicts a wide range of phenomena .
Furthermore, in contrast to the approach of Dalrymple et al. [1991], the treatment directly encodes the intuitive distinction between full NPs and the referential elements that corefer with them through what we term role linking.	full NPs	referential elements	compare	{'e1': {'word': 'full NPs', 'word_index': [(23, 24)], 'id': 'E93-1025.4'}, 'e2': {'word': 'referential elements', 'word_index': [(27, 28)], 'id': 'E93-1025.5'}, 'entity_replacement': {'23:24': 'ENTITY', '27:28': 'ENTITYOTHER', '37:38': 'ENTITYUNRELATED'}}	Furthermore , in contrast to the approach of Dalrymple et al. [ 1991 ] , the treatment directly encodes the intuitive distinction between full NPs and the referential elements that corefer with them through what we term role linking .
The correct predictions for several problematic examples of ellipsis naturally result.	predictions	ellipsis	model-feature	{'e1': {'word': 'predictions', 'word_index': [(2, 2)], 'id': 'E93-1025.7'}, 'e2': {'word': 'ellipsis', 'word_index': [(8, 8)], 'id': 'E93-1025.8'}, 'entity_replacement': {'2:2': 'ENTITY', '8:8': 'ENTITYOTHER'}}	The correct predictions for several problematic examples of ellipsis naturally result .
Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval.	chunks of words	sentences	part_whole	{'e1': {'word': 'chunks of words', 'word_index': [(3, 5)], 'id': 'E99-1023.2'}, 'e2': {'word': 'sentences', 'word_index': [(1, 1)], 'id': 'E99-1023.1'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '3:5': 'ENTITY', '12:12': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED'}}	Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval .
"(Ramshaw and Marcus, 1995) have introduced a ""convenient"" data representation for chunking by converting it to a tagging task."	data representation	chunking	usage	{'e1': {'word': 'data representation', 'word_index': [(13, 14)], 'id': 'E99-1023.6'}, 'e2': {'word': 'chunking', 'word_index': [(16, 16)], 'id': 'E99-1023.7'}, 'entity_replacement': {'13:14': 'ENTITY', '16:16': 'ENTITYOTHER', '22:23': 'ENTITYUNRELATED'}}	"( Ramshaw and Marcus , 1995 ) have introduced a "" convenient "" data representation for chunking by converting it to a tagging task ."
In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks.	data representations	noun phrase chunks	model-feature	{'e1': {'word': 'data representations', 'word_index': [(8, 9)], 'id': 'E99-1023.9'}, 'e2': {'word': 'noun phrase chunks', 'word_index': [(15, 17)], 'id': 'E99-1023.10'}, 'entity_replacement': {'8:9': 'ENTITY', '15:17': 'ENTITYOTHER'}}	In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks .
We will show that the data representation choice has a minor influence on chunking performance.	data representation choice	chunking performance	result	{'e1': {'word': 'data representation choice', 'word_index': [(5, 7)], 'id': 'E99-1023.11'}, 'e2': {'word': 'chunking performance', 'word_index': [(13, 14)], 'id': 'E99-1023.12'}, 'entity_replacement': {'5:7': 'ENTITY', '13:14': 'ENTITYOTHER'}}	We will show that the data representation choice has a minor influence on chunking performance .
However, equipped with the most suitabledata representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.	memory-based learning chunker	chunking results	result	{'e1': {'word': 'memory-based learning chunker', 'word_index': [(11, 15)], 'id': 'E99-1023.14'}, 'e2': {'word': 'chunking results', 'word_index': [(23, 24)], 'id': 'E99-1023.15'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '11:15': 'ENTITY', '23:24': 'ENTITYOTHER', '27:29': 'ENTITYUNRELATED'}}	However , equipped with the most suitable data representation , our memory - based learning chunker was able to improve the best published chunking results for a standard data set .
In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language.	statistical and constraint-based disambiguation	part-of-speech tagging	usage	{'e1': {'word': 'statistical and constraint-based disambiguation', 'word_index': [(16, 21)], 'id': 'E95-1021.2'}, 'e2': {'word': 'part-of-speech tagging', 'word_index': [(9, 14)], 'id': 'E95-1021.1'}, 'entity_replacement': {'9:14': 'ENTITYOTHER', '16:21': 'ENTITY', '24:24': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	In this paper we compare two competing approaches to part - of - speech tagging , statistical and constraint - based disambiguation , using French as our test language .
In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language.	French	test language	usage	{'e1': {'word': 'French', 'word_index': [(23, 23)], 'id': 'E95-1021.3'}, 'e2': {'word': 'test language', 'word_index': [(26, 27)], 'id': 'E95-1021.4'}, 'entity_replacement': {'9:13': 'ENTITYUNRELATED', '15:20': 'ENTITYUNRELATED', '23:23': 'ENTITY', '26:27': 'ENTITYOTHER'}}	In this paper we compare two competing approaches to part -of - speech tagging , statistical and constraint - based disambiguation , using French as our test language .
We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model.	constraint system	statistical model	compare	{'e1': {'word': 'constraint system', 'word_index': [(19, 20)], 'id': 'E95-1021.5'}, 'e2': {'word': 'statistical model', 'word_index': [(40, 41)], 'id': 'E95-1021.6'}, 'entity_replacement': {'19:20': 'ENTITY', '40:41': 'ENTITYOTHER'}}	We imposed a time limit on our experiment : the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy - to - implement statistical model .
The accuracy of the statistical method is reasonably good, comparable to taggers for English.	statistical method	taggers	compare	{'e1': {'word': 'statistical method', 'word_index': [(4, 5)], 'id': 'E95-1021.8'}, 'e2': {'word': 'taggers', 'word_index': [(12, 12)], 'id': 'E95-1021.9'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '4:5': 'ENTITY', '12:12': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED'}}	The accuracy of the statistical method is reasonably good , comparable to taggers for English .
In order to build robust automatic abstracting systems, there is a need for better training resources than are currently available.	training resources	automatic abstracting systems	usage	{'e1': {'word': 'training resources', 'word_index': [(15, 16)], 'id': 'E99-1015.2'}, 'e2': {'word': 'automatic abstracting systems', 'word_index': [(5, 7)], 'id': 'E99-1015.1'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '15:16': 'ENTITY'}}	In order to build robust automatic abstracting systems , there is a need for better training resources than are currently available .
In this paper, we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way.	annotation scheme	resource	usage	{'e1': {'word': 'annotation scheme', 'word_index': [(7, 8)], 'id': 'E99-1015.3'}, 'e2': {'word': 'resource', 'word_index': [(20, 20)], 'id': 'E99-1015.4'}, 'entity_replacement': {'7:8': 'ENTITY', '20:20': 'ENTITYOTHER'}}	In this paper , we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way .
This paper describes an implemented program that takes a tagged text corpus and generates a partial list of the subcategorization frames in which each verb occurs.	subcategorization frames	verb	model-feature	{'e1': {'word': 'subcategorization frames', 'word_index': [(19, 20)], 'id': 'H91-1067.2'}, 'e2': {'word': 'verb', 'word_index': [(24, 24)], 'id': 'H91-1067.3'}, 'entity_replacement': {'9:11': 'ENTITYUNRELATED', '19:20': 'ENTITY', '24:24': 'ENTITYOTHER'}}	This paper describes an implemented program that takes a tagged text corpus and generates a partial list of the subcategorization frames in which each verb occurs .
The completeness of the output list increases monotonically with the total occurrences of each verb in the training corpus.	verb	training corpus	part_whole	{'e1': {'word': 'verb', 'word_index': [(14, 14)], 'id': 'H91-1067.5'}, 'e2': {'word': 'training corpus', 'word_index': [(17, 18)], 'id': 'H91-1067.6'}, 'entity_replacement': {'11:11': 'ENTITYUNRELATED', '14:14': 'ENTITY', '17:18': 'ENTITYOTHER'}}	The completeness of the output list increases monotonically with the total occurrences of each verb in the training corpus .
We focus on the problem of building large repositories of lexical conceptual structure (LCS) representations for verbs in multiple languages.	lexical conceptual structure (LCS) representations	verbs	model-feature	{'e1': {'word': 'lexical conceptual structure (LCS) representations', 'word_index': [(10, 16)], 'id': 'A97-1021.2'}, 'e2': {'word': 'verbs', 'word_index': [(18, 18)], 'id': 'A97-1021.3'}, 'entity_replacement': {'8:8': 'ENTITYUNRELATED', '10:16': 'ENTITY', '18:18': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED'}}	We focus on the problem of building large repositories of lexical conceptual structure ( LCS ) representations for verbs in multiple languages .
Our acquisition program - LEXICALL - takes, as input, the result of previous work on verb classification and thematic grid tagging, and outputs LCS representations for different languages.	thematic grid tagging	LCS representations	usage	{'e1': {'word': 'thematic grid tagging', 'word_index': [(20, 22)], 'id': 'A97-1021.9'}, 'e2': {'word': 'LCS representations', 'word_index': [(26, 27)], 'id': 'A97-1021.10'}, 'entity_replacement': {'1:5': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '20:22': 'ENTITY', '26:27': 'ENTITYOTHER', '30:30': 'ENTITYUNRELATED'}}	Our acquisition program - LEXICALL - takes , as input , the result of previous work on verb classification and thematic grid tagging , and outputs LCS representations for different languages .
These representations have been ported into English, Arabic and Spanish lexicons, each containing approximately 9000 verbs.	representations	English, Arabic and Spanish lexicons	usage	{'e1': {'word': 'representations', 'word_index': [(1, 1)], 'id': 'A97-1021.12'}, 'e2': {'word': 'English, Arabic and Spanish lexicons', 'word_index': [(6, 11)], 'id': 'A97-1021.13'}, 'entity_replacement': {'1:1': 'ENTITY', '6:11': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED'}}	These representations have been ported into English , Arabic and Spanish lexicons , each containing approximately 9000 verbs .
We are currently using these lexicons in an operational foreign language tutoring and machine translation.	lexicons	operational foreign language tutoring	usage	{'e1': {'word': 'lexicons', 'word_index': [(5, 5)], 'id': 'A97-1021.15'}, 'e2': {'word': 'operational foreign language tutoring', 'word_index': [(8, 11)], 'id': 'A97-1021.16'}, 'entity_replacement': {'5:5': 'ENTITY', '8:11': 'ENTITYOTHER', '13:14': 'ENTITYUNRELATED'}}	We are currently using these lexicons in an operational foreign language tutoring and machine translation .
We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.	algorithm for translation lexicon acquisition (SABLE)	translation lexicons	usage	{'e1': {'word': 'algorithm for translation lexicon acquisition (SABLE)', 'word_index': [(6, 13)], 'id': 'A97-1050.1'}, 'e2': {'word': 'translation lexicons', 'word_index': [(25, 26)], 'id': 'A97-1050.3'}, 'entity_replacement': {'6:13': 'ENTITY', '21:21': 'ENTITYUNRELATED', '25:26': 'ENTITYOTHER', '30:30': 'ENTITYUNRELATED', '37:37': 'ENTITYUNRELATED', '42:44': 'ENTITYUNRELATED'}}	We investigate the utility of an algorithm for translation lexicon acquisition ( SABLE ) , used previously on a very large corpus to acquire general translation lexicons , when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons .
We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.	algorithm	domain-specific translation lexicons	usage	{'e1': {'word': 'algorithm', 'word_index': [(30, 30)], 'id': 'A97-1050.4'}, 'e2': {'word': 'domain-specific translation lexicons', 'word_index': [(42, 44)], 'id': 'A97-1050.6'}, 'entity_replacement': {'6:13': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '30:30': 'ENTITY', '37:37': 'ENTITYUNRELATED', '42:44': 'ENTITYOTHER'}}	We investigate the utility of an algorithm for translation lexicon acquisition ( SABLE ) , used previously on a very large corpus to acquire general translation lexicons , when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons .
English is shown to be trans-context-free on the basis of coordinations of the respectively type that involve strictly syntactic cross-serial agreement.	coordinations	strictly syntactic cross-serial agreement	model-feature	{'e1': {'word': 'coordinations', 'word_index': [(14, 14)], 'id': 'J87-1003.2'}, 'e2': {'word': 'strictly syntactic cross-serial agreement', 'word_index': [(21, 25)], 'id': 'J87-1003.3'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '14:14': 'ENTITY', '21:25': 'ENTITYOTHER'}}	English is shown to be trans - context - free on the basis of coordinations of the respectively type that involve strictly syntactic cross -serial agreement .
The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like grammatical gender in languages such as French, is partly arbitrary.	number	nouns	model-feature	{'e1': {'word': 'number', 'word_index': [(5, 5)], 'id': 'J87-1003.5'}, 'e2': {'word': 'nouns', 'word_index': [(7, 7)], 'id': 'J87-1003.6'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:5': 'ENTITY', '7:7': 'ENTITYOTHER', '9:10': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .
The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like grammatical gender in languages such as French, is partly arbitrary.	grammatical number	English	part_whole	{'e1': {'word': 'grammatical number', 'word_index': [(20, 21)], 'id': 'J87-1003.8'}, 'e2': {'word': 'English', 'word_index': [(23, 23)], 'id': 'J87-1003.9'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '20:21': 'ENTITY', '23:23': 'ENTITYOTHER', '26:27': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .
The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like grammatical gender in languages such as French, is partly arbitrary.	grammatical gender	French	part_whole	{'e1': {'word': 'grammatical gender', 'word_index': [(26, 27)], 'id': 'J87-1003.10'}, 'e2': {'word': 'French', 'word_index': [(32, 32)], 'id': 'J87-1003.12'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:5': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:27': 'ENTITY', '29:29': 'ENTITYUNRELATED', '32:32': 'ENTITYOTHER'}}	The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .
The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem.	grammatical sentences	English	part_whole	{'e1': {'word': 'grammatical sentences', 'word_index': [(31, 32)], 'id': 'J87-1003.15'}, 'e2': {'word': 'English', 'word_index': [(26, 26)], 'id': 'J87-1003.14'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '26:26': 'ENTITYOTHER', '31:32': 'ENTITY', '41:42': 'ENTITYUNRELATED', '49:49': 'ENTITYUNRELATED', '64:64': 'ENTITYUNRELATED', '69:69': 'ENTITYUNRELATED', '72:72': 'ENTITYUNRELATED', '78:78': 'ENTITYUNRELATED'}}	The formal proof , which makes crucial use of the Interchange Lemma of Ogden et al. , is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other ; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively , whereas other arguments have foundered on this problem .
The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem.	conjuncts	coordinate phrases	part_whole	{'e1': {'word': 'conjuncts', 'word_index': [(49, 49)], 'id': 'J87-1003.17'}, 'e2': {'word': 'coordinate phrases', 'word_index': [(41, 42)], 'id': 'J87-1003.16'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED', '41:42': 'ENTITYOTHER', '49:49': 'ENTITY', '64:64': 'ENTITYUNRELATED', '69:69': 'ENTITYUNRELATED', '72:72': 'ENTITYUNRELATED', '78:78': 'ENTITYUNRELATED'}}	The formal proof , which makes crucial use of the Interchange Lemma of Ogden et al. , is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other ; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively , whereas other arguments have foundered on this problem .
The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem.	conjuncts	constructions	part_whole	{'e1': {'word': 'conjuncts', 'word_index': [(69, 69)], 'id': 'J87-1003.19'}, 'e2': {'word': 'constructions', 'word_index': [(64, 64)], 'id': 'J87-1003.18'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '31:32': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED', '49:49': 'ENTITYUNRELATED', '64:64': 'ENTITYOTHER', '69:69': 'ENTITY', '72:72': 'ENTITYUNRELATED', '78:78': 'ENTITYUNRELATED'}}	The formal proof , which makes crucial use of the Interchange Lemma of Ogden et al. , is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other ; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively , whereas other arguments have foundered on this problem .
Towards deep analysis of compositional classes of paraphrases, we have examined a class-oriented framework for collecting paraphrase examples, in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement.	class-oriented framework	paraphrase examples	usage	{'e1': {'word': 'class-oriented framework', 'word_index': [(13, 15)], 'id': 'I05-5004.2'}, 'e2': {'word': 'paraphrase examples', 'word_index': [(18, 19)], 'id': 'I05-5004.3'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '13:15': 'ENTITY', '18:19': 'ENTITYOTHER', '23:24': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '35:37': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED'}}	Towards deep analysis of compositional classes of paraphrases , we have examined a class -oriented framework for collecting paraphrase examples , in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement .
Towards deep analysis of compositional classes of paraphrases, we have examined a class-oriented framework for collecting paraphrase examples, in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement.	sentential paraphrases	paraphrase class	part_whole	{'e1': {'word': 'sentential paraphrases', 'word_index': [(23, 24)], 'id': 'I05-5004.4'}, 'e2': {'word': 'paraphrase class', 'word_index': [(29, 30)], 'id': 'I05-5004.5'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '13:15': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED', '23:24': 'ENTITY', '29:30': 'ENTITYOTHER', '35:37': 'ENTITYUNRELATED', '39:40': 'ENTITYUNRELATED'}}	Towards deep analysis of compositional classes of paraphrases , we have examined a class -oriented framework for collecting paraphrase examples , in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement .
A flexible parser can deal with input that deviates from its grammar, in addition to input that conforms to it.	grammar	flexible parser	part_whole	{'e1': {'word': 'grammar', 'word_index': [(11, 11)], 'id': 'P81-1033.2'}, 'e2': {'word': 'flexible parser', 'word_index': [(1, 2)], 'id': 'P81-1033.1'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '11:11': 'ENTITY'}}	A flexible parser can deal with input that deviates from its grammar , in addition to input that conforms to it .
Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to.	construction-specific approach	flexible parsing	usage	{'e1': {'word': 'construction-specific approach', 'word_index': [(9, 12)], 'id': 'P81-1033.8'}, 'e2': {'word': 'flexible parsing', 'word_index': [(14, 15)], 'id': 'P81-1033.9'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '9:12': 'ENTITY', '14:15': 'ENTITYOTHER', '18:20': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '29:30': 'ENTITYUNRELATED', '35:35': 'ENTITYUNRELATED', '39:39': 'ENTITYUNRELATED'}}	Focused interaction of this kind is facilitated by a construction - specific approach to flexible parsing , with specialized parsing techniques for each type of construction , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .
Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to.	specialized parsing techniques	construction	usage	{'e1': {'word': 'specialized parsing techniques', 'word_index': [(17, 19)], 'id': 'P81-1033.10'}, 'e2': {'word': 'construction', 'word_index': [(24, 24)], 'id': 'P81-1033.11'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '9:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '17:19': 'ENTITY', '24:24': 'ENTITYOTHER', '28:29': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED'}}	Focused interaction of this kind is facilitated by a construction -specific approach to flexible parsing , with specialized parsing techniques for each type of construction , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .
Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to.	ambiguity representations	ambiguity	model-feature	{'e1': {'word': 'ambiguity representations', 'word_index': [(28, 29)], 'id': 'P81-1033.12'}, 'e2': {'word': 'ambiguity', 'word_index': [(34, 34)], 'id': 'P81-1033.13'}, 'entity_replacement': {'0:1': 'ENTITYUNRELATED', '9:11': 'ENTITYUNRELATED', '13:14': 'ENTITYUNRELATED', '17:19': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '28:29': 'ENTITY', '34:34': 'ENTITYOTHER', '38:38': 'ENTITYUNRELATED'}}	Focused interaction of this kind is facilitated by a construction -specific approach to flexible parsing , with specialized parsing techniques for each type of construction , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .
A construction-specific approach also aids in task-specific language development by allowing a language definition that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism, thus greatly speeding the testing of changes to the language definition.	construction-specific approach	task-specific language development	usage	{'e1': {'word': 'construction-specific approach', 'word_index': [(1, 3)], 'id': 'P81-1033.15'}, 'e2': {'word': 'task-specific language development', 'word_index': [(7, 11)], 'id': 'P81-1033.16'}, 'entity_replacement': {'1:3': 'ENTITY', '7:11': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '34:36': 'ENTITYUNRELATED', '42:42': 'ENTITYUNRELATED', '47:48': 'ENTITYUNRELATED'}}	A construction -specific approach also aids in task - specific language development by allowing a language definition that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism , thus greatly speeding the testing of changes to the language definition .
Building on previous work at Carnegie-Mellon University e.g. [4, 5, 8], Plume's approach to parsing is based on semantic caseframe instantiation.	semantic caseframe instantiation	Plume's approach to parsing	usage	"{'e1': {'word': 'semantic caseframe instantiation', 'word_index': [(25, 27)], 'id': 'P85-1019.4'}, 'e2': {'word': ""Plume's approach to parsing"", 'word_index': [(17, 21)], 'id': 'P85-1019.3'}, 'entity_replacement': {'17:21': 'ENTITYOTHER', '25:27': 'ENTITY'}}"	Building on previous work at Carnegie- Mellon University e.g. [ 4 , 5 , 8 ] , Plume 's approach to parsing is based on semantic caseframe instantiation .
While Plume is well adapted to simple declarative and imperative utterances, it handles passives, relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage.	Plume	syntactic coverage	result	{'e1': {'word': 'Plume', 'word_index': [(1, 1)], 'id': 'P85-1019.9'}, 'e2': {'word': 'syntactic coverage', 'word_index': [(28, 29)], 'id': 'P85-1019.14'}, 'entity_replacement': {'1:1': 'ENTITY', '7:10': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '28:29': 'ENTITYOTHER'}}	While Plume is well adapted to simple declarative and imperative utterances , it handles passives , relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage .
Languages differ in the concepts and real-world entities for which they have words and grammatical constructs.	words	Languages	part_whole	{'e1': {'word': 'words', 'word_index': [(14, 14)], 'id': 'P91-1025.4'}, 'e2': {'word': 'Languages', 'word_index': [(0, 0)], 'id': 'P91-1025.1'}, 'entity_replacement': {'0:0': 'ENTITYOTHER', '4:4': 'ENTITYUNRELATED', '6:9': 'ENTITYUNRELATED', '14:14': 'ENTITY', '16:17': 'ENTITYUNRELATED'}}	Languages differ in the concepts and real - world entities for which they have words and grammatical constructs .
Therefore translation must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language.	meaning	source language text	model-feature	{'e1': {'word': 'meaning', 'word_index': [(10, 10)], 'id': 'P91-1025.7'}, 'e2': {'word': 'source language text', 'word_index': [(13, 15)], 'id': 'P91-1025.8'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '10:10': 'ENTITY', '13:15': 'ENTITYOTHER', '24:25': 'ENTITYUNRELATED'}}	Therefore translation must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language .
We propose a translation framework based on Situation Theory.	Situation Theory	translation framework	usage	{'e1': {'word': 'Situation Theory', 'word_index': [(7, 8)], 'id': 'P91-1025.11'}, 'e2': {'word': 'translation framework', 'word_index': [(3, 4)], 'id': 'P91-1025.10'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '7:8': 'ENTITY'}}	We propose a translation framework based on Situation Theory .
The basic ingredients are an information lattice, a representation scheme for utterances embedded in contexts, and a mismatch resolution scheme defined in terms of information flow.	representation scheme	utterances	model-feature	{'e1': {'word': 'representation scheme', 'word_index': [(9, 10)], 'id': 'P91-1025.13'}, 'e2': {'word': 'utterances', 'word_index': [(12, 12)], 'id': 'P91-1025.14'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '9:10': 'ENTITY', '12:12': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '19:21': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	The basic ingredients are an information lattice , a representation scheme for utterances embedded in contexts , and a mismatch resolution scheme defined in terms of information flow .
The basic ingredients are an information lattice, a representation scheme for utterances embedded in contexts, and a mismatch resolution scheme defined in terms of information flow.	information flow	mismatch resolution scheme	model-feature	{'e1': {'word': 'information flow', 'word_index': [(26, 27)], 'id': 'P91-1025.17'}, 'e2': {'word': 'mismatch resolution scheme', 'word_index': [(19, 21)], 'id': 'P91-1025.16'}, 'entity_replacement': {'5:6': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '12:12': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED', '19:21': 'ENTITYOTHER', '26:27': 'ENTITY'}}	The basic ingredients are an information lattice , a representation scheme for utterances embedded in contexts , and a mismatch resolution scheme defined in terms of information flow .
We motivate our approach with examples of translation between English and Japanese.	translation	English	usage	{'e1': {'word': 'translation', 'word_index': [(7, 7)], 'id': 'P91-1025.18'}, 'e2': {'word': 'English', 'word_index': [(9, 9)], 'id': 'P91-1025.19'}, 'entity_replacement': {'7:7': 'ENTITY', '9:9': 'ENTITYOTHER', '11:11': 'ENTITYUNRELATED'}}	We motivate our approach with examples of translation between English and Japanese .
Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual.	knowledge	Large-scale natural language generation	usage	{'e1': {'word': 'knowledge', 'word_index': [(11, 11)], 'id': 'P95-1034.2'}, 'e2': {'word': 'Large-scale natural language generation', 'word_index': [(0, 3)], 'id': 'P95-1034.1'}, 'entity_replacement': {'0:3': 'ENTITYOTHER', '11:11': 'ENTITY'}}	Large-scale natural language generation requires the integration of vast amounts of knowledge : lexical , grammatical , and conceptual .
A robust generator must be able to operate well even when pieces of knowledge are missing.	knowledge	robust generator	usage	{'e1': {'word': 'knowledge', 'word_index': [(13, 13)], 'id': 'P95-1034.4'}, 'e2': {'word': 'robust generator', 'word_index': [(1, 2)], 'id': 'P95-1034.3'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '13:13': 'ENTITY'}}	A robust generator must be able to operate well even when pieces of knowledge are missing .
To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods.	statistical methods	hybrid generator	usage	{'e1': {'word': 'statistical methods', 'word_index': [(21, 22)], 'id': 'P95-1034.8'}, 'e2': {'word': 'hybrid generator', 'word_index': [(9, 10)], 'id': 'P95-1034.6'}, 'entity_replacement': {'9:10': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '21:22': 'ENTITY'}}	To attack these problems , we have built a hybrid generator , in which gaps in symbolic knowledge are filled by statistical methods .
We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.</abstract>	portability	generators	model-feature	{'e1': {'word': 'portability', 'word_index': [(18, 18)], 'id': 'P95-1034.11'}, 'e2': {'word': 'generators', 'word_index': [(14, 14)], 'id': 'P95-1034.10'}, 'entity_replacement': {'5:7': 'ENTITYUNRELATED', '14:14': 'ENTITYOTHER', '18:18': 'ENTITY', '23:23': 'ENTITYUNRELATED'}}	We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability , even when perfect knowledge is in principle obtainable . < / abstract >
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.	languages	alphabets	model-feature	{'e1': {'word': 'languages', 'word_index': [(10, 10)], 'id': 'P97-1017.3'}, 'e2': {'word': 'alphabets', 'word_index': [(13, 13)], 'id': 'P97-1017.4'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '10:10': 'ENTITY', '13:13': 'ENTITYOTHER', '15:16': 'ENTITYUNRELATED'}}	It is challenging to translate names and technical terms across languages with different alphabets and sound inventories .
For example, computer in English comes out as ~ i/l:::'=--~-- (konpyuutaa) in Japanese.	English	Japanese	compare	{'e1': {'word': 'English', 'word_index': [(5, 5)], 'id': 'P97-1017.7'}, 'e2': {'word': 'Japanese', 'word_index': [(16, 16)], 'id': 'P97-1017.8'}, 'entity_replacement': {'5:5': 'ENTITY', '16:16': 'ENTITYOTHER'}}	For example , computer in English comes out as ~ i/ l:::'=--~-- ( konpyuutaa ) in Japanese .
We describe and evaluate a method for performing backwards transliterations by machine.	machine	backwards transliterations	usage	{'e1': {'word': 'machine', 'word_index': [(11, 11)], 'id': 'P97-1017.14'}, 'e2': {'word': 'backwards transliterations', 'word_index': [(8, 9)], 'id': 'P97-1017.13'}, 'entity_replacement': {'8:9': 'ENTITYOTHER', '11:11': 'ENTITY'}}	We describe and evaluate a method for performing backwards transliterations by machine .
This method uses a generative model, incorporating several distinct stages in the transliteration process.	generative model	transliteration process	usage	{'e1': {'word': 'generative model', 'word_index': [(4, 5)], 'id': 'P97-1017.15'}, 'e2': {'word': 'transliteration process', 'word_index': [(13, 14)], 'id': 'P97-1017.16'}, 'entity_replacement': {'4:5': 'ENTITY', '13:14': 'ENTITYOTHER'}}	This method uses a generative model , incorporating several distinct stages in the transliteration process .
Although adequate models of human language for syntactic analysis and semantic interpretation are of at least context-free complexity, for applications such as speech processing in which speed is important finite-state models are often preferred.	finite-state models	speech processing	usage	{'e1': {'word': 'finite-state models', 'word_index': [(32, 35)], 'id': 'P97-1058.6'}, 'e2': {'word': 'speech processing', 'word_index': [(25, 26)], 'id': 'P97-1058.5'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '7:8': 'ENTITYUNRELATED', '10:11': 'ENTITYUNRELATED', '16:19': 'ENTITYUNRELATED', '25:26': 'ENTITYOTHER', '32:35': 'ENTITY'}}	Although adequate models of human language for syntactic analysis and semantic interpretation are of at least context - free complexity , for applications such as speech processing in which speed is important finite - state models are often preferred .
These requirements may be reconciled by using the more complex grammar to automatically derive a finite-state approximation which can then be used as a filter to guide speech recognition or to reject many hypotheses at an early stage of processing.	finite-state approximation	speech recognition	usage	{'e1': {'word': 'finite-state approximation', 'word_index': [(15, 18)], 'id': 'P97-1058.8'}, 'e2': {'word': 'speech recognition', 'word_index': [(29, 30)], 'id': 'P97-1058.9'}, 'entity_replacement': {'10:10': 'ENTITYUNRELATED', '15:18': 'ENTITY', '29:30': 'ENTITYOTHER'}}	These requirements may be reconciled by using the more complex grammar to automatically derive a finite - state approximation which can then be used as a filter to guide speech recognition or to reject many hypotheses at an early stage of processing .
We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word.	statistical model	Japanese unknown words	model-feature	{'e1': {'word': 'statistical model', 'word_index': [(3, 4)], 'id': 'P99-1036.1'}, 'e2': {'word': 'Japanese unknown words', 'word_index': [(6, 8)], 'id': 'P99-1036.2'}, 'entity_replacement': {'3:4': 'ENTITY', '6:8': 'ENTITYOTHER', '14:17': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word .
We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word.	character types	word	part_whole	{'e1': {'word': 'character types', 'word_index': [(21, 22)], 'id': 'P99-1036.4'}, 'e2': {'word': 'word', 'word_index': [(26, 26)], 'id': 'P99-1036.5'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '6:8': 'ENTITYUNRELATED', '14:17': 'ENTITYUNRELATED', '21:22': 'ENTITY', '26:26': 'ENTITYOTHER'}}	We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word .
The point is quite simple: different character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana).	ideograms	Japanese script	part_whole	{'e1': {'word': 'ideograms', 'word_index': [(27, 27)], 'id': 'P99-1036.9'}, 'e2': {'word': 'Japanese script', 'word_index': [(23, 24)], 'id': 'P99-1036.8'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER', '27:27': 'ENTITY', '29:29': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '36:36': 'ENTITYUNRELATED', '38:38': 'ENTITYUNRELATED'}}	The point is quite simple : different character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese ( kanji ) and phonograms like English ( katakana ) .
This paper discusses a decision-tree approach to the problem of assigning probabilities to words following a given text.	decision-tree approach	probabilities	usage	{'e1': {'word': 'decision-tree approach', 'word_index': [(4, 7)], 'id': 'P99-1080.1'}, 'e2': {'word': 'probabilities', 'word_index': [(13, 13)], 'id': 'P99-1080.2'}, 'entity_replacement': {'4:7': 'ENTITY', '13:13': 'ENTITYOTHER', '15:15': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED'}}	This paper discusses a decision - tree approach to the problem of assigning probabilities to words following a given text .
This paper discusses a decision-tree approach to the problem of assigning probabilities to words following a given text.	words	text	part_whole	{'e1': {'word': 'words', 'word_index': [(15, 15)], 'id': 'P99-1080.3'}, 'e2': {'word': 'text', 'word_index': [(19, 19)], 'id': 'P99-1080.4'}, 'entity_replacement': {'4:7': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '15:15': 'ENTITY', '19:19': 'ENTITYOTHER'}}	This paper discusses a decision - tree approach to the problem of assigning probabilities to words following a given text .
This poster paper describes a full scale two-level morphological description (Karttunen, 1983; Koskenniemi, 1983) of Turkish word structures.	full scale two-level morphological description	Turkish word structures	model-feature	{'e1': {'word': 'full scale two-level morphological description', 'word_index': [(5, 10)], 'id': 'E93-1066.1'}, 'e2': {'word': 'Turkish word structures', 'word_index': [(21, 23)], 'id': 'E93-1066.2'}, 'entity_replacement': {'5:10': 'ENTITY', '21:23': 'ENTITYOTHER'}}	This poster paper describes a full scale two -level morphological description ( Karttunen , 1983 ; Koskenniemi , 1983 ) of Turkish word structures .
The description has been implemented using the PC-KIMMO environment (Antworth, 1990) and is based on a root word lexicon of about 23,000 roots words.	roots words	root word lexicon	part_whole	{'e1': {'word': 'roots words', 'word_index': [(25, 26)], 'id': 'E93-1066.5'}, 'e2': {'word': 'root word lexicon', 'word_index': [(19, 21)], 'id': 'E93-1066.4'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '19:21': 'ENTITYOTHER', '25:26': 'ENTITY'}}	The description has been implemented using the PC-KIMMO environment ( Antworth , 1990 ) and is based on a root word lexicon of about 23,000 roots words .
Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words.	Turkish	agglutinative language	model-feature	{'e1': {'word': 'Turkish', 'word_index': [(0, 0)], 'id': 'E93-1066.7'}, 'e2': {'word': 'agglutinative language', 'word_index': [(3, 4)], 'id': 'E93-1066.8'}, 'entity_replacement': {'0:0': 'ENTITY', '3:4': 'ENTITYOTHER', '6:7': 'ENTITYUNRELATED', '10:16': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED'}}	Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words .
The surface realizations of morphological constructions are constrained and modified by a number of phonetic rules such as vowel harmony.	surface realizations	morphological constructions	model-feature	{'e1': {'word': 'surface realizations', 'word_index': [(1, 2)], 'id': 'E93-1066.21'}, 'e2': {'word': 'morphological constructions', 'word_index': [(4, 5)], 'id': 'E93-1066.22'}, 'entity_replacement': {'1:2': 'ENTITY', '4:5': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '18:19': 'ENTITYUNRELATED'}}	The surface realizations of morphological constructions are constrained and modified by a number of phonetic rules such as vowel harmony .
The TIPSTER Architecture has been designed to enable a variety of different text applications to use a set of common text processing modules.	common text processing modules	text applications	usage	{'e1': {'word': 'common text processing modules', 'word_index': [(19, 22)], 'id': 'X96-1041.3'}, 'e2': {'word': 'text applications', 'word_index': [(12, 13)], 'id': 'X96-1041.2'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '12:13': 'ENTITYOTHER', '19:22': 'ENTITY'}}	The TIPSTER Architecture has been designed to enable a variety of different text applications to use a set of common text processing modules .
Since user interfaces work best when customized for particular applications , it is appropriator that no particular user interface styles or conventions are described in the TIPSTER Architecture specification.	TIPSTER Architecture specification	user interface styles or conventions	topic	{'e1': {'word': 'TIPSTER Architecture specification', 'word_index': [(26, 28)], 'id': 'X96-1041.7'}, 'e2': {'word': 'user interface styles or conventions', 'word_index': [(17, 21)], 'id': 'X96-1041.6'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '8:9': 'ENTITYUNRELATED', '17:21': 'ENTITYOTHER', '26:28': 'ENTITY'}}	Since user interfaces work best when customized for particular applications , it is appropriator that no particular user interface styles or conventions are described in the TIPSTER Architecture specification .
However, the Computing Research Laboratory (CRL) has constructed several TIPSTER applications that use a common set of configurable Graphical User Interface (GUI) functions.	Graphical User Interface (GUI) functions	TIPSTER applications	usage	{'e1': {'word': 'Graphical User Interface (GUI) functions', 'word_index': [(21, 27)], 'id': 'X96-1041.10'}, 'e2': {'word': 'TIPSTER applications', 'word_index': [(12, 13)], 'id': 'X96-1041.9'}, 'entity_replacement': {'3:8': 'ENTITYUNRELATED', '12:13': 'ENTITYOTHER', '21:27': 'ENTITY'}}	However , the Computing Research Laboratory ( CRL ) has constructed several TIPSTER applications that use a common set of configurable Graphical User Interface ( GUI ) functions .
These GUIs were constructed using CRL's TIPSTER User Interface Toolkit (TUIT).	CRL's TIPSTER User Interface Toolkit (TUIT)	GUIs	usage	"{'e1': {'word': ""CRL's TIPSTER User Interface Toolkit (TUIT)"", 'word_index': [(5, 13)], 'id': 'X96-1041.12'}, 'e2': {'word': 'GUIs', 'word_index': [(1, 1)], 'id': 'X96-1041.11'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '5:13': 'ENTITY'}}"	These GUIs were constructed using CRL 's TIPSTER User Interface Toolkit ( TUIT ) .
TUIT is a software library that can be used to construct multilingual TIPSTER user interfaces for a set of common user tasks.	software library	multilingual TIPSTER user interfaces	usage	{'e1': {'word': 'software library', 'word_index': [(3, 4)], 'id': 'X96-1041.14'}, 'e2': {'word': 'multilingual TIPSTER user interfaces', 'word_index': [(11, 14)], 'id': 'X96-1041.15'}, 'entity_replacement': {'0:0': 'ENTITYUNRELATED', '3:4': 'ENTITY', '11:14': 'ENTITYOTHER'}}	TUIT is a software library that can be used to construct multilingual TIPSTER user interfaces for a set of common user tasks .
This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish.	shallow techniques	deep processing	usage	{'e1': {'word': 'shallow techniques', 'word_index': [(11, 12)], 'id': 'C02-1071.2'}, 'e2': {'word': 'deep processing', 'word_index': [(6, 7)], 'id': 'C02-1071.1'}, 'entity_replacement': {'6:7': 'ENTITYOTHER', '11:12': 'ENTITY', '17:18': 'ENTITYUNRELATED', '22:26': 'ENTITYUNRELATED', '33:39': 'ENTITYUNRELATED'}}	This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish .
This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish.	linguistic PoS tagger and chunker	NLP system	part_whole	{'e1': {'word': 'linguistic PoS tagger and chunker', 'word_index': [(22, 26)], 'id': 'C02-1071.4'}, 'e2': {'word': 'NLP system', 'word_index': [(17, 18)], 'id': 'C02-1071.3'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '17:18': 'ENTITYOTHER', '22:26': 'ENTITY', '33:39': 'ENTITYUNRELATED'}}	This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish .
Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the precision of the grammar.	robustness	linguistic processing	model-feature	{'e1': {'word': 'robustness', 'word_index': [(17, 17)], 'id': 'C02-1071.7'}, 'e2': {'word': 'linguistic processing', 'word_index': [(20, 21)], 'id': 'C02-1071.8'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '17:17': 'ENTITY', '20:21': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '29:29': 'ENTITYUNRELATED', '32:32': 'ENTITYUNRELATED'}}	Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the precision of the grammar .
Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the precision of the grammar.	grammar	precision	result	{'e1': {'word': 'grammar', 'word_index': [(32, 32)], 'id': 'C02-1071.11'}, 'e2': {'word': 'precision', 'word_index': [(29, 29)], 'id': 'C02-1071.10'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '29:29': 'ENTITYOTHER', '32:32': 'ENTITY'}}	Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the precision of the grammar .
In this paper, we compare the performance of a state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken language and in generating sub-categorization cues from written and spoken language.	statistical parser	written and spoken language	usage	{'e1': {'word': 'statistical parser', 'word_index': [(17, 18)], 'id': 'P06-2067.1'}, 'e2': {'word': 'written and spoken language', 'word_index': [(26, 29)], 'id': 'P06-2067.2'}, 'entity_replacement': {'17:18': 'ENTITY', '26:29': 'ENTITYOTHER', '33:35': 'ENTITYUNRELATED', '37:40': 'ENTITYUNRELATED'}}	In this paper , we compare the performance of a state - of - the - art statistical parser ( Bikel , 2004 ) in parsing written and spoken language and in generating sub- categorization cues from written and spoken language .
Although Bikel's parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language.	Bikel's parser	accuracy	result	"{'e1': {'word': ""Bikel's parser"", 'word_index': [(1, 3)], 'id': 'P06-2067.5'}, 'e2': {'word': 'accuracy', 'word_index': [(7, 7)], 'id': 'P06-2067.6'}, 'entity_replacement': {'1:3': 'ENTITY', '7:7': 'ENTITYOTHER', '10:11': 'ENTITYUNRELATED', '17:17': 'ENTITYUNRELATED', '20:21': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED'}}"	Although Bikel 's parser achieves a higher accuracy for parsing written language , it achieves a higher accuracy when extracting subcategorization cues from spoken language .
Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language.	extracting subcategorization frames	written texts	usage	{'e1': {'word': 'extracting subcategorization frames', 'word_index': [(8, 10)], 'id': 'P06-2067.11'}, 'e2': {'word': 'written texts', 'word_index': [(14, 15)], 'id': 'P06-2067.12'}, 'entity_replacement': {'8:10': 'ENTITY', '14:15': 'ENTITYOTHER', '20:21': 'ENTITYUNRELATED'}}	Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language .
Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues.	punctuation	parsing	usage	{'e1': {'word': 'punctuation', 'word_index': [(7, 7)], 'id': 'P06-2067.14'}, 'e2': {'word': 'parsing', 'word_index': [(10, 10)], 'id': 'P06-2067.15'}, 'entity_replacement': {'7:7': 'ENTITY', '10:10': 'ENTITYOTHER', '12:12': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED'}}	Additionally , we explore the utility of punctuation in helping parsing and extraction of subcategorization cues .
Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language.	subcategorization cues	spoken language	part_whole	{'e1': {'word': 'subcategorization cues', 'word_index': [(15, 16)], 'id': 'P06-2067.20'}, 'e2': {'word': 'spoken language', 'word_index': [(18, 19)], 'id': 'P06-2067.21'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '15:16': 'ENTITY', '18:19': 'ENTITYOTHER'}}	Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language .
This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers.	punctuation	spoken corpora	part_whole	{'e1': {'word': 'punctuation', 'word_index': [(9, 9)], 'id': 'P06-2067.22'}, 'e2': {'word': 'spoken corpora', 'word_index': [(12, 13)], 'id': 'P06-2067.23'}, 'entity_replacement': {'9:9': 'ENTITY', '12:13': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED'}}	This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers .
This paper describes a characters-based Chinese collocation system and discusses the advantages of it over a traditional word-based system.	characters-based Chinese collocation system	word-based system	compare	{'e1': {'word': 'characters-based Chinese collocation system', 'word_index': [(4, 9)], 'id': 'C94-1088.1'}, 'e2': {'word': 'word-based system', 'word_index': [(19, 22)], 'id': 'C94-1088.2'}, 'entity_replacement': {'4:9': 'ENTITY', '19:22': 'ENTITYOTHER'}}	This paper describes a characters - based Chinese collocation system and discusses the advantages of it over a traditional word - based system .
Since wordbreaks are not conventionally marked in Chinese text corpora, a character-based collocation system has the dual advantages of avoiding pre-processing distortion and directly accessing sub-lexical information.	wordbreaks	Chinese text corpora	part_whole	{'e1': {'word': 'wordbreaks', 'word_index': [(1, 1)], 'id': 'C94-1088.3'}, 'e2': {'word': 'Chinese text corpora', 'word_index': [(7, 9)], 'id': 'C94-1088.4'}, 'entity_replacement': {'1:1': 'ENTITY', '7:9': 'ENTITYOTHER', '12:16': 'ENTITYUNRELATED', '23:24': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED'}}	Since wordbreaks are not conventionally marked in Chinese text corpora , a character - based collocation system has the dual advantages of avoiding pre-processing distortion and directly accessing sub-lexical information .
Furthermore, word-based collocational properties can be obtained through an auxiliary module of automatic segmentation.	automatic segmentation	word-based collocational properties	usage	{'e1': {'word': 'automatic segmentation', 'word_index': [(15, 16)], 'id': 'C94-1088.9'}, 'e2': {'word': 'word-based collocational properties', 'word_index': [(2, 6)], 'id': 'C94-1088.8'}, 'entity_replacement': {'2:6': 'ENTITYOTHER', '15:16': 'ENTITY'}}	Furthermore , word - based collocational properties can be obtained through an auxiliary module of automatic segmentation .
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.	bit-vector-based CKY-style parser	context-free parsing	usage	{'e1': {'word': 'bit-vector-based CKY-style parser', 'word_index': [(2, 8)], 'id': 'C04-1024.1'}, 'e2': {'word': 'context-free parsing', 'word_index': [(10, 13)], 'id': 'C04-1024.2'}, 'entity_replacement': {'2:8': 'ENTITY', '10:13': 'ENTITYOTHER'}}	An efficient bit-vector - based CKY - style parser for context - free parsing is presented .
The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences.	parse forest representation	analyses for large treebank grammars	model-feature	{'e1': {'word': 'parse forest representation', 'word_index': [(5, 7)], 'id': 'C04-1024.4'}, 'e2': {'word': 'analyses for large treebank grammars', 'word_index': [(14, 18)], 'id': 'C04-1024.5'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '5:7': 'ENTITY', '14:18': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED'}}	The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences .
The parser uses bit-vector operations to parallelise the basic parsing operations.	bit-vector operations	parser	usage	{'e1': {'word': 'bit-vector operations', 'word_index': [(3, 5)], 'id': 'C04-1024.8'}, 'e2': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'C04-1024.7'}, 'entity_replacement': {'1:1': 'ENTITYOTHER', '3:5': 'ENTITY', '9:11': 'ENTITYUNRELATED'}}	The parser uses bit- vector operations to parallelise the basic parsing operations .
We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.	language model	answers	usage	{'e1': {'word': 'language model', 'word_index': [(24, 25)], 'id': 'N04-1008.4'}, 'e2': {'word': 'answers', 'word_index': [(27, 27)], 'id': 'N04-1008.5'}, 'entity_replacement': {'3:8': 'ENTITYUNRELATED', '16:19': 'ENTITYUNRELATED', '24:25': 'ENTITY', '27:27': 'ENTITYOTHER', '30:31': 'ENTITYUNRELATED', '33:36': 'ENTITYUNRELATED', '41:41': 'ENTITYUNRELATED', '45:48': 'ENTITYUNRELATED'}}	We focus on FAQ - like questions and answers , and build our system around a noisy - channel architecture which exploits both a language model for answers and a transformation model for answer / question terms , trained on a corpus of 1 million question / answer pairs collected from the Web .
We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.	transformation model	answer/question terms	usage	{'e1': {'word': 'transformation model', 'word_index': [(30, 31)], 'id': 'N04-1008.6'}, 'e2': {'word': 'answer/question terms', 'word_index': [(33, 36)], 'id': 'N04-1008.7'}, 'entity_replacement': {'3:8': 'ENTITYUNRELATED', '16:19': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '30:31': 'ENTITY', '33:36': 'ENTITYOTHER', '41:41': 'ENTITYUNRELATED', '45:48': 'ENTITYUNRELATED'}}	We focus on FAQ - like questions and answers , and build our system around a noisy - channel architecture which exploits both a language model for answers and a transformation model for answer / question terms , trained on a corpus of 1 million question / answer pairs collected from the Web .
We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.	question/answer pairs	corpus	part_whole	{'e1': {'word': 'question/answer pairs', 'word_index': [(45, 48)], 'id': 'N04-1008.9'}, 'e2': {'word': 'corpus', 'word_index': [(41, 41)], 'id': 'N04-1008.8'}, 'entity_replacement': {'3:8': 'ENTITYUNRELATED', '16:19': 'ENTITYUNRELATED', '24:25': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '33:36': 'ENTITYUNRELATED', '41:41': 'ENTITYOTHER', '45:48': 'ENTITY'}}	We focus on FAQ - like questions and answers , and build our system around a noisy - channel architecture which exploits both a language model for answers and a transformation model for answer / question terms , trained on a corpus of 1 million question / answer pairs collected from the Web .
The applicability of many current information extraction techniques is severely limited by the need for supervised training data.	supervised training data	information extraction techniques	usage	{'e1': {'word': 'supervised training data', 'word_index': [(15, 17)], 'id': 'P05-1046.2'}, 'e2': {'word': 'information extraction techniques', 'word_index': [(5, 7)], 'id': 'P05-1046.1'}, 'entity_replacement': {'5:7': 'ENTITYOTHER', '15:17': 'ENTITY'}}	The applicability of many current information extraction techniques is severely limited by the need for supervised training data .
We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion.	prior knowledge	field structured extraction tasks	usage	{'e1': {'word': 'prior knowledge', 'word_index': [(21, 22)], 'id': 'P05-1046.4'}, 'e2': {'word': 'field structured extraction tasks', 'word_index': [(5, 8)], 'id': 'P05-1046.3'}, 'entity_replacement': {'5:8': 'ENTITYOTHER', '21:22': 'ENTITY'}}	We demonstrate that for certain field structured extraction tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .
Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains.	generative model	field structured text	model-feature	{'e1': {'word': 'generative model', 'word_index': [(10, 11)], 'id': 'P05-1046.6'}, 'e2': {'word': 'field structured text', 'word_index': [(13, 15)], 'id': 'P05-1046.7'}, 'entity_replacement': {'1:6': 'ENTITYUNRELATED', '10:11': 'ENTITY', '13:15': 'ENTITYOTHER', '18:20': 'ENTITYUNRELATED'}}	Although hidden Markov models ( HMMs ) provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains .
In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.	unsupervised methods	accuracies	result	{'e1': {'word': 'unsupervised methods', 'word_index': [(7, 8)], 'id': 'P05-1046.10'}, 'e2': {'word': 'accuracies', 'word_index': [(11, 11)], 'id': 'P05-1046.11'}, 'entity_replacement': {'7:8': 'ENTITY', '11:11': 'ENTITYOTHER', '14:15': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '30:31': 'ENTITYUNRELATED', '40:41': 'ENTITYUNRELATED'}}	In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .
In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.	labeled data	semi-supervised methods	usage	{'e1': {'word': 'labeled data', 'word_index': [(40, 41)], 'id': 'P05-1046.16'}, 'e2': {'word': 'semi-supervised methods', 'word_index': [(30, 31)], 'id': 'P05-1046.15'}, 'entity_replacement': {'7:8': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '14:15': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED', '30:31': 'ENTITYOTHER', '40:41': 'ENTITY'}}	In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .
Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.	independent classifiers	semantic role labeling	usage	{'e1': {'word': 'independent classifiers', 'word_index': [(15, 16)], 'id': 'P05-1073.2'}, 'e2': {'word': 'semantic role labeling', 'word_index': [(6, 8)], 'id': 'P05-1073.1'}, 'entity_replacement': {'6:8': 'ENTITYOTHER', '15:16': 'ENTITY', '22:24': 'ENTITYUNRELATED', '26:27': 'ENTITYUNRELATED'}}	Despite much recent progress on accurate semantic role labeling , previous work has largely used independent classifiers , possibly combined with separate label sequence models via Viterbi decoding .
This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.	dependencies	arguments	model-feature	{'e1': {'word': 'dependencies', 'word_index': [(21, 21)], 'id': 'P05-1073.6'}, 'e2': {'word': 'arguments', 'word_index': [(23, 23)], 'id': 'P05-1073.7'}, 'entity_replacement': {'11:13': 'ENTITYUNRELATED', '21:21': 'ENTITY', '23:23': 'ENTITYOTHER'}}	This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure , with strong dependencies between arguments .
We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.	joint model	argument frames	model-feature	{'e1': {'word': 'joint model', 'word_index': [(6, 7)], 'id': 'P05-1073.8'}, 'e2': {'word': 'argument frames', 'word_index': [(9, 10)], 'id': 'P05-1073.9'}, 'entity_replacement': {'6:7': 'ENTITY', '9:10': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '20:22': 'ENTITYUNRELATED'}}	We show how to build a joint model of argument frames , incorporating novel features that model these interactions into discriminative log-linear models .
We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.	features	discriminative log-linear models	part_whole	{'e1': {'word': 'features', 'word_index': [(14, 14)], 'id': 'P05-1073.10'}, 'e2': {'word': 'discriminative log-linear models', 'word_index': [(20, 22)], 'id': 'P05-1073.11'}, 'entity_replacement': {'6:7': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '14:14': 'ENTITY', '20:22': 'ENTITYOTHER'}}	We show how to build a joint model of argument frames , incorporating novel features that model these interactions into discriminative log-linear models .
This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.	gold-standard parse trees	PropBank	part_whole	{'e1': {'word': 'gold-standard parse trees', 'word_index': [(29, 33)], 'id': 'P05-1073.16'}, 'e2': {'word': 'PropBank', 'word_index': [(35, 35)], 'id': 'P05-1073.17'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '16:17': 'ENTITYUNRELATED', '27:27': 'ENTITYUNRELATED', '29:33': 'ENTITY', '35:35': 'ENTITYOTHER'}}	This system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state - of - the art independent classifier for gold - standard parse trees on PropBank .
It enables us to select a concise set of reading texts (from a target corpus) that contains all the target vocabulary to be learned.	texts	target corpus	part_whole	{'e1': {'word': 'texts', 'word_index': [(10, 10)], 'id': 'P05-3030.2'}, 'e2': {'word': 'target corpus', 'word_index': [(14, 15)], 'id': 'P05-3030.3'}, 'entity_replacement': {'10:10': 'ENTITY', '14:15': 'ENTITYOTHER', '21:22': 'ENTITYUNRELATED'}}	It enables us to select a concise set of reading texts ( from a target corpus ) that contains all the target vocabulary to be learned .
We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia, a free-content encyclopedia, as the target corpus.	vocabulary	target vocabulary	usage	{'e1': {'word': 'vocabulary', 'word_index': [(4, 4)], 'id': 'P05-3030.5'}, 'e2': {'word': 'target vocabulary', 'word_index': [(12, 13)], 'id': 'P05-3030.6'}, 'entity_replacement': {'4:4': 'ENTITY', '12:13': 'ENTITYOTHER', '16:17': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia , a free - content encyclopedia , as the target corpus .
We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia, a free-content encyclopedia, as the target corpus.	English Wikipedia	target corpus	usage	{'e1': {'word': 'English Wikipedia', 'word_index': [(16, 17)], 'id': 'P05-3030.7'}, 'e2': {'word': 'target corpus', 'word_index': [(27, 28)], 'id': 'P05-3030.8'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '16:17': 'ENTITY', '27:28': 'ENTITYOTHER'}}	We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia , a free - content encyclopedia , as the target corpus .
Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary.	Procedural Systemic Grammar	syntactic analyzer	usage	{'e1': {'word': 'Procedural Systemic Grammar', 'word_index': [(18, 20)], 'id': 'E83-1029.3'}, 'e2': {'word': 'syntactic analyzer', 'word_index': [(12, 13)], 'id': 'E83-1029.2'}, 'entity_replacement': {'12:13': 'ENTITYOTHER', '18:20': 'ENTITY', '23:24': 'ENTITYUNRELATED', '28:30': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED'}}	Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .
Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary.	Conceptual Dependency Theory	semantic analyzer	usage	{'e1': {'word': 'Conceptual Dependency Theory', 'word_index': [(28, 30)], 'id': 'E83-1029.5'}, 'e2': {'word': 'semantic analyzer', 'word_index': [(23, 24)], 'id': 'E83-1029.4'}, 'entity_replacement': {'12:13': 'ENTITYUNRELATED', '18:20': 'ENTITYUNRELATED', '23:24': 'ENTITYOTHER', '28:30': 'ENTITY', '34:34': 'ENTITYUNRELATED'}}	Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .
A proposal to deal with French tenses in the framework of Discourse Representation Theory is presented, as it has been implemented for a fragment at the IMS.	Discourse Representation Theory	French tenses	model-feature	{'e1': {'word': 'Discourse Representation Theory', 'word_index': [(11, 13)], 'id': 'E89-1006.2'}, 'e2': {'word': 'French tenses', 'word_index': [(5, 6)], 'id': 'E89-1006.1'}, 'entity_replacement': {'5:6': 'ENTITYOTHER', '11:13': 'ENTITY', '27:27': 'ENTITYUNRELATED'}}	A proposal to deal with French tenses in the framework of Discourse Representation Theory is presented , as it has been implemented for a fragment at the IMS .
Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text.	meaning	tenses	model-feature	{'e1': {'word': 'meaning', 'word_index': [(7, 7)], 'id': 'E89-1006.6'}, 'e2': {'word': 'tenses', 'word_index': [(10, 10)], 'id': 'E89-1006.7'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '7:7': 'ENTITY', '10:10': 'ENTITYOTHER', '26:26': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '44:44': 'ENTITYUNRELATED', '47:47': 'ENTITYUNRELATED', '50:51': 'ENTITYUNRELATED', '55:55': 'ENTITYUNRELATED'}}	Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text .
Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text.	meaning	text	model-feature	{'e1': {'word': 'meaning', 'word_index': [(31, 31)], 'id': 'E89-1006.9'}, 'e2': {'word': 'text', 'word_index': [(34, 34)], 'id': 'E89-1006.10'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '31:31': 'ENTITY', '34:34': 'ENTITYOTHER', '44:44': 'ENTITYUNRELATED', '47:47': 'ENTITYUNRELATED', '50:51': 'ENTITYUNRELATED', '55:55': 'ENTITYUNRELATED'}}	Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text .
Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text.	events	sentence	part_whole	{'e1': {'word': 'events', 'word_index': [(44, 44)], 'id': 'E89-1006.11'}, 'e2': {'word': 'sentence', 'word_index': [(47, 47)], 'id': 'E89-1006.12'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '44:44': 'ENTITY', '47:47': 'ENTITYOTHER', '50:51': 'ENTITYUNRELATED', '55:55': 'ENTITYUNRELATED'}}	Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text .
Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text.	event structure	text	model-feature	{'e1': {'word': 'event structure', 'word_index': [(50, 51)], 'id': 'E89-1006.13'}, 'e2': {'word': 'text', 'word_index': [(55, 55)], 'id': 'E89-1006.14'}, 'entity_replacement': {'3:3': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '31:31': 'ENTITYUNRELATED', '34:34': 'ENTITYUNRELATED', '44:44': 'ENTITYUNRELATED', '47:47': 'ENTITYUNRELATED', '50:51': 'ENTITY', '55:55': 'ENTITYOTHER'}}	Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text .
Thereby a system of relevant times provided by the preceeding text and by the temporal adverbials of the sentence being processed is used.	temporal adverbials	sentence	part_whole	{'e1': {'word': 'temporal adverbials', 'word_index': [(14, 15)], 'id': 'E89-1006.17'}, 'e2': {'word': 'sentence', 'word_index': [(18, 18)], 'id': 'E89-1006.18'}, 'entity_replacement': {'2:5': 'ENTITYUNRELATED', '10:10': 'ENTITYUNRELATED', '14:15': 'ENTITY', '18:18': 'ENTITYOTHER'}}	Thereby a system of relevant times provided by the preceeding text and by the temporal adverbials of the sentence being processed is used .
In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the resolution component and not in the process of syntactic analysis.	meaning	tenses	model-feature	{'e1': {'word': 'meaning', 'word_index': [(11, 11)], 'id': 'E89-1006.26'}, 'e2': {'word': 'tenses', 'word_index': [(14, 14)], 'id': 'E89-1006.27'}, 'entity_replacement': {'11:11': 'ENTITY', '14:14': 'ENTITYOTHER', '19:20': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the resolution component and not in the process of syntactic analysis .
In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the resolution component and not in the process of syntactic analysis.	resolution component	syntactic analysis	compare	{'e1': {'word': 'resolution component', 'word_index': [(19, 20)], 'id': 'E89-1006.28'}, 'e2': {'word': 'syntactic analysis', 'word_index': [(27, 28)], 'id': 'E89-1006.29'}, 'entity_replacement': {'11:11': 'ENTITYUNRELATED', '14:14': 'ENTITYUNRELATED', '19:20': 'ENTITY', '27:28': 'ENTITYOTHER'}}	In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the resolution component and not in the process of syntactic analysis .
The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously, and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT (LF).	languages	grammatical frameworks	model-feature	{'e1': {'word': 'languages', 'word_index': [(5, 5)], 'id': 'E93-1004.7'}, 'e2': {'word': 'grammatical frameworks', 'word_index': [(12, 13)], 'id': 'E93-1004.8'}, 'entity_replacement': {'5:5': 'ENTITY', '12:13': 'ENTITYOTHER', '28:28': 'ENTITYUNRELATED', '33:36': 'ENTITYUNRELATED'}}	The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously , and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT ( LF ) .
The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously, and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT (LF).	GPSG	LT (LF)	compare	{'e1': {'word': 'GPSG', 'word_index': [(28, 28)], 'id': 'E93-1004.9'}, 'e2': {'word': 'LT (LF)', 'word_index': [(33, 36)], 'id': 'E93-1004.10'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '28:28': 'ENTITY', '33:36': 'ENTITYOTHER'}}	The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously , and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT ( LF ) .
One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality (EDOL).	Tree Adjoining Grammars	extended domain of locality (EDOL)	model-feature	{'e1': {'word': 'Tree Adjoining Grammars', 'word_index': [(6, 8)], 'id': 'E99-1029.1'}, 'e2': {'word': 'extended domain of locality (EDOL)', 'word_index': [(14, 20)], 'id': 'E99-1029.2'}, 'entity_replacement': {'6:8': 'ENTITY', '14:20': 'ENTITYOTHER'}}	One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality ( EDOL ) .
We consider how this can be exploited to limit the need for feature structure unification during parsing.	feature structure unification	parsing	usage	{'e1': {'word': 'feature structure unification', 'word_index': [(12, 14)], 'id': 'E99-1029.3'}, 'e2': {'word': 'parsing', 'word_index': [(16, 16)], 'id': 'E99-1029.4'}, 'entity_replacement': {'12:14': 'ENTITY', '16:16': 'ENTITYOTHER'}}	We consider how this can be exploited to limit the need for feature structure unification during parsing .
We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways.	LEXSYS	XTAG	compare	{'e1': {'word': 'LEXSYS', 'word_index': [(11, 11)], 'id': 'E99-1029.6'}, 'e2': {'word': 'XTAG', 'word_index': [(13, 13)], 'id': 'E99-1029.7'}, 'entity_replacement': {'6:9': 'ENTITYUNRELATED', '11:11': 'ENTITY', '13:13': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED'}}	We compare two wide - coverage lexicalized grammars of English , LEXSYS and XTAG , finding that the two grammars exploit EDOL in different ways .
We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways.	EDOL	grammars	usage	{'e1': {'word': 'EDOL', 'word_index': [(21, 21)], 'id': 'E99-1029.9'}, 'e2': {'word': 'grammars', 'word_index': [(19, 19)], 'id': 'E99-1029.8'}, 'entity_replacement': {'6:9': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '13:13': 'ENTITYUNRELATED', '19:19': 'ENTITYOTHER', '21:21': 'ENTITY'}}	We compare two wide - coverage lexicalized grammars of English , LEXSYS and XTAG , finding that the two grammars exploit EDOL in different ways .
We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model.	dependency-based grammar model	sentence-level and text-level anaphora	model-feature	{'e1': {'word': 'dependency-based grammar model', 'word_index': [(19, 23)], 'id': 'E95-1033.2'}, 'e2': {'word': 'sentence-level and text-level anaphora', 'word_index': [(6, 13)], 'id': 'E95-1033.1'}, 'entity_replacement': {'6:13': 'ENTITYOTHER', '19:23': 'ENTITY'}}	We provide a unified account of sentence - level and text - level anaphora within the framework of a dependency - based grammar model .
Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model.	Grosz-Sidner-style focus model	text-level anaphora	model-feature	{'e1': {'word': 'Grosz-Sidner-style focus model', 'word_index': [(29, 35)], 'id': 'E95-1033.7'}, 'e2': {'word': 'text-level anaphora', 'word_index': [(19, 22)], 'id': 'E95-1033.6'}, 'entity_replacement': {'2:3': 'ENTITYUNRELATED', '5:6': 'ENTITYUNRELATED', '11:14': 'ENTITYUNRELATED', '19:22': 'ENTITYOTHER', '29:35': 'ENTITY'}}	Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB 's binding theory , while those for text - level anaphora incorporate an adapted version of a Grosz - Sidner - style focus model .
In contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering, our approach attempts to express the speech knowledge within a formal framework using well-defined mathematical tools.	knowledge engineering	heuristic rules	usage	{'e1': {'word': 'knowledge engineering', 'word_index': [(18, 19)], 'id': 'H89-1027.4'}, 'e2': {'word': 'heuristic rules', 'word_index': [(12, 13)], 'id': 'H89-1027.3'}, 'entity_replacement': {'12:13': 'ENTITYOTHER', '18:19': 'ENTITY', '27:28': 'ENTITYUNRELATED'}}	In contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering , our approach attempts to express the speech knowledge within a formal framework using well - defined mathematical tools .
In our system, features and decision strategies are discovered and trained automatically, using a large body of speech data.	speech data	decision strategies	usage	{'e1': {'word': 'speech data', 'word_index': [(19, 20)], 'id': 'H89-1027.8'}, 'e2': {'word': 'decision strategies', 'word_index': [(6, 7)], 'id': 'H89-1027.7'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '6:7': 'ENTITYOTHER', '19:20': 'ENTITY'}}	In our system , features and decision strategies are discovered and trained automatically , using a large body of speech data .
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses.	WordNet	sense resolution	usage	{'e1': {'word': 'WordNet', 'word_index': [(11, 11)], 'id': 'H91-1077.2'}, 'e2': {'word': 'sense resolution', 'word_index': [(3, 4)], 'id': 'H91-1077.1'}, 'entity_replacement': {'3:4': 'ENTITYOTHER', '11:11': 'ENTITY', '17:18': 'ENTITYUNRELATED', '21:22': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED', '38:39': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED'}}	A method of sense resolution is proposed that is based on WordNet , an on - line lexical database that incorporates semantic relations ( synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment ) as labeled pointers between word senses .
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses.	semantic relations	lexical database	part_whole	{'e1': {'word': 'semantic relations', 'word_index': [(21, 22)], 'id': 'H91-1077.4'}, 'e2': {'word': 'lexical database', 'word_index': [(17, 18)], 'id': 'H91-1077.3'}, 'entity_replacement': {'3:4': 'ENTITYUNRELATED', '11:11': 'ENTITYUNRELATED', '17:18': 'ENTITYOTHER', '21:22': 'ENTITY', '24:24': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED', '28:28': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '32:35': 'ENTITYUNRELATED', '38:39': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED'}}	A method of sense resolution is proposed that is based on WordNet , an on - line lexical database that incorporates semantic relations ( synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment ) as labeled pointers between word senses .
With WordNet, it is easy to retrieve sets of semantically related words, a facility that will be used for sense resolution during text processing, as follows.	semantically related words	WordNet	part_whole	{'e1': {'word': 'semantically related words', 'word_index': [(11, 13)], 'id': 'H91-1077.13'}, 'e2': {'word': 'WordNet', 'word_index': [(1, 2)], 'id': 'H91-1077.12'}, 'entity_replacement': {'1:2': 'ENTITYOTHER', '11:13': 'ENTITY', '22:23': 'ENTITYUNRELATED', '25:26': 'ENTITYUNRELATED'}}	With Word Net , it is easy to retrieve sets of semantically related words , a facility that will be used for sense resolution during text processing , as follows .
With WordNet, it is easy to retrieve sets of semantically related words, a facility that will be used for sense resolution during text processing, as follows.	sense resolution	text processing	part_whole	{'e1': {'word': 'sense resolution', 'word_index': [(22, 23)], 'id': 'H91-1077.14'}, 'e2': {'word': 'text processing', 'word_index': [(25, 26)], 'id': 'H91-1077.15'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '11:13': 'ENTITYUNRELATED', '22:23': 'ENTITY', '25:26': 'ENTITYOTHER'}}	With Word Net , it is easy to retrieve sets of semantically related words , a facility that will be used for sense resolution during text processing , as follows .
When a word with multiple senses is encountered, one of two procedures will be followed.	senses	word	model-feature	{'e1': {'word': 'senses', 'word_index': [(5, 5)], 'id': 'H91-1077.17'}, 'e2': {'word': 'word', 'word_index': [(2, 2)], 'id': 'H91-1077.16'}, 'entity_replacement': {'2:2': 'ENTITYOTHER', '5:5': 'ENTITY'}}	When a word with multiple senses is encountered , one of two procedures will be followed .
Either, (1) words related in meaning to the alternative senses of the polysemous word will be retrieved; new strings will be derived by substituting these related words into the context of the polysemous word; a large textual corpus will then be searched for these derived strings; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus.	derived strings	textual corpus	part_whole	{'e1': {'word': 'derived strings', 'word_index': [(49, 50)], 'id': 'H91-1077.27'}, 'e2': {'word': 'textual corpus', 'word_index': [(41, 42)], 'id': 'H91-1077.26'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED', '41:42': 'ENTITYOTHER', '49:50': 'ENTITY', '54:54': 'ENTITYUNRELATED', '62:63': 'ENTITYUNRELATED', '71:71': 'ENTITYUNRELATED'}}	Either , ( 1 ) words related in meaning to the alternative senses of the polysemous word will be retrieved ; new strings will be derived by substituting these related words into the context of the polysemous word ; a large textual corpus will then be searched for these derived strings ; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus .
Either, (1) words related in meaning to the alternative senses of the polysemous word will be retrieved; new strings will be derived by substituting these related words into the context of the polysemous word; a large textual corpus will then be searched for these derived strings; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus.	derived string	corpus	part_whole	{'e1': {'word': 'derived string', 'word_index': [(62, 63)], 'id': 'H91-1077.29'}, 'e2': {'word': 'corpus', 'word_index': [(71, 71)], 'id': 'H91-1077.30'}, 'entity_replacement': {'5:5': 'ENTITYUNRELATED', '8:8': 'ENTITYUNRELATED', '11:12': 'ENTITYUNRELATED', '15:16': 'ENTITYUNRELATED', '22:22': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '33:33': 'ENTITYUNRELATED', '36:37': 'ENTITYUNRELATED', '41:42': 'ENTITYUNRELATED', '49:50': 'ENTITYUNRELATED', '54:54': 'ENTITYUNRELATED', '62:63': 'ENTITY', '71:71': 'ENTITYOTHER'}}	Either , ( 1 ) words related in meaning to the alternative senses of the polysemous word will be retrieved ; new strings will be derived by substituting these related words into the context of the polysemous word ; a large textual corpus will then be searched for these derived strings ; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus .
Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.	context	polysemous word	model-feature	{'e1': {'word': 'context', 'word_index': [(6, 6)], 'id': 'H91-1077.31'}, 'e2': {'word': 'polysemous word', 'word_index': [(9, 10)], 'id': 'H91-1077.32'}, 'entity_replacement': {'6:6': 'ENTITY', '9:10': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED', '48:48': 'ENTITYUNRELATED', '51:52': 'ENTITYUNRELATED', '55:56': 'ENTITYUNRELATED', '60:60': 'ENTITYUNRELATED', '68:68': 'ENTITYUNRELATED', '71:71': 'ENTITYUNRELATED', '76:76': 'ENTITYUNRELATED', '89:90': 'ENTITYUNRELATED', '92:93': 'ENTITYUNRELATED', '95:97': 'ENTITYUNRELATED'}}	Or , ( 2 ) the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; Word Net will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this procedure could have practical applications to problems of information retrieval , mechanical translation , intelligent tutoring systems , and elsewhere .
Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.	context	words	model-feature	{'e1': {'word': 'context', 'word_index': [(30, 30)], 'id': 'H91-1077.35'}, 'e2': {'word': 'words', 'word_index': [(24, 24)], 'id': 'H91-1077.34'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYOTHER', '30:30': 'ENTITY', '35:36': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED', '48:48': 'ENTITYUNRELATED', '51:52': 'ENTITYUNRELATED', '55:56': 'ENTITYUNRELATED', '60:60': 'ENTITYUNRELATED', '68:68': 'ENTITYUNRELATED', '71:71': 'ENTITYUNRELATED', '76:76': 'ENTITYUNRELATED', '89:90': 'ENTITYUNRELATED', '92:93': 'ENTITYUNRELATED', '95:97': 'ENTITYUNRELATED'}}	Or , ( 2 ) the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; Word Net will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this procedure could have practical applications to problems of information retrieval , mechanical translation , intelligent tutoring systems , and elsewhere .
Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.	WordNet	semantic distance	usage	{'e1': {'word': 'WordNet', 'word_index': [(35, 35)], 'id': 'H91-1077.36'}, 'e2': {'word': 'semantic distance', 'word_index': [(43, 44)], 'id': 'H91-1077.37'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '35:35': 'ENTITY', '43:44': 'ENTITYOTHER', '47:47': 'ENTITYUNRELATED', '50:51': 'ENTITYUNRELATED', '54:55': 'ENTITYUNRELATED', '59:59': 'ENTITYUNRELATED', '67:67': 'ENTITYUNRELATED', '70:70': 'ENTITYUNRELATED', '75:75': 'ENTITYUNRELATED', '88:89': 'ENTITYUNRELATED', '91:92': 'ENTITYUNRELATED', '94:96': 'ENTITYUNRELATED'}}	Or , ( 2 ) the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this procedure could have practical applications to problems of information retrieval , mechanical translation , intelligent tutoring systems , and elsewhere .
Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.	context	words	model-feature	{'e1': {'word': 'context', 'word_index': [(76, 76)], 'id': 'H91-1077.44'}, 'e2': {'word': 'words', 'word_index': [(71, 71)], 'id': 'H91-1077.43'}, 'entity_replacement': {'6:6': 'ENTITYUNRELATED', '9:10': 'ENTITYUNRELATED', '21:21': 'ENTITYUNRELATED', '24:24': 'ENTITYUNRELATED', '30:30': 'ENTITYUNRELATED', '35:36': 'ENTITYUNRELATED', '44:45': 'ENTITYUNRELATED', '48:48': 'ENTITYUNRELATED', '51:52': 'ENTITYUNRELATED', '55:56': 'ENTITYUNRELATED', '60:60': 'ENTITYUNRELATED', '68:68': 'ENTITYUNRELATED', '71:71': 'ENTITYOTHER', '76:76': 'ENTITY', '89:90': 'ENTITYUNRELATED', '92:93': 'ENTITYUNRELATED', '95:97': 'ENTITYUNRELATED'}}	Or , ( 2 ) the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; Word Net will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this procedure could have practical applications to problems of information retrieval , mechanical translation , intelligent tutoring systems , and elsewhere .
In this paper, we want to show how the morphological component of an existing NLP-system for Dutch (Dutch Medical Language Processor - DMLP) has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system (Linguistic String Project - Medical Language Processor) of the New York University.	morphological component	NLP-system for Dutch (Dutch Medical Language Processor - DMLP)	part_whole	{'e1': {'word': 'morphological component', 'word_index': [(10, 11)], 'id': 'A97-1027.1'}, 'e2': {'word': 'NLP-system for Dutch (Dutch Medical Language Processor - DMLP)', 'word_index': [(15, 27)], 'id': 'A97-1027.2'}, 'entity_replacement': {'10:11': 'ENTITY', '15:27': 'ENTITYOTHER', '41:43': 'ENTITYUNRELATED', '46:58': 'ENTITYUNRELATED'}}	In this paper , we want to show how the morphological component of an existing NLP - system for Dutch ( Dutch Medical Language Processor - DMLP ) has been extended in order to produce output that is compatible with the language independent modules of the LSP - MLP system ( Linguistic String Project - Medical Language Processor ) of the New York University .
In this paper, we want to show how the morphological component of an existing NLP-system for Dutch (Dutch Medical Language Processor - DMLP) has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system (Linguistic String Project - Medical Language Processor) of the New York University.	language independent modules	LSP-MLP system (Linguistic String Project - Medical Language Processor)	part_whole	{'e1': {'word': 'language independent modules', 'word_index': [(41, 43)], 'id': 'A97-1027.3'}, 'e2': {'word': 'LSP-MLP system (Linguistic String Project - Medical Language Processor)', 'word_index': [(46, 58)], 'id': 'A97-1027.4'}, 'entity_replacement': {'10:11': 'ENTITYUNRELATED', '15:27': 'ENTITYUNRELATED', '41:43': 'ENTITY', '46:58': 'ENTITYOTHER'}}	In this paper , we want to show how the morphological component of an existing NLP - system for Dutch ( Dutch Medical Language Processor - DMLP ) has been extended in order to produce output that is compatible with the language independent modules of the LSP - MLP system ( Linguistic String Project - Medical Language Processor ) of the New York University .
The former can take advantage of the language independent developments of the latter, while focusing on idiosyncrasies for Dutch.	idiosyncrasies	Dutch	part_whole	{'e1': {'word': 'idiosyncrasies', 'word_index': [(17, 17)], 'id': 'A97-1027.6'}, 'e2': {'word': 'Dutch', 'word_index': [(19, 19)], 'id': 'A97-1027.7'}, 'entity_replacement': {'7:9': 'ENTITYUNRELATED', '17:17': 'ENTITY', '19:19': 'ENTITYOTHER'}}	The former can take advantage of the language independent developments of the latter , while focusing on idiosyncrasies for Dutch .
We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora.	textual corpora	subcategorization dictionary	usage	{'e1': {'word': 'textual corpora', 'word_index': [(14, 15)], 'id': 'A97-1052.2'}, 'e2': {'word': 'subcategorization dictionary', 'word_index': [(11, 12)], 'id': 'A97-1052.1'}, 'entity_replacement': {'11:12': 'ENTITYOTHER', '14:15': 'ENTITY'}}	We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora .
Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English.	relative frequency of occurrence	subcategorization classes	model-feature	{'e1': {'word': 'relative frequency of occurrence', 'word_index': [(5, 8)], 'id': 'A97-1052.4'}, 'e2': {'word': 'subcategorization classes', 'word_index': [(14, 15)], 'id': 'A97-1052.5'}, 'entity_replacement': {'1:2': 'ENTITYUNRELATED', '5:8': 'ENTITY', '14:15': 'ENTITYOTHER', '17:17': 'ENTITYUNRELATED'}}	Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English .
An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes.	multiple complementation patterns	verbs	model-feature	{'e1': {'word': 'multiple complementation patterns', 'word_index': [(12, 14)], 'id': 'A97-1052.8'}, 'e2': {'word': 'verbs', 'word_index': [(9, 9)], 'id': 'A97-1052.7'}, 'entity_replacement': {'9:9': 'ENTITYOTHER', '12:14': 'ENTITY', '21:21': 'ENTITYUNRELATED', '37:38': 'ENTITYUNRELATED'}}	An initial experiment , on a sample of 14 verbs which exhibit multiple complementation patterns , demonstrates that the technique achieves accuracy comparable to previous approaches , which are all limited to a highly restricted set of subcategorization classes .
We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount</abstract>	subcategorization dictionary	accuracy	result	{'e1': {'word': 'subcategorization dictionary', 'word_index': [(5, 6)], 'id': 'A97-1052.11'}, 'e2': {'word': 'accuracy', 'word_index': [(13, 13)], 'id': 'A97-1052.12'}, 'entity_replacement': {'5:6': 'ENTITY', '13:13': 'ENTITYOTHER', '16:16': 'ENTITYUNRELATED'}}	We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount < / abstract >
This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns.	phrasal patterns	dictionary word sense definitions	usage	{'e1': {'word': 'phrasal patterns', 'word_index': [(16, 17)], 'id': 'J87-3001.2'}, 'e2': {'word': 'dictionary word sense definitions', 'word_index': [(4, 7)], 'id': 'J87-3001.1'}, 'entity_replacement': {'4:7': 'ENTITYOTHER', '16:17': 'ENTITY'}}	This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns .
An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English.	definitions	Longman Dictionary of Contemporary English	part_whole	{'e1': {'word': 'definitions', 'word_index': [(11, 11)], 'id': 'J87-3001.3'}, 'e2': {'word': 'Longman Dictionary of Contemporary English', 'word_index': [(14, 18)], 'id': 'J87-3001.4'}, 'entity_replacement': {'11:11': 'ENTITY', '14:18': 'ENTITYOTHER'}}	An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English .
A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions.	restricted vocabulary	word sense definitions	usage	{'e1': {'word': 'restricted vocabulary', 'word_index': [(16, 17)], 'id': 'J87-3001.6'}, 'e2': {'word': 'word sense definitions', 'word_index': [(20, 22)], 'id': 'J87-3001.7'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '16:17': 'ENTITY', '20:22': 'ENTITYOTHER'}}	A property of this dictionary , exploited by the system , is that it uses a restricted vocabulary in its word sense definitions .
The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary.	senses	word senses	model-feature	{'e1': {'word': 'senses', 'word_index': [(23, 23)], 'id': 'J87-3001.10'}, 'e2': {'word': 'word senses', 'word_index': [(17, 18)], 'id': 'J87-3001.9'}, 'entity_replacement': {'14:14': 'ENTITYUNRELATED', '17:18': 'ENTITYOTHER', '23:23': 'ENTITY', '25:25': 'ENTITYUNRELATED', '28:29': 'ENTITYUNRELATED'}}	The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary .
The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary.	words	restricted vocabulary	part_whole	{'e1': {'word': 'words', 'word_index': [(25, 25)], 'id': 'J87-3001.11'}, 'e2': {'word': 'restricted vocabulary', 'word_index': [(28, 29)], 'id': 'J87-3001.12'}, 'entity_replacement': {'14:14': 'ENTITYUNRELATED', '17:18': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '25:25': 'ENTITY', '28:29': 'ENTITYOTHER'}}	The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary .
This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism.	analysis mechanism	definitions	topic	{'e1': {'word': 'analysis mechanism', 'word_index': [(24, 25)], 'id': 'J87-3001.17'}, 'e2': {'word': 'definitions', 'word_index': [(8, 8)], 'id': 'J87-3001.16'}, 'entity_replacement': {'8:8': 'ENTITYOTHER', '24:25': 'ENTITY'}}	This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible , resulting in a relatively robust analysis mechanism .
Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions.	robustness problems	natural language processing systems	model-feature	{'e1': {'word': 'robustness problems', 'word_index': [(6, 7)], 'id': 'J87-3001.18'}, 'e2': {'word': 'natural language processing systems', 'word_index': [(12, 15)], 'id': 'J87-3001.19'}, 'entity_replacement': {'6:7': 'ENTITY', '12:15': 'ENTITYOTHER', '21:21': 'ENTITYUNRELATED', '25:25': 'ENTITYUNRELATED', '27:28': 'ENTITYUNRELATED'}}	Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems : coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions .
This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts.	latent variable model	evaluation method	usage	{'e1': {'word': 'latent variable model', 'word_index': [(8, 10)], 'id': 'I05-5009.2'}, 'e2': {'word': 'evaluation method', 'word_index': [(4, 5)], 'id': 'I05-5009.1'}, 'entity_replacement': {'4:5': 'ENTITYOTHER', '8:10': 'ENTITY', '12:12': 'ENTITYUNRELATED', '15:15': 'ENTITYUNRELATED'}}	This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts .
This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts.	contexts	paraphrases	model-feature	{'e1': {'word': 'contexts', 'word_index': [(15, 15)], 'id': 'I05-5009.4'}, 'e2': {'word': 'paraphrases', 'word_index': [(12, 12)], 'id': 'I05-5009.3'}, 'entity_replacement': {'4:5': 'ENTITYUNRELATED', '8:10': 'ENTITYUNRELATED', '12:12': 'ENTITYOTHER', '15:15': 'ENTITY'}}	This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts .
We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred.	context	sentence	model-feature	{'e1': {'word': 'context', 'word_index': [(4, 4)], 'id': 'I05-5009.5'}, 'e2': {'word': 'sentence', 'word_index': [(7, 7)], 'id': 'I05-5009.6'}, 'entity_replacement': {'4:4': 'ENTITY', '7:7': 'ENTITYOTHER', '12:13': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred .
We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred.	latent variable	model	part_whole	{'e1': {'word': 'latent variable', 'word_index': [(12, 13)], 'id': 'I05-5009.7'}, 'e2': {'word': 'model', 'word_index': [(16, 16)], 'id': 'I05-5009.8'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '12:13': 'ENTITY', '16:16': 'ENTITYOTHER', '19:19': 'ENTITYUNRELATED', '23:23': 'ENTITYUNRELATED', '26:26': 'ENTITYUNRELATED'}}	We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred .
We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred.	likelihood	variable	model-feature	{'e1': {'word': 'likelihood', 'word_index': [(23, 23)], 'id': 'I05-5009.10'}, 'e2': {'word': 'variable', 'word_index': [(26, 26)], 'id': 'I05-5009.11'}, 'entity_replacement': {'4:4': 'ENTITYUNRELATED', '7:7': 'ENTITYUNRELATED', '12:13': 'ENTITYUNRELATED', '16:16': 'ENTITYUNRELATED', '19:19': 'ENTITYUNRELATED', '23:23': 'ENTITY', '26:26': 'ENTITYOTHER'}}	We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred .
A paraphrase is evaluated for whether its sentences are used in the same context.	sentences	context	model-feature	{'e1': {'word': 'sentences', 'word_index': [(7, 7)], 'id': 'I05-5009.13'}, 'e2': {'word': 'context', 'word_index': [(13, 13)], 'id': 'I05-5009.14'}, 'entity_replacement': {'1:1': 'ENTITYUNRELATED', '7:7': 'ENTITY', '13:13': 'ENTITYOTHER'}}	A paraphrase is evaluated for whether its sentences are used in the same context .
The results also revealed an upper bound of accuracy of 77% with the method when using only topic information.	topic information	accuracy	result	{'e1': {'word': 'topic information', 'word_index': [(18, 19)], 'id': 'I05-5009.19'}, 'e2': {'word': 'accuracy', 'word_index': [(8, 8)], 'id': 'I05-5009.17'}, 'entity_replacement': {'8:8': 'ENTITYOTHER', '14:14': 'ENTITYUNRELATED', '18:19': 'ENTITY'}}	The results also revealed an upper bound of accuracy of 77 % with the method when using only topic information .
An extension to the GPSG grammatical formalism is proposed, allowing non-terminals to consist of finite sequences of category labels, and allowing schematic variables to range over such sequences.	category labels	non-terminals	part_whole	{'e1': {'word': 'category labels', 'word_index': [(18, 19)], 'id': 'P83-1003.3'}, 'e2': {'word': 'non-terminals', 'word_index': [(11, 11)], 'id': 'P83-1003.2'}, 'entity_replacement': {'4:6': 'ENTITYUNRELATED', '11:11': 'ENTITYOTHER', '18:19': 'ENTITY', '23:24': 'ENTITYUNRELATED'}}	An extension to the GPSG grammatical formalism is proposed , allowing non-terminals to consist of finite sequences of category labels , and allowing schematic variables to range over such sequences .
The extension is shown to be sufficient to provide a strongly adequate grammar for crossed serial dependencies, as found in e.g. Dutch subordinate clauses.	grammar	crossed serial dependencies	usage	{'e1': {'word': 'grammar', 'word_index': [(12, 12)], 'id': 'P83-1003.5'}, 'e2': {'word': 'crossed serial dependencies', 'word_index': [(14, 16)], 'id': 'P83-1003.6'}, 'entity_replacement': {'12:12': 'ENTITY', '14:16': 'ENTITYOTHER', '22:24': 'ENTITYUNRELATED'}}	The extension is shown to be sufficient to provide a strongly adequate grammar for crossed serial dependencies , as found in e.g. Dutch subordinate clauses .
The extension is shown to be parseable by a simple extension to an existing parsing method for GPSG.	parsing method	GPSG	usage	{'e1': {'word': 'parsing method', 'word_index': [(14, 15)], 'id': 'P83-1003.10'}, 'e2': {'word': 'GPSG', 'word_index': [(17, 17)], 'id': 'P83-1003.11'}, 'entity_replacement': {'14:15': 'ENTITY', '17:17': 'ENTITYOTHER'}}	The extension is shown to be parseable by a simple extension to an existing parsing method for GPSG .
