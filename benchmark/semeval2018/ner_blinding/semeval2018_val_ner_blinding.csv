original_sentence	e1	e2	relation_type	metadata	preprocessed_sentence
In Semantic Role Labeling (SRL), arguments are usually limited in a syntax subtree.	arguments	syntax subtree	part_whole	{'e1': {'word': 'arguments', 'word_index': [(3, 3)], 'id': 'C08-1105.2'}, 'e2': {'word': 'syntax subtree', 'word_index': [(6, 6)], 'id': 'C08-1105.3'}}	O Role Labeling O in a O O O O O O O O O O O O O O O
It is reasonable to label arguments locally in such a sub-tree rather than a whole tree.	arguments	sub-tree	part_whole	{'e1': {'word': 'arguments', 'word_index': [(3, 3)], 'id': 'C08-1105.4'}, 'e2': {'word': 'sub-tree', 'word_index': [(6, 6)], 'id': 'C08-1105.5'}}	O reasonable to O locally in O . O O O O O O O O O O O O O O O O
The anchor group approach achieves an accuracy of 87.75% and the single anchor approach achieves 83.63%.	anchor group approach	accuracy	result	{'e1': {'word': 'anchor group approach', 'word_index': [(0, 1)], 'id': 'C08-1105.15'}, 'e2': {'word': 'accuracy', 'word_index': [(3, 3)], 'id': 'C08-1105.16'}}	O approach achieves O the single O O O O O O S-CARDINAL O O O O O O O S-CARDINAL E-PERCENT O
Experimental results also indicate that the prediction of MP improves semantic role labeling.	prediction of MP	semantic role labeling	result	{'e1': {'word': 'prediction of MP', 'word_index': [(0, 0)], 'id': 'C08-1105.18'}, 'e2': {'word': 'semantic role labeling', 'word_index': [(0, 1)], 'id': 'C08-1105.19'}}	O labeling . O O O O O O O O O O O O O
Recently the LATL has undertaken the development of a multilingual translation system based on a symbolic parsing technology and on a transfer-based translation model.	symbolic parsing technology	multilingual translation system	usage	{'e1': {'word': 'symbolic parsing technology', 'word_index': [(6, 6)], 'id': 'L08-1579.2'}, 'e2': {'word': 'multilingual translation system', 'word_index': [(2, 3)], 'id': 'L08-1579.1'}}	O a multilingual O on a O on a O translation model O O O O O O O O O O O O O O O O O O O O O O O
This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning.	term weight learning	verbal word senses	usage	{'e1': {'word': 'term weight learning', 'word_index': [(5, 6)], 'id': 'E99-1028.2'}, 'e2': {'word': 'verbal word senses', 'word_index': [(3, 3)], 'id': 'E99-1028.1'}}	O learning algorithm O using term O O O O O O O O O O O O O O
This is quite different from current approaches to semantic parsing or chunking that depend on full statistical syntactic parsers that require tree bank style annotation.	statistical syntactic parsers	chunking	usage	{'e1': {'word': 'statistical syntactic parsers', 'word_index': [(8, 9)], 'id': 'N04-4037.10'}, 'e2': {'word': 'chunking', 'word_index': [(6, 6)], 'id': 'N04-4037.9'}}	O from current O semantic parsing O full statistical O O O O O O O O O O O O O O O O O O O O O O O
We compare it with a recently proposed word-by-word semantic chunker and present results that show that the phrase-by-phrase approach performs better than its word-by-word counterpart.	phrase-by-phrase approach	word-by-word counterpart	compare	{'e1': {'word': 'phrase-by-phrase approach', 'word_index': [(9, 13)], 'id': 'N04-4037.13'}, 'e2': {'word': 'word-by-word counterpart', 'word_index': [(15, 16)], 'id': 'N04-4037.14'}}	O it with O semantic chunker O results that O - by- O approach performs O counterpart . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
The primary objective of this basic research is to develop improved methods and models for acoustic recognition of continuous speech.	acoustic recognition	continuous speech	usage	{'e1': {'word': 'acoustic recognition', 'word_index': [(6, 6)], 'id': 'H91-1080.1'}, 'e2': {'word': 'continuous speech', 'word_index': [(6, 6)], 'id': 'H91-1080.2'}}	O of this O and models O O O O O O O O O O O O O O O O O O O
The work has focussed on developing accurate and detailed mathematical models of phonemes and their coarticulation for the purpose of large-vocabulary continuous speech recognition.	phonemes	large-vocabulary continuous speech recognition	usage	{'e1': {'word': 'phonemes', 'word_index': [(6, 6)], 'id': 'H91-1080.3'}, 'e2': {'word': 'large-vocabulary continuous speech recognition', 'word_index': [(9, 11)], 'id': 'H91-1080.4'}}	O focussed on O mathematical models O their coarticulation O speech recognition O O O O O O O O O O O O O O O O O O O O O O O
KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus.	word list	corpus	part_whole	{'e1': {'word': 'word list', 'word_index': [(0, 0)], 'id': 'W00-0902.5'}, 'e2': {'word': 'corpus', 'word_index': [(3, 3)], 'id': 'W00-0902.6'}}	S-PERSON has been O corpus which O researcher is O with a O a reference O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus.	word list	reference corpus	part_whole	{'e1': {'word': 'word list', 'word_index': [(12, 12)], 'id': 'W00-0902.8'}, 'e2': {'word': 'reference corpus', 'word_index': [(14, 15)], 'id': 'W00-0902.9'}}	S-PERSON has been O corpus which O researcher is O with a O a reference O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Five English corpora were compared to reference corpora of various sizes (varying from two to 100 times larger than the study corpus).	English corpora	reference corpora	compare	{'e1': {'word': 'English corpora', 'word_index': [(0, 0)], 'id': 'W00-0902.15'}, 'e2': {'word': 'reference corpora', 'word_index': [(3, 3)], 'id': 'W00-0902.16'}}	S-CARDINAL compared to S-LANGUAGE from two O corpus ) O O O O O O O O O O O S-CARDINAL O S-CARDINAL O O O O O O O O
The results indicate that a reference corpus that is five times as large as the study corpus yielded a larger number of keywords than a smaller reference corpus.	keywords	corpus	part_whole	{'e1': {'word': 'keywords', 'word_index': [(7, 7)], 'id': 'W00-0902.20'}, 'e2': {'word': 'corpus', 'word_index': [(6, 6)], 'id': 'W00-0902.19'}}	O that a O large as O keywords than O . O O O O O S-CARDINAL O O O O O O O O O O O O O O O O O O O
The implication is that a larger reference corpus is not always better than a smaller one, for WordSmith Tools Keywords analysis, while a reference corpus that is less than five times the size of the study corpus may not be reliable.	reference corpus	study corpus	compare	{'e1': {'word': 'reference corpus', 'word_index': [(9, 10)], 'id': 'W00-0902.28'}, 'e2': {'word': 'study corpus', 'word_index': [(16, 17)], 'id': 'W00-0902.29'}}	O that a O , for O Keywords analysis O corpus that O less than O study corpus O O O O O O O O O S-CARDINAL O O S-PERSON O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O O
In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a non-lexicalised and a lexicalised mode.	dependency analyser	Italian	usage	{'e1': {'word': 'dependency analyser', 'word_index': [(6, 7)], 'id': 'W02-1501.1'}, 'e2': {'word': 'Italian', 'word_index': [(9, 9)], 'id': 'W02-1501.2'}}	O paper we O evaluation of O analyser of O runs in O mode . O O O O O O O O O O O S-NORP O O O O O O O O O O O
Results shed light on the contribution of types of lexical information to parsing.	lexical information	parsing	usage	{'e1': {'word': 'lexical information', 'word_index': [(3, 3)], 'id': 'W02-1501.5'}, 'e2': {'word': 'parsing', 'word_index': [(3, 3)], 'id': 'W02-1501.6'}}	O types of O . O O O O O O O O O O O O
In international phonology and speech synthesis research, it has been suggested that the relative informativeness of a word can be used to predict pitch prominence.	relative informativeness	word	model-feature	{'e1': {'word': 'relative informativeness', 'word_index': [(3, 3)], 'id': 'W99-0619.3'}, 'e2': {'word': 'word', 'word_index': [(5, 5)], 'id': 'W99-0619.4'}}	O phonology and O a word O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper, we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted measures of informativeness.	measures	informativeness	model-feature	{'e1': {'word': 'measures', 'word_index': [(13, 13)], 'id': 'W99-0619.7'}, 'e2': {'word': 'informativeness', 'word_index': [(15, 15)], 'id': 'W99-0619.8'}}	O paper , O evidence to O a correlation O employing two O measures of O O O O O O O O O O O O O O S-CARDINAL O O O O O O
Our experiments show that there is a positive correlation between the informativeness of a word and its pitch accent assignment.	informativeness	word	model-feature	{'e1': {'word': 'informativeness', 'word_index': [(3, 3)], 'id': 'W99-0619.9'}, 'e2': {'word': 'word', 'word_index': [(3, 3)], 'id': 'W99-0619.10'}}	O that there O its pitch O O O O O O O O O O O O O O O O O O O
The computation of word informativeness is inexpensive and can be incorporated into speech synthesis systems easily.	word informativeness	speech synthesis systems	usage	{'e1': {'word': 'word informativeness', 'word_index': [(1, 2)], 'id': 'W99-0619.14'}, 'e2': {'word': 'speech synthesis systems', 'word_index': [(3, 3)], 'id': 'W99-0619.15'}}	O word informativeness O . O O O O O O O O O O O O O O O
"""An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed."	parsing algorithm	augmented context-free grammars	usage	{'e1': {'word': 'parsing algorithm', 'word_index': [(3, 3)], 'id': 'J87-1004.1'}, 'e2': {'word': 'augmented context-free grammars', 'word_index': [(5, 6)], 'id': 'J87-1004.2'}}	O An efficient O for augmented O application to O O O O O O O O O O O O O O O O O O O O O O O
The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar.	generalized LR parsing algorithm	LR shift-reduce parsing table	usage	{'e1': {'word': 'generalized LR parsing algorithm', 'word_index': [(2, 3)], 'id': 'J87-1004.4'}, 'e2': {'word': 'LR shift-reduce parsing table', 'word_index': [(3, 6)], 'id': 'J87-1004.5'}}	O a generalized O - reduce O ( possibly O multiple entries O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables.	LR parsing tables	chart parsing algorithm	usage	{'e1': {'word': 'LR parsing tables', 'word_index': [(9, 9)], 'id': 'J87-1004.14'}, 'e2': {'word': 'chart parsing algorithm', 'word_index': [(6, 6)], 'id': 'J87-1004.13'}}	O also view O as an O efficiently guided O . O O O O O O O O O O O O O O O O
"Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, based on the technique developed at CMU."""	on-line parser	Japanese language	usage	{'e1': {'word': 'on-line parser', 'word_index': [(1, 4)], 'id': 'J87-1004.21'}, 'e2': {'word': 'Japanese language', 'word_index': [(6, 7)], 'id': 'J87-1004.22'}}	"O on - O parser for O language is O technique developed O . "" O O O O S-NORP O O O O O O O O O O O O O O O S-ORG O O"
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).	hybrid statistical and structural semantic model	multi-stage spoken language understanding (SLU)	usage	{'e1': {'word': 'hybrid statistical and structural semantic model', 'word_index': [(1, 3)], 'id': 'W04-3002.1'}, 'e2': {'word': 'multi-stage spoken language understanding (SLU)', 'word_index': [(3, 6)], 'id': 'W04-3002.2'}}	O hybrid statistical O language understanding O O O O O O O O O O O O O O S-ORG O O
The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted.	weighted finite-state transducer (WFST)-based parser	SLU	usage	{'e1': {'word': 'weighted finite-state transducer (WFST)-based parser', 'word_index': [(3, 8)], 'id': 'W04-3002.4'}, 'e2': {'word': 'SLU', 'word_index': [(3, 3)], 'id': 'W04-3002.3'}}	O of this S-ORDINAL - state O based parser O encodes the O to be S-ORG O O O O O O O O O O O O O O O O O O O O O O O O O
The proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger.	n-gram semantic tagger	regular grammar	usage	{'e1': {'word': 'n-gram semantic tagger', 'word_index': [(5, 6)], 'id': 'W04-3002.7'}, 'e2': {'word': 'regular grammar', 'word_index': [(3, 3)], 'id': 'W04-3002.6'}}	O improves the O known n-gram O O O O O O O O O O O O O O O O
This paper presents a corpus-based account of structural priming in human sentence processing, focusing on the role that syntactic representations play in such an account.	structural priming	sentence processing	model-feature	{'e1': {'word': 'structural priming', 'word_index': [(3, 3)], 'id': 'W06-1637.2'}, 'e2': {'word': 'sentence processing', 'word_index': [(6, 6)], 'id': 'W06-1637.3'}}	O corpus - O in human O syntactic representations O in such O O O O O O O O O O O O O O O O O O O O O O O O O
We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue, annotated syntactically with Combinatory Categorial Grammar (CCG) derivations.	spontaneous spoken dialogue	corpus	part_whole	{'e1': {'word': 'spontaneous spoken dialogue', 'word_index': [(4, 6)], 'id': 'W06-1637.7'}, 'e2': {'word': 'corpus', 'word_index': [(3, 3)], 'id': 'W06-1637.6'}}	O the strength O spontaneous spoken O syntactically with O O O O O O O O O O O O O O O O O O O O S-ORG O O O
In particular, we present evidence for priming between lexical and syntactic categories encoding partially satisfied sub-categorization frames, and we show that priming effects exist both for incremental and normal-form CCG derivations.	priming effects	incremental and normal-form CCG derivations	model-feature	{'e1': {'word': 'priming effects', 'word_index': [(12, 12)], 'id': 'W06-1637.13'}, 'e2': {'word': 'incremental and normal-form CCG derivations', 'word_index': [(12, 15)], 'id': 'W06-1637.14'}}	O , we O partially satisfied O frames , O show that O normal - O O O O O O O O O O O O O O O O O O O O O O O O O O O O S-ORG O O
This paper describes the application of an ensemble of indexing and classification systems, which have been shown to be successful in information retrieval and classification of medical literature, to a new task of assigning ICD-9-CM codes to the clinical history and impression sections of radiology reports.	information retrieval	medical literature	usage	{'e1': {'word': 'information retrieval', 'word_index': [(7, 8)], 'id': 'W07-1014.1'}, 'e2': {'word': 'medical literature', 'word_index': [(10, 11)], 'id': 'W07-1014.2'}}	O application of O classification systems O information retrieval O medical literature O to the O and impression O radiology reports O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
This type of summary could serve as an effective navigation tool for accessing information in long texts, such as books.	summary	texts	model-feature	{'e1': {'word': 'summary', 'word_index': [(0, 0)], 'id': 'P07-1069.2'}, 'e2': {'word': 'texts', 'word_index': [(9, 9)], 'id': 'P07-1069.4'}}	O could serve O tool for O in long O O O O O O O O O O O O O O O O O O O
To generate a coherent table-of-contents, we need to capture both global dependencies across different titles in the table and local constraints within sections.	titles	table-of-contents	usage	{'e1': {'word': 'titles', 'word_index': [(9, 9)], 'id': 'P07-1069.8'}, 'e2': {'word': 'table-of-contents', 'word_index': [(3, 3)], 'id': 'P07-1069.6'}}	O a coherent O to capture O global dependencies O constraints within O O O O O O O O O O O O O O O O O O O O O O O O O
First, we compare cruiser with a baseline system-initiative DS, and show that users prefer cruiser.	cruiser	system-initiative DS	compare	{'e1': {'word': 'cruiser', 'word_index': [(0, 0)], 'id': 'P08-1055.7'}, 'e2': {'word': 'system-initiative DS', 'word_index': [(4, 6)], 'id': 'P08-1055.8'}}	S-ORDINAL with a O system - O , and O O O O O O O O O O O O O O O O O
To improve the Mandarin large vocabulary continuous speech recognition (LVCSR), a unified framework based approach is introduced to exploit multi-level linguistic knowledge.	multi-level linguistic knowledge	large vocabulary continuous speech recognition (LVCSR)	usage	{'e1': {'word': 'multi-level linguistic knowledge', 'word_index': [(9, 9)], 'id': 'D08-1086.3'}, 'e2': {'word': 'large vocabulary continuous speech recognition (LVCSR)', 'word_index': [(3, 4)], 'id': 'D08-1086.2'}}	O the Mandarin O ) , O based approach S-LANGUAGE O O O O O O O O O O O O O O O O O O O O O O
In this framework, each knowledge source is represented by a Weighted Finite State Transducer (WFST), and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources.	Weighted Finite State Transducer (WFST)	knowledge source	model-feature	{'e1': {'word': 'Weighted Finite State Transducer (WFST)', 'word_index': [(6, 6)], 'id': 'D08-1086.5'}, 'e2': {'word': 'knowledge source', 'word_index': [(3, 3)], 'id': 'D08-1086.4'}}	O framework , O represented by O and then O are combined O analyzer for O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Due to the uniform transducer representation, any knowledge source can be easily integrated into the analyzer, as long as it can be encoded into WFSTs.	knowledge source	WFSTs	usage	{'e1': {'word': 'knowledge source', 'word_index': [(5, 6)], 'id': 'D08-1086.8'}, 'e2': {'word': 'WFSTs', 'word_index': [(12, 12)], 'id': 'D08-1086.9'}}	O uniform transducer O any knowledge O easily integrated O it can O O O O O O O O O O O O O O O O O O O O O O O O
In this paper we discuss algorithms for clustering words into classes from unlabeled text using unsupervised algorithms, based on distributional and morphological information.	words	unlabeled text	part_whole	{'e1': {'word': 'words', 'word_index': [(4, 4)], 'id': 'E03-1009.1'}, 'e2': {'word': 'unlabeled text', 'word_index': [(6, 6)], 'id': 'E03-1009.2'}}	O paper we O words into O unsupervised algorithms O on distributional O O O O O O O O O O O O O O O O O O O O O
"""This paper presents an unsupervised relation extraction algorithm, which induces relations between entity pairs by grouping them into a ""natural"" number of clusters based on the similarity of their contexts."	unsupervised relation extraction algorithm	entity pairs	usage	{'e1': {'word': 'unsupervised relation extraction algorithm', 'word_index': [(3, 5)], 'id': 'I05-2045.2'}, 'e2': {'word': 'entity pairs', 'word_index': [(7, 8)], 'id': 'I05-2045.4'}}	O This paper O extraction algorithm O entity pairs O number of O on the O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
This paper presents a method that measures the similarity between compound nouns in different languages to locate translation equivalents from corpora.	translation equivalents	corpora	part_whole	{'e1': {'word': 'translation equivalents', 'word_index': [(6, 6)], 'id': 'C02-1065.5'}, 'e2': {'word': 'corpora', 'word_index': [(6, 6)], 'id': 'C02-1065.6'}}	O method that O nouns in O . O O O O O O O O O O O O O O O O O O O
The method uses information from unrelated corpora in different languages that do not have to be parallel.	corpora	languages	model-feature	{'e1': {'word': 'corpora', 'word_index': [(3, 3)], 'id': 'C02-1065.7'}, 'e2': {'word': 'languages', 'word_index': [(3, 3)], 'id': 'C02-1065.8'}}	O information from O do not O . O O O O O O O O O O O O O O O
The method compares the contexts of target compound nouns and translation candidates in the word or semantic attribute level.	contexts	translation candidates	compare	{'e1': {'word': 'contexts', 'word_index': [(2, 2)], 'id': 'C02-1065.10'}, 'e2': {'word': 'translation candidates', 'word_index': [(3, 4)], 'id': 'C02-1065.12'}}	O the contexts O candidates in O O O O O O O O O O O O O O O O O O
In this paper, we show how this measuring method can be applied to select the best English translation candidate for Japanese compound nouns in more than 70% of the cases.	English translation	Japanese compound nouns	usage	{'e1': {'word': 'English translation', 'word_index': [(9, 10)], 'id': 'C02-1065.15'}, 'e2': {'word': 'Japanese compound nouns', 'word_index': [(12, 14)], 'id': 'C02-1065.16'}}	O paper , O measuring method O the best O translation candidate O compound nouns O % of O O O O O O O O O O O S-LANGUAGE O O O S-NORP O O O O O S-CARDINAL O O O O O
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text	text mining	statistical Arabic NLP	part_whole	{'e1': {'word': 'text mining', 'word_index': [(0, 0)], 'id': 'L08-1611.2'}, 'e2': {'word': 'statistical Arabic NLP', 'word_index': [(0, 0)], 'id': 'L08-1611.1'}}	O specific , O the tools O language factorization S-LANGUAGE text O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text	statistical processing	language factorization(s)	usage	{'e1': {'word': 'statistical processing', 'word_index': [(6, 6)], 'id': 'L08-1611.3'}, 'e2': {'word': 'language factorization(s)', 'word_index': [(7, 9)], 'id': 'L08-1611.4'}}	O specific , O the tools O language factorization S-LANGUAGE text O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
While building this LR, we had to go beyond the conventional exclusive collection of words from dictionaries and thesauri that cannot alone produce a satisfactory coverage of this highly inflective and derivative language.	words	dictionaries	part_whole	{'e1': {'word': 'words', 'word_index': [(4, 4)], 'id': 'L08-1611.9'}, 'e2': {'word': 'dictionaries', 'word_index': [(6, 6)], 'id': 'L08-1611.10'}}	O we had O words from O cannot alone O satisfactory coverage O this highly O derivative language O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
With the aid of the same large-scale Arabic morphological analyzer and PoS tagger in the runtime, the possible senses of virtually any given Arabic word are retrievable.	PoS tagger	Arabic word	usage	{'e1': {'word': 'PoS tagger', 'word_index': [(6, 7)], 'id': 'L08-1611.21'}, 'e2': {'word': 'Arabic word', 'word_index': [(15, 15)], 'id': 'L08-1611.22'}}	O the same O Arabic morphological O tagger in O , the O virtually any O retrievable . O O O S-LANGUAGE O O O O O O O O O O O O O O O O S-LANGUAGE O O O O
Similarly to the well-established ROVER approach of ( Fiscus, 1997 ) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network.	confusion network	consensus translation	usage	{'e1': {'word': 'confusion network', 'word_index': [(15, 15)], 'id': 'E06-1005.7'}, 'e2': {'word': 'consensus translation', 'word_index': [(9, 9)], 'id': 'E06-1005.6'}}	O ( Fiscus O ) for O hypotheses , O computed by O on a O O O O O O O S-DATE O O O O O O O O O O O O O O O O O O O
To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.	statistical alignment algorithm	word alignments	usage	{'e1': {'word': 'statistical alignment algorithm', 'word_index': [(9, 9)], 'id': 'E06-1005.11'}, 'e2': {'word': 'word alignments', 'word_index': [(3, 4)], 'id': 'E06-1005.9'}}	O the confusion O alignments of O translation hypotheses O word reordering O O O O O O O O O O O O O O O O O O O O O O O O O
The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.	document	alignment	usage	{'e1': {'word': 'document', 'word_index': [(3, 3)], 'id': 'E06-1005.13'}, 'e2': {'word': 'alignment', 'word_index': [(11, 11)], 'id': 'E06-1005.16'}}	O a whole O sentence is O account to O the alignment O O O O O O O O O O O O O O O O O O
The method was also tested in the framework of multi-source and speech translation.	method	multi-source and speech translation	usage	{'e1': {'word': 'method', 'word_index': [(0, 0)], 'id': 'E06-1005.20'}, 'e2': {'word': 'multi-source and speech translation', 'word_index': [(3, 5)], 'id': 'E06-1005.21'}}	O also tested O speech translation O O O O O O O O O O O O
We present a two stage parser that recovers Penn Treebank style syntactic analyses of new sentences including skeletal syntactic structure, and, for the first time, both function tags and empty categories.	syntactic analyses	sentences	usage	{'e1': {'word': 'syntactic analyses', 'word_index': [(4, 5)], 'id': 'N06-1024.3'}, 'e2': {'word': 'sentences', 'word_index': [(8, 8)], 'id': 'N06-1024.4'}}	O a two O syntactic analyses O new sentences S-CARDINAL structure , O time , O . O O S-ORG S-PERSON O O O O O O O O O O O O O O O S-ORDINAL O O O O O O O O O
The accuracy of the first-stage parser on the standard Parseval metric matches that of the ( Collins, 2003 ) parser on which it is based, despite the data fragmentation caused by the greatly enriched space of possible node labels.	parser	parser	compare	{'e1': {'word': 'parser', 'word_index': [(3, 3)], 'id': 'N06-1024.9'}, 'e2': {'word': 'parser', 'word_index': [(10, 10)], 'id': 'N06-1024.11'}}	O the first O matches that O ( Collins O parser on S-ORDINAL despite the O fragmentation caused O of possible O O O O S-PERSON O O O O O O S-PERSON O S-DATE O O O O O O O O O O O O O O O O O O O O O O O
Both anaphora resolution and prepositional phrase (PP) attachment are the most frequent ambiguities in natural language processing.	ambiguities	natural language processing	model-feature	{'e1': {'word': 'ambiguities', 'word_index': [(4, 4)], 'id': 'E95-1041.3'}, 'e2': {'word': 'natural language processing', 'word_index': [(6, 6)], 'id': 'E95-1041.4'}}	O prepositional phrase O ambiguities in O O O O O O O O O O O O O O O O O O
Just as with speaker adaptation in speaker-independent system, two vocabulary adaptation algorithms [5] are implemented in order to tailor the VI subword models to the target vocabulary.	vocabulary adaptation algorithms	VI subword models	usage	{'e1': {'word': 'vocabulary adaptation algorithms', 'word_index': [(6, 7)], 'id': 'H92-1033.4'}, 'e2': {'word': 'VI subword models', 'word_index': [(12, 12)], 'id': 'H92-1033.5'}}	O adaptation in O independent system O algorithms [ O tailor the O O O O O O O S-CARDINAL O O O O O O O O O O O O O O O O O O O O O
Over the past 9 years, the Applied Science and Engineering Laboratories (ASEL) at the University of Delaware and the duPont Hospital for Children, has been involved with applying natural language processing (NLP) technologies to the field of AAC.	natural language processing (NLP) technologies	AAC	usage	{'e1': {'word': 'natural language processing (NLP) technologies', 'word_index': [(18, 21)], 'id': 'W97-0503.2'}, 'e2': {'word': 'AAC', 'word_index': [(23, 23)], 'id': 'W97-0503.3'}}	O years , O and Engineering O at the O of Delaware S-DATE Children , O been involved O language processing O of AAC O O O O O O O O O O O S-GPE O O O O O O O O O O O O O O O O O O O O O O O O O O
One of the major projects at ASEL (The COMPAN-SION project) has been concerned with the application of primarily lexical semantics and sentence generation technology to expand telegraphic input into full sentences.	sentence generation technology	telegraphic input	usage	{'e1': {'word': 'sentence generation technology', 'word_index': [(12, 12)], 'id': 'W97-0503.5'}, 'e2': {'word': 'telegraphic input', 'word_index': [(14, 15)], 'id': 'W97-0503.6'}}	S-CARDINAL major projects O ( The O project ) O application of O expand telegraphic O full sentences O O O S-PERSON O O O O O O O O O O O O O O O O O O O O O O O O O O
We view the entire problem as series of classification problems and employ memory-based learning (MBL) to resolve them.	memory-based learning (MBL)	classification problems	usage	{'e1': {'word': 'memory-based learning (MBL)', 'word_index': [(6, 9)], 'id': 'W00-1210.3'}, 'e2': {'word': 'classification problems', 'word_index': [(4, 5)], 'id': 'W00-1210.2'}}	O the entire O classification problems O - based O them . O O O O O O O O O O O O O O O O O O O
The problem of word segmentation affects all aspects of Chinese language processing, including the development of text-to-speech synthesis systems.	word segmentation	Chinese language processing	part_whole	{'e1': {'word': 'word segmentation', 'word_index': [(1, 2)], 'id': 'W02-1813.1'}, 'e2': {'word': 'Chinese language processing', 'word_index': [(3, 3)], 'id': 'W02-1813.2'}}	O word segmentation O , including O of text O systems . O O O O O S-NORP O O O O O O O O O O O O O O O
This paper treats the classification of the semantic functions performed by adnominal constituents in Japanese, where many parts of speech act as adnominal constituents.	adnominal constituents	Japanese	part_whole	{'e1': {'word': 'adnominal constituents', 'word_index': [(4, 5)], 'id': 'W00-0110.3'}, 'e2': {'word': 'Japanese', 'word_index': [(6, 6)], 'id': 'W00-0110.4'}}	O classification of O adnominal constituents O of speech O constituents . O O O O O O O O O O S-NORP O O O O O O O O O O O
"adjectives and ""noun + NO"" (in English ""of + noun"") structures, which have a broad range of semantic functions, are discussed."	semantic functions	"""noun + NO"" (in English ""of + noun"") structures"	model-feature	"{'e1': {'word': 'semantic functions', 'word_index': [(10, 11)], 'id': 'W00-0110.11'}, 'e2': {'word': '""noun + NO"" (in English ""of + noun"") structures', 'word_index': [(0, 5)], 'id': 'W00-0110.10'}}"	"O "" of O ) structures O which have O semantic functions O are discussed O O O O S-LANGUAGE O O O O O O O O O O O O O O O O O O O O"
The feasibility of this was verified with a self-organizing semantic map based on a neural network model.	neural network model	self-organizing semantic map	usage	{'e1': {'word': 'neural network model', 'word_index': [(4, 6)], 'id': 'W00-0110.15'}, 'e2': {'word': 'self-organizing semantic map', 'word_index': [(3, 3)], 'id': 'W00-0110.14'}}	O this was O neural network O O O O O O O O O O O O O O O O O O
Recent corpus-based work on word sense disambiguation explores the application of statistical pattern recognition procedures to lexical co-occurrence data from very large text databases.	statistical pattern recognition procedures	lexical co-occurrence data	usage	{'e1': {'word': 'statistical pattern recognition procedures', 'word_index': [(3, 6)], 'id': 'J95-1001.2'}, 'e2': {'word': 'lexical co-occurrence data', 'word_index': [(8, 9)], 'id': 'J95-1001.3'}}	O word sense O pattern recognition O to lexical O text databases O O O O O O O O O O O O O O O O O O O O O O O
Statistical methods play a definite role in this work, helping to organize and analyze data, but the disambiguation method itself does not employ statistical data or decision criteria.	statistical data	disambiguation method	usage	{'e1': {'word': 'statistical data', 'word_index': [(6, 7)], 'id': 'J95-1001.12'}, 'e2': {'word': 'disambiguation method', 'word_index': [(3, 4)], 'id': 'J95-1001.11'}}	O to organize O method itself O data or O criteria . O O O O O O O O O O O O O O O O O O O O O O O O O O O
The approach is illustrated by an experiment discriminating among the senses of adjectives, which have been relatively neglected in work on sense disambiguation.	senses	adjectives	model-feature	{'e1': {'word': 'senses', 'word_index': [(3, 3)], 'id': 'J95-1001.17'}, 'e2': {'word': 'adjectives', 'word_index': [(3, 3)], 'id': 'J95-1001.18'}}	O illustrated by O , which O relatively neglected O O O O O O O O O O O O O O O O O O O O O O
In particular, the paper assesses the potential of nouns for discriminating among the senses of adjectives that modify them.	senses	adjectives	model-feature	{'e1': {'word': 'senses', 'word_index': [(4, 4)], 'id': 'J95-1001.21'}, 'e2': {'word': 'adjectives', 'word_index': [(6, 6)], 'id': 'J95-1001.22'}}	O , the O senses of O that modify O O O O O O O O O O O O O O O O O O
This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: and About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur.	ambiguous adjectives	English	part_whole	{'e1': {'word': 'ambiguous adjectives', 'word_index': [(3, 3)], 'id': 'J95-1001.23'}, 'e2': {'word': 'English', 'word_index': [(5, 5)], 'id': 'J95-1001.24'}}	O on an O in English O About three O of these O be disambiguated O by the O which they O O S-CARDINAL O O O O O O O S-LANGUAGE O O O S-CARDINAL O O O O O O O O O O O O O O O O O O O O O O O O O O O O
This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: and About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur.	nouns	syntactic constructions	model-feature	{'e1': {'word': 'nouns', 'word_index': [(18, 18)], 'id': 'J95-1001.26'}, 'e2': {'word': 'syntactic constructions', 'word_index': [(18, 18)], 'id': 'J95-1001.27'}}	O on an O in English O About three O of these O be disambiguated O by the O which they O O S-CARDINAL O O O O O O O S-LANGUAGE O O O S-CARDINAL O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Furthermore, a small number of semantic attributes supply a compact means of representing the noun clues in a very few rules.	semantic attributes	noun	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(0, 0)], 'id': 'J95-1001.29'}, 'e2': {'word': 'noun', 'word_index': [(5, 5)], 'id': 'J95-1001.30'}}	O means of O the noun O in a O O O O O O O O O O O O O O O O O O O O
The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined syntactic relations to the ambiguous adjective.	sense	ambiguous modified noun	model-feature	{'e1': {'word': 'sense', 'word_index': [(0, 0)], 'id': 'J95-1001.34'}, 'e2': {'word': 'ambiguous modified noun', 'word_index': [(2, 3)], 'id': 'J95-1001.35'}}	O an ambiguous O to determine O semantic attribute O of a O and grammatical O and sometimes O when they O , well O syntactic relations O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined syntactic relations to the ambiguous adjective.	semantic attribute	target adjective	model-feature	{'e1': {'word': 'semantic attribute', 'word_index': [(7, 8)], 'id': 'J95-1001.36'}, 'e2': {'word': 'target adjective', 'word_index': [(12, 12)], 'id': 'J95-1001.38'}}	O an ambiguous O to determine O semantic attribute O of a O and grammatical O and sometimes O when they O , well O syntactic relations O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
This paper presents our method of incorporating character clustering based on mutual information into Decision-Tree Dictionary-less morphological analysis.	mutual information	character clustering	usage	{'e1': {'word': 'mutual information', 'word_index': [(4, 5)], 'id': 'P98-1108.7'}, 'e2': {'word': 'character clustering', 'word_index': [(3, 3)], 'id': 'P98-1108.6'}}	O method of O mutual information O analysis . O O O O O O O O O O O O O O O O O O O O
By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text.	tagging	text	usage	{'e1': {'word': 'tagging', 'word_index': [(9, 9)], 'id': 'P98-1108.12'}, 'e2': {'word': 'text', 'word_index': [(9, 9)], 'id': 'P98-1108.14'}}	O natural classes O our morphological O tokenizing and O O O O O O O O O O O O O O O O O O S-NORP O O
As a case study in one direction, we discuss the recent development of an automatic method for evaluating definition questions based on n-gram overlap, a commonly-used technique in summarization evaluation.	n-gram overlap	automatic method for evaluating definition questions	usage	{'e1': {'word': 'n-gram overlap', 'word_index': [(14, 15)], 'id': 'W05-0906.4'}, 'e2': {'word': 'automatic method for evaluating definition questions', 'word_index': [(9, 12)], 'id': 'W05-0906.3'}}	O case study O one direction O recent development O evaluating definition O on n-gram S-CARDINAL commonly - O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
SYSTRAN'S Chinese word segmentation is one important component of its Chinese-English machine translation system.	Chinese word segmentation	Chinese-English machine translation system	part_whole	{'e1': {'word': 'Chinese word segmentation', 'word_index': [(0, 0)], 'id': 'W03-1729.1'}, 'e2': {'word': 'Chinese-English machine translation system', 'word_index': [(2, 3)], 'id': 'W03-1729.2'}}	S-GPE its Chinese S-NORP O O O S-CARDINAL O O O O S-NORP O S-LANGUAGE O O O O
The Chinese word segmentation module uses a rule-based approach, based on a large dictionary and fine-grained linguistic rules.	rule-based approach	Chinese word segmentation	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(3, 3)], 'id': 'W03-1729.4'}, 'e2': {'word': 'Chinese word segmentation', 'word_index': [(0, 1)], 'id': 'W03-1729.3'}}	O segmentation module S-NORP based on O fine - O O O O O O O O O O O O O O O O O O O O O
It works on general-purpose texts from different Chinese-speaking regions, with comparable performance.	Chinese-speaking regions	general-purpose texts	model-feature	{'e1': {'word': 'Chinese-speaking regions', 'word_index': [(4, 6)], 'id': 'W03-1729.8'}, 'e2': {'word': 'general-purpose texts', 'word_index': [(2, 3)], 'id': 'W03-1729.7'}}	O on general O Chinese -speaking O with comparable O O O O O O S-NORP O O O O O O O O
ParaMor, our minimally supervised morphology induction algorithm, retrusses the word forms of raw text corpora back onto their paradigmatic skeletons; performing on par with state-of-the-art minimally supervised morphology induction algorithms at morphological analysis of English and German.	morphological analysis	English	usage	{'e1': {'word': 'morphological analysis', 'word_index': [(16, 16)], 'id': 'W07-1315.6'}, 'e2': {'word': 'English', 'word_index': [(16, 16)], 'id': 'W07-1315.7'}}	O O algorithm , O the word O text corpora O with state O algorithms at O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O S-LANGUAGE O S-NORP O
And with these structures in hand, ParaMor then annotates word forms with morpheme boundaries.	morpheme boundaries	word forms	model-feature	{'e1': {'word': 'morpheme boundaries', 'word_index': [(6, 6)], 'id': 'W07-1315.12'}, 'e2': {'word': 'word forms', 'word_index': [(6, 6)], 'id': 'W07-1315.11'}}	O structures in O then annotates O . O O O O O O O O O O O O O O
To set ParaMor 's few free parameters we analyze a training corpus of Spanish.	Spanish	training corpus	part_whole	{'e1': {'word': 'Spanish', 'word_index': [(7, 7)], 'id': 'W07-1315.14'}, 'e2': {'word': 'training corpus', 'word_index': [(6, 6)], 'id': 'W07-1315.13'}}	O ParaMor 's O we analyze O O O O O O O O O O O O S-LANGUAGE O
Without adjusting parameters, we induce the morphological structure of English and German.	morphological structure	English	model-feature	{'e1': {'word': 'morphological structure', 'word_index': [(1, 2)], 'id': 'W07-1315.15'}, 'e2': {'word': 'English', 'word_index': [(3, 3)], 'id': 'W07-1315.16'}}	O morphological structure O O O O O O O O O S-LANGUAGE O S-NORP O
The other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores.	distance features	multi-level confidence scores	usage	{'e1': {'word': 'distance features', 'word_index': [(6, 6)], 'id': 'P08-2055.8'}, 'e2': {'word': 'multi-level confidence scores', 'word_index': [(6, 6)], 'id': 'P08-2055.9'}}	O of this O novel long O O O O O O O O O O O O O O O O O O O O O
Using Conditional Maximum Entropy (CME) classifier with all the selected features, we reached an annotation error rate of 26.0% in the SWBD corpus, compared with a subtree error rate of 41.91%, a closely related benchmark with the Charniak parser from ( Kahn et al., 2005 ).	Conditional Maximum Entropy (CME) classifier	annotation error rate	result	{'e1': {'word': 'Conditional Maximum Entropy (CME) classifier', 'word_index': [(0, 3)], 'id': 'P08-2055.10'}, 'e2': {'word': 'annotation error rate', 'word_index': [(3, 5)], 'id': 'P08-2055.11'}}	O CME ) O error rate O , compared O % , O closely related O Charniak parser O ( Kahn O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O O O S-CARDINAL O O O O O O O O S-PERSON O O O S-PERSON O O O O S-DATE O O
Coreference resolution systems usually attempt to find a suitable antecedent for (almost) every noun phrase	Coreference resolution systems	noun phrase	usage	{'e1': {'word': 'Coreference resolution systems', 'word_index': [(0, 0)], 'id': 'P03-2012.1'}, 'e2': {'word': 'noun phrase', 'word_index': [(3, 3)], 'id': 'P03-2012.2'}}	O ( almost O O O O O O O O O O O O O O O O
We use a small training corpus (MUC-7), but also acquire some data from the Internet.	data	Internet	part_whole	{'e1': {'word': 'data', 'word_index': [(9, 9)], 'id': 'P03-2012.8'}, 'e2': {'word': 'Internet', 'word_index': [(12, 12)], 'id': 'P03-2012.9'}}	O a small O MUC - O ) , O from the O O O O O O O O O O O O O O O O O O
Combining our classifiers sequentially, we achieve 88.9% precision and 84.6% recall for discourse new entities.	classifiers	precision	result	{'e1': {'word': 'classifiers', 'word_index': [(0, 0)], 'id': 'P03-2012.10'}, 'e2': {'word': 'precision', 'word_index': [(1, 1)], 'id': 'P03-2012.11'}}	O precision and O for discourse O O O O O S-CARDINAL O O O S-CARDINAL O O O O O O O
We expect our classifiers to provide a good prefiltering for coreference resolution systems, improving both their speed and performance.	classifiers	coreference resolution systems	usage	{'e1': {'word': 'classifiers', 'word_index': [(2, 2)], 'id': 'P03-2012.14'}, 'e2': {'word': 'coreference resolution systems', 'word_index': [(4, 6)], 'id': 'P03-2012.15'}}	O our classifiers O coreference resolution O both their O O O O O O O O O O O O O O O O O O
MBDP-1 is a knowledge-free segmentation algorithm that bootstraps its own lexicon, which starts out empty.	knowledge-free segmentation algorithm	lexicon	usage	{'e1': {'word': 'knowledge-free segmentation algorithm', 'word_index': [(1, 4)], 'id': 'P01-1013.3'}, 'e2': {'word': 'lexicon', 'word_index': [(8, 8)], 'id': 'P01-1013.4'}}	O knowledge - O algorithm that O own lexicon O which starts O O O O O O O O O O O O O O O O
In this paper, we present methods that allow the users of a natural language processor (NLP) to define, inspect, and modify any case frame information associated with the words and phrases known to the system.	case frame information	words	model-feature	{'e1': {'word': 'case frame information', 'word_index': [(15, 15)], 'id': 'C86-1108.2'}, 'e2': {'word': 'words', 'word_index': [(18, 18)], 'id': 'C86-1108.3'}}	O paper , O allow the O processor ( O ) to O inspect , O with the O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
The number and sizes of parallel corpora keep growing, which makes it necessary to have automatic methods of processing them: combining, checking and improving corpora quality, etc.	corpora quality	parallel corpora	model-feature	{'e1': {'word': 'corpora quality', 'word_index': [(13, 14)], 'id': 'L08-1114.2'}, 'e2': {'word': 'parallel corpora', 'word_index': [(3, 3)], 'id': 'L08-1114.1'}}	O sizes of O makes it O automatic methods O , checking O corpora quality O O O O O O O O O O O O O O O O O O O O O O O O O O O
The method takes into consideration slight differences in the source documents, different levels of segmentation of the input corpora, encoding differences and other aspects of the task.	segmentation	input corpora	usage	{'e1': {'word': 'segmentation', 'word_index': [(6, 6)], 'id': 'L08-1114.10'}, 'e2': {'word': 'input corpora', 'word_index': [(7, 8)], 'id': 'L08-1114.11'}}	O into consideration O , different O input corpora O other aspects O O O O O O O O O O O O O O O O O O O O O O O O O O
In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts.	Estonian-English	JRC-Acquis corpus	part_whole	{'e1': {'word': 'Estonian-English', 'word_index': [(3, 5)], 'id': 'L08-1114.12'}, 'e2': {'word': 'JRC-Acquis corpus', 'word_index': [(6, 8)], 'id': 'L08-1114.13'}}	O first experiment O - English S-ORDINAL Acquis corpus O O O S-NORP O S-LANGUAGE O O O S-ORG O O O O O O O O O O O O
In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts.	legislation texts	corpus	part_whole	{'e1': {'word': 'legislation texts', 'word_index': [(9, 9)], 'id': 'L08-1114.15'}, 'e2': {'word': 'corpus', 'word_index': [(9, 9)], 'id': 'L08-1114.14'}}	O first experiment O - English S-ORDINAL Acquis corpus O O O S-NORP O S-LANGUAGE O O O S-ORG O O O O O O O O O O O O
The generation module supports the seamless integration of full grammar rules, templates and canned text.	grammar rules	generation module	usage	{'e1': {'word': 'grammar rules', 'word_index': [(3, 3)], 'id': 'E03-1019.6'}, 'e2': {'word': 'generation module', 'word_index': [(0, 0)], 'id': 'E03-1019.5'}}	O supports the O text . O O O O O O O O O O O O O O O
Ambiguity is the fundamental property of natural language	Ambiguity	natural language	model-feature	{'e1': {'word': 'Ambiguity', 'word_index': [(0, 0)], 'id': 'C02-1079.1'}, 'e2': {'word': 'natural language', 'word_index': [(0, 0)], 'id': 'C02-1079.2'}}	O O O O O O O O
Perhaps, the most burdensome case of ambiguity manifests itself on the syntactic level of analysis.	ambiguity	syntactic level of analysis	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(1, 1)], 'id': 'C02-1079.3'}, 'e2': {'word': 'syntactic level of analysis', 'word_index': [(6, 7)], 'id': 'C02-1079.4'}}	O ambiguity manifests O on the O analysis . O O O O O O O O O O O O O O
The presented methods are based on language specific features of synthetical languages and they improve the results of simple stochastic approaches.	language specific features	synthetical languages	model-feature	{'e1': {'word': 'language specific features', 'word_index': [(3, 3)], 'id': 'C02-1079.7'}, 'e2': {'word': 'synthetical languages', 'word_index': [(3, 3)], 'id': 'C02-1079.8'}}	O are based O improve the O O O O O O O O O O O O O O O O O O O O
The texts are in English and Czech.	texts	English	model-feature	{'e1': {'word': 'texts', 'word_index': [(0, 0)], 'id': 'L08-1197.3'}, 'e2': {'word': 'English', 'word_index': [(2, 2)], 'id': 'L08-1197.4'}}	O in English O O O S-LANGUAGE O S-NORP O
This paper presents techniques for multimedia annotation and their application to video summarization and translation.	multimedia annotation	video summarization and translation	usage	{'e1': {'word': 'multimedia annotation', 'word_index': [(2, 3)], 'id': 'C02-1098.2'}, 'e2': {'word': 'video summarization and translation', 'word_index': [(4, 6)], 'id': 'C02-1098.3'}}	O for multimedia O video summarization O O O O O O O O O O O O O O
A video scene description consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes.	semi-automatically detected keyframes	video scene description	part_whole	{'e1': {'word': 'semi-automatically detected keyframes', 'word_index': [(3, 4)], 'id': 'C02-1098.12'}, 'e2': {'word': 'video scene description', 'word_index': [(1, 3)], 'id': 'C02-1098.11'}}	O video scene O keyframes of O clip and O O O O O O O O O O O O O O O O O O O
The text data in the multimedia annotation are syntactically and semantically structured using linguistic annotation.	text data	syntactically and semantically structured	model-feature	{'e1': {'word': 'text data', 'word_index': [(0, 0)], 'id': 'C02-1098.14'}, 'e2': {'word': 'syntactically and semantically structured', 'word_index': [(3, 6)], 'id': 'C02-1098.16'}}	O in the O and semantically O . O O O O O O O O O O O O O
The proposed multimedia summarization works upon a multimodal document that consists of a video, keyframes of scenes, and transcripts of the scenes.	 multimedia summarization	multimodal document	usage	{'e1': {'word': ' multimedia summarization', 'word_index': [(0, 1)], 'id': 'C02-1098.18'}, 'e2': {'word': 'multimodal document', 'word_index': [(3, 3)], 'id': 'C02-1098.19'}}	O summarization works O video , O O O O O O O O O O O O O O O O O O O O O O O
The multimedia translation automatically generates several versions of multimedia content in different languages.	languages	multimedia content	model-feature	{'e1': {'word': 'languages', 'word_index': [(3, 3)], 'id': 'C02-1098.26'}, 'e2': {'word': 'multimedia content', 'word_index': [(3, 3)], 'id': 'C02-1098.25'}}	O automatically generates O O O O O O O O O O O O O
When machine translation (MT) knowledge is automatically constructed from bilingual corpora, redundant rules are acquired due to translation variety.	bilingual corpora	machine translation (MT) knowledge	usage	{'e1': {'word': 'bilingual corpora', 'word_index': [(3, 3)], 'id': 'E03-1029.2'}, 'e2': {'word': 'machine translation (MT) knowledge', 'word_index': [(0, 3)], 'id': 'E03-1029.1'}}	O MT ) O , redundant O O O O O O O O O O O O O O O O O O O O O
These rules increase ambiguity or cause incorrect MT results.	rules	ambiguity	result	{'e1': {'word': 'rules', 'word_index': [(0, 0)], 'id': 'E03-1029.4'}, 'e2': {'word': 'ambiguity', 'word_index': [(0, 0)], 'id': 'E03-1029.5'}}	O cause incorrect O O O O O O O O O
"To overcome this problem, we constrain the sentences used for knowledge extraction to ""the appropriate bilingual sentences for the MT""."	sentences	knowledge extraction	usage	{'e1': {'word': 'sentences', 'word_index': [(3, 3)], 'id': 'E03-1029.7'}, 'e2': {'word': 'knowledge extraction', 'word_index': [(3, 4)], 'id': 'E03-1029.8'}}	O this problem O extraction to O sentences for O O O O O O O O O O O O O O O O O O O O O
"To overcome this problem, we constrain the sentences used for knowledge extraction to ""the appropriate bilingual sentences for the MT""."	bilingual sentences	MT	usage	{'e1': {'word': 'bilingual sentences', 'word_index': [(6, 7)], 'id': 'E03-1029.9'}, 'e2': {'word': 'MT', 'word_index': [(9, 9)], 'id': 'E03-1029.10'}}	O this problem O extraction to O sentences for O O O O O O O O O O O O O O O O O O O O O
For the creation of these scholarly digital editions the AAC edition philosophy and edition principles have been applied whereby new corpus research methods have been made use of for questions of computational philology and textual studies in a digital environment.	AAC edition philosophy and edition principles	scholarly digital editions	usage	{'e1': {'word': 'AAC edition philosophy and edition principles', 'word_index': [(5, 6)], 'id': 'L08-1405.9'}, 'e2': {'word': 'scholarly digital editions', 'word_index': [(3, 3)], 'id': 'L08-1405.8'}}	O of these O the AAC O whereby new O methods have O questions of O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
The evaluation results show our system can achieve an F measure of 0.9400.967 for different testing corpora.	system	F measure	result	{'e1': {'word': 'system', 'word_index': [(3, 3)], 'id': 'I05-3026.5'}, 'e2': {'word': 'F measure', 'word_index': [(3, 3)], 'id': 'I05-3026.6'}}	O show our O testing corpora O O O O O O O O O O S-TIME O O O O O
In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others.	machine learning algorithm	shallow semantic parsing	usage	{'e1': {'word': 'machine learning algorithm', 'word_index': [(3, 5)], 'id': 'N04-1030.1'}, 'e2': {'word': 'shallow semantic parsing', 'word_index': [(6, 6)], 'id': 'N04-1030.2'}}	O paper , O learning algorithm O extending the O of Gildea O ( 2002 O ) and O . O O O O O O O O O O O O S-PERSON O S-PERSON O S-DATE O O S-PERSON O O O O S-DATE O O O O
Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers.	Support Vector Machines	algorithm	usage	{'e1': {'word': 'Support Vector Machines', 'word_index': [(3, 3)], 'id': 'N04-1030.4'}, 'e2': {'word': 'algorithm', 'word_index': [(0, 0)], 'id': 'N04-1030.3'}}	O based on O in performance O classifiers . O O O O O O O O O O O O O O O O O
We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.	test set	AQUAINT corpus	part_whole	{'e1': {'word': 'test set', 'word_index': [(6, 6)], 'id': 'N04-1030.7'}, 'e2': {'word': 'AQUAINT corpus', 'word_index': [(9, 9)], 'id': 'N04-1030.8'}}	O performance improvements O new features O drawn from O O O O O O O O O O O O O O O O O O O O O O O O
Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences.	language model	speech understanding	usage	{'e1': {'word': 'language model', 'word_index': [(2, 3)], 'id': 'A94-1010.1'}, 'e2': {'word': 'speech understanding', 'word_index': [(5, 6)], 'id': 'A94-1010.2'}}	O of language O in speech O imperfect modeling O O O O O O O O O O O O O O O O O
I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into sub corpora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster.	sentences	training corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(6, 6)], 'id': 'A94-1010.5'}, 'e2': {'word': 'training corpus', 'word_index': [(7, 8)], 'id': 'A94-1010.6'}}	O argue that O by clustering O training corpus O on the O separate language O each cluster O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model.	clustering	contextual effects	model-feature	{'e1': {'word': 'clustering', 'word_index': [(0, 0)], 'id': 'A94-1010.11'}, 'e2': {'word': 'contextual effects', 'word_index': [(4, 5)], 'id': 'A94-1010.12'}}	O offers a O contextual effects O therefore significantly O O O O O O O O O O O O O O O O O O O O
It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclustered model.	clustering	model	result	{'e1': {'word': 'clustering', 'word_index': [(17, 17)], 'id': 'A94-1010.16'}, 'e2': {'word': 'model', 'word_index': [(19, 19)], 'id': 'A94-1010.17'}}	O offers a O gather evidence O context - O model using O reward the O if clustering O model , O the existence O exploited by O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain.	clustering	models	result	{'e1': {'word': 'clustering', 'word_index': [(3, 3)], 'id': 'A94-1010.20'}, 'e2': {'word': 'models', 'word_index': [(6, 6)], 'id': 'A94-1010.21'}}	O for these O improves some O others for O O O O O O O O O O O O O O O O O O O O
This paper presents a parsing system for the detection of syntactic errors.	parsing system	detection of syntactic errors	usage	{'e1': {'word': 'parsing system', 'word_index': [(1, 2)], 'id': 'A00-3005.1'}, 'e2': {'word': 'detection of syntactic errors', 'word_index': [(3, 4)], 'id': 'A00-3005.2'}}	O parsing system O errors . O O O O O O O O O O O
It combines a robust partial parser which obtains the main sentence components and a finite-state parser used for the description of syntactic error patterns.	finite-state parser	syntactic error patterns	usage	{'e1': {'word': 'finite-state parser', 'word_index': [(6, 9)], 'id': 'A00-3005.5'}, 'e2': {'word': 'syntactic error patterns', 'word_index': [(10, 12)], 'id': 'A00-3005.6'}}	O a robust O and a O - state O syntactic error O O O O O O O O O O O O O O O O O O O O O O O
The system has been tested on a corpus of real texts, containing both correct and incorrect sentences, with promising results.	texts	corpus	part_whole	{'e1': {'word': 'texts', 'word_index': [(3, 3)], 'id': 'A00-3005.9'}, 'e2': {'word': 'corpus', 'word_index': [(3, 3)], 'id': 'A00-3005.8'}}	O been tested O , containing O incorrect sentences O . O O O O O O O O O O O O O O O O O O O
The objectives of this project are to advance our understanding of the merits of current text analysis techniques, as applied to the performance of realistic text analysis tasks, and to achieve this understanding by means of a sound performance evaluation methodology.	text analysis techniques	realistic text analysis tasks	usage	{'e1': {'word': 'text analysis techniques', 'word_index': [(4, 6)], 'id': 'H92-1111.1'}, 'e2': {'word': 'realistic text analysis tasks', 'word_index': [(10, 12)], 'id': 'H92-1111.3'}}	O this project O text analysis O as applied O realistic text O understanding by O sound performance O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Talk'n'Travel is a fully conversational, mixed-initiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English.	general spoken English	questions	model-feature	{'e1': {'word': 'general spoken English', 'word_index': [(21, 21)], 'id': 'A00-1010.8'}, 'e2': {'word': 'questions', 'word_index': [(18, 18)], 'id': 'A00-1010.7'}}	O is a O , mixed O initiative system O the constraints O travel plan O arbitrary order O , in O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O S-LANGUAGE O
The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met.	plan-based agenda mechanism	system	usage	{'e1': {'word': 'plan-based agenda mechanism', 'word_index': [(3, 3)], 'id': 'A00-1010.10'}, 'e2': {'word': 'system', 'word_index': [(0, 0)], 'id': 'A00-1010.9'}}	O according to O , rather O to negotiate O can be O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
As mentioned in Mitkov (1996), solving the anaphora and extracting the antecedent are key issues in a correct translation.	anaphora	translation	part_whole	{'e1': {'word': 'anaphora', 'word_index': [(3, 3)], 'id': 'W99-0210.3'}, 'e2': {'word': 'translation', 'word_index': [(9, 9)], 'id': 'W99-0210.5'}}	O in Mitkov O the antecedent O issues in S-PERSON O S-DATE O O O O O O O O O O O O O O O O O
The SS stores the lexical, syntactic, morphologic and semantic information of every constituent of the grammar.	lexical, syntactic, morphologic and semantic information	constituent	model-feature	{'e1': {'word': 'lexical, syntactic, morphologic and semantic information', 'word_index': [(2, 6)], 'id': 'W99-0210.10'}, 'e2': {'word': 'constituent', 'word_index': [(6, 6)], 'id': 'W99-0210.11'}}	O the lexical O , morphologic O of the O O O O O O O O O O O O O O O O
This mechanism could be added to a MT system such as an additional module to solve anaphora generation problem.	mechanism	MT system	usage	{'e1': {'word': 'mechanism', 'word_index': [(0, 0)], 'id': 'W99-0210.23'}, 'e2': {'word': 'MT system', 'word_index': [(3, 3)], 'id': 'W99-0210.24'}}	O added to O solve anaphora O O O O O O O O O O O O O O O O O O
This paper addresses language engineering infrastructure issues by considering whether standard V&amp;V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&amp;V in the context of language processing systems.	standard V&amp;V methods	evaluation practices	compare	{'e1': {'word': 'standard V&amp;V methods', 'word_index': [(3, 6)], 'id': 'W01-0906.6'}, 'e2': {'word': 'evaluation practices', 'word_index': [(6, 6)], 'id': 'W01-0906.7'}}	O engineering infrastructure O V&amp ; O commonly used O for applying O O O O O O O S-PERSON O O O O O O O O O O O O O O O O O O O O O O S-PERSON O O O O O O O O O O
This paper addresses language engineering infrastructure issues by considering whether standard V&amp;V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&amp;V in the context of language processing systems.	V&amp;V	language processing systems	usage	{'e1': {'word': 'V&amp;V', 'word_index': [(12, 12)], 'id': 'W01-0906.9'}, 'e2': {'word': 'language processing systems', 'word_index': [(12, 12)], 'id': 'W01-0906.10'}}	O engineering infrastructure O V&amp ; O commonly used O for applying O O O O O O O S-PERSON O O O O O O O O O O O O O O O O O O O O O O S-PERSON O O O O O O O O O O
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text.	paragraphs	document	part_whole	{'e1': {'word': 'paragraphs', 'word_index': [(6, 6)], 'id': 'W03-1102.1'}, 'e2': {'word': 'document', 'word_index': [(10, 10)], 'id': 'W03-1102.2'}}	O paper , O approach for O from the O document to O summary for O O O O O O O O O O O O O O O O O O O S-NORP O O
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text.	summary	Thai text	model-feature	{'e1': {'word': 'summary', 'word_index': [(13, 13)], 'id': 'W03-1102.3'}, 'e2': {'word': 'Thai text', 'word_index': [(15, 15)], 'id': 'W03-1102.4'}}	O paper , O approach for O from the O document to O summary for O O O O O O O O O O O O O O O O O O O S-NORP O O
The idea of our approach is to exploit both the local and global properties of paragraphs.	local and global properties	paragraphs	model-feature	{'e1': {'word': 'local and global properties', 'word_index': [(5, 7)], 'id': 'W03-1102.5'}, 'e2': {'word': 'paragraphs', 'word_index': [(9, 9)], 'id': 'W03-1102.6'}}	O our approach O the local O properties of O O O O O O O O O O O O O O
The local property can be considered as clusters of significant words within each paragraph, while the global property can be thought of as relations of all paragraphs in a document.	clusters	significant words	model-feature	{'e1': {'word': 'clusters', 'word_index': [(3, 3)], 'id': 'W03-1102.8'}, 'e2': {'word': 'significant words', 'word_index': [(3, 4)], 'id': 'W03-1102.9'}}	O can be O words within O be thought O of all O a document O O O O O O O O O O O O O O O O O O O O O O O O O O O
Syntax-based Machine Translation systems have recently become a focus of research with much hope that they will outperform traditional Phrase- Based Statistical Machine Translation (PBSMT)	Syntax-based Machine Translation systems	traditional Phrase- Based Statistical Machine Translation (PBSMT)	compare	{'e1': {'word': 'Syntax-based Machine Translation systems', 'word_index': [(0, 0)], 'id': 'W08-0410.1'}, 'e2': {'word': 'traditional Phrase- Based Statistical Machine Translation (PBSMT)', 'word_index': [(9, 12)], 'id': 'W08-0410.2'}}	O have recently O a focus O that they O Machine Translation O O O O O O O O O O O O O O O O O O O O O O O O S-PERSON O
Toward this goal, we present a method for analyzing the morphosyntactic content of language from an Elicitation Corpus such as the one included in the LDC's upcoming LCTL language packs.	morphosyntactic content	Elicitation Corpus	part_whole	{'e1': {'word': 'morphosyntactic content', 'word_index': [(3, 4)], 'id': 'W08-0410.3'}, 'e2': {'word': 'Elicitation Corpus', 'word_index': [(6, 7)], 'id': 'W08-0410.5'}}	O a method O content of O Corpus such O the one O the LDC O O O O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O
By providing this tool that can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved.	rich features	structure-based MT models	usage	{'e1': {'word': 'rich features', 'word_index': [(5, 6)], 'id': 'W08-0410.12'}, 'e2': {'word': 'structure-based MT models', 'word_index': [(3, 3)], 'id': 'W08-0410.11'}}	O this tool O these rich O the discriminative O can be O O O O O O O O O O O O O O O O O O O O O O O O O
This article outlines a quantitative method for segmenting texts into thematically coherent units.	quantitative method	texts	usage	{'e1': {'word': 'quantitative method', 'word_index': [(1, 2)], 'id': 'P98-2243.1'}, 'e2': {'word': 'texts', 'word_index': [(3, 3)], 'id': 'P98-2243.2'}}	O quantitative method O . O O O O O O O O O O O O
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.	network of lexical collocations	method	usage	{'e1': {'word': 'network of lexical collocations', 'word_index': [(2, 3)], 'id': 'P98-2243.5'}, 'e2': {'word': 'method', 'word_index': [(0, 0)], 'id': 'P98-2243.4'}}	O a network O thematic coherence O text from O cohesiveness of O words . O O O O O O O O O O O O O O O O O O O O O O O O
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.	lexical cohesiveness	words	model-feature	{'e1': {'word': 'lexical cohesiveness', 'word_index': [(9, 10)], 'id': 'P98-2243.8'}, 'e2': {'word': 'words', 'word_index': [(13, 13)], 'id': 'P98-2243.9'}}	O a network O thematic coherence O text from O cohesiveness of O words . O O O O O O O O O O O O O O O O O O O O O O O O
We also present the results of an experiment about locating boundaries between a series of concatened texts.	boundaries	texts	part_whole	{'e1': {'word': 'boundaries', 'word_index': [(7, 7)], 'id': 'P98-2243.10'}, 'e2': {'word': 'texts', 'word_index': [(7, 7)], 'id': 'P98-2243.11'}}	O present the O O about locating O . O O O O O O O O O O O O O O O
In this work, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves.	model	sense assignment	usage	{'e1': {'word': 'model', 'word_index': [(3, 3)], 'id': 'W04-1908.1'}, 'e2': {'word': 'sense assignment', 'word_index': [(5, 6)], 'id': 'W04-1908.2'}}	O work , O for sense O assigning senses O the contexts O words appear O O O O O O O O O O O O O O O O O O O O O O O O O O
In this work, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves.	words	contexts	model-feature	{'e1': {'word': 'words', 'word_index': [(13, 13)], 'id': 'W04-1908.5'}, 'e2': {'word': 'contexts', 'word_index': [(11, 11)], 'id': 'W04-1908.4'}}	O work , O for sense O assigning senses O the contexts O words appear O O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper we describe a morphological analysis method based on a maximum entropy model.	maximum entropy model	morphological analysis method	usage	{'e1': {'word': 'maximum entropy model', 'word_index': [(6, 6)], 'id': 'W01-0512.2'}, 'e2': {'word': 'morphological analysis method', 'word_index': [(3, 4)], 'id': 'W01-0512.1'}}	O paper we O method based O . O O O O O O O O O O O O O
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	model	method	usage	{'e1': {'word': 'model', 'word_index': [(1, 1)], 'id': 'W01-0512.4'}, 'e2': {'word': 'method', 'word_index': [(0, 0)], 'id': 'W01-0512.3'}}	O model that O with a O information but O also identify O characteristics . O O O O O O O O O O O O O O O O O O O O O O O O O O
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	lexical information	dictionary	part_whole	{'e1': {'word': 'lexical information', 'word_index': [(6, 7)], 'id': 'W01-0512.6'}, 'e2': {'word': 'dictionary', 'word_index': [(3, 3)], 'id': 'W01-0512.5'}}	O model that O with a O information but O also identify O characteristics . O O O O O O O O O O O O O O O O O O O O O O O O O O
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	characteristics	unknown words	model-feature	{'e1': {'word': 'characteristics', 'word_index': [(13, 13)], 'id': 'W01-0512.8'}, 'e2': {'word': 'unknown words', 'word_index': [(12, 12)], 'id': 'W01-0512.7'}}	O model that O with a O information but O also identify O characteristics . O O O O O O O O O O O O O O O O O O O O O O O O O O
Finally, we present Corporator, an Open Source software which was designed for collecting corpus from RSS feeds.	corpus	RSS feeds	part_whole	{'e1': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'W06-1707.10'}, 'e2': {'word': 'RSS feeds', 'word_index': [(9, 9)], 'id': 'W06-1707.11'}}	O Open Source O which was O collecting corpus O O O O O O O O O O O O O O O O O
Several SVMs are trained using information from pyramids of summary content units.	summary content units	SVMs	usage	{'e1': {'word': 'summary content units', 'word_index': [(3, 3)], 'id': 'P07-2015.4'}, 'e2': {'word': 'SVMs', 'word_index': [(0, 0)], 'id': 'P07-2015.3'}}	O pyramids of O O O O O O O O O O O O
Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.	performance	DUC-2005	compare	{'e1': {'word': 'performance', 'word_index': [(0, 0)], 'id': 'P07-2015.5'}, 'e2': {'word': 'DUC-2005', 'word_index': [(3, 3)], 'id': 'P07-2015.6'}}	O the best O , an O method for O O O O O O O O O S-DATE O O O S-PERSON O O O O O O O O O O O O
Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.	automatic scoring method	pyramid evaluation	usage	{'e1': {'word': 'automatic scoring method', 'word_index': [(6, 7)], 'id': 'P07-2015.9'}, 'e2': {'word': 'pyramid evaluation', 'word_index': [(9, 9)], 'id': 'P07-2015.10'}}	O the best O , an O method for O O O O O O O O O S-DATE O O O S-PERSON O O O O O O O O O O O O
We present a novel unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees.	unsupervised method	sentence compression	usage	{'e1': {'word': 'unsupervised method', 'word_index': [(3, 3)], 'id': 'W08-1105.1'}, 'e2': {'word': 'sentence compression', 'word_index': [(3, 3)], 'id': 'W08-1105.2'}}	O a novel O on a O tree representation O subtrees . O O O O O O O O O O O O O O O O O O O
We demonstrate that the choice of the parser affects the performance of the system.	parser	performance	result	{'e1': {'word': 'parser', 'word_index': [(3, 3)], 'id': 'W08-1105.8'}, 'e2': {'word': 'performance', 'word_index': [(3, 3)], 'id': 'W08-1105.9'}}	O that the O O O O O O O O O O O O O O
We also apply the method to German and report the results of an evaluation with humans.	method	German	usage	{'e1': {'word': 'method', 'word_index': [(3, 3)], 'id': 'W08-1105.11'}, 'e2': {'word': 'German', 'word_index': [(3, 3)], 'id': 'W08-1105.12'}}	O apply the O O report the O humans . O O O S-NORP O O O O O O O O O O
Our approach even outperformed the hand coded system on NER in Spanish, and it achieved high accuracies in Portuguese.	NER	Spanish	usage	{'e1': {'word': 'NER', 'word_index': [(3, 3)], 'id': 'P05-2005.5'}, 'e2': {'word': 'Spanish', 'word_index': [(3, 3)], 'id': 'P05-2005.6'}}	O outperformed the O and it O Portuguese . O O O O O O O O S-LANGUAGE O O O O O O O S-LANGUAGE O
A karaka based approach to parsing of Indian languages is described.	parsing	Indian languages	usage	{'e1': {'word': 'parsing', 'word_index': [(3, 3)], 'id': 'C90-3005.2'}, 'e2': {'word': 'Indian languages', 'word_index': [(3, 3)], 'id': 'C90-3005.3'}}	O karaka based O is described O O O O O S-NORP O O O O
It has been used for building a parser of Hindi for a prototype Machine Translation system.	parser	Hindi	usage	{'e1': {'word': 'parser', 'word_index': [(4, 4)], 'id': 'C90-3005.4'}, 'e2': {'word': 'Hindi', 'word_index': [(6, 6)], 'id': 'C90-3005.5'}}	O been used O parser of O Machine Translation O O O O O O S-LANGUAGE O O O O O O O
This paper presents our work on the detection of temporal information in web pages.	detection	web pages	usage	{'e1': {'word': 'detection', 'word_index': [(3, 3)], 'id': 'L08-1559.1'}, 'e2': {'word': 'web pages', 'word_index': [(5, 6)], 'id': 'L08-1559.3'}}	O work on O in web O O O O O O O O O O O O O
The pages examined within the scope of this study were taken from the tourism sector and the temporal information in question is thus particular to this area.	temporal information	pages	part_whole	{'e1': {'word': 'temporal information', 'word_index': [(6, 6)], 'id': 'L08-1559.5'}, 'e2': {'word': 'pages', 'word_index': [(0, 0)], 'id': 'L08-1559.4'}}	O within the O taken from O question is O O O O O O O O O O O O O O O O O O O O O O O O O
The differences that exist between extraction from plain textual data and extraction from the web are brought to light.	extraction	plain textual data	usage	{'e1': {'word': 'extraction', 'word_index': [(3, 3)], 'id': 'L08-1559.6'}, 'e2': {'word': 'plain textual data', 'word_index': [(3, 3)], 'id': 'L08-1559.7'}}	O exist between O brought to O O O O O O O O O O O O O O O O O O
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	patterns	symbolic approach	usage	{'e1': {'word': 'patterns', 'word_index': [(3, 3)], 'id': 'L08-1559.16'}, 'e2': {'word': 'symbolic approach', 'word_index': [(2, 3)], 'id': 'L08-1559.15'}}	O a symbolic O rules for O detection , O method is O O O O O O O O O O O O O O O O O O O O O O O O O O O
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	rules	detection	usage	{'e1': {'word': 'rules', 'word_index': [(4, 4)], 'id': 'L08-1559.17'}, 'e2': {'word': 'detection', 'word_index': [(7, 7)], 'id': 'L08-1559.18'}}	O a symbolic O rules for O detection , O method is O O O O O O O O O O O O O O O O O O O O O O O O O O O
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	annotation	temporal expressions	usage	{'e1': {'word': 'annotation', 'word_index': [(9, 9)], 'id': 'L08-1559.20'}, 'e2': {'word': 'temporal expressions', 'word_index': [(9, 9)], 'id': 'L08-1559.21'}}	O a symbolic O rules for O detection , O method is O O O O O O O O O O O O O O O O O O O O O O O O O O O
The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German.	read and spontaneous speech	German Ph@ttSessionz speech database	part_whole	{'e1': {'word': 'read and spontaneous speech', 'word_index': [(4, 6)], 'id': 'L08-1196.2'}, 'e2': {'word': 'German Ph@ttSessionz speech database', 'word_index': [(3, 3)], 'id': 'L08-1196.1'}}	O of the S-ORDINAL read and O is the O of its O . S-NORP O O O O O O O O O S-CARDINAL O O O O O O O O O O O S-NORP O
In this paper, we present a cross-sectional study of f0 measurements on this database.	cross-sectional study	f0 measurements	topic	{'e1': {'word': 'cross-sectional study', 'word_index': [(3, 5)], 'id': 'L08-1196.7'}, 'e2': {'word': 'f0 measurements', 'word_index': [(6, 6)], 'id': 'L08-1196.8'}}	O paper , O -sectional study O database . O O O O O O O O O O O O O O O
Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers.	relative f0 variability	male and female speakers	model-feature	{'e1': {'word': 'relative f0 variability', 'word_index': [(6, 7)], 'id': 'L08-1196.11'}, 'e2': {'word': 'male and female speakers', 'word_index': [(9, 9)], 'id': 'L08-1196.12'}}	O is little O in the O variability for O O O O O O O O O O O O O O O O O O O O O O
The study provides statistically reliable voice parameters of adolescent speakers for German.	voice parameters	adolescent speakers	model-feature	{'e1': {'word': 'voice parameters', 'word_index': [(3, 3)], 'id': 'L08-1196.17'}, 'e2': {'word': 'adolescent speakers', 'word_index': [(3, 3)], 'id': 'L08-1196.18'}}	O statistically reliable O for German O O O O O O O O O S-NORP O
The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability.	utterances	user input	part_whole	{'e1': {'word': 'utterances', 'word_index': [(6, 6)], 'id': 'L08-1196.22'}, 'e2': {'word': 'user input', 'word_index': [(5, 6)], 'id': 'L08-1196.21'}}	O contribute to O restricting user O with low O O O O O O O O O O O O O O O O O O O
The platform will support researchers and engineers with well-developed and standardized resources and application tools thereby avoiding duplicate activities from scratch and amplifying overall effort on the domain.	standardized resources	platform	usage	{'e1': {'word': 'standardized resources', 'word_index': [(3, 4)], 'id': 'C96-2185.9'}, 'e2': {'word': 'platform', 'word_index': [(0, 0)], 'id': 'C96-2185.8'}}	O support researchers O resources and O duplicate activities O the domain O O O O O O O O O O O O O O O O O O O O O O O O O O O
We present in this article, as a part of aspectual operation system, a generation system of iterative expressions using a set of operators called iterative operators.	generation system	aspectual operation system	part_whole	{'e1': {'word': 'generation system', 'word_index': [(7, 8)], 'id': 'E83-1003.2'}, 'e2': {'word': 'aspectual operation system', 'word_index': [(3, 5)], 'id': 'E83-1003.1'}}	O in this O operation system O generation system O a set O O O O O O O O O O O O O O O O O O O O O O O O O
The classification has been carried out especially in consideration of the durative / non-durative character of the denoted events and also in consideration of existence / non-existence of a culmination point (or a boundary) in the events.	durative / non-durative character	events	model-feature	{'e1': {'word': 'durative / non-durative character', 'word_index': [(3, 3)], 'id': 'E83-1003.11'}, 'e2': {'word': 'events', 'word_index': [(3, 3)], 'id': 'E83-1003.12'}}	O been carried O and also O existence / O point ( O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper, we show that time-synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.	cross-word triphones	time-synchronous one-pass decoding	usage	{'e1': {'word': 'cross-word triphones', 'word_index': [(7, 10)], 'id': 'H94-1080.16'}, 'e2': {'word': 'time-synchronous one-pass decoding', 'word_index': [(3, 6)], 'id': 'H94-1080.15'}}	O paper , O - synchronous O cross - O triphones and O language model O a dynamically O network . O O O S-CARDINAL O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper, we show that time-synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.	tree-structured network	trigram language model	usage	{'e1': {'word': 'tree-structured network', 'word_index': [(18, 19)], 'id': 'H94-1080.18'}, 'e2': {'word': 'trigram language model', 'word_index': [(12, 14)], 'id': 'H94-1080.17'}}	O paper , O - synchronous O cross - O triphones and O language model O a dynamically O network . O O O S-CARDINAL O O O O O O O O O O O O O O O O O O O O O O O O O
It was included in the HTK large vocabulary speech recognition system used for the 1993 ARPA WSJ evaluation and experimental results are presented for that task.	HTK large vocabulary speech recognition system	1993 ARPA WSJ evaluation	usage	{'e1': {'word': 'HTK large vocabulary speech recognition system', 'word_index': [(3, 6)], 'id': 'H94-1080.20'}, 'e2': {'word': '1993 ARPA WSJ evaluation', 'word_index': [(6, 7)], 'id': 'H94-1080.21'}}	O included in O vocabulary speech O evaluation and O are presented O . O O O O O O O O O S-DATE S-ORG S-ORG O O O O O O O O O O
Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model.	parallel bi-lingual data	statistical machine translation systems	usage	{'e1': {'word': 'parallel bi-lingual data', 'word_index': [(0, 0)], 'id': 'D08-1090.2'}, 'e2': {'word': 'statistical machine translation systems', 'word_index': [(0, 0)], 'id': 'D08-1090.1'}}	O train a O model . O O O O O O O O O O O O O O O O
While bi-lingual parallel data are expensive to generate, monolingual data are relatively common.	bi-lingual parallel data	monolingual data	compare	{'e1': {'word': 'bi-lingual parallel data', 'word_index': [(0, 0)], 'id': 'D08-1090.4'}, 'e2': {'word': 'monolingual data', 'word_index': [(3, 3)], 'id': 'D08-1090.5'}}	O expensive to O O O O O O O O O O O O O O
Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language.	monolingual data	language model	usage	{'e1': {'word': 'monolingual data', 'word_index': [(0, 0)], 'id': 'D08-1090.6'}, 'e2': {'word': 'language model', 'word_index': [(3, 3)], 'id': 'D08-1090.7'}}	O have been O in the O O O O O O O O O O O O O O O O O O O
This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories.	monolingual target data	statistical machine translation system	usage	{'e1': {'word': 'monolingual target data', 'word_index': [(3, 3)], 'id': 'D08-1090.9'}, 'e2': {'word': 'statistical machine translation system', 'word_index': [(6, 6)], 'id': 'D08-1090.10'}}	O novel method O to improve O news stories O O O O O O O O O O O O O O O O O O O O O O
For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to thesource documents.	documents	source documents	compare	{'e1': {'word': 'documents', 'word_index': [(9, 9)], 'id': 'D08-1090.18'}, 'e2': {'word': 'source documents', 'word_index': [(11, 12)], 'id': 'D08-1090.19'}}	O document that O a large O language is O the source O O O O O O O O O O O O O O O O O O O O O O O O O O O O
These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document.	documents	MT system	usage	{'e1': {'word': 'documents', 'word_index': [(0, 0)], 'id': 'D08-1090.20'}, 'e2': {'word': 'MT system', 'word_index': [(3, 3)], 'id': 'D08-1090.21'}}	O to adapt O texts that O document . O O O O O O O O O O O O O O O O O O O O
Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system.	language and translation models	baseline system	compare	{'e1': {'word': 'language and translation models', 'word_index': [(0, 0)], 'id': 'D08-1090.23'}, 'e2': {'word': 'baseline system', 'word_index': [(3, 3)], 'id': 'D08-1090.24'}}	O substantial gains O O O O O O O O O O O O O O O O O O
This paper describes an unsupervised knowledge-lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus.	ambiguous word	corpus	part_whole	{'e1': {'word': 'ambiguous word', 'word_index': [(6, 6)], 'id': 'E06-2007.3'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'E06-2007.4'}}	O unsupervised knowledge O determining the O used in O corpus . O O O O O O O O O O O O O O O O O O O O O O O O
This paper describes the Unisys MUC-3 text understanding system, a system based upon a three-tiered approach to text processing in which a powerful knowledge-based form of information retrieval plays a central role.	three-tiered approach	system	usage	{'e1': {'word': 'three-tiered approach', 'word_index': [(6, 6)], 'id': 'M91-1032.3'}, 'e2': {'word': 'system', 'word_index': [(6, 6)], 'id': 'M91-1032.2'}}	O Unisys MUC O , a O text processing O powerful knowledge S-ORG plays a O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O O O O O O O O O O O O
A decision was made to focus on the development of a knowledge-based information retrieval component, and this precluded the integration of Pundit into the prototype.	Pundit	prototype	part_whole	{'e1': {'word': 'Pundit', 'word_index': [(12, 12)], 'id': 'M91-1032.23'}, 'e2': {'word': 'prototype', 'word_index': [(14, 14)], 'id': 'M91-1032.24'}}	O decision was O knowledge - O component , O integration of O the prototype O O O O O O O O O O O O O O O O O O O O O O O O
ARBITER is a Prolog program that extracts assertions about macromolecular binding relationships from biomedical text.	macromolecular binding relationships	biomedical text	part_whole	{'e1': {'word': 'macromolecular binding relationships', 'word_index': [(3, 4)], 'id': 'A00-1026.3'}, 'e2': {'word': 'biomedical text', 'word_index': [(6, 7)], 'id': 'A00-1026.4'}}	O assertions about O relationships from O text . O O O O O O O O O O O O O
After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted.	ARBITER	MEDLINE abstracts	usage	{'e1': {'word': 'ARBITER', 'word_index': [(2, 2)], 'id': 'A00-1026.8'}, 'e2': {'word': 'MEDLINE abstracts', 'word_index': [(3, 3)], 'id': 'A00-1026.9'}}	O of ARBITER O , during O almost 25,000 O a database O O O O O O O O O O S-CARDINAL O O O O O O S-CARDINAL O O O O O O O O O O O O O O
After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted.	macro-molecular function	database	part_whole	{'e1': {'word': 'macro-molecular function', 'word_index': [(12, 12)], 'id': 'A00-1026.12'}, 'e2': {'word': 'database', 'word_index': [(11, 11)], 'id': 'A00-1026.11'}}	O of ARBITER O , during O almost 25,000 O a database O O O O O O O O O O S-CARDINAL O O O O O O S-CARDINAL O O O O O O O O O O O O O O
The resolution of lexical ambiguity is important for most natural language processing tasks, and a range of computational techniques have been proposed for its solution.	lexical ambiguity	natural language processing tasks	part_whole	{'e1': {'word': 'lexical ambiguity', 'word_index': [(1, 2)], 'id': 'H92-1046.2'}, 'e2': {'word': 'natural language processing tasks', 'word_index': [(3, 3)], 'id': 'H92-1046.3'}}	O lexical ambiguity O a range O techniques have O O O O O O O O O O O O O O O O O O O O O O O O
In this paper, we describe a method for lexical disambiguation of text using the definitions in a machine-readable dictionary together with the technique of simulated annealing.	definitions	lexical disambiguation	usage	{'e1': {'word': 'definitions', 'word_index': [(7, 7)], 'id': 'H92-1046.7'}, 'e2': {'word': 'lexical disambiguation', 'word_index': [(5, 6)], 'id': 'H92-1046.5'}}	O paper , O for lexical O definitions in O machine - O together with O O O O O O O O O O O O O O O O O O O O O O O O O
Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.	hand coding	lexical entries	usage	{'e1': {'word': 'hand coding', 'word_index': [(10, 11)], 'id': 'H92-1046.23'}, 'e2': {'word': 'lexical entries', 'word_index': [(13, 14)], 'id': 'H92-1046.24'}}	O on a O to those O automatic method O hand coding O lexical entries O . O O S-CARDINAL O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.	hand tagging	text	usage	{'e1': {'word': 'hand tagging', 'word_index': [(15, 15)], 'id': 'H92-1046.25'}, 'e2': {'word': 'text', 'word_index': [(15, 15)], 'id': 'H92-1046.26'}}	O on a O to those O automatic method O hand coding O lexical entries O . O O S-CARDINAL O O O O O O O O O O O O O O O O O O O O O O O O O O O O
To date, this array of formal and natural language processing technologies has been used to perform mass changes to legacy textual databases and to facilitate user interfacing to relational databases and software applications.	formal and natural language processing technologies	legacy textual databases	usage	{'e1': {'word': 'formal and natural language processing technologies', 'word_index': [(3, 7)], 'id': 'W97-0909.3'}, 'e2': {'word': 'legacy textual databases', 'word_index': [(12, 12)], 'id': 'W97-0909.4'}}	O , this O natural language O technologies has O mass changes O to facilitate O to relational O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations.	Rhetorical Structure trees	parallel corpus	part_whole	{'e1': {'word': 'Rhetorical Structure trees', 'word_index': [(5, 6)], 'id': 'W00-1403.3'}, 'e2': {'word': 'parallel corpus', 'word_index': [(3, 3)], 'id': 'W00-1403.2'}}	O discourse annotation O of Rhetorical O their corresponding O O O O O O O O O S-PERSON O O O O O O S-NORP O O O O S-LANGUAGE O O
The approach to knowledge representation taken in a multi-modal multi-domain dialogue system - SmartKom - is presented.	knowledge representation	multi-modal multi-domain dialogue system - SmartKom -	usage	{'e1': {'word': 'knowledge representation', 'word_index': [(1, 2)], 'id': 'W03-0903.1'}, 'e2': {'word': 'multi-modal multi-domain dialogue system - SmartKom -', 'word_index': [(3, 5)], 'id': 'W03-0903.2'}}	O knowledge representation O SmartKom - O . O O O O O O O O O O O O O O O O
This paper presents intial work on a system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admission Test (LSAT) and the analytic section of the Graduate Record Exam (GRE).	logic puzzles	Law School Admission Test (LSAT)	part_whole	{'e1': {'word': 'logic puzzles', 'word_index': [(10, 11)], 'id': 'W04-0902.5'}, 'e2': {'word': 'Law School Admission Test (LSAT)', 'word_index': [(15, 20)], 'id': 'W04-0902.6'}}	O work on O robust , O semantics and O logic puzzles O as the O Admission Test O LSAT ) O Graduate Record O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O S-PERSON O O O O S-ORG O O O O O O O O O O O O O O
We highlight key challenges, and discuss the representations and performance of the prototype system.	performance	prototype system	model-feature	{'e1': {'word': 'performance', 'word_index': [(3, 3)], 'id': 'W04-0902.9'}, 'e2': {'word': 'prototype system', 'word_index': [(4, 5)], 'id': 'W04-0902.10'}}	O key challenges O prototype system O O O O O O O O O O O O O O
In this paper, a new parsing model is proposed to formulate the complete chunking problem as a series of boundary detection subtasks.	chunking problem	parsing model	model-feature	{'e1': {'word': 'chunking problem', 'word_index': [(6, 7)], 'id': 'W06-0113.11'}, 'e2': {'word': 'parsing model', 'word_index': [(3, 3)], 'id': 'W06-0113.10'}}	O paper , O is proposed O problem as O series of O detection subtasks O O O O O O O O O O O O O O O O O O O
By applying SVM algorithm to these subtasks, we have achieved the best F-Score of 76.56% and 82.26% respectively.	SVM algorithm	F-Score	result	{'e1': {'word': 'SVM algorithm', 'word_index': [(1, 2)], 'id': 'W06-0113.17'}, 'e2': {'word': 'F-Score', 'word_index': [(5, 6)], 'id': 'W06-0113.18'}}	O SVM algorithm O best F- O % and O O O O O O O O O O O O O O S-CARDINAL O O S-CARDINAL O O O
In this paper, we introduce a WordNet-based measure of semantic relatedness by combining the structure and content of WordNet with co-occurrence information derived from raw text.	co-occurrence information	raw text	part_whole	{'e1': {'word': 'co-occurrence information', 'word_index': [(14, 15)], 'id': 'W06-2501.4'}, 'e2': {'word': 'raw text', 'word_index': [(15, 15)], 'id': 'W06-2501.5'}}	O paper , O - based O combining the O and content O with co-occurrence O O O O O O O O O O O O O O O O O O O O O O O O O
We use the co-occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet.	gloss vectors	concept	model-feature	{'e1': {'word': 'gloss vectors', 'word_index': [(8, 9)], 'id': 'W06-2501.8'}, 'e2': {'word': 'concept', 'word_index': [(9, 9)], 'id': 'W06-2501.9'}}	O the co-occurrence O the Word O build gloss O O O O O O O O O O O O O O O O O O O O
Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors.	Numeric scores of relatedness	concepts	model-feature	{'e1': {'word': 'Numeric scores of relatedness', 'word_index': [(0, 0)], 'id': 'W06-2501.11'}, 'e2': {'word': 'concepts', 'word_index': [(3, 3)], 'id': 'W06-2501.12'}}	O a pair O of the O their respective O O O O O O O O O O O O O O O O O O O O O
We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness.	semantic relatedness	word sense disambiguation algorithm	usage	{'e1': {'word': 'semantic relatedness', 'word_index': [(15, 15)], 'id': 'W06-2501.18'}, 'e2': {'word': 'word sense disambiguation algorithm', 'word_index': [(12, 15)], 'id': 'W06-2501.17'}}	O that this O other measures O judgments of O that it O sense disambiguation O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co–occurrence information.	co–occurrence information	plain text corpus	part_whole	{'e1': {'word': 'co–occurrence information', 'word_index': [(12, 12)], 'id': 'W06-2501.23'}, 'e2': {'word': 'plain text corpus', 'word_index': [(5, 7)], 'id': 'W06-2501.22'}}	O , it O any plain O corpus can O to derive O . O O O O O O O O O O O O O O O O O O O O
This paper describes our system as used in the RTE3 task.	system	RTE3 task	usage	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'W07-1403.1'}, 'e2': {'word': 'RTE3 task', 'word_index': [(3, 3)], 'id': 'W07-1403.2'}}	O system as O . O O O O O O O S-PERSON O O
The system maps premise and hypothesis pairs into an abstract knowledge representation (AKR) and then performs entailment and contradiction detection (ecd) on the resulting AKRs.	abstract knowledge representation (AKR)	premise and hypothesis pairs	model-feature	{'e1': {'word': 'abstract knowledge representation (AKR)', 'word_index': [(3, 6)], 'id': 'W07-1403.5'}, 'e2': {'word': 'premise and hypothesis pairs', 'word_index': [(1, 3)], 'id': 'W07-1403.4'}}	O premise and O representation ( O performs entailment O the resulting O O O O O O O O O O O O O O O O O O O O O O O O O O
Two versions of ECD were used in RTE3, one with strict ECD and one with looser ECD.	ECD	RTE3	usage	{'e1': {'word': 'ECD', 'word_index': [(1, 1)], 'id': 'W07-1403.8'}, 'e2': {'word': 'RTE3', 'word_index': [(3, 3)], 'id': 'W07-1403.9'}}	S-CARDINAL ECD were O and one O ECD . O O O O S-PERSON O S-CARDINAL O O O O S-CARDINAL O O O O
We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax.	natural language strings	semantics	model-feature	{'e1': {'word': 'natural language strings', 'word_index': [(9, 10)], 'id': 'P07-1105.3'}, 'e2': {'word': 'semantics', 'word_index': [(12, 12)], 'id': 'P07-1105.4'}}	O a new O grammar from O small number O strings annotated O about natural O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper, we propose a novel graph based sentence ranking algorithm, namely PNR2, for update summarization.	graph based sentence ranking algorithm	update summarization	usage	{'e1': {'word': 'graph based sentence ranking algorithm', 'word_index': [(4, 6)], 'id': 'C08-1062.7'}, 'e2': {'word': 'update summarization', 'word_index': [(10, 11)], 'id': 'C08-1062.9'}}	O paper , O graph based O PNR2 , O update summarization O O O O O O O O O O O O O O O O O
This paper discusses the application of the Expectation-Maximization (EM) clustering algorithm to the task of Chinese verb sense discrimination.	Expectation-Maximization (EM) clustering algorithm	Chinese verb sense discrimination	usage	{'e1': {'word': 'Expectation-Maximization (EM) clustering algorithm', 'word_index': [(3, 6)], 'id': 'P04-1038.1'}, 'e2': {'word': 'Chinese verb sense discrimination', 'word_index': [(6, 7)], 'id': 'P04-1038.2'}}	O application of O EM ) O discrimination . O O O O O O O O O O O O O O O O S-NORP O O O O
The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.	rich linguistic features	model	usage	{'e1': {'word': 'rich linguistic features', 'word_index': [(1, 3)], 'id': 'P04-1038.4'}, 'e2': {'word': 'model', 'word_index': [(0, 0)], 'id': 'P04-1038.3'}}	O rich linguistic O argument structure O O O O O O O O O O O O O O O O
The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.	predicate-argument structure information	target verbs	model-feature	{'e1': {'word': 'predicate-argument structure information', 'word_index': [(3, 6)], 'id': 'P04-1038.5'}, 'e2': {'word': 'target verbs', 'word_index': [(6, 6)], 'id': 'P04-1038.6'}}	O rich linguistic O argument structure O O O O O O O O O O O O O O O O
A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.	semantic taxonomy	Chinese nouns	model-feature	{'e1': {'word': 'semantic taxonomy', 'word_index': [(1, 2)], 'id': 'P04-1038.7'}, 'e2': {'word': 'Chinese nouns', 'word_index': [(3, 3)], 'id': 'P04-1038.8'}}	O semantic taxonomy O based on O to provide O the model S-NORP O O O O O O O O S-CARDINAL O S-NORP O O O O O O O O O O O O O
A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.	semantic features	model	usage	{'e1': {'word': 'semantic features', 'word_index': [(9, 9)], 'id': 'P04-1038.10'}, 'e2': {'word': 'model', 'word_index': [(11, 11)], 'id': 'P04-1038.11'}}	O semantic taxonomy O based on O to provide O the model S-NORP O O O O O O O O S-CARDINAL O S-NORP O O O O O O O O O O O O O
We further enhanced the model with certain fine-grained semantic categories called lexical sets.	fine-grained semantic categories	model	usage	{'e1': {'word': 'fine-grained semantic categories', 'word_index': [(3, 4)], 'id': 'P04-1038.20'}, 'e2': {'word': 'model', 'word_index': [(3, 3)], 'id': 'P04-1038.19'}}	O enhanced the O categories called O O O O O O O O O O O O O O
Our results indicate that these lexical sets improve the model's performance for the three most challenging verbs chosen from the first set of experiments.	lexical sets	model	result	{'e1': {'word': 'lexical sets', 'word_index': [(3, 3)], 'id': 'P04-1038.22'}, 'e2': {'word': 'model', 'word_index': [(3, 3)], 'id': 'P04-1038.23'}}	O that these O for the O set of O O O O O O O O O O O S-CARDINAL O O O O O O S-ORDINAL O O O O
This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar.	dependency structure grammar	speech recognition and understanding	usage	{'e1': {'word': 'dependency structure grammar', 'word_index': [(6, 6)], 'id': 'C88-1082.3'}, 'e2': {'word': 'speech recognition and understanding', 'word_index': [(3, 5)], 'id': 'C88-1082.2'}}	O efficient linguistic O and understanding O O O O O O O O O O O O O O O O O
After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships.	phoneme recognition	phrase recognition	usage	{'e1': {'word': 'phoneme recognition', 'word_index': [(3, 3)], 'id': 'C88-1082.9'}, 'e2': {'word': 'phrase recognition', 'word_index': [(0, 1)], 'id': 'C88-1082.8'}}	O recognition based O extracts the O likelihood of O the linguistic O relationships . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
A fast parsing algorithm using breadth-first search is also proposed.	breadth-first search	parsing algorithm	usage	{'e1': {'word': 'breadth-first search', 'word_index': [(3, 4)], 'id': 'C88-1082.18'}, 'e2': {'word': 'parsing algorithm', 'word_index': [(2, 3)], 'id': 'C88-1082.17'}}	O fast parsing O search is O O O O O S-ORDINAL O O O O O O
The predictor pre-selects the phrase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing.	transition rules	predictor	usage	{'e1': {'word': 'transition rules', 'word_index': [(3, 3)], 'id': 'C88-1082.21'}, 'e2': {'word': 'predictor', 'word_index': [(0, 0)], 'id': 'C88-1082.19'}}	O the phrase O to reduce O O O O O O O O O O O O O O O O O O O O
The experimental results show that it greatly increases the accuracy of speech recognitions, and the breadth-first parsing algorithm and predictor increase processing speed.	predictor	processing speed	result	{'e1': {'word': 'predictor', 'word_index': [(6, 6)], 'id': 'C88-1082.29'}, 'e2': {'word': 'processing speed', 'word_index': [(6, 6)], 'id': 'C88-1082.30'}}	O show that O first parsing O O O O O O O O O O O O O O O O S-ORDINAL O O O O O O O O
In the EU-funded project, QALL-ME, a domain-specific ontology was developed and applied for question answering in the domain of tourism, along with the assistance of two upper ontologies for concept expansion and reasoning.	domain-specific ontology	question answering	usage	{'e1': {'word': 'domain-specific ontology', 'word_index': [(8, 9)], 'id': 'L08-1178.6'}, 'e2': {'word': 'question answering', 'word_index': [(12, 12)], 'id': 'L08-1178.7'}}	O EU - O QALL - S-ORG a domain-specific O was developed O the domain O of two O ontologies for O . O O O O O O O O O O O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O
The design of the ontology is presented in the paper, and a semi-automatic alignment procedure is described with some alignment results given as well.	semi-automatic alignment procedure	alignment results	result	{'e1': {'word': 'semi-automatic alignment procedure', 'word_index': [(6, 7)], 'id': 'L08-1178.14'}, 'e2': {'word': 'alignment results', 'word_index': [(10, 11)], 'id': 'L08-1178.15'}}	O the ontology O and a O procedure is O alignment results O O O O O O O O O O O O O O O O O O O O O O
Furthermore, the aligned ontology was used to semantically annotate original data obtained from the tourism web sites and natural language questions.	natural language questions	data	part_whole	{'e1': {'word': 'natural language questions', 'word_index': [(7, 9)], 'id': 'L08-1178.18'}, 'e2': {'word': 'data', 'word_index': [(1, 1)], 'id': 'L08-1178.17'}}	O data obtained O the tourism O natural language O O O O O O O O O O O O O O O O O O O O
The storage schema of the annotated data and the data access method for retrieving answers from the annotated data are also reported in the paper.	data access method	annotated data	usage	{'e1': {'word': 'data access method', 'word_index': [(3, 3)], 'id': 'L08-1178.21'}, 'e2': {'word': 'annotated data', 'word_index': [(6, 6)], 'id': 'L08-1178.22'}}	O of the O for retrieving O also reported O paper . O O O O O O O O O O O O O O O O O O O O O O O
In particular we discuss a natural language generation system that is composed of SPoT, a trainable sentence planner, and FERGUS, a stochastic surface realizer.	SPoT	natural language generation system	part_whole	{'e1': {'word': 'SPoT', 'word_index': [(3, 3)], 'id': 'C02-1138.9'}, 'e2': {'word': 'natural language generation system', 'word_index': [(3, 3)], 'id': 'C02-1138.8'}}	O we discuss O , a O planner , O . O O O O O O O O O O O O O O O O O O S-PERSON O O O O O O
We show how these stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system.	NLG components	real-time dialog system	part_whole	{'e1': {'word': 'NLG components', 'word_index': [(12, 12)], 'id': 'C02-1138.13'}, 'e2': {'word': 'real-time dialog system', 'word_index': [(15, 18)], 'id': 'C02-1138.14'}}	O how these O be made O , that O new domains O be integrated O - time O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
In the current work, we produce a manual segmentation of laughter in a large corpus of interactive multi-party seminars, which promises to be a valuable resource for acoustic modeling purposes.	interactive multi-party seminars	corpus	part_whole	{'e1': {'word': 'interactive multi-party seminars', 'word_index': [(8, 9)], 'id': 'L08-1016.10'}, 'e2': {'word': 'corpus', 'word_index': [(6, 6)], 'id': 'L08-1016.9'}}	O current work O a manual O of interactive O promises to O a valuable O acoustic modeling O O O O O O O O O O O O O O O O O O O O O O O O O O O
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further.	grammar PCFG*	extended GLR parsing algorithm	usage	{'e1': {'word': 'grammar PCFG*', 'word_index': [(3, 4)], 'id': 'C02-2028.2'}, 'e2': {'word': 'extended GLR parsing algorithm', 'word_index': [(1, 3)], 'id': 'C02-2028.1'}}	O extended GLR O * that O and extends O . O O O O O O O O O O O O S-PERSON O O O O O O O O O
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	PCFG	grammar PCFG*	usage	{'e1': {'word': 'PCFG', 'word_index': [(6, 6)], 'id': 'C02-2028.5'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(3, 3)], 'id': 'C02-2028.4'}}	O define a O that is O only probability O also frequency O rule . O O O O O O O O O O O O O O O O O O O O O
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	frequency	rule	model-feature	{'e1': {'word': 'frequency', 'word_index': [(11, 11)], 'id': 'C02-2028.7'}, 'e2': {'word': 'rule', 'word_index': [(13, 13)], 'id': 'C02-2028.8'}}	O define a O that is O only probability O also frequency O rule . O O O O O O O O O O O O O O O O O O O O O
So our syntactic parsing system is implemented based on rule-based approach and statistics approach.	rule-based approach	syntactic parsing system	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(6, 6)], 'id': 'C02-2028.10'}, 'e2': {'word': 'syntactic parsing system', 'word_index': [(1, 3)], 'id': 'C02-2028.9'}}	O syntactic parsing O based on O O O O O O O O O O O O O O O
In this paper, we discuss lemma identification in Japanese morphological analysis, which is crucial for a proper formulation of morphological analysis that benefits not only NLP researchers but also corpus linguists.	lemma identification	Japanese morphological analysis	usage	{'e1': {'word': 'lemma identification', 'word_index': [(3, 3)], 'id': 'L08-1535.1'}, 'e2': {'word': 'Japanese morphological analysis', 'word_index': [(5, 6)], 'id': 'L08-1535.2'}}	O paper , O in Japanese O crucial for O proper formulation O analysis that O corpus linguists O O O S-NORP O O O O O O O O O O O O O O O O O O O O O O O O
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	words	vocabulary	part_whole	{'e1': {'word': 'words', 'word_index': [(4, 4)], 'id': 'L08-1535.8'}, 'e2': {'word': 'vocabulary', 'word_index': [(3, 3)], 'id': 'L08-1535.6'}}	O variation in S-NORP words of O sometimes happens O form corresponds O and that O lemmas with O and / O O O O O S-NORP O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	writing form	lemma	model-feature	{'e1': {'word': 'writing form', 'word_index': [(9, 10)], 'id': 'L08-1535.9'}, 'e2': {'word': 'lemma', 'word_index': [(12, 12)], 'id': 'L08-1535.10'}}	O variation in S-NORP words of O sometimes happens O form corresponds O and that O lemmas with O and / O O O O O S-NORP O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	writing form	lemmas	model-feature	{'e1': {'word': 'writing form', 'word_index': [(15, 15)], 'id': 'L08-1535.11'}, 'e2': {'word': 'lemmas', 'word_index': [(16, 16)], 'id': 'L08-1535.12'}}	O variation in S-NORP words of O sometimes happens O form corresponds O and that O lemmas with O and / O O O O O S-NORP O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O
The mapping from a writing form onto a lemma is important in linguistic analysis of corpora.	linguistic analysis	corpora	topic	{'e1': {'word': 'linguistic analysis', 'word_index': [(4, 5)], 'id': 'L08-1535.17'}, 'e2': {'word': 'corpora', 'word_index': [(6, 6)], 'id': 'L08-1535.18'}}	O a writing O linguistic analysis O O O O O O O O O O O O O O O
The current study focuses on disambiguation of heteronyms, words with the same writing form but with different word forms.	disambiguation	heteronyms	usage	{'e1': {'word': 'disambiguation', 'word_index': [(3, 3)], 'id': 'L08-1535.19'}, 'e2': {'word': 'heteronyms', 'word_index': [(3, 3)], 'id': 'L08-1535.20'}}	O focuses on O same writing O forms . O O O O O O O O O O O O O O O O O O
To resolve heteronym ambiguity, we make use of goshu information, the classification of words based on their origin.	origin	words	model-feature	{'e1': {'word': 'origin', 'word_index': [(6, 6)], 'id': 'L08-1535.28'}, 'e2': {'word': 'words', 'word_index': [(6, 6)], 'id': 'L08-1535.27'}}	O heteronym ambiguity O , the O O O O O O O O O O O O O O O O O O O
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	words	goshu classes	part_whole	{'e1': {'word': 'words', 'word_index': [(0, 0)], 'id': 'L08-1535.29'}, 'e2': {'word': 'goshu classes', 'word_index': [(2, 3)], 'id': 'L08-1535.30'}}	O some goshu O more likely O compound words O classes , O model based O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	words	classes	part_whole	{'e1': {'word': 'words', 'word_index': [(9, 9)], 'id': 'L08-1535.32'}, 'e2': {'word': 'classes', 'word_index': [(10, 10)], 'id': 'L08-1535.33'}}	O some goshu O more likely O compound words O classes , O model based O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	CRFs	statistical model	usage	{'e1': {'word': 'CRFs', 'word_index': [(15, 15)], 'id': 'L08-1535.35'}, 'e2': {'word': 'statistical model', 'word_index': [(12, 13)], 'id': 'L08-1535.34'}}	O some goshu O more likely O compound words O classes , O model based O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.	goshu information	performance	result	{'e1': {'word': 'goshu information', 'word_index': [(0, 0)], 'id': 'L08-1535.38'}, 'e2': {'word': 'performance', 'word_index': [(1, 1)], 'id': 'L08-1535.39'}}	O performance of O that goshu O identification task O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.	information	lemma identification task	usage	{'e1': {'word': 'information', 'word_index': [(6, 6)], 'id': 'L08-1535.42'}, 'e2': {'word': 'lemma identification task', 'word_index': [(6, 8)], 'id': 'L08-1535.43'}}	O performance of O that goshu O identification task O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
An event detection algorithm identifies the collocations that may cause an event in a specific timestamp.	event detection algorithm	collocations	usage	{'e1': {'word': 'event detection algorithm', 'word_index': [(0, 2)], 'id': 'L08-1003.6'}, 'e2': {'word': 'collocations', 'word_index': [(3, 3)], 'id': 'L08-1003.7'}}	O detection algorithm O cause an O O O O O O O O O O O O O O O
An event summarization algorithm retrieves a set of collocations which describe an event.	event summarization algorithm	collocations	usage	{'e1': {'word': 'event summarization algorithm', 'word_index': [(0, 2)], 'id': 'L08-1003.8'}, 'e2': {'word': 'collocations', 'word_index': [(3, 3)], 'id': 'L08-1003.9'}}	O summarization algorithm O which describe O O O O O O O O O O O O
We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis (LSA) to predict team performance.	Latent Semantic Analysis (LSA)	tagging	usage	{'e1': {'word': 'Latent Semantic Analysis (LSA)', 'word_index': [(3, 6)], 'id': 'N04-4025.2'}, 'e2': {'word': 'tagging', 'word_index': [(3, 3)], 'id': 'N04-4025.1'}}	O two approaches O Semantic Analysis S-CARDINAL to predict O O O O O O O O O O O O O O O O O O O
A huge amount of translation work needs to be done when creating and updating technical documentation.	translation work	technical documentation	usage	{'e1': {'word': 'translation work', 'word_index': [(3, 3)], 'id': 'A94-1044.3'}, 'e2': {'word': 'technical documentation', 'word_index': [(6, 7)], 'id': 'A94-1044.4'}}	O huge amount O to be O documentation . O O O O O O O O O O O O O O
The objective of this project is a pilot study of several new ideas for the automatic adaptation and improvement of natural language processing (NLP) systems.	pilot study	automatic adaptation	topic	{'e1': {'word': 'pilot study', 'word_index': [(3, 3)], 'id': 'H91-1079.1'}, 'e2': {'word': 'automatic adaptation', 'word_index': [(5, 6)], 'id': 'H91-1079.2'}}	O this project O the automatic O improvement of O NLP ) O O O O O O O O O O O O O O O O O O O O O O O O
The effort focuses particularly on automatically inferring the meaning of new words in context and on developing partial interpretations of language that is either fragmentary or beyond the capability of the NLP system to understand.	meaning	words	model-feature	{'e1': {'word': 'meaning', 'word_index': [(3, 3)], 'id': 'H91-1079.5'}, 'e2': {'word': 'words', 'word_index': [(4, 4)], 'id': 'H91-1079.6'}}	O particularly on O words in O language that O understand . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
The NLP system uses large annotated corpora, such as those being developed under the DARPA-funded TREE-BANK project at the University of Pennsylvania, to adapt by acquiring syntactic and semantic information from the annotated examples.	large annotated corpora	NLP system	usage	{'e1': {'word': 'large annotated corpora', 'word_index': [(2, 3)], 'id': 'H91-1079.13'}, 'e2': {'word': 'NLP system', 'word_index': [(0, 0)], 'id': 'H91-1079.12'}}	O uses large O such as O - funded O University of O acquiring syntactic O O O O O O O O O O S-ORG O O O O O O O O S-GPE O O O O O O O O O O O O O O
The NLP system uses large annotated corpora, such as those being developed under the DARPA-funded TREE-BANK project at the University of Pennsylvania, to adapt by acquiring syntactic and semantic information from the annotated examples.	syntactic and semantic information	annotated examples	part_whole	{'e1': {'word': 'syntactic and semantic information', 'word_index': [(14, 15)], 'id': 'H91-1079.15'}, 'e2': {'word': 'annotated examples', 'word_index': [(15, 15)], 'id': 'H91-1079.16'}}	O uses large O such as O - funded O University of O acquiring syntactic O O O O O O O O O O S-ORG O O O O O O O O S-GPE O O O O O O O O O O O O O O
Statistical language modeling, based on probability estimates derived from the large corpora, will provide a means of ranking alternative interpretations of fragments.	probability estimates	Statistical language modeling	usage	{'e1': {'word': 'probability estimates', 'word_index': [(0, 0)], 'id': 'H91-1079.18'}, 'e2': {'word': 'Statistical language modeling', 'word_index': [(0, 0)], 'id': 'H91-1079.17'}}	O large corpora O interpretations of O O O O O O O O O O O O O O O O O O O O O O O
Lexicalized concepts are organized by semantic relations (synonymy, antonymy, hyponymy, meronymy, etc.)	semantic relations	Lexicalized concepts	model-feature	{'e1': {'word': 'semantic relations', 'word_index': [(0, 0)], 'id': 'H92-1116.6'}, 'e2': {'word': 'Lexicalized concepts', 'word_index': [(0, 0)], 'id': 'H92-1116.5'}}	O , hyponymy O O O O O O O O O O O O O O O O O O
Work under this grant is intended to extend and upgrade WordNet, to make it generally available, and to develop it as a tool for use in practical applications.	WordNet	applications	usage	{'e1': {'word': 'WordNet', 'word_index': [(3, 3)], 'id': 'H92-1116.14'}, 'e2': {'word': 'applications', 'word_index': [(14, 14)], 'id': 'H92-1116.15'}}	O is intended O , to O , and O tool for O practical applications O O O O O O O O O O O O O O O O O O O O O O O O O O
In order to make it available for information retrieval and machine translation, a system is being developed English text as input and automatically gives as output the same text augmented by syntactic and semantic anotations that disambiguate all of the substantive words.	syntactic and semantic anotations	substantive words	model-feature	{'e1': {'word': 'syntactic and semantic anotations', 'word_index': [(15, 17)], 'id': 'H92-1116.20'}, 'e2': {'word': 'substantive words', 'word_index': [(21, 21)], 'id': 'H92-1116.21'}}	O to make O and machine O a system O text as O automatically gives O semantic anotations O of the O O O O O O O O O O O S-LANGUAGE O O O O O O O O O O O O O O O O O O O O O O O O O
Initially, the semantic tagging is being done manually so that we can (1) obtain extensive experience with the tagging process and (2) create a database of correctly tagged text for use in testing proposals for automatic sense disambiguation.	text	database	part_whole	{'e1': {'word': 'text', 'word_index': [(12, 12)], 'id': 'H92-1116.25'}, 'e2': {'word': 'database', 'word_index': [(12, 12)], 'id': 'H92-1116.24'}}	O so that O can ( O extensive experience O create a O in testing O automatic sense O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Review previous designs involving TIPSTER technology, to support you design process.Determine if your application can benefit from upgrading to advanced TIPSTER technology that has been developed since your application was implemented.	TIPSTER technology	application	usage	{'e1': {'word': 'TIPSTER technology', 'word_index': [(6, 6)], 'id': 'X96-1060.8'}, 'e2': {'word': 'application', 'word_index': [(4, 4)], 'id': 'X96-1060.7'}}	O , to O application can O that has O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Accurate lemmatization of German nouns mandates the use of a lexicon.	lemmatization	German nouns	usage	{'e1': {'word': 'lemmatization', 'word_index': [(0, 0)], 'id': 'H05-1080.1'}, 'e2': {'word': 'German nouns', 'word_index': [(0, 0)], 'id': 'H05-1080.2'}}	O of a O O S-NORP O O O O O O O O
We present a self-learning lemmatizer capable of automatically creating a full-form lexicon by processing German documents.	self-learning lemmatizer	German documents	usage	{'e1': {'word': 'self-learning lemmatizer', 'word_index': [(2, 3)], 'id': 'H05-1080.5'}, 'e2': {'word': 'German documents', 'word_index': [(9, 9)], 'id': 'H05-1080.7'}}	O a self O a full O form lexicon O . O O O O O O O O O O O O O O S-NORP O O
In this paper we describe how Why-Atlas creates and utilizes a proof-based representation of student essays.	proof-based representation	Why-Atlas	usage	{'e1': {'word': 'proof-based representation', 'word_index': [(6, 8)], 'id': 'W02-0211.6'}, 'e2': {'word': 'Why-Atlas', 'word_index': [(3, 4)], 'id': 'W02-0211.5'}}	O paper we O Atlas creates O based representation O essays . O O O O O O O O O O O O O O O O O
We describe how it creates the proof given the output of sentence-level understanding, how it uses the proofs to give students feedback, some preliminary runtime measures, and the work we are currently doing to derive additional benefits from a proof-based approach for tutoring applications.	proof-based approach	tutoring applications	usage	{'e1': {'word': 'proof-based approach', 'word_index': [(21, 21)], 'id': 'W02-0211.10'}, 'e2': {'word': 'tutoring applications', 'word_index': [(22, 23)], 'id': 'W02-0211.11'}}	O how it O - level O it uses O to give O , and O are currently O from a O tutoring applications O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Compared with syntactic knowledge, semantic knowledge is more difficult to annotate, for ambiguity problem is more serious.	syntactic knowledge	semantic knowledge	compare	{'e1': {'word': 'syntactic knowledge', 'word_index': [(0, 0)], 'id': 'W03-1712.13'}, 'e2': {'word': 'semantic knowledge', 'word_index': [(0, 0)], 'id': 'W03-1712.14'}}	O more difficult O ambiguity problem O O O O O O O O O O O O O O O O O O
Finally, we will compare our corpus with other well-known corpora.	corpus	corpora	compare	{'e1': {'word': 'corpus', 'word_index': [(0, 0)], 'id': 'W03-1712.17'}, 'e2': {'word': 'corpora', 'word_index': [(6, 6)], 'id': 'W03-1712.18'}}	O with other O - known O O O O O O O O O O O O
These techniques, developed within the Augmented Transition Network (ATN) model, are shown to be adequate to handle many of these cases.	techniques	Augmented Transition Network (ATN) model	usage	{'e1': {'word': 'techniques', 'word_index': [(0, 0)], 'id': 'J81-2002.7'}, 'e2': {'word': 'Augmented Transition Network (ATN) model', 'word_index': [(2, 3)], 'id': 'J81-2002.8'}}	O the Augmented O be adequate O handle many O O O O O O O S-ORG O O O O O O O O O O O O O O O
Once identified, reference chains can be extended across segment boundaries, thus enabling the application of CT over the entire discourse.	CT	discourse	usage	{'e1': {'word': 'CT', 'word_index': [(5, 5)], 'id': 'P98-1044.13'}, 'e2': {'word': 'discourse', 'word_index': [(9, 9)], 'id': 'P98-1044.14'}}	O chains can O of CT O the entire O O O O O O O O O O O O O O O O O O O O
We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains.	CT	global discourse	usage	{'e1': {'word': 'CT', 'word_index': [(6, 6)], 'id': 'P98-1044.17'}, 'e2': {'word': 'global discourse', 'word_index': [(9, 9)], 'id': 'P98-1044.18'}}	O the processes O trees and O be applied O O O O O O O O O O O O O O O O O O O O O O O O
We also define a discourse smoothness index which can be used to compare different discourse structures and interpretations, and show how VT can be used to abstract a span of text in the context of the whole discourse.	discourse smoothness index	discourse structures and interpretations	usage	{'e1': {'word': 'discourse smoothness index', 'word_index': [(3, 3)], 'id': 'P98-1044.20'}, 'e2': {'word': 'discourse structures and interpretations', 'word_index': [(7, 9)], 'id': 'P98-1044.21'}}	O define a O O can be O and interpretations O and show O of text O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports in order to satisfy a given scenario.	natural language	exploratory questions	model-feature	{'e1': {'word': 'natural language', 'word_index': [(9, 9)], 'id': 'W04-2507.7'}, 'e2': {'word': 'exploratory questions', 'word_index': [(8, 9)], 'id': 'W04-2507.6'}}	S-PERSON answering technology O allow analysts O complex exploratory O their briefing O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space.	data-driven semantics	system	usage	{'e1': {'word': 'data-driven semantics', 'word_index': [(2, 3)], 'id': 'W04-2507.12'}, 'e2': {'word': 'system', 'word_index': [(0, 0)], 'id': 'W04-2507.11'}}	O novel data-driven O with the O scope and O answer space O O O O O O O O O O O O O O O O O O O O O O O
One is that it resulted in the first freely distributable corpus of fully anonymized clinical text.	fully anonymized clinical text	corpus	part_whole	{'e1': {'word': 'fully anonymized clinical text', 'word_index': [(6, 9)], 'id': 'W07-1013.4'}, 'e2': {'word': 'corpus', 'word_index': [(6, 6)], 'id': 'W07-1013.3'}}	S-CARDINAL it resulted O first freely O anonymized clinical O O O O S-ORDINAL O O O O O O O O O
The other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels.	set of labels	categorization	usage	{'e1': {'word': 'set of labels', 'word_index': [(9, 9)], 'id': 'W07-1013.7'}, 'e2': {'word': 'categorization', 'word_index': [(5, 5)], 'id': 'W07-1013.6'}}	O feature of O required categorization O a large O O O O O O O O O O O O O O O O O O O O O
Many systems performed at levels approaching the inter-coder agreement, suggesting that human-like performance on this task is within the reach of currently available technologies.	human-like performance	currently available technologies	compare	{'e1': {'word': 'human-like performance', 'word_index': [(5, 6)], 'id': 'W07-1013.12'}, 'e2': {'word': 'currently available technologies', 'word_index': [(9, 10)], 'id': 'W07-1013.13'}}	O levels approaching O that human O reach of O technologies . O O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper we describe automatic information nuggetization and its application to text comparison.	automatic information nuggetization	text comparison	usage	{'e1': {'word': 'automatic information nuggetization', 'word_index': [(3, 3)], 'id': 'N07-2055.1'}, 'e2': {'word': 'text comparison', 'word_index': [(6, 6)], 'id': 'N07-2055.2'}}	O paper we O and its O O O O O O O O O O O O O
More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material.	machine-generated nuggets	evaluation material	usage	{'e1': {'word': 'machine-generated nuggets', 'word_index': [(3, 3)], 'id': 'N07-2055.3'}, 'e2': {'word': 'evaluation material', 'word_index': [(5, 6)], 'id': 'N07-2055.4'}}	O take a O create evaluation O . O O O O O O O O O O O O O O O O O O O
A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.	semiautomatic annotation scheme	gold-standard data	usage	{'e1': {'word': 'semiautomatic annotation scheme', 'word_index': [(1, 3)], 'id': 'N07-2055.5'}, 'e2': {'word': 'gold-standard data', 'word_index': [(3, 3)], 'id': 'N07-2055.6'}}	O semiautomatic annotation O agreement . O O O O O O O O O O O O O O O O
This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts.	features	retrieval	usage	{'e1': {'word': 'features', 'word_index': [(6, 6)], 'id': 'P07-3007.1'}, 'e2': {'word': 'retrieval', 'word_index': [(6, 6)], 'id': 'P07-3007.2'}}	O results of O different kinds O . O O O O O O O O O O O O O O O S-NORP O O O
We assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR, but with some distinct features.	retrieval	IR	part_whole	{'e1': {'word': 'retrieval', 'word_index': [(3, 3)], 'id': 'P07-3007.4'}, 'e2': {'word': 'IR', 'word_index': [(10, 10)], 'id': 'P07-3007.6'}}	O that the O ( OIR O as a O IR , O features . O O O O O O O O O O O O O O O O O O O O O O O O
This paper discusses the supervised learning of morphology using stochastic transducers, trained using the Expectation-Maximization (EM) algorithm.	stochastic transducers	supervised learning	usage	{'e1': {'word': 'stochastic transducers', 'word_index': [(3, 3)], 'id': 'P02-1065.3'}, 'e2': {'word': 'supervised learning', 'word_index': [(1, 2)], 'id': 'P02-1065.1'}}	O supervised learning O , trained O . O O O O O O O O O O O O O O O O O O O O
These are evaluated and compared ondata sets from English, German, Slovene and Arabic.	data sets	English	part_whole	{'e1': {'word': 'data sets', 'word_index': [(2, 3)], 'id': 'P02-1065.9'}, 'e2': {'word': 'English', 'word_index': [(3, 3)], 'id': 'P02-1065.10'}}	O on data O , German O O O O O O O S-LANGUAGE O S-NORP O S-NORP O S-LANGUAGE O
Speech recognition problems are a reality in current spoken dialogue systems	Speech recognition problems	spoken dialogue systems	part_whole	{'e1': {'word': 'Speech recognition problems', 'word_index': [(0, 0)], 'id': 'P06-1025.1'}, 'e2': {'word': 'spoken dialogue systems', 'word_index': [(3, 3)], 'id': 'P06-1025.2'}}	O in current O O O O O O O O O O
We apply Chi Square (%2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns.	Chi Square (%2) analysis	corpus of speech-based computer tutoring dialogues	usage	{'e1': {'word': 'Chi Square (%2) analysis', 'word_index': [(1, 3)], 'id': 'P06-1025.5'}, 'e2': {'word': 'corpus of speech-based computer tutoring dialogues', 'word_index': [(6, 9)], 'id': 'P06-1025.6'}}	O Chi Square O to a O - based O dependencies both O and across O . O O O O O O O O O O O O O O O O O O O O O O O
In an interlingual knowledge-based machine translation system, ambiguity arises when the source language analyzer produces more than one interlingua expression for a source sentence.	interlingua expression	source sentence	model-feature	{'e1': {'word': 'interlingua expression', 'word_index': [(8, 9)], 'id': 'C94-1012.4'}, 'e2': {'word': 'source sentence', 'word_index': [(9, 9)], 'id': 'C94-1012.5'}}	O interlingual knowledge O machine translation O one interlingua O O O O O O O O O O O O O O O O O S-CARDINAL O O O O O O O
We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence.	test sentences	corpus	part_whole	{'e1': {'word': 'test sentences', 'word_index': [(6, 6)], 'id': 'C94-1012.12'}, 'e2': {'word': 'corpus', 'word_index': [(4, 4)], 'id': 'C94-1012.11'}}	O test these O corpus of O order to O methods reduce O . O O O O O O O O O O O O O O O O O O O O O O O O O O
We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence.	disambiguation methods	parses	result	{'e1': {'word': 'disambiguation methods', 'word_index': [(9, 10)], 'id': 'C94-1012.13'}, 'e2': {'word': 'parses', 'word_index': [(12, 12)], 'id': 'C94-1012.14'}}	O test these O corpus of O order to O methods reduce O . O O O O O O O O O O O O O O O O O O O O O O O O O O
The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding.	dynamic programs	machine translation decoding	usage	{'e1': {'word': 'dynamic programs', 'word_index': [(3, 4)], 'id': 'D08-1012.3'}, 'e2': {'word': 'machine translation decoding', 'word_index': [(6, 7)], 'id': 'D08-1012.4'}}	O tree transducer O programs for O decoding . O O O O O O O O O O O O O O O O O O O O
In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language.	clustered encoding	encoding-based methods	usage	{'e1': {'word': 'clustered encoding', 'word_index': [(9, 9)], 'id': 'D08-1012.10'}, 'e2': {'word': 'encoding-based methods', 'word_index': [(7, 9)], 'id': 'D08-1012.9'}}	O to previous O , we O encoding - O of the O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.	decoding cascade for trigram language models	bigram-to-trigram decoder	compare	{'e1': {'word': 'decoding cascade for trigram language models', 'word_index': [(0, 2)], 'id': 'D08-1012.16'}, 'e2': {'word': 'bigram-to-trigram decoder', 'word_index': [(9, 9)], 'id': 'D08-1012.18'}}	O language models O faster than O pass alone O . O O O O O O O O O O O O O O O O O O O O O O O
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain.	pronoun resolution	biomedical texts	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(3, 4)], 'id': 'L08-1071.1'}, 'e2': {'word': 'biomedical texts', 'word_index': [(6, 6)], 'id': 'L08-1071.2'}}	O findings on O resolution for O resolution for O . O O O O O O O O O O O O O O O O O O O O O O O
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain.	pronoun resolution	newswire domain	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(6, 7)], 'id': 'L08-1071.3'}, 'e2': {'word': 'newswire domain', 'word_index': [(9, 9)], 'id': 'L08-1071.4'}}	O findings on O resolution for O resolution for O . O O O O O O O O O O O O O O O O O O O O O O O
Sharing portions of grammars across languages greatly reduces the costs of multilingual grammar engineering.	grammars	multilingual grammar engineering	result	{'e1': {'word': 'grammars', 'word_index': [(0, 0)], 'id': 'C00-1005.1'}, 'e2': {'word': 'multilingual grammar engineering', 'word_index': [(3, 3)], 'id': 'C00-1005.3'}}	O reduces the O O O O O O O O O O O O O O
Taking grammatical relatedness seriously, we are particularly interested in designing linguistically motivated grammatical resources for Slavic languages to be used in applied and theoretical computational linguistics.	linguistically motivated grammatical resources	applied and theoretical computational linguistics	usage	{'e1': {'word': 'linguistically motivated grammatical resources', 'word_index': [(3, 3)], 'id': 'C00-1005.8'}, 'e2': {'word': 'applied and theoretical computational linguistics', 'word_index': [(6, 6)], 'id': 'C00-1005.10'}}	O are particularly O be used O O O O O O O O O O O O O O S-NORP O O O O O O O O O O O
"On the basis of Slavic data, we show how a domain ontology conceptualising morpho-syntactic ""building blocks"" can serve as a basis of a shared grammar of Slavic."	domain ontology	shared grammar of Slavic	usage	{'e1': {'word': 'domain ontology', 'word_index': [(6, 6)], 'id': 'C00-1005.18'}, 'e2': {'word': 'shared grammar of Slavic', 'word_index': [(13, 15)], 'id': 'C00-1005.20'}}	"O basis of O we show O morpho-syntactic "" O "" can S-NORP shared grammar O O O O O O O O O O O O O O O O O O O O O O O O S-NORP O"
We are currently developing MiniSTEx, a spatiotemporal annotation system to handle temporal and/or geospatial information directly and indirectly expressed in texts.	temporal and/or geospatial information	texts	part_whole	{'e1': {'word': 'temporal and/or geospatial information', 'word_index': [(6, 6)], 'id': 'L08-1561.6'}, 'e2': {'word': 'texts', 'word_index': [(9, 9)], 'id': 'L08-1561.7'}}	O currently developing O spatiotemporal annotation O directly and O S-ORG O O O O O O O O O O O O O O O O O O O O
A first version of MiniSTEx was originally developed for Dutch, keeping in mind that it should also be useful for other European languages, and for multilingual applications.	MiniSTEx	Dutch	usage	{'e1': {'word': 'MiniSTEx', 'word_index': [(3, 3)], 'id': 'L08-1561.11'}, 'e2': {'word': 'Dutch', 'word_index': [(5, 5)], 'id': 'L08-1561.12'}}	O first version S-ORDINAL for Dutch O also be O other European S-ORG O O O O S-NORP O O O O O O O O O O O O S-NORP O O O O O O O
In this paper we describe a technique for improving the performance of an information extraction system for speech data by explicitly modeling the errors in the recognizer output.	information extraction system	speech data	usage	{'e1': {'word': 'information extraction system', 'word_index': [(6, 7)], 'id': 'H01-1034.4'}, 'e2': {'word': 'speech data', 'word_index': [(9, 9)], 'id': 'H01-1034.5'}}	O paper we O improving the O system for O by explicitly O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper, we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list.	QA system	type questions	usage	{'e1': {'word': 'QA system', 'word_index': [(3, 4)], 'id': 'I05-2006.1'}, 'e2': {'word': 'type questions', 'word_index': [(6, 6)], 'id': 'I05-2006.2'}}	O paper , O system which O based on O confirmed knowledge O was developed O mailing list O O O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper, we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list.	confirmed knowledge base	mails	part_whole	{'e1': {'word': 'confirmed knowledge base', 'word_index': [(10, 12)], 'id': 'I05-2006.3'}, 'e2': {'word': 'mails', 'word_index': [(15, 15)], 'id': 'I05-2006.4'}}	O paper , O system which O based on O confirmed knowledge O was developed O mailing list O O O O O O O O O O O O O O O O O O O O O O O O O O O
We first discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents.	knowledge base	natural language documents	part_whole	{'e1': {'word': 'knowledge base', 'word_index': [(3, 4)], 'id': 'I05-2006.6'}, 'e2': {'word': 'natural language documents', 'word_index': [(6, 6)], 'id': 'I05-2006.7'}}	O discuss a S-ORDINAL base by O in natural O documents . O O O O O O O O O O O O O O O O O O O
We first discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents.	wrong information	natural language documents	part_whole	{'e1': {'word': 'wrong information', 'word_index': [(6, 6)], 'id': 'I05-2006.8'}, 'e2': {'word': 'natural language documents', 'word_index': [(8, 10)], 'id': 'I05-2006.9'}}	O discuss a S-ORDINAL base by O in natural O documents . O O O O O O O O O O O O O O O O O O O
Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails.	wrong information	mails	part_whole	{'e1': {'word': 'wrong information', 'word_index': [(5, 6)], 'id': 'I05-2006.10'}, 'e2': {'word': 'mails', 'word_index': [(7, 7)], 'id': 'I05-2006.11'}}	O a method O detecting wrong O mails posted O base by O these mails O O O O O O O O O O O O O O O O O O O O O O
Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails.	knowledge base	mails	part_whole	{'e1': {'word': 'knowledge base', 'word_index': [(9, 10)], 'id': 'I05-2006.13'}, 'e2': {'word': 'mails', 'word_index': [(14, 14)], 'id': 'I05-2006.14'}}	O a method O detecting wrong O mails posted O base by O these mails O O O O O O O O O O O O O O O O O O O O O O
Finally, we show that question and answer mails posted to a mailing list can be used as a knowledge base for a QA system.	knowledge base	QA system	usage	{'e1': {'word': 'knowledge base', 'word_index': [(9, 10)], 'id': 'I05-2006.17'}, 'e2': {'word': 'QA system', 'word_index': [(12, 12)], 'id': 'I05-2006.18'}}	O answer mails O to a O can be O base for O O O O O O O O O O O O O O O O O O O O O O
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.	reading difficulty	text passage	model-feature	{'e1': {'word': 'reading difficulty', 'word_index': [(3, 3)], 'id': 'N04-1025.1'}, 'e2': {'word': 'text passage', 'word_index': [(5, 6)], 'id': 'N04-1025.2'}}	O a new O a text O , by O terms of O O O O O O O O O O O O O O O O O O O O O O O O O
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.	statistical language modeling	readability	model-feature	{'e1': {'word': 'statistical language modeling', 'word_index': [(12, 12)], 'id': 'N04-1025.4'}, 'e2': {'word': 'readability', 'word_index': [(9, 9)], 'id': 'N04-1025.3'}}	O a new O a text O , by O terms of O O O O O O O O O O O O O O O O O O O O O O O O O
We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.	language models	multinomial naive Bayes classification	usage	{'e1': {'word': 'language models', 'word_index': [(9, 9)], 'id': 'N04-1025.6'}, 'e2': {'word': 'multinomial naive Bayes classification', 'word_index': [(3, 6)], 'id': 'N04-1025.5'}}	O a measure O naive Bayes O that combines O likely grade O . O O O O O O S-PERSON O O O O O O O O O O O O O O O O O O
We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.	semantic variables	readability measures	part_whole	{'e1': {'word': 'semantic variables', 'word_index': [(6, 6)], 'id': 'N04-1025.10'}, 'e2': {'word': 'readability measures', 'word_index': [(6, 6)], 'id': 'N04-1025.11'}}	O predictions for O our performance O O O O O O S-LANGUAGE O O O O O O O O O O O O O O O
Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).	language modeling approach	Web documents	usage	{'e1': {'word': 'language modeling approach', 'word_index': [(6, 6)], 'id': 'N04-1025.17'}, 'e2': {'word': 'Web documents', 'word_index': [(8, 9)], 'id': 'N04-1025.18'}}	O such as O passages , O for Web O words ) O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O S-CARDINAL O O O
Syntactic and semantic information are both represented in the grammar in a uniform manner, similar to HPSG ( Pollard and Sag, 1987 ).	grammar	Syntactic and semantic information	model-feature	{'e1': {'word': 'grammar', 'word_index': [(1, 1)], 'id': 'M93-1024.7'}, 'e2': {'word': 'Syntactic and semantic information', 'word_index': [(0, 0)], 'id': 'M93-1024.6'}}	O grammar in O , similar O ) . O O O O O O O O O O O O O O S-ORG O S-PERSON O O O S-DATE O O
LINK has been used in several information extraction applications.	LINK	information extraction applications	usage	{'e1': {'word': 'LINK', 'word_index': [(0, 0)], 'id': 'M93-1024.9'}, 'e2': {'word': 'information extraction applications', 'word_index': [(3, 3)], 'id': 'M93-1024.10'}}	O in several O . O O O O O O O O
In a project with General Motors, LINK was used to process terse free-form descriptions of symptoms displayed by malfunctioning automobiles, and the repairs which fixed them.	LINK	free-form descriptions	usage	{'e1': {'word': 'LINK', 'word_index': [(6, 6)], 'id': 'M93-1024.11'}, 'e2': {'word': 'free-form descriptions', 'word_index': [(6, 9)], 'id': 'M93-1024.12'}}	O project with O Motors , O - form O by malfunctioning O them . O O O O O O O O O O O O O O O O O O O O O O O O O O
Reduplication, a central instance of prosodic morphology, is particularly challenging for state-of-the-art computational morphology, since it involves copying of some part of a phonological string	Reduplication	prosodic morphology	part_whole	{'e1': {'word': 'Reduplication', 'word_index': [(0, 0)], 'id': 'A00-2039.1'}, 'e2': {'word': 'prosodic morphology', 'word_index': [(0, 0)], 'id': 'A00-2039.2'}}	O state - O - the O art computational O some part O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper I advocate a finite-state method that combines enriched lexical representations via intersection to implement the copying.	enriched lexical representations	finite-state method	usage	{'e1': {'word': 'enriched lexical representations', 'word_index': [(6, 6)], 'id': 'A00-2039.6'}, 'e2': {'word': 'finite-state method', 'word_index': [(3, 5)], 'id': 'A00-2039.5'}}	O paper I O state method O via intersection O implement the O O O O O O O O O O O O O O O O O O
The proposal includes a resource-conscious variant of automata and can benefit from the existence of lazy algorithms.	lazy algorithms	resource-conscious variant of automata	usage	{'e1': {'word': 'lazy algorithms', 'word_index': [(6, 6)], 'id': 'A00-2039.8'}, 'e2': {'word': 'resource-conscious variant of automata', 'word_index': [(2, 3)], 'id': 'A00-2039.7'}}	O a resource O from the O O O O O O O O O O O O O O O O O O
"These quick relevancy judgements require two steps: (1) recognizing an expression that is highly relevant to the given domain, e.g. ""were killed"" in the domain of terrorism, and (2) verifying that the context surrounding the expression is consistent with the relevancy guidelines for the domain, e.g. ""5 soldiers were killed by guerrillas"" is not consistent with the terrorism domain since victims of terrorist acts must be civilians."	expression	given domain	model-feature	{'e1': {'word': 'expression', 'word_index': [(5, 5)], 'id': 'H92-1094.3'}, 'e2': {'word': 'given domain', 'word_index': [(6, 6)], 'id': 'H92-1094.4'}}	"O two steps O an expression O e.g. "" O ( 2 O expression is S-CARDINAL relevancy guidelines O "" 5 O were killed O guerrillas "" O not consistent O the terrorism O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O"
The Relevancy Signatures Algorithm attempts to simulate the first step in this process by deriving reliable relevancy cues from a corpus of training texts and using these cues to quickly identify new texts that are highly likely to be relevant.	reliable relevancy cues	corpus of training texts	part_whole	{'e1': {'word': 'reliable relevancy cues', 'word_index': [(5, 6)], 'id': 'H92-1094.12'}, 'e2': {'word': 'corpus of training texts', 'word_index': [(6, 6)], 'id': 'H92-1094.13'}}	O Algorithm attempts O deriving reliable O these cues O to be O O O O S-ORDINAL O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Our system accepts an entire English news story (text) as query, and retrieves relevant Chinese broadcast news stories (audio) from the document collection.	relevant Chinese broadcast news stories (audio)	document collection	part_whole	{'e1': {'word': 'relevant Chinese broadcast news stories (audio)', 'word_index': [(6, 9)], 'id': 'H01-1050.6'}, 'e2': {'word': 'document collection', 'word_index': [(12, 12)], 'id': 'H01-1050.7'}}	O an entire O as query O stories ( O from the O S-LANGUAGE O O O O O O O O O O O S-NORP O O O O O O O O O O O
The English queries are translated into Chinese by means of a dictionary-based approach, where we have integrated phrase-based translation with word-by-word translation.	phrase-based translation	dictionary-based approach	usage	{'e1': {'word': 'phrase-based translation', 'word_index': [(6, 9)], 'id': 'H01-1050.16'}, 'e2': {'word': 'dictionary-based approach', 'word_index': [(3, 6)], 'id': 'H01-1050.15'}}	O are translated S-LANGUAGE - based O - based O - by O O S-NORP O O O O O O O O O O O O O O O O O O O O O O O O O
Untranslatable named entities are transliterated by a novel subword translation technique.	novel subword translation technique	Untranslatable named entities	usage	{'e1': {'word': 'novel subword translation technique', 'word_index': [(0, 0)], 'id': 'H01-1050.19'}, 'e2': {'word': 'Untranslatable named entities', 'word_index': [(0, 0)], 'id': 'H01-1050.18'}}	O O O O O O O O O O O O
Experimental results demonstrate that the use of phrase-based translation and subword translation gave performance gains, and multi-scale retrieval outperforms word-based retrieval.	multi-scale retrieval	word-based retrieval	compare	{'e1': {'word': 'multi-scale retrieval', 'word_index': [(3, 3)], 'id': 'H01-1050.26'}, 'e2': {'word': 'word-based retrieval', 'word_index': [(5, 6)], 'id': 'H01-1050.27'}}	O subword translation O outperforms word O O O O O O O O O O O O O O O O O O O O O O O O O
But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English.	features	English past tense	model-feature	{'e1': {'word': 'features', 'word_index': [(3, 3)], 'id': 'W98-1240.10'}, 'e2': {'word': 'English past tense', 'word_index': [(1, 3)], 'id': 'W98-1240.9'}}	O English past O in its O rules with S-LANGUAGE in many O trivial morphological O vestigal nature O within modern O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O S-LANGUAGE O
But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English.	vestigal nature	inflectional morphology	model-feature	{'e1': {'word': 'vestigal nature', 'word_index': [(16, 17)], 'id': 'W98-1240.14'}, 'e2': {'word': 'inflectional morphology', 'word_index': [(18, 18)], 'id': 'W98-1240.15'}}	O English past O in its O rules with S-LANGUAGE in many O trivial morphological O vestigal nature O within modern O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O S-LANGUAGE O
We define, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages.	shallow syntactic analysis	statistical machine translation	usage	{'e1': {'word': 'shallow syntactic analysis', 'word_index': [(9, 9)], 'id': 'W03-1002.2'}, 'e2': {'word': 'statistical machine translation', 'word_index': [(4, 6)], 'id': 'W03-1002.1'}}	O , implement O statistical machine O , which O - speech O chunking ) O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.	machine translation systems	language pairs	usage	{'e1': {'word': 'machine translation systems', 'word_index': [(3, 3)], 'id': 'W08-0309.2'}, 'e2': {'word': 'language pairs', 'word_index': [(5, 6)], 'id': 'W08-0309.3'}}	O translation quality O 10 language O , German O and Spanish O O O O O O O S-CARDINAL O O O O S-NORP O S-LANGUAGE O S-NORP O S-NORP O S-NORP O O S-LANGUAGE O
We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information.	intra- and inter-annotator agreement	manual evaluation methodology	usage	{'e1': {'word': 'intra- and inter-annotator agreement', 'word_index': [(3, 4)], 'id': 'W08-0309.18'}, 'e2': {'word': 'manual evaluation methodology', 'word_index': [(2, 3)], 'id': 'W08-0309.17'}}	O our manual O agreement , O information . O O O O O O O O O O O O O O O O
In this paper, we describe issues in the translation of proper names from English to Chinese which we have faced in constructing a system for multilingual text generation supporting both languages.	translation	proper names	usage	{'e1': {'word': 'translation', 'word_index': [(5, 5)], 'id': 'P98-2220.1'}, 'e2': {'word': 'proper names', 'word_index': [(6, 6)], 'id': 'P98-2220.2'}}	O paper , O the translation O to Chinese O we have O constructing a O . O O O O O O O O S-LANGUAGE O S-NORP O O O O O O O O O O O O O O O O
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.	backward maximum matching	CWS	usage	{'e1': {'word': 'backward maximum matching', 'word_index': [(3, 3)], 'id': 'W06-0119.6'}, 'e2': {'word': 'CWS', 'word_index': [(0, 0)], 'id': 'W06-0119.5'}}	O based on O with word O ( WSM O Chinese unknown O . O O O O O O O O O O O O O O S-NORP O O O O
The Arabic language has far richer systems of inflection and derivation than English which has very little morphology.	systems of inflection and derivation	Arabic language	model-feature	{'e1': {'word': 'systems of inflection and derivation', 'word_index': [(3, 3)], 'id': 'W06-3103.2'}, 'e2': {'word': 'Arabic language', 'word_index': [(0, 0)], 'id': 'W06-3103.1'}}	O has far S-LANGUAGE than English O O O O O O O O O O S-LANGUAGE O O O O O O
Segmentation of inflected Arabic words is a way to smooth its highly morphological nature.	Segmentation	inflected Arabic words	usage	{'e1': {'word': 'Segmentation', 'word_index': [(0, 0)], 'id': 'W06-3103.8'}, 'e2': {'word': 'inflected Arabic words', 'word_index': [(0, 0)], 'id': 'W06-3103.9'}}	O morphological nature O O S-LANGUAGE O O O O O O O O O O O
In this paper, we describe some statistically and linguistically motivated methods for Arabic word segmentation.	statistically and linguistically motivated methods	Arabic word segmentation	usage	{'e1': {'word': 'statistically and linguistically motivated methods', 'word_index': [(3, 6)], 'id': 'W06-3103.11'}, 'e2': {'word': 'Arabic word segmentation', 'word_index': [(6, 7)], 'id': 'W06-3103.12'}}	O paper , O and linguistically O segmentation . O O O O O O O O O O S-LANGUAGE O O O
Then, we show the efficiency of proposed methods on the Arabic-English BTEC and NIST tasks.	proposed methods	Arabic-English BTEC and NIST tasks	usage	{'e1': {'word': 'proposed methods', 'word_index': [(4, 5)], 'id': 'W06-3103.13'}, 'e2': {'word': 'Arabic-English BTEC and NIST tasks', 'word_index': [(7, 9)], 'id': 'W06-3103.14'}}	O the efficiency O proposed methods O Arabic-English BTEC O O O O O O O O O O O S-ORG O O
Developing a high-quality lexicon is often the first step towards building a POS tagger, which is in turn the front-end to many NLP systems.	high-quality lexicon	POS tagger	usage	{'e1': {'word': 'high-quality lexicon', 'word_index': [(0, 0)], 'id': 'W06-1647.4'}, 'e2': {'word': 'POS tagger', 'word_index': [(5, 6)], 'id': 'W06-1647.5'}}	O step towards O a POS O in turn O front - O O O O O S-ORDINAL O O O O O O O O O O O O O O O O O O O O
We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Transductive SVMs, Spectral Graph Transducers, and a novel Transductive Clustering method.	Transductive SVMs	Spectral Graph Transducers	compare	{'e1': {'word': 'Transductive SVMs', 'word_index': [(9, 9)], 'id': 'W06-1647.10'}, 'e2': {'word': 'Spectral Graph Transducers', 'word_index': [(10, 12)], 'id': 'W06-1647.11'}}	O the lexicon O learning problem O comparisons on O Spectral Graph O O O O O O O O O O O O S-CARDINAL O O O O O O O O O O O O O O O O O
We present an API for computing the semantic relatedness of words in Wikipedia.	API	semantic relatedness	usage	{'e1': {'word': 'API', 'word_index': [(2, 2)], 'id': 'P07-2013.1'}, 'e2': {'word': 'semantic relatedness', 'word_index': [(3, 3)], 'id': 'P07-2013.2'}}	O an API O in Wikipedia O S-ORG O O O O O O O O O O
This paper presents a general platform, namely synchronous tree sequence substitution grammar (STSSG), for the grammar comparison study in Translational Equivalence Modeling (TEM) and Statistical Machine Translation (SMT).	synchronous tree sequence substitution grammar (STSSG)	grammar comparison study	usage	{'e1': {'word': 'synchronous tree sequence substitution grammar (STSSG)', 'word_index': [(3, 6)], 'id': 'C08-1138.1'}, 'e2': {'word': 'grammar comparison study', 'word_index': [(6, 7)], 'id': 'C08-1138.2'}}	O general platform O substitution grammar O study in O Equivalence Modeling O ( SMT O O O O O O O O O O O O O O O O O O O O O O O O O O O O O S-ORG O O
Experimental results show that the STSSG is able to better explain the data in parallel corpora than other grammars.	STSSG	other grammars	compare	{'e1': {'word': 'STSSG', 'word_index': [(0, 0)], 'id': 'C08-1138.11'}, 'e2': {'word': 'other grammars', 'word_index': [(3, 3)], 'id': 'C08-1138.13'}}	O data in O O O O O O O O O O O O O O O O O O O
Our study further finds that the complexity of structure divergence is much higher than suggested in literature, which imposes a big challenge to syntactic transformation-based SMT.	structure divergence	syntactic transformation-based SMT	result	{'e1': {'word': 'structure divergence', 'word_index': [(3, 3)], 'id': 'C08-1138.14'}, 'e2': {'word': 'syntactic transformation-based SMT', 'word_index': [(9, 12)], 'id': 'C08-1138.15'}}	O finds that O is much O imposes a O - based O O O O O O O O O O O O O O O O O O O O O O O O S-ORG O
This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms.	unsupervised method	automatic sentence extraction	usage	{'e1': {'word': 'unsupervised method', 'word_index': [(2, 3)], 'id': 'P04-3020.1'}, 'e2': {'word': 'automatic sentence extraction', 'word_index': [(3, 3)], 'id': 'P04-3020.2'}}	O innovative unsupervised O using graph O O O O O O O O O O O O O O O O
We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.	method	text summarization task	usage	{'e1': {'word': 'method', 'word_index': [(2, 2)], 'id': 'P04-3020.4'}, 'e2': {'word': 'text summarization task', 'word_index': [(3, 3)], 'id': 'P04-3020.5'}}	O the method O , and O results obtained O on established O O O O O O O O O O O O O O O O O O O O O O O O O
Preferred antecedents are a subset of the possible antecedents, selected by the application of extralinguistic knowledge.	Preferred antecedents	possible antecedents	part_whole	{'e1': {'word': 'Preferred antecedents', 'word_index': [(0, 0)], 'id': 'C90-2017.7'}, 'e2': {'word': 'possible antecedents', 'word_index': [(0, 0)], 'id': 'C90-2017.8'}}	O , selected O O O O O O O O O O O O O O O O O
This paper presents a syntactic description of a fragment of German that has been worked out within the machine translation project Eurotra.	syntactic description	German	model-feature	{'e1': {'word': 'syntactic description', 'word_index': [(1, 2)], 'id': 'C88-2123.1'}, 'e2': {'word': 'German', 'word_index': [(3, 3)], 'id': 'C88-2123.2'}}	O syntactic description O that has O Eurotra . O O O O O O O S-NORP O O O O O O O O O O S-PERSON O
It represents the syntactic part of the German module of this multilingual translation system.	syntactic part	German module	part_whole	{'e1': {'word': 'syntactic part', 'word_index': [(2, 3)], 'id': 'C88-2123.4'}, 'e2': {'word': 'German module', 'word_index': [(3, 3)], 'id': 'C88-2123.5'}}	O the syntactic O . O O O O O S-NORP O O O O O O O
We present an approach for querying collections of heterogeneous linguistic corpora that are annotated on multiple layers using arbitrary XML-based markup languages.	multiple layers	heterogeneous linguistic corpora	model-feature	{'e1': {'word': 'multiple layers', 'word_index': [(7, 8)], 'id': 'L08-1190.2'}, 'e2': {'word': 'heterogeneous linguistic corpora', 'word_index': [(3, 3)], 'id': 'L08-1190.1'}}	O an approach O that are O multiple layers O O O O O O O O O O O O O O O O O O O O O O
An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion.	OWL ontology	markup languages	usage	{'e1': {'word': 'OWL ontology', 'word_index': [(0, 1)], 'id': 'L08-1190.4'}, 'e2': {'word': 'markup languages', 'word_index': [(6, 6)], 'id': 'L08-1190.5'}}	O ontology provides O on the O querying framework O - based O expansion . O O O O O O O O O O O O O O O O O O O O O O O O O O O
An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion.	ontology-based query expansion	querying framework	usage	{'e1': {'word': 'ontology-based query expansion', 'word_index': [(9, 13)], 'id': 'L08-1190.7'}, 'e2': {'word': 'querying framework', 'word_index': [(7, 8)], 'id': 'L08-1190.6'}}	O ontology provides O on the O querying framework O - based O expansion . O O O O O O O O O O O O O O O O O O O O O O O O O O O
This interface can also be used for ontology-based querying of multiple corpora simultaneously.	ontology-based querying	corpora	usage	{'e1': {'word': 'ontology-based querying', 'word_index': [(3, 3)], 'id': 'L08-1190.12'}, 'e2': {'word': 'corpora', 'word_index': [(3, 3)], 'id': 'L08-1190.13'}}	O be used O . O O O O O O O O O O O O O O
We also evaluate our model on the related role-labelling task, and compare it with a standard role labeller.	model	standard role labeller	compare	{'e1': {'word': 'model', 'word_index': [(3, 3)], 'id': 'E06-1044.5'}, 'e2': {'word': 'standard role labeller', 'word_index': [(8, 10)], 'id': 'E06-1044.7'}}	O evaluate our O O role - O standard role O O O O O O O O O O O O O O O O O O O
For both tasks, our model benefits from class-based smoothing, which allows it to make correct argument-specific predictions despite a severe sparse data problem.	class-based smoothing	model	result	{'e1': {'word': 'class-based smoothing', 'word_index': [(3, 6)], 'id': 'E06-1044.10'}, 'e2': {'word': 'model', 'word_index': [(3, 3)], 'id': 'E06-1044.9'}}	O , our O - based O to make O argument -specific O severe sparse O O O O O O O O O O O O O O O O O O O O O O O O
The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.	sparse data	standard labeller	result	{'e1': {'word': 'sparse data', 'word_index': [(3, 3)], 'id': 'E06-1044.14'}, 'e2': {'word': 'standard labeller', 'word_index': [(0, 0)], 'id': 'E06-1044.13'}}	O suffers from O cues , O O O O O O O O O O O O O O O O O O O
The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.	syntactic cues	prediction task	usage	{'e1': {'word': 'syntactic cues', 'word_index': [(3, 4)], 'id': 'E06-1044.15'}, 'e2': {'word': 'prediction task', 'word_index': [(6, 6)], 'id': 'E06-1044.16'}}	O suffers from O cues , O O O O O O O O O O O O O O O O O O O
In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step.	'Pendulum Algorithm'	word sets	usage	"{'e1': {'word': ""'Pendulum Algorithm'"", 'word_index': [(6, 6)], 'id': 'C04-1178.6'}, 'e2': {'word': 'word sets', 'word_index': [(6, 6)], 'id': 'C04-1178.7'}}"	O bootstrapping fashion O the so O statistics on O error propagation O by a O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step.	co-occurrence statistics	large un-annotated corpus	part_whole	{'e1': {'word': 'co-occurrence statistics', 'word_index': [(6, 7)], 'id': 'C04-1178.8'}, 'e2': {'word': 'large un-annotated corpus', 'word_index': [(9, 9)], 'id': 'C04-1178.9'}}	O bootstrapping fashion O the so O statistics on O error propagation O by a O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
The first algorithm, phone-dependent cepstral compensation, is similar in concept to the previously-described MFCDCN method, except that cepstral compensation vectors are selected according to the current phonetic hypothesis, rather than on the basis of SNR or VQ codeword identity.	phone-dependent cepstral compensation	MFCDCN method	compare	{'e1': {'word': 'phone-dependent cepstral compensation', 'word_index': [(2, 3)], 'id': 'H94-1066.4'}, 'e2': {'word': 'MFCDCN method', 'word_index': [(6, 6)], 'id': 'H94-1066.5'}}	O , phone S-ORDINAL is similar O , except O cepstral compensation O the current O hypothesis , O codeword identity O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Use of the various compensation algorithms in consort produces a reduction of error rates for SPHINX-II by as much as 40 percent relative to the rate achieved with cepstral mean normalization alone, in both development test sets and in the context of the 1993 ARPA CSR evaluations.	compensation algorithms	reduction of error rates	result	{'e1': {'word': 'compensation algorithms', 'word_index': [(2, 3)], 'id': 'H94-1066.12'}, 'e2': {'word': 'reduction of error rates', 'word_index': [(6, 8)], 'id': 'H94-1066.13'}}	O various compensation O consort produces O error rates O as 40 O in both O ARPA CSR O O O O O O O O O O O O O O O O S-CARDINAL E-PERCENT O O O O O O O O O O O O O O O O O O O O O O S-DATE S-ORG O O O
One of the critical components of the CLT is a speech recognition system which is used to track the child's progress during oral reading and to provide sufficient information to detect reading miscues.	reading miscues	oral reading	part_whole	{'e1': {'word': 'reading miscues', 'word_index': [(12, 12)], 'id': 'C04-1182.7'}, 'e2': {'word': 'oral reading', 'word_index': [(11, 12)], 'id': 'C04-1182.6'}}	S-CARDINAL critical components O CLT is O system which O during oral O . O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
In this paper, we extend on prior work by examining a novel labeling of children's oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors.	labeling	oral reading audio data	model-feature	{'e1': {'word': 'labeling', 'word_index': [(6, 6)], 'id': 'C04-1182.8'}, 'e2': {'word': 'oral reading audio data', 'word_index': [(9, 12)], 'id': 'C04-1182.9'}}	O paper , O work by O children 's O reading audio O order to O most significantly O recognition errors O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O
Next, we consider the problem of detecting miscues during oral reading.	miscues	oral reading	part_whole	{'e1': {'word': 'miscues', 'word_index': [(5, 5)], 'id': 'C04-1182.14'}, 'e2': {'word': 'oral reading', 'word_index': [(6, 7)], 'id': 'C04-1182.15'}}	O the problem O detecting miscues O reading . O O O O O O O O O O
