original_sentence	e1	e2	relation_type	metadata	preprocessed_sentence
In Semantic Role Labeling (SRL), arguments are usually limited in a syntax subtree.	arguments	syntax subtree	part_whole	{'e1': {'word': 'arguments', 'word_index': [(3, 3)], 'id': 'C08-1105.2'}, 'e2': {'word': 'syntax subtree', 'word_index': [(9, 9)], 'id': 'C08-1105.3'}}	In ENTITYUNRELATED , ENTITY are usually limited in a ENTITYOTHER .
It is reasonable to label arguments locally in such a sub-tree rather than a whole tree.	arguments	sub-tree	part_whole	{'e1': {'word': 'arguments', 'word_index': [(5, 5)], 'id': 'C08-1105.4'}, 'e2': {'word': 'sub-tree', 'word_index': [(10, 10)], 'id': 'C08-1105.5'}}	It is reasonable to label ENTITY locally in such a ENTITYOTHER rather than a ENTITYUNRELATED .
The anchor group approach achieves an accuracy of 87.75% and the single anchor approach achieves 83.63%.	anchor group approach	accuracy	result	{'e1': {'word': 'anchor group approach', 'word_index': [(1, 1)], 'id': 'C08-1105.15'}, 'e2': {'word': 'accuracy', 'word_index': [(4, 4)], 'id': 'C08-1105.16'}}	The ENTITY achieves an ENTITYOTHER of 87.75 % and the ENTITYUNRELATED achieves 83.63 %.
Experimental results also indicate that the prediction of MP improves semantic role labeling.	prediction of MP	semantic role labeling	result	{'e1': {'word': 'prediction of MP', 'word_index': [(6, 6)], 'id': 'C08-1105.18'}, 'e2': {'word': 'semantic role labeling', 'word_index': [(8, 8)], 'id': 'C08-1105.19'}}	Experimental results also indicate that the ENTITY improves ENTITYOTHER .
Recently the LATL has undertaken the development of a multilingual translation system based on a symbolic parsing technology and on a transfer-based translation model.	symbolic parsing technology	multilingual translation system	usage	{'e1': {'word': 'symbolic parsing technology', 'word_index': [(13, 13)], 'id': 'L08-1579.2'}, 'e2': {'word': 'multilingual translation system', 'word_index': [(9, 9)], 'id': 'L08-1579.1'}}	Recently the LATL has undertaken the development of a ENTITYOTHER based on a ENTITY and on a ENTITYUNRELATED .
This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning.	term weight learning	verbal word senses	usage	{'e1': {'word': 'term weight learning', 'word_index': [(10, 10)], 'id': 'E99-1028.2'}, 'e2': {'word': 'verbal word senses', 'word_index': [(8, 8)], 'id': 'E99-1028.1'}}	This paper describes unsupervised learning algorithm for disambiguating ENTITYOTHER using ENTITY .
This is quite different from current approaches to semantic parsing or chunking that depend on full statistical syntactic parsers that require tree bank style annotation.	statistical syntactic parsers	chunking	usage	{'e1': {'word': 'statistical syntactic parsers', 'word_index': [(15, 15)], 'id': 'N04-4037.10'}, 'e2': {'word': 'chunking', 'word_index': [(10, 10)], 'id': 'N04-4037.9'}}	This is quite different from current approaches to ENTITYUNRELATED or ENTITYOTHER that depend on full ENTITY that require ENTITYUNRELATED style annotation .
We compare it with a recently proposed word-by-word semantic chunker and present results that show that the phrase-by-phrase approach performs better than its word-by-word counterpart.	phrase-by-phrase approach	word-by-word counterpart	compare	{'e1': {'word': 'phrase-by-phrase approach', 'word_index': [(15, 15)], 'id': 'N04-4037.13'}, 'e2': {'word': 'word-by-word counterpart', 'word_index': [(20, 20)], 'id': 'N04-4037.14'}}	We compare it with a recently proposed ENTITYUNRELATED and present results that show that the ENTITY performs better than its ENTITYOTHER .
The primary objective of this basic research is to develop improved methods and models for acoustic recognition of continuous speech.	acoustic recognition	continuous speech	usage	{'e1': {'word': 'acoustic recognition', 'word_index': [(15, 15)], 'id': 'H91-1080.1'}, 'e2': {'word': 'continuous speech', 'word_index': [(17, 17)], 'id': 'H91-1080.2'}}	The primary objective of this basic research is to develop improved methods and models for ENTITY of ENTITYOTHER .
The work has focussed on developing accurate and detailed mathematical models of phonemes and their coarticulation for the purpose of large-vocabulary continuous speech recognition.	phonemes	large-vocabulary continuous speech recognition	usage	{'e1': {'word': 'phonemes', 'word_index': [(12, 12)], 'id': 'H91-1080.3'}, 'e2': {'word': 'large-vocabulary continuous speech recognition', 'word_index': [(20, 20)], 'id': 'H91-1080.4'}}	The work has focussed on developing accurate and detailed mathematical models of ENTITY and their coarticulation for the purpose of ENTITYOTHER .
KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus.	word list	corpus	part_whole	{'e1': {'word': 'word list', 'word_index': [(3, 3)], 'id': 'W00-0902.5'}, 'e2': {'word': 'corpus', 'word_index': [(13, 13)], 'id': 'W00-0902.6'}}	ENTITYUNRELATED compares a ENTITY extracted from what has been called ' the study ENTITYOTHER ' ( the ENTITYUNRELATED which the researcher is interested in describing ) with a ENTITYUNRELATED made from a ENTITYUNRELATED .
KeyWords compares a word list extracted from what has been called 'the study corpus' (the corpus which the researcher is interested in describing) with a word list made from a reference corpus.	word list	reference corpus	part_whole	{'e1': {'word': 'word list', 'word_index': [(28, 28)], 'id': 'W00-0902.8'}, 'e2': {'word': 'reference corpus', 'word_index': [(32, 32)], 'id': 'W00-0902.9'}}	ENTITYUNRELATED compares a ENTITYUNRELATED extracted from what has been called ' the study ENTITYUNRELATED ' ( the ENTITYUNRELATED which the researcher is interested in describing ) with a ENTITY made from a ENTITYOTHER .
Five English corpora were compared to reference corpora of various sizes (varying from two to 100 times larger than the study corpus).	English corpora	reference corpora	compare	{'e1': {'word': 'English corpora', 'word_index': [(1, 1)], 'id': 'W00-0902.15'}, 'e2': {'word': 'reference corpora', 'word_index': [(5, 5)], 'id': 'W00-0902.16'}}	Five ENTITY were compared to ENTITYOTHER of various sizes ( varying from two to 100 times larger than the study ENTITYUNRELATED ) .
The results indicate that a reference corpus that is five times as large as the study corpus yielded a larger number of keywords than a smaller reference corpus.	keywords	corpus	part_whole	{'e1': {'word': 'keywords', 'word_index': [(21, 21)], 'id': 'W00-0902.20'}, 'e2': {'word': 'corpus', 'word_index': [(15, 15)], 'id': 'W00-0902.19'}}	The results indicate that a ENTITYUNRELATED that is five times as large as the study ENTITYOTHER yielded a larger number of ENTITY than a smaller ENTITYUNRELATED .
The implication is that a larger reference corpus is not always better than a smaller one, for WordSmith Tools Keywords analysis, while a reference corpus that is less than five times the size of the study corpus may not be reliable.	reference corpus	study corpus	compare	{'e1': {'word': 'reference corpus', 'word_index': [(22, 22)], 'id': 'W00-0902.28'}, 'e2': {'word': 'study corpus', 'word_index': [(33, 33)], 'id': 'W00-0902.29'}}	The implication is that a larger ENTITYUNRELATED is not always better than a smaller one , for ENTITYUNRELATED ENTITYUNRELATED , while a ENTITY that is less than five times the size of the ENTITYOTHER may not be reliable .
In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a non-lexicalised and a lexicalised mode.	dependency analyser	Italian	usage	{'e1': {'word': 'dependency analyser', 'word_index': [(13, 13)], 'id': 'W02-1501.1'}, 'e2': {'word': 'Italian', 'word_index': [(15, 15)], 'id': 'W02-1501.2'}}	In the paper we report a qualitative evaluation of the performance of a ENTITY of ENTITYOTHER that runs in both a ENTITYUNRELATED and a ENTITYUNRELATED mode .
Results shed light on the contribution of types of lexical information to parsing.	lexical information	parsing	usage	{'e1': {'word': 'lexical information', 'word_index': [(9, 9)], 'id': 'W02-1501.5'}, 'e2': {'word': 'parsing', 'word_index': [(11, 11)], 'id': 'W02-1501.6'}}	Results shed light on the contribution of types of ENTITY to ENTITYOTHER .
In international phonology and speech synthesis research, it has been suggested that the relative informativeness of a word can be used to predict pitch prominence.	relative informativeness	word	model-feature	{'e1': {'word': 'relative informativeness', 'word_index': [(12, 12)], 'id': 'W99-0619.3'}, 'e2': {'word': 'word', 'word_index': [(15, 15)], 'id': 'W99-0619.4'}}	In ENTITYUNRELATED and ENTITYUNRELATED research , it has been suggested that the ENTITY of a ENTITYOTHER can be used to predict ENTITYUNRELATED .
In this paper, we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted measures of informativeness.	measures	informativeness	model-feature	{'e1': {'word': 'measures', 'word_index': [(22, 22)], 'id': 'W99-0619.7'}, 'e2': {'word': 'informativeness', 'word_index': [(24, 24)], 'id': 'W99-0619.8'}}	In this paper , we provide some empirical evidence to support the existence of such a correlation by employing two widely accepted ENTITY of ENTITYOTHER .
Our experiments show that there is a positive correlation between the informativeness of a word and its pitch accent assignment.	informativeness	word	model-feature	{'e1': {'word': 'informativeness', 'word_index': [(11, 11)], 'id': 'W99-0619.9'}, 'e2': {'word': 'word', 'word_index': [(14, 14)], 'id': 'W99-0619.10'}}	Our experiments show that there is a positive correlation between the ENTITY of a ENTITYOTHER and its ENTITYUNRELATED .
The computation of word informativeness is inexpensive and can be incorporated into speech synthesis systems easily.	word informativeness	speech synthesis systems	usage	{'e1': {'word': 'word informativeness', 'word_index': [(3, 3)], 'id': 'W99-0619.14'}, 'e2': {'word': 'speech synthesis systems', 'word_index': [(11, 11)], 'id': 'W99-0619.15'}}	The computation of ENTITY is inexpensive and can be incorporated into ENTITYOTHER easily .
"""An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed."	parsing algorithm	augmented context-free grammars	usage	{'e1': {'word': 'parsing algorithm', 'word_index': [(3, 3)], 'id': 'J87-1004.1'}, 'e2': {'word': 'augmented context-free grammars', 'word_index': [(5, 5)], 'id': 'J87-1004.2'}}	""" An efficient ENTITY for ENTITYOTHER is introduced , and its application to ENTITYUNRELATED discussed ."
The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar.	generalized LR parsing algorithm	LR shift-reduce parsing table	usage	{'e1': {'word': 'generalized LR parsing algorithm', 'word_index': [(4, 4)], 'id': 'J87-1004.4'}, 'e2': {'word': 'LR shift-reduce parsing table', 'word_index': [(9, 9)], 'id': 'J87-1004.5'}}	The algorithm is a ENTITY , which precomputes an ENTITYOTHER ( possibly with multiple entries ) from a given ENTITYUNRELATED .
We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables.	LR parsing tables	chart parsing algorithm	usage	{'e1': {'word': 'LR parsing tables', 'word_index': [(14, 14)], 'id': 'J87-1004.14'}, 'e2': {'word': 'chart parsing algorithm', 'word_index': [(10, 10)], 'id': 'J87-1004.13'}}	We can also view our parsing algorithm as an extended ENTITYOTHER efficiently guided by ENTITY .
"Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, based on the technique developed at CMU."""	on-line parser	Japanese language	usage	{'e1': {'word': 'on-line parser', 'word_index': [(4, 4)], 'id': 'J87-1004.21'}, 'e2': {'word': 'Japanese language', 'word_index': [(6, 6)], 'id': 'J87-1004.22'}}	"Also , a commercial ENTITY for ENTITYOTHER is being built by Intelligent Technology Incorporation , based on the technique developed at CMU . """
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).	hybrid statistical and structural semantic model	multi-stage spoken language understanding (SLU)	usage	{'e1': {'word': 'hybrid statistical and structural semantic model', 'word_index': [(4, 4)], 'id': 'W04-3002.1'}, 'e2': {'word': 'multi-stage spoken language understanding (SLU)', 'word_index': [(6, 6)], 'id': 'W04-3002.2'}}	This article proposes a ENTITY for ENTITYOTHER .
The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted.	weighted finite-state transducer (WFST)-based parser	SLU	usage	{'e1': {'word': 'weighted finite-state transducer (WFST)-based parser', 'word_index': [(8, 8)], 'id': 'W04-3002.4'}, 'e2': {'word': 'SLU', 'word_index': [(5, 5)], 'id': 'W04-3002.3'}}	The first stage of this ENTITYOTHER utilizes a ENTITY , which encodes the ENTITYUNRELATED of concepts to be extracted .
The proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger.	n-gram semantic tagger	regular grammar	usage	{'e1': {'word': 'n-gram semantic tagger', 'word_index': [(13, 13)], 'id': 'W04-3002.7'}, 'e2': {'word': 'regular grammar', 'word_index': [(5, 5)], 'id': 'W04-3002.6'}}	The proposed method improves the ENTITYOTHER model by incorporating a well - known ENTITY .
This paper presents a corpus-based account of structural priming in human sentence processing, focusing on the role that syntactic representations play in such an account.	structural priming	sentence processing	model-feature	{'e1': {'word': 'structural priming', 'word_index': [(7, 7)], 'id': 'W06-1637.2'}, 'e2': {'word': 'sentence processing', 'word_index': [(10, 10)], 'id': 'W06-1637.3'}}	This paper presents a ENTITYUNRELATED account of ENTITY in human ENTITYOTHER , focusing on the role that ENTITYUNRELATED play in such an account .
We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue, annotated syntactically with Combinatory Categorial Grammar (CCG) derivations.	spontaneous spoken dialogue	corpus	part_whole	{'e1': {'word': 'spontaneous spoken dialogue', 'word_index': [(10, 10)], 'id': 'W06-1637.7'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'W06-1637.6'}}	We estimate the strength of ENTITYUNRELATED from a ENTITYOTHER of ENTITY , annotated syntactically with ENTITYUNRELATED derivations .
In particular, we present evidence for priming between lexical and syntactic categories encoding partially satisfied sub-categorization frames, and we show that priming effects exist both for incremental and normal-form CCG derivations.	priming effects	incremental and normal-form CCG derivations	model-feature	{'e1': {'word': 'priming effects', 'word_index': [(20, 20)], 'id': 'W06-1637.13'}, 'e2': {'word': 'incremental and normal-form CCG derivations', 'word_index': [(24, 24)], 'id': 'W06-1637.14'}}	In particular , we present evidence for ENTITYUNRELATED between ENTITYUNRELATED encoding partially satisfied sub-categorization frames , and we show that ENTITY exist both for ENTITYOTHER .
This paper describes the application of an ensemble of indexing and classification systems, which have been shown to be successful in information retrieval and classification of medical literature, to a new task of assigning ICD-9-CM codes to the clinical history and impression sections of radiology reports.	information retrieval	medical literature	usage	{'e1': {'word': 'information retrieval', 'word_index': [(22, 22)], 'id': 'W07-1014.1'}, 'e2': {'word': 'medical literature', 'word_index': [(26, 26)], 'id': 'W07-1014.2'}}	This paper describes the application of an ensemble of indexing and classification systems , which have been shown to be successful in ENTITY and classification of ENTITYOTHER , to a new task of assigning ENTITYUNRELATED to the ENTITYUNRELATED and impression sections of radiology ENTITYUNRELATED .
This type of summary could serve as an effective navigation tool for accessing information in long texts, such as books.	summary	texts	model-feature	{'e1': {'word': 'summary', 'word_index': [(3, 3)], 'id': 'P07-1069.2'}, 'e2': {'word': 'texts', 'word_index': [(16, 16)], 'id': 'P07-1069.4'}}	This type of ENTITY could serve as an effective navigation tool for accessing ENTITYUNRELATED in long ENTITYOTHER , such as ENTITYUNRELATED .
To generate a coherent table-of-contents, we need to capture both global dependencies across different titles in the table and local constraints within sections.	titles	table-of-contents	usage	{'e1': {'word': 'titles', 'word_index': [(14, 14)], 'id': 'P07-1069.8'}, 'e2': {'word': 'table-of-contents', 'word_index': [(4, 4)], 'id': 'P07-1069.6'}}	To generate a coherent ENTITYOTHER , we need to capture both ENTITYUNRELATED across different ENTITY in the table and local constraints within sections .
First, we compare cruiser with a baseline system-initiative DS, and show that users prefer cruiser.	cruiser	system-initiative DS	compare	{'e1': {'word': 'cruiser', 'word_index': [(4, 4)], 'id': 'P08-1055.7'}, 'e2': {'word': 'system-initiative DS', 'word_index': [(8, 8)], 'id': 'P08-1055.8'}}	First , we compare ENTITY with a baseline ENTITYOTHER , and show that users prefer ENTITYUNRELATED .
To improve the Mandarin large vocabulary continuous speech recognition (LVCSR), a unified framework based approach is introduced to exploit multi-level linguistic knowledge.	multi-level linguistic knowledge	large vocabulary continuous speech recognition (LVCSR)	usage	{'e1': {'word': 'multi-level linguistic knowledge', 'word_index': [(15, 15)], 'id': 'D08-1086.3'}, 'e2': {'word': 'large vocabulary continuous speech recognition (LVCSR)', 'word_index': [(4, 4)], 'id': 'D08-1086.2'}}	To improve the ENTITYUNRELATED ENTITYOTHER , a unified framework based approach is introduced to exploit ENTITY .
In this framework, each knowledge source is represented by a Weighted Finite State Transducer (WFST), and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources.	Weighted Finite State Transducer (WFST)	knowledge source	model-feature	{'e1': {'word': 'Weighted Finite State Transducer (WFST)', 'word_index': [(10, 10)], 'id': 'D08-1086.5'}, 'e2': {'word': 'knowledge source', 'word_index': [(5, 5)], 'id': 'D08-1086.4'}}	In this framework , each ENTITYOTHER is represented by a ENTITY , and then they are combined to obtain a so-called analyzer for integrating ENTITYUNRELATED .
Due to the uniform transducer representation, any knowledge source can be easily integrated into the analyzer, as long as it can be encoded into WFSTs.	knowledge source	WFSTs	usage	{'e1': {'word': 'knowledge source', 'word_index': [(8, 8)], 'id': 'D08-1086.8'}, 'e2': {'word': 'WFSTs', 'word_index': [(25, 25)], 'id': 'D08-1086.9'}}	Due to the uniform ENTITYUNRELATED representation , any ENTITY can be easily integrated into the analyzer , as long as it can be encoded into ENTITYOTHER .
In this paper we discuss algorithms for clustering words into classes from unlabeled text using unsupervised algorithms, based on distributional and morphological information.	words	unlabeled text	part_whole	{'e1': {'word': 'words', 'word_index': [(8, 8)], 'id': 'E03-1009.1'}, 'e2': {'word': 'unlabeled text', 'word_index': [(12, 12)], 'id': 'E03-1009.2'}}	In this paper we discuss algorithms for clustering ENTITY into classes from ENTITYOTHER using unsupervised algorithms , based on ENTITYUNRELATED .
"""This paper presents an unsupervised relation extraction algorithm, which induces relations between entity pairs by grouping them into a ""natural"" number of clusters based on the similarity of their contexts."	unsupervised relation extraction algorithm	entity pairs	usage	{'e1': {'word': 'unsupervised relation extraction algorithm', 'word_index': [(5, 5)], 'id': 'I05-2045.2'}, 'e2': {'word': 'entity pairs', 'word_index': [(11, 11)], 'id': 'I05-2045.4'}}	""" This paper presents an ENTITY , which induces ENTITYUNRELATED between ENTITYOTHER by grouping them into a "" natural "" number of clusters based on the similarity of their ENTITYUNRELATED ."
This paper presents a method that measures the similarity between compound nouns in different languages to locate translation equivalents from corpora.	translation equivalents	corpora	part_whole	{'e1': {'word': 'translation equivalents', 'word_index': [(16, 16)], 'id': 'C02-1065.5'}, 'e2': {'word': 'corpora', 'word_index': [(18, 18)], 'id': 'C02-1065.6'}}	This paper presents a method that measures the similarity between ENTITYUNRELATED in different languages to locate ENTITY from ENTITYOTHER .
The method uses information from unrelated corpora in different languages that do not have to be parallel.	corpora	languages	model-feature	{'e1': {'word': 'corpora', 'word_index': [(6, 6)], 'id': 'C02-1065.7'}, 'e2': {'word': 'languages', 'word_index': [(9, 9)], 'id': 'C02-1065.8'}}	The method uses information from unrelated ENTITY in different ENTITYOTHER that do not have to be parallel .
The method compares the contexts of target compound nouns and translation candidates in the word or semantic attribute level.	contexts	translation candidates	compare	{'e1': {'word': 'contexts', 'word_index': [(4, 4)], 'id': 'C02-1065.10'}, 'e2': {'word': 'translation candidates', 'word_index': [(9, 9)], 'id': 'C02-1065.12'}}	The method compares the ENTITY of target ENTITYUNRELATED and ENTITYOTHER in the ENTITYUNRELATED or ENTITYUNRELATED level .
In this paper, we show how this measuring method can be applied to select the best English translation candidate for Japanese compound nouns in more than 70% of the cases.	English translation	Japanese compound nouns	usage	{'e1': {'word': 'English translation', 'word_index': [(17, 17)], 'id': 'C02-1065.15'}, 'e2': {'word': 'Japanese compound nouns', 'word_index': [(20, 20)], 'id': 'C02-1065.16'}}	In this paper , we show how this measuring method can be applied to select the best ENTITY candidate for ENTITYOTHER in more than 70 % of the cases .
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text	text mining	statistical Arabic NLP	part_whole	{'e1': {'word': 'text mining', 'word_index': [(7, 7)], 'id': 'L08-1611.2'}, 'e2': {'word': 'statistical Arabic NLP', 'word_index': [(2, 2)], 'id': 'L08-1611.1'}}	Applications of ENTITYOTHER in general , and ENTITY in specific , along with the tools underneath perform much better as the ENTITYUNRELATED operates on deeper ENTITYUNRELATED than on ENTITYUNRELATED
Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorization(s) than on raw text	statistical processing	language factorization(s)	usage	{'e1': {'word': 'statistical processing', 'word_index': [(21, 21)], 'id': 'L08-1611.3'}, 'e2': {'word': 'language factorization(s)', 'word_index': [(25, 25)], 'id': 'L08-1611.4'}}	Applications of ENTITYUNRELATED in general , and ENTITYUNRELATED in specific , along with the tools underneath perform much better as the ENTITY operates on deeper ENTITYOTHER than on ENTITYUNRELATED
While building this LR, we had to go beyond the conventional exclusive collection of words from dictionaries and thesauri that cannot alone produce a satisfactory coverage of this highly inflective and derivative language.	words	dictionaries	part_whole	{'e1': {'word': 'words', 'word_index': [(15, 15)], 'id': 'L08-1611.9'}, 'e2': {'word': 'dictionaries', 'word_index': [(17, 17)], 'id': 'L08-1611.10'}}	While building this ENTITYUNRELATED , we had to go beyond the conventional exclusive collection of ENTITY from ENTITYOTHER and ENTITYUNRELATED that cannot alone produce a satisfactory coverage of this highly ENTITYUNRELATED .
With the aid of the same large-scale Arabic morphological analyzer and PoS tagger in the runtime, the possible senses of virtually any given Arabic word are retrievable.	PoS tagger	Arabic word	usage	{'e1': {'word': 'PoS tagger', 'word_index': [(8, 8)], 'id': 'L08-1611.21'}, 'e2': {'word': 'Arabic word', 'word_index': [(20, 20)], 'id': 'L08-1611.22'}}	With the aid of the same ENTITYUNRELATED and ENTITY in the runtime , the possible senses of virtually any given ENTITYOTHER are retrievable .
Similarly to the well-established ROVER approach of ( Fiscus, 1997 ) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network.	confusion network	consensus translation	usage	{'e1': {'word': 'confusion network', 'word_index': [(25, 25)], 'id': 'E06-1005.7'}, 'e2': {'word': 'consensus translation', 'word_index': [(18, 18)], 'id': 'E06-1005.6'}}	Similarly to the well - established ENTITYUNRELATED of ( Fiscus , 1997 ) for combining ENTITYUNRELATED , the ENTITYOTHER is computed by voting on a ENTITY .
To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.	statistical alignment algorithm	word alignments	usage	{'e1': {'word': 'statistical alignment algorithm', 'word_index': [(16, 16)], 'id': 'E06-1005.11'}, 'e2': {'word': 'word alignments', 'word_index': [(8, 8)], 'id': 'E06-1005.9'}}	To create the ENTITYUNRELATED , we produce pairwise ENTITYOTHER of the original ENTITYUNRELATED with an enhanced ENTITY that explicitly models ENTITYUNRELATED .
The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.	document	alignment	usage	{'e1': {'word': 'document', 'word_index': [(5, 5)], 'id': 'E06-1005.13'}, 'e2': {'word': 'alignment', 'word_index': [(20, 20)], 'id': 'E06-1005.16'}}	The context of a whole ENTITY of ENTITYUNRELATED rather than a single ENTITYUNRELATED is taken into account to produce the ENTITYOTHER .
The method was also tested in the framework of multi-source and speech translation.	method	multi-source and speech translation	usage	{'e1': {'word': 'method', 'word_index': [(1, 1)], 'id': 'E06-1005.20'}, 'e2': {'word': 'multi-source and speech translation', 'word_index': [(9, 9)], 'id': 'E06-1005.21'}}	The ENTITY was also tested in the framework of ENTITYOTHER .
We present a two stage parser that recovers Penn Treebank style syntactic analyses of new sentences including skeletal syntactic structure, and, for the first time, both function tags and empty categories.	syntactic analyses	sentences	usage	{'e1': {'word': 'syntactic analyses', 'word_index': [(10, 10)], 'id': 'N06-1024.3'}, 'e2': {'word': 'sentences', 'word_index': [(13, 13)], 'id': 'N06-1024.4'}}	We present a two stage ENTITYUNRELATED that recovers ENTITYUNRELATED style ENTITY of new ENTITYOTHER including skeletal ENTITYUNRELATED , and , for the first time , both ENTITYUNRELATED and ENTITYUNRELATED .
The accuracy of the first-stage parser on the standard Parseval metric matches that of the ( Collins, 2003 ) parser on which it is based, despite the data fragmentation caused by the greatly enriched space of possible node labels.	parser	parser	compare	{'e1': {'word': 'parser', 'word_index': [(7, 7)], 'id': 'N06-1024.9'}, 'e2': {'word': 'parser', 'word_index': [(21, 21)], 'id': 'N06-1024.11'}}	The ENTITYUNRELATED of the first - stage ENTITY on the standard ENTITYUNRELATED matches that of the ( Collins , 2003 ) ENTITYOTHER on which it is based , despite the ENTITYUNRELATED caused by the greatly enriched space of possible node labels .
Both anaphora resolution and prepositional phrase (PP) attachment are the most frequent ambiguities in natural language processing.	ambiguities	natural language processing	model-feature	{'e1': {'word': 'ambiguities', 'word_index': [(8, 8)], 'id': 'E95-1041.3'}, 'e2': {'word': 'natural language processing', 'word_index': [(10, 10)], 'id': 'E95-1041.4'}}	Both ENTITYUNRELATED and ENTITYUNRELATED are the most frequent ENTITY in ENTITYOTHER .
Just as with speaker adaptation in speaker-independent system, two vocabulary adaptation algorithms [5] are implemented in order to tailor the VI subword models to the target vocabulary.	vocabulary adaptation algorithms	VI subword models	usage	{'e1': {'word': 'vocabulary adaptation algorithms', 'word_index': [(8, 8)], 'id': 'H92-1033.4'}, 'e2': {'word': 'VI subword models', 'word_index': [(19, 19)], 'id': 'H92-1033.5'}}	Just as with ENTITYUNRELATED in ENTITYUNRELATED , two ENTITY [ 5 ] are implemented in order to tailor the ENTITYOTHER to the ENTITYUNRELATED .
Over the past 9 years, the Applied Science and Engineering Laboratories (ASEL) at the University of Delaware and the duPont Hospital for Children, has been involved with applying natural language processing (NLP) technologies to the field of AAC.	natural language processing (NLP) technologies	AAC	usage	{'e1': {'word': 'natural language processing (NLP) technologies', 'word_index': [(32, 32)], 'id': 'W97-0503.2'}, 'e2': {'word': 'AAC', 'word_index': [(37, 37)], 'id': 'W97-0503.3'}}	Over the past 9 years , the Applied Science and Engineering Laboratories ( ASEL ) at the University of Delaware and the duPont Hospital for Children , has been involved with applying ENTITY to the field of ENTITYOTHER .
One of the major projects at ASEL (The COMPAN-SION project) has been concerned with the application of primarily lexical semantics and sentence generation technology to expand telegraphic input into full sentences.	sentence generation technology	telegraphic input	usage	{'e1': {'word': 'sentence generation technology', 'word_index': [(24, 24)], 'id': 'W97-0503.5'}, 'e2': {'word': 'telegraphic input', 'word_index': [(27, 27)], 'id': 'W97-0503.6'}}	One of the major projects at ASEL ( The COMPAN - SION project ) has been concerned with the application of primarily ENTITYUNRELATED and ENTITY to expand ENTITYOTHER into full ENTITYUNRELATED .
We view the entire problem as series of classification problems and employ memory-based learning (MBL) to resolve them.	memory-based learning (MBL)	classification problems	usage	{'e1': {'word': 'memory-based learning (MBL)', 'word_index': [(11, 11)], 'id': 'W00-1210.3'}, 'e2': {'word': 'classification problems', 'word_index': [(8, 8)], 'id': 'W00-1210.2'}}	We view the entire problem as series of ENTITYOTHER and employ ENTITY to resolve them .
The problem of word segmentation affects all aspects of Chinese language processing, including the development of text-to-speech synthesis systems.	word segmentation	Chinese language processing	part_whole	{'e1': {'word': 'word segmentation', 'word_index': [(3, 3)], 'id': 'W02-1813.1'}, 'e2': {'word': 'Chinese language processing', 'word_index': [(8, 8)], 'id': 'W02-1813.2'}}	The problem of ENTITY affects all aspects of ENTITYOTHER , including the development of ENTITYUNRELATED .
This paper treats the classification of the semantic functions performed by adnominal constituents in Japanese, where many parts of speech act as adnominal constituents.	adnominal constituents	Japanese	part_whole	{'e1': {'word': 'adnominal constituents', 'word_index': [(10, 10)], 'id': 'W00-0110.3'}, 'e2': {'word': 'Japanese', 'word_index': [(12, 12)], 'id': 'W00-0110.4'}}	This paper treats the ENTITYUNRELATED of the ENTITYUNRELATED performed by ENTITY in ENTITYOTHER , where many ENTITYUNRELATED act as ENTITYUNRELATED .
"adjectives and ""noun + NO"" (in English ""of + noun"") structures, which have a broad range of semantic functions, are discussed."	semantic functions	"""noun + NO"" (in English ""of + noun"") structures"	model-feature	"{'e1': {'word': 'semantic functions', 'word_index': [(10, 10)], 'id': 'W00-0110.11'}, 'e2': {'word': '""noun + NO"" (in English ""of + noun"") structures', 'word_index': [(2, 2)], 'id': 'W00-0110.10'}}"	ENTITYUNRELATED and ENTITYOTHER , which have a broad range of ENTITY , are discussed .
The feasibility of this was verified with a self-organizing semantic map based on a neural network model.	neural network model	self-organizing semantic map	usage	{'e1': {'word': 'neural network model', 'word_index': [(12, 12)], 'id': 'W00-0110.15'}, 'e2': {'word': 'self-organizing semantic map', 'word_index': [(8, 8)], 'id': 'W00-0110.14'}}	The feasibility of this was verified with a ENTITYOTHER based on a ENTITY .
Recent corpus-based work on word sense disambiguation explores the application of statistical pattern recognition procedures to lexical co-occurrence data from very large text databases.	statistical pattern recognition procedures	lexical co-occurrence data	usage	{'e1': {'word': 'statistical pattern recognition procedures', 'word_index': [(11, 11)], 'id': 'J95-1001.2'}, 'e2': {'word': 'lexical co-occurrence data', 'word_index': [(13, 13)], 'id': 'J95-1001.3'}}	Recent corpus - based work on ENTITYUNRELATED explores the application of ENTITY to ENTITYOTHER from ENTITYUNRELATED .
Statistical methods play a definite role in this work, helping to organize and analyze data, but the disambiguation method itself does not employ statistical data or decision criteria.	statistical data	disambiguation method	usage	{'e1': {'word': 'statistical data', 'word_index': [(23, 23)], 'id': 'J95-1001.12'}, 'e2': {'word': 'disambiguation method', 'word_index': [(18, 18)], 'id': 'J95-1001.11'}}	ENTITYUNRELATED play a definite role in this work , helping to organize and analyze data , but the ENTITYOTHER itself does not employ ENTITY or ENTITYUNRELATED .
The approach is illustrated by an experiment discriminating among the senses of adjectives, which have been relatively neglected in work on sense disambiguation.	senses	adjectives	model-feature	{'e1': {'word': 'senses', 'word_index': [(10, 10)], 'id': 'J95-1001.17'}, 'e2': {'word': 'adjectives', 'word_index': [(12, 12)], 'id': 'J95-1001.18'}}	The approach is illustrated by an experiment discriminating among the ENTITY of ENTITYOTHER , which have been relatively neglected in work on ENTITYUNRELATED .
In particular, the paper assesses the potential of nouns for discriminating among the senses of adjectives that modify them.	senses	adjectives	model-feature	{'e1': {'word': 'senses', 'word_index': [(14, 14)], 'id': 'J95-1001.21'}, 'e2': {'word': 'adjectives', 'word_index': [(16, 16)], 'id': 'J95-1001.22'}}	In particular , the paper assesses the potential of ENTITYUNRELATED for discriminating among the ENTITY of ENTITYOTHER that modify them .
This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: and About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur.	ambiguous adjectives	English	part_whole	{'e1': {'word': 'ambiguous adjectives', 'word_index': [(14, 14)], 'id': 'J95-1001.23'}, 'e2': {'word': 'English', 'word_index': [(16, 16)], 'id': 'J95-1001.24'}}	This assessment is based on an empirical study of five of the most frequent ENTITY in ENTITYOTHER : and About three - quarters of all instances of these ENTITYUNRELATED can be disambiguated almost errorlessly by the ENTITYUNRELATED they modify or by the ENTITYUNRELATED in which they occur .
This assessment is based on an empirical study of five of the most frequent ambiguous adjectives in English: and About three-quarters of all instances of these adjectives can be disambiguated almost errorlessly by the nouns they modify or by the syntactic constructions in which they occur.	nouns	syntactic constructions	model-feature	{'e1': {'word': 'nouns', 'word_index': [(36, 36)], 'id': 'J95-1001.26'}, 'e2': {'word': 'syntactic constructions', 'word_index': [(42, 42)], 'id': 'J95-1001.27'}}	This assessment is based on an empirical study of five of the most frequent ENTITYUNRELATED in ENTITYUNRELATED : and About three - quarters of all instances of these ENTITYUNRELATED can be disambiguated almost errorlessly by the ENTITY they modify or by the ENTITYOTHER in which they occur .
Furthermore, a small number of semantic attributes supply a compact means of representing the noun clues in a very few rules.	semantic attributes	noun	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(6, 6)], 'id': 'J95-1001.29'}, 'e2': {'word': 'noun', 'word_index': [(14, 14)], 'id': 'J95-1001.30'}}	Furthermore , a small number of ENTITY supply a compact means of representing the ENTITYOTHER clues in a very few ENTITYUNRELATED .
The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined syntactic relations to the ambiguous adjective.	sense	ambiguous modified noun	model-feature	{'e1': {'word': 'sense', 'word_index': [(1, 1)], 'id': 'J95-1001.34'}, 'e2': {'word': 'ambiguous modified noun', 'word_index': [(4, 4)], 'id': 'J95-1001.35'}}	The ENTITY of an ENTITYOTHER may be needed to determine the relevant ENTITYUNRELATED for ENTITYUNRELATED of a ENTITYUNRELATED ; and other ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED all show evidence of high reliability , and sometimes of high applicability , when they stand in specific , well - defined ENTITYUNRELATED to the ENTITYUNRELATED .
The sense of an ambiguous modified noun may be needed to determine the relevant semantic attribute for disambiguation of a target adjective; and other adjectives, verbs, and grammatical constructions all show evidence of high reliability, and sometimes of high applicability, when they stand in specific, well-defined syntactic relations to the ambiguous adjective.	semantic attribute	target adjective	model-feature	{'e1': {'word': 'semantic attribute', 'word_index': [(12, 12)], 'id': 'J95-1001.36'}, 'e2': {'word': 'target adjective', 'word_index': [(17, 17)], 'id': 'J95-1001.38'}}	The ENTITYUNRELATED of an ENTITYUNRELATED may be needed to determine the relevant ENTITY for ENTITYUNRELATED of a ENTITYOTHER ; and other ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED all show evidence of high reliability , and sometimes of high applicability , when they stand in specific , well - defined ENTITYUNRELATED to the ENTITYUNRELATED .
This paper presents our method of incorporating character clustering based on mutual information into Decision-Tree Dictionary-less morphological analysis.	mutual information	character clustering	usage	{'e1': {'word': 'mutual information', 'word_index': [(10, 10)], 'id': 'P98-1108.7'}, 'e2': {'word': 'character clustering', 'word_index': [(7, 7)], 'id': 'P98-1108.6'}}	This paper presents our method of incorporating ENTITYOTHER based on ENTITY into ENTITYUNRELATED .
By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text.	tagging	text	usage	{'e1': {'word': 'tagging', 'word_index': [(18, 18)], 'id': 'P98-1108.12'}, 'e2': {'word': 'text', 'word_index': [(20, 20)], 'id': 'P98-1108.14'}}	By using ENTITYUNRELATED , we have confirmed that our ENTITYUNRELATED has been significantly improved in both ENTITYUNRELATED and ENTITY ENTITYUNRELATED ENTITYOTHER .
As a case study in one direction, we discuss the recent development of an automatic method for evaluating definition questions based on n-gram overlap, a commonly-used technique in summarization evaluation.	n-gram overlap	automatic method for evaluating definition questions	usage	{'e1': {'word': 'n-gram overlap', 'word_index': [(18, 18)], 'id': 'W05-0906.4'}, 'e2': {'word': 'automatic method for evaluating definition questions', 'word_index': [(15, 15)], 'id': 'W05-0906.3'}}	As a case study in one direction , we discuss the recent development of an ENTITYOTHER based on ENTITY , a commonly - used technique in ENTITYUNRELATED .
SYSTRAN'S Chinese word segmentation is one important component of its Chinese-English machine translation system.	Chinese word segmentation	Chinese-English machine translation system	part_whole	{'e1': {'word': 'Chinese word segmentation', 'word_index': [(1, 1)], 'id': 'W03-1729.1'}, 'e2': {'word': 'Chinese-English machine translation system', 'word_index': [(8, 8)], 'id': 'W03-1729.2'}}	SYSTRAN'S ENTITY is one important component of its ENTITYOTHER .
The Chinese word segmentation module uses a rule-based approach, based on a large dictionary and fine-grained linguistic rules.	rule-based approach	Chinese word segmentation	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(5, 5)], 'id': 'W03-1729.4'}, 'e2': {'word': 'Chinese word segmentation', 'word_index': [(1, 1)], 'id': 'W03-1729.3'}}	The ENTITYOTHER module uses a ENTITY , based on a ENTITYUNRELATED and ENTITYUNRELATED .
It works on general-purpose texts from different Chinese-speaking regions, with comparable performance.	Chinese-speaking regions	general-purpose texts	model-feature	{'e1': {'word': 'Chinese-speaking regions', 'word_index': [(6, 6)], 'id': 'W03-1729.8'}, 'e2': {'word': 'general-purpose texts', 'word_index': [(3, 3)], 'id': 'W03-1729.7'}}	It works on ENTITYOTHER from different ENTITY , with comparable ENTITYUNRELATED .
ParaMor, our minimally supervised morphology induction algorithm, retrusses the word forms of raw text corpora back onto their paradigmatic skeletons; performing on par with state-of-the-art minimally supervised morphology induction algorithms at morphological analysis of English and German.	morphological analysis	English	usage	{'e1': {'word': 'morphological analysis', 'word_index': [(29, 29)], 'id': 'W07-1315.6'}, 'e2': {'word': 'English', 'word_index': [(31, 31)], 'id': 'W07-1315.7'}}	ParaMor , our ENTITYUNRELATED , retrusses the ENTITYUNRELATED of ENTITYUNRELATED back onto their paradigmatic skeletons ; performing on par with state - of - the - art ENTITYUNRELATED at ENTITY of ENTITYOTHER and ENTITYUNRELATED .
And with these structures in hand, ParaMor then annotates word forms with morpheme boundaries.	morpheme boundaries	word forms	model-feature	{'e1': {'word': 'morpheme boundaries', 'word_index': [(13, 13)], 'id': 'W07-1315.12'}, 'e2': {'word': 'word forms', 'word_index': [(11, 11)], 'id': 'W07-1315.11'}}	And with these structures in hand , Para Mor then annotates ENTITYOTHER with ENTITY .
To set ParaMor 's few free parameters we analyze a training corpus of Spanish.	Spanish	training corpus	part_whole	{'e1': {'word': 'Spanish', 'word_index': [(12, 12)], 'id': 'W07-1315.14'}, 'e2': {'word': 'training corpus', 'word_index': [(10, 10)], 'id': 'W07-1315.13'}}	To set ParaMor 's few free parameters we analyze a ENTITYOTHER of ENTITY .
Without adjusting parameters, we induce the morphological structure of English and German.	morphological structure	English	model-feature	{'e1': {'word': 'morphological structure', 'word_index': [(7, 7)], 'id': 'W07-1315.15'}, 'e2': {'word': 'English', 'word_index': [(9, 9)], 'id': 'W07-1315.16'}}	Without adjusting parameters , we induce the ENTITY of ENTITYOTHER and ENTITYUNRELATED .
The other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores.	distance features	multi-level confidence scores	usage	{'e1': {'word': 'distance features', 'word_index': [(12, 12)], 'id': 'P08-2055.8'}, 'e2': {'word': 'multi-level confidence scores', 'word_index': [(18, 18)], 'id': 'P08-2055.9'}}	The other contribution of this work is that we incorporated novel long ENTITY to address challenges in computing ENTITYOTHER .
Using Conditional Maximum Entropy (CME) classifier with all the selected features, we reached an annotation error rate of 26.0% in the SWBD corpus, compared with a subtree error rate of 41.91%, a closely related benchmark with the Charniak parser from ( Kahn et al., 2005 ).	Conditional Maximum Entropy (CME) classifier	annotation error rate	result	{'e1': {'word': 'Conditional Maximum Entropy (CME) classifier', 'word_index': [(1, 1)], 'id': 'P08-2055.10'}, 'e2': {'word': 'annotation error rate', 'word_index': [(11, 11)], 'id': 'P08-2055.11'}}	Using ENTITY with all the selected features , we reached an ENTITYOTHER of 26.0 % in the ENTITYUNRELATED , compared with a ENTITYUNRELATED of 41.91 % , a closely related benchmark with the ENTITYUNRELATED from ( Kahn et al. , 2005 ) .
Coreference resolution systems usually attempt to find a suitable antecedent for (almost) every noun phrase	Coreference resolution systems	noun phrase	usage	{'e1': {'word': 'Coreference resolution systems', 'word_index': [(0, 0)], 'id': 'P03-2012.1'}, 'e2': {'word': 'noun phrase', 'word_index': [(13, 13)], 'id': 'P03-2012.2'}}	ENTITY usually attempt to find a suitable antecedent for ( almost ) every ENTITYOTHER
We use a small training corpus (MUC-7), but also acquire some data from the Internet.	data	Internet	part_whole	{'e1': {'word': 'data', 'word_index': [(10, 10)], 'id': 'P03-2012.8'}, 'e2': {'word': 'Internet', 'word_index': [(13, 13)], 'id': 'P03-2012.9'}}	We use a small ENTITYUNRELATED , but also acquire some ENTITY from the ENTITYOTHER .
Combining our classifiers sequentially, we achieve 88.9% precision and 84.6% recall for discourse new entities.	classifiers	precision	result	{'e1': {'word': 'classifiers', 'word_index': [(2, 2)], 'id': 'P03-2012.10'}, 'e2': {'word': 'precision', 'word_index': [(9, 9)], 'id': 'P03-2012.11'}}	Combining our ENTITY sequentially , we achieve 88.9 % ENTITYOTHER and 84.6 % ENTITYUNRELATED for ENTITYUNRELATED .
We expect our classifiers to provide a good prefiltering for coreference resolution systems, improving both their speed and performance.	classifiers	coreference resolution systems	usage	{'e1': {'word': 'classifiers', 'word_index': [(3, 3)], 'id': 'P03-2012.14'}, 'e2': {'word': 'coreference resolution systems', 'word_index': [(10, 10)], 'id': 'P03-2012.15'}}	We expect our ENTITY to provide a good prefiltering for ENTITYOTHER , improving both their ENTITYUNRELATED and ENTITYUNRELATED .
MBDP-1 is a knowledge-free segmentation algorithm that bootstraps its own lexicon, which starts out empty.	knowledge-free segmentation algorithm	lexicon	usage	{'e1': {'word': 'knowledge-free segmentation algorithm', 'word_index': [(4, 4)], 'id': 'P01-1013.3'}, 'e2': {'word': 'lexicon', 'word_index': [(9, 9)], 'id': 'P01-1013.4'}}	MBDP -1 is a ENTITY that bootstraps its own ENTITYOTHER , which starts out empty .
In this paper, we present methods that allow the users of a natural language processor (NLP) to define, inspect, and modify any case frame information associated with the words and phrases known to the system.	case frame information	words	model-feature	{'e1': {'word': 'case frame information', 'word_index': [(22, 22)], 'id': 'C86-1108.2'}, 'e2': {'word': 'words', 'word_index': [(26, 26)], 'id': 'C86-1108.3'}}	In this paper , we present methods that allow the users of a ENTITYUNRELATED to define , inspect , and modify any ENTITY associated with the ENTITYOTHER and ENTITYUNRELATED known to the system .
The number and sizes of parallel corpora keep growing, which makes it necessary to have automatic methods of processing them: combining, checking and improving corpora quality, etc.	corpora quality	parallel corpora	model-feature	{'e1': {'word': 'corpora quality', 'word_index': [(26, 26)], 'id': 'L08-1114.2'}, 'e2': {'word': 'parallel corpora', 'word_index': [(5, 5)], 'id': 'L08-1114.1'}}	The number and sizes of ENTITYOTHER keep growing , which makes it necessary to have automatic methods of processing them : combining , checking and improving ENTITY , etc.
The method takes into consideration slight differences in the source documents, different levels of segmentation of the input corpora, encoding differences and other aspects of the task.	segmentation	input corpora	usage	{'e1': {'word': 'segmentation', 'word_index': [(14, 14)], 'id': 'L08-1114.10'}, 'e2': {'word': 'input corpora', 'word_index': [(17, 17)], 'id': 'L08-1114.11'}}	The method takes into consideration slight differences in the ENTITYUNRELATED , different levels of ENTITY of the ENTITYOTHER , encoding differences and other aspects of the task .
In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts.	Estonian-English	JRC-Acquis corpus	part_whole	{'e1': {'word': 'Estonian-English', 'word_index': [(6, 6)], 'id': 'L08-1114.12'}, 'e2': {'word': 'JRC-Acquis corpus', 'word_index': [(10, 10)], 'id': 'L08-1114.13'}}	In the first experiment , the ENTITY part of the ENTITYOTHER was combined with another ENTITYUNRELATED of ENTITYUNRELATED .
In the first experiment, the Estonian-English part of the JRC-Acquis corpus was combined with another corpus of legislation texts.	legislation texts	corpus	part_whole	{'e1': {'word': 'legislation texts', 'word_index': [(17, 17)], 'id': 'L08-1114.15'}, 'e2': {'word': 'corpus', 'word_index': [(15, 15)], 'id': 'L08-1114.14'}}	In the first experiment , the ENTITYUNRELATED part of the ENTITYUNRELATED was combined with another ENTITYOTHER of ENTITY .
The generation module supports the seamless integration of full grammar rules, templates and canned text.	grammar rules	generation module	usage	{'e1': {'word': 'grammar rules', 'word_index': [(8, 8)], 'id': 'E03-1019.6'}, 'e2': {'word': 'generation module', 'word_index': [(1, 1)], 'id': 'E03-1019.5'}}	The ENTITYOTHER supports the seamless integration of full ENTITY , ENTITYUNRELATED and ENTITYUNRELATED .
Ambiguity is the fundamental property of natural language	Ambiguity	natural language	model-feature	{'e1': {'word': 'Ambiguity', 'word_index': [(0, 0)], 'id': 'C02-1079.1'}, 'e2': {'word': 'natural language', 'word_index': [(6, 6)], 'id': 'C02-1079.2'}}	ENTITY is the fundamental property of ENTITYOTHER
Perhaps, the most burdensome case of ambiguity manifests itself on the syntactic level of analysis.	ambiguity	syntactic level of analysis	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(7, 7)], 'id': 'C02-1079.3'}, 'e2': {'word': 'syntactic level of analysis', 'word_index': [(12, 12)], 'id': 'C02-1079.4'}}	Perhaps , the most burdensome case of ENTITY manifests itself on the ENTITYOTHER .
The presented methods are based on language specific features of synthetical languages and they improve the results of simple stochastic approaches.	language specific features	synthetical languages	model-feature	{'e1': {'word': 'language specific features', 'word_index': [(6, 6)], 'id': 'C02-1079.7'}, 'e2': {'word': 'synthetical languages', 'word_index': [(8, 8)], 'id': 'C02-1079.8'}}	The presented methods are based on ENTITY of ENTITYOTHER and they improve the results of simple ENTITYUNRELATED .
The texts are in English and Czech.	texts	English	model-feature	{'e1': {'word': 'texts', 'word_index': [(1, 1)], 'id': 'L08-1197.3'}, 'e2': {'word': 'English', 'word_index': [(4, 4)], 'id': 'L08-1197.4'}}	The ENTITY are in ENTITYOTHER and ENTITYUNRELATED .
This paper presents techniques for multimedia annotation and their application to video summarization and translation.	multimedia annotation	video summarization and translation	usage	{'e1': {'word': 'multimedia annotation', 'word_index': [(5, 5)], 'id': 'C02-1098.2'}, 'e2': {'word': 'video summarization and translation', 'word_index': [(10, 10)], 'id': 'C02-1098.3'}}	This paper presents techniques for ENTITY and their application to ENTITYOTHER .
A video scene description consists of semi-automatically detected keyframes of each scene in a video clip and time codes of scenes.	semi-automatically detected keyframes	video scene description	part_whole	{'e1': {'word': 'semi-automatically detected keyframes', 'word_index': [(4, 4)], 'id': 'C02-1098.12'}, 'e2': {'word': 'video scene description', 'word_index': [(1, 1)], 'id': 'C02-1098.11'}}	A ENTITYOTHER consists of ENTITY of each scene in a video clip and time codes of scenes .
The text data in the multimedia annotation are syntactically and semantically structured using linguistic annotation.	text data	syntactically and semantically structured	model-feature	{'e1': {'word': 'text data', 'word_index': [(1, 1)], 'id': 'C02-1098.14'}, 'e2': {'word': 'syntactically and semantically structured', 'word_index': [(6, 6)], 'id': 'C02-1098.16'}}	The ENTITY in the ENTITYUNRELATED are ENTITYOTHER using ENTITYUNRELATED .
The proposed multimedia summarization works upon a multimodal document that consists of a video, keyframes of scenes, and transcripts of the scenes.	 multimedia summarization	multimodal document	usage	{'e1': {'word': ' multimedia summarization', 'word_index': [(2, 2)], 'id': 'C02-1098.18'}, 'e2': {'word': 'multimodal document', 'word_index': [(6, 6)], 'id': 'C02-1098.19'}}	The proposed ENTITY works upon a ENTITYOTHER that consists of a video , ENTITYUNRELATED of ENTITYUNRELATED , and ENTITYUNRELATED of the ENTITYUNRELATED .
The multimedia translation automatically generates several versions of multimedia content in different languages.	languages	multimedia content	model-feature	{'e1': {'word': 'languages', 'word_index': [(10, 10)], 'id': 'C02-1098.26'}, 'e2': {'word': 'multimedia content', 'word_index': [(7, 7)], 'id': 'C02-1098.25'}}	The ENTITYUNRELATED automatically generates several versions of ENTITYOTHER in different ENTITY .
When machine translation (MT) knowledge is automatically constructed from bilingual corpora, redundant rules are acquired due to translation variety.	bilingual corpora	machine translation (MT) knowledge	usage	{'e1': {'word': 'bilingual corpora', 'word_index': [(6, 6)], 'id': 'E03-1029.2'}, 'e2': {'word': 'machine translation (MT) knowledge', 'word_index': [(1, 1)], 'id': 'E03-1029.1'}}	When ENTITYOTHER is automatically constructed from ENTITY , ENTITYUNRELATED are acquired due to translation variety .
These rules increase ambiguity or cause incorrect MT results.	rules	ambiguity	result	{'e1': {'word': 'rules', 'word_index': [(1, 1)], 'id': 'E03-1029.4'}, 'e2': {'word': 'ambiguity', 'word_index': [(3, 3)], 'id': 'E03-1029.5'}}	These ENTITY increase ENTITYOTHER or cause incorrect ENTITYUNRELATED .
"To overcome this problem, we constrain the sentences used for knowledge extraction to ""the appropriate bilingual sentences for the MT""."	sentences	knowledge extraction	usage	{'e1': {'word': 'sentences', 'word_index': [(8, 8)], 'id': 'E03-1029.7'}, 'e2': {'word': 'knowledge extraction', 'word_index': [(11, 11)], 'id': 'E03-1029.8'}}	"To overcome this problem , we constrain the ENTITY used for ENTITYOTHER to "" the appropriate ENTITYUNRELATED for the ENTITYUNRELATED "" ."
"To overcome this problem, we constrain the sentences used for knowledge extraction to ""the appropriate bilingual sentences for the MT""."	bilingual sentences	MT	usage	{'e1': {'word': 'bilingual sentences', 'word_index': [(16, 16)], 'id': 'E03-1029.9'}, 'e2': {'word': 'MT', 'word_index': [(19, 19)], 'id': 'E03-1029.10'}}	"To overcome this problem , we constrain the ENTITYUNRELATED used for ENTITYUNRELATED to "" the appropriate ENTITY for the ENTITYOTHER "" ."
For the creation of these scholarly digital editions the AAC edition philosophy and edition principles have been applied whereby new corpus research methods have been made use of for questions of computational philology and textual studies in a digital environment.	AAC edition philosophy and edition principles	scholarly digital editions	usage	{'e1': {'word': 'AAC edition philosophy and edition principles', 'word_index': [(7, 7)], 'id': 'L08-1405.9'}, 'e2': {'word': 'scholarly digital editions', 'word_index': [(5, 5)], 'id': 'L08-1405.8'}}	For the creation of these ENTITYOTHER the ENTITY have been applied whereby new ENTITYUNRELATED have been made use of for questions of ENTITYUNRELATED and ENTITYUNRELATED in a digital environment .
The evaluation results show our system can achieve an F measure of 0.9400.967 for different testing corpora.	system	F measure	result	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'I05-3026.5'}, 'e2': {'word': 'F measure', 'word_index': [(9, 9)], 'id': 'I05-3026.6'}}	The evaluation results show our ENTITY can achieve an ENTITYOTHER of 0.9400.967 for different ENTITYUNRELATED .
In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others.	machine learning algorithm	shallow semantic parsing	usage	{'e1': {'word': 'machine learning algorithm', 'word_index': [(7, 7)], 'id': 'N04-1030.1'}, 'e2': {'word': 'shallow semantic parsing', 'word_index': [(9, 9)], 'id': 'N04-1030.2'}}	In this paper , we propose a ENTITY for ENTITYOTHER , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al. ( 2003 ) and others .
Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers.	Support Vector Machines	algorithm	usage	{'e1': {'word': 'Support Vector Machines', 'word_index': [(5, 5)], 'id': 'N04-1030.4'}, 'e2': {'word': 'algorithm', 'word_index': [(1, 1)], 'id': 'N04-1030.3'}}	Our ENTITYOTHER is based on ENTITY which we show give an improvement in performance over earlier ENTITYUNRELATED .
We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.	test set	AQUAINT corpus	part_whole	{'e1': {'word': 'test set', 'word_index': [(19, 19)], 'id': 'N04-1030.7'}, 'e2': {'word': 'AQUAINT corpus', 'word_index': [(23, 23)], 'id': 'N04-1030.8'}}	We show performance improvements through a number of new ENTITYUNRELATED and measure their ability to generalize to a new ENTITY drawn from the ENTITYOTHER .
Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra-sentential contextual influences.	language model	speech understanding	usage	{'e1': {'word': 'language model', 'word_index': [(5, 5)], 'id': 'A94-1010.1'}, 'e2': {'word': 'speech understanding', 'word_index': [(8, 8)], 'id': 'A94-1010.2'}}	Many of the kinds of ENTITY used in ENTITYOTHER suffer from imperfect ENTITYUNRELATED of ENTITYUNRELATED .
I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into sub corpora on the criterion of entropy reduction, and calculating separate language model parameters for each cluster.	sentences	training corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(11, 11)], 'id': 'A94-1010.5'}, 'e2': {'word': 'training corpus', 'word_index': [(14, 14)], 'id': 'A94-1010.6'}}	I argue that this problem can be addressed by clustering the ENTITY in a ENTITYOTHER automatically into ENTITYUNRELATED on the criterion of ENTITYUNRELATED , and calculating separate ENTITYUNRELATED for each ENTITYUNRELATED .
This kind of clustering offers a way to represent important contextual effects and can therefore significantly improve the performance of a model.	clustering	contextual effects	model-feature	{'e1': {'word': 'clustering', 'word_index': [(3, 3)], 'id': 'A94-1010.11'}, 'e2': {'word': 'contextual effects', 'word_index': [(10, 10)], 'id': 'A94-1010.12'}}	This kind of ENTITY offers a way to represent important ENTITYOTHER and can therefore significantly improve the performance of a ENTITYUNRELATED .
It also offers a reasonably automatic means to gather evidence on whether a more complex, context-sensitive model using the same general kind of linguistic information is likely to reward the effort that would be required to develop it: if clustering improves the performance of a model, this proves the existence of further context dependencies, not exploited by the unclustered model.	clustering	model	result	{'e1': {'word': 'clustering', 'word_index': [(36, 36)], 'id': 'A94-1010.16'}, 'e2': {'word': 'model', 'word_index': [(42, 42)], 'id': 'A94-1010.17'}}	It also offers a reasonably automatic means to gather evidence on whether a ENTITYUNRELATED using the same general kind of ENTITYUNRELATED is likely to reward the effort that would be required to develop it : if ENTITY improves the performance of a ENTITYOTHER , this proves the existence of further ENTITYUNRELATED , not exploited by the ENTITYUNRELATED .
As evidence for these claims, I present results showing that clustering improves some models but not others for the ATIS domain.	clustering	models	result	{'e1': {'word': 'clustering', 'word_index': [(11, 11)], 'id': 'A94-1010.20'}, 'e2': {'word': 'models', 'word_index': [(14, 14)], 'id': 'A94-1010.21'}}	As evidence for these claims , I present results showing that ENTITY improves some ENTITYOTHER but not others for the ENTITYUNRELATED .
This paper presents a parsing system for the detection of syntactic errors.	parsing system	detection of syntactic errors	usage	{'e1': {'word': 'parsing system', 'word_index': [(4, 4)], 'id': 'A00-3005.1'}, 'e2': {'word': 'detection of syntactic errors', 'word_index': [(7, 7)], 'id': 'A00-3005.2'}}	This paper presents a ENTITY for the ENTITYOTHER .
It combines a robust partial parser which obtains the main sentence components and a finite-state parser used for the description of syntactic error patterns.	finite-state parser	syntactic error patterns	usage	{'e1': {'word': 'finite-state parser', 'word_index': [(10, 10)], 'id': 'A00-3005.5'}, 'e2': {'word': 'syntactic error patterns', 'word_index': [(16, 16)], 'id': 'A00-3005.6'}}	It combines a ENTITYUNRELATED which obtains the ENTITYUNRELATED and a ENTITY used for the description of ENTITYOTHER .
The system has been tested on a corpus of real texts, containing both correct and incorrect sentences, with promising results.	texts	corpus	part_whole	{'e1': {'word': 'texts', 'word_index': [(10, 10)], 'id': 'A00-3005.9'}, 'e2': {'word': 'corpus', 'word_index': [(7, 7)], 'id': 'A00-3005.8'}}	The ENTITYUNRELATED has been tested on a ENTITYOTHER of real ENTITY , containing both ENTITYUNRELATED , with promising results .
The objectives of this project are to advance our understanding of the merits of current text analysis techniques, as applied to the performance of realistic text analysis tasks, and to achieve this understanding by means of a sound performance evaluation methodology.	text analysis techniques	realistic text analysis tasks	usage	{'e1': {'word': 'text analysis techniques', 'word_index': [(15, 15)], 'id': 'H92-1111.1'}, 'e2': {'word': 'realistic text analysis tasks', 'word_index': [(23, 23)], 'id': 'H92-1111.3'}}	The objectives of this project are to advance our understanding of the merits of current ENTITY , as applied to the ENTITYUNRELATED of ENTITYOTHER , and to achieve this understanding by means of a sound ENTITYUNRELATED .
Talk'n'Travel is a fully conversational, mixed-initiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English.	general spoken English	questions	model-feature	{'e1': {'word': 'general spoken English', 'word_index': [(27, 27)], 'id': 'A00-1010.8'}, 'e2': {'word': 'questions', 'word_index': [(22, 22)], 'id': 'A00-1010.7'}}	ENTITYUNRELATED is a fully ENTITYUNRELATED that allows the ENTITYUNRELATED to specify the ENTITYUNRELATED on his travel plan in arbitrary order , ask ENTITYOTHER , etc. , in ENTITY .
The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met.	plan-based agenda mechanism	system	usage	{'e1': {'word': 'plan-based agenda mechanism', 'word_index': [(6, 6)], 'id': 'A00-1010.10'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'A00-1010.9'}}	The ENTITYOTHER operates according to a ENTITY , rather than a ENTITYUNRELATED , and attempts to negotiate with the ENTITYUNRELATED when not all of his ENTITYUNRELATED can be met .
As mentioned in Mitkov (1996), solving the anaphora and extracting the antecedent are key issues in a correct translation.	anaphora	translation	part_whole	{'e1': {'word': 'anaphora', 'word_index': [(10, 10)], 'id': 'W99-0210.3'}, 'e2': {'word': 'translation', 'word_index': [(21, 21)], 'id': 'W99-0210.5'}}	As mentioned in Mitkov ( 1996 ) , solving the ENTITY and extracting the ENTITYUNRELATED are key issues in a correct ENTITYOTHER .
The SS stores the lexical, syntactic, morphologic and semantic information of every constituent of the grammar.	lexical, syntactic, morphologic and semantic information	constituent	model-feature	{'e1': {'word': 'lexical, syntactic, morphologic and semantic information', 'word_index': [(4, 4)], 'id': 'W99-0210.10'}, 'e2': {'word': 'constituent', 'word_index': [(7, 7)], 'id': 'W99-0210.11'}}	The ENTITYUNRELATED stores the ENTITY of every ENTITYOTHER of the ENTITYUNRELATED .
This mechanism could be added to a MT system such as an additional module to solve anaphora generation problem.	mechanism	MT system	usage	{'e1': {'word': 'mechanism', 'word_index': [(1, 1)], 'id': 'W99-0210.23'}, 'e2': {'word': 'MT system', 'word_index': [(7, 7)], 'id': 'W99-0210.24'}}	This ENTITY could be added to a ENTITYOTHER such as an additional module to solve ENTITYUNRELATED .
This paper addresses language engineering infrastructure issues by considering whether standard V&amp;V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&amp;V in the context of language processing systems.	standard V&amp;V methods	evaluation practices	compare	{'e1': {'word': 'standard V&amp;V methods', 'word_index': [(7, 7)], 'id': 'W01-0906.6'}, 'e2': {'word': 'evaluation practices', 'word_index': [(13, 13)], 'id': 'W01-0906.7'}}	This paper addresses ENTITYUNRELATED by considering whether ENTITY are fundamentally different than the ENTITYOTHER commonly used for ENTITYUNRELATED , and proposes practical approaches for applying ENTITYUNRELATED in the context of ENTITYUNRELATED .
This paper addresses language engineering infrastructure issues by considering whether standard V&amp;V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&amp;V in the context of language processing systems.	V&amp;V	language processing systems	usage	{'e1': {'word': 'V&amp;V', 'word_index': [(25, 25)], 'id': 'W01-0906.9'}, 'e2': {'word': 'language processing systems', 'word_index': [(30, 30)], 'id': 'W01-0906.10'}}	This paper addresses ENTITYUNRELATED by considering whether ENTITYUNRELATED are fundamentally different than the ENTITYUNRELATED commonly used for ENTITYUNRELATED , and proposes practical approaches for applying ENTITY in the context of ENTITYOTHER .
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text.	paragraphs	document	part_whole	{'e1': {'word': 'paragraphs', 'word_index': [(14, 14)], 'id': 'W03-1102.1'}, 'e2': {'word': 'document', 'word_index': [(18, 18)], 'id': 'W03-1102.2'}}	In this paper , we propose a practical approach for extracting the most relevant ENTITY from the original ENTITYOTHER to form a ENTITYUNRELATED for ENTITYUNRELATED .
In this paper, we propose a practical approach for extracting the most relevant paragraphs from the original document to form a summary for Thai text.	summary	Thai text	model-feature	{'e1': {'word': 'summary', 'word_index': [(22, 22)], 'id': 'W03-1102.3'}, 'e2': {'word': 'Thai text', 'word_index': [(24, 24)], 'id': 'W03-1102.4'}}	In this paper , we propose a practical approach for extracting the most relevant ENTITYUNRELATED from the original ENTITYUNRELATED to form a ENTITY for ENTITYOTHER .
The idea of our approach is to exploit both the local and global properties of paragraphs.	local and global properties	paragraphs	model-feature	{'e1': {'word': 'local and global properties', 'word_index': [(10, 10)], 'id': 'W03-1102.5'}, 'e2': {'word': 'paragraphs', 'word_index': [(12, 12)], 'id': 'W03-1102.6'}}	The idea of our approach is to exploit both the ENTITY of ENTITYOTHER .
The local property can be considered as clusters of significant words within each paragraph, while the global property can be thought of as relations of all paragraphs in a document.	clusters	significant words	model-feature	{'e1': {'word': 'clusters', 'word_index': [(6, 6)], 'id': 'W03-1102.8'}, 'e2': {'word': 'significant words', 'word_index': [(8, 8)], 'id': 'W03-1102.9'}}	The ENTITYUNRELATED can be considered as ENTITY of ENTITYOTHER within each ENTITYUNRELATED , while the ENTITYUNRELATED can be thought of as ENTITYUNRELATED of all ENTITYUNRELATED in a ENTITYUNRELATED .
Syntax-based Machine Translation systems have recently become a focus of research with much hope that they will outperform traditional Phrase- Based Statistical Machine Translation (PBSMT)	Syntax-based Machine Translation systems	traditional Phrase- Based Statistical Machine Translation (PBSMT)	compare	{'e1': {'word': 'Syntax-based Machine Translation systems', 'word_index': [(0, 0)], 'id': 'W08-0410.1'}, 'e2': {'word': 'traditional Phrase- Based Statistical Machine Translation (PBSMT)', 'word_index': [(15, 15)], 'id': 'W08-0410.2'}}	ENTITY have recently become a focus of research with much hope that they will outperform ENTITYOTHER
Toward this goal, we present a method for analyzing the morphosyntactic content of language from an Elicitation Corpus such as the one included in the LDC's upcoming LCTL language packs.	morphosyntactic content	Elicitation Corpus	part_whole	{'e1': {'word': 'morphosyntactic content', 'word_index': [(11, 11)], 'id': 'W08-0410.3'}, 'e2': {'word': 'Elicitation Corpus', 'word_index': [(16, 16)], 'id': 'W08-0410.5'}}	Toward this goal , we present a method for analyzing the ENTITY of ENTITYUNRELATED from an ENTITYOTHER such as the one included in the ENTITYUNRELATED upcoming ENTITYUNRELATED .
By providing this tool that can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved.	rich features	structure-based MT models	usage	{'e1': {'word': 'rich features', 'word_index': [(10, 10)], 'id': 'W08-0410.12'}, 'e2': {'word': 'structure-based MT models', 'word_index': [(7, 7)], 'id': 'W08-0410.11'}}	By providing this tool that can augment ENTITYOTHER with these ENTITY , we believe the ENTITYUNRELATED of current ENTITYUNRELATED can be improved .
This article outlines a quantitative method for segmenting texts into thematically coherent units.	quantitative method	texts	usage	{'e1': {'word': 'quantitative method', 'word_index': [(4, 4)], 'id': 'P98-2243.1'}, 'e2': {'word': 'texts', 'word_index': [(7, 7)], 'id': 'P98-2243.2'}}	This article outlines a ENTITY for segmenting ENTITYOTHER into ENTITYUNRELATED .
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.	network of lexical collocations	method	usage	{'e1': {'word': 'network of lexical collocations', 'word_index': [(5, 5)], 'id': 'P98-2243.5'}, 'e2': {'word': 'method', 'word_index': [(1, 1)], 'id': 'P98-2243.4'}}	This ENTITYOTHER relies on a ENTITY to compute the ENTITYUNRELATED of the different parts of a ENTITYUNRELATED from the ENTITYUNRELATED of their ENTITYUNRELATED .
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.	lexical cohesiveness	words	model-feature	{'e1': {'word': 'lexical cohesiveness', 'word_index': [(19, 19)], 'id': 'P98-2243.8'}, 'e2': {'word': 'words', 'word_index': [(22, 22)], 'id': 'P98-2243.9'}}	This ENTITYUNRELATED relies on a ENTITYUNRELATED to compute the ENTITYUNRELATED of the different parts of a ENTITYUNRELATED from the ENTITY of their ENTITYOTHER .
We also present the results of an experiment about locating boundaries between a series of concatened texts.	boundaries	texts	part_whole	{'e1': {'word': 'boundaries', 'word_index': [(10, 10)], 'id': 'P98-2243.10'}, 'e2': {'word': 'texts', 'word_index': [(16, 16)], 'id': 'P98-2243.11'}}	We also present the results of an experiment about locating ENTITY between a series of concatened ENTITYOTHER .
In this work, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves.	model	sense assignment	usage	{'e1': {'word': 'model', 'word_index': [(7, 7)], 'id': 'W04-1908.1'}, 'e2': {'word': 'sense assignment', 'word_index': [(9, 9)], 'id': 'W04-1908.2'}}	In this work , we introduce a ENTITY for ENTITYOTHER which relies on assigning ENTITYUNRELATED to the ENTITYUNRELATED within which ENTITYUNRELATED appear , rather than to the ENTITYUNRELATED themselves .
In this work, we introduce a model for sense assignment which relies on assigning senses to the contexts within which words appear, rather than to the words themselves.	words	contexts	model-feature	{'e1': {'word': 'words', 'word_index': [(20, 20)], 'id': 'W04-1908.5'}, 'e2': {'word': 'contexts', 'word_index': [(17, 17)], 'id': 'W04-1908.4'}}	In this work , we introduce a ENTITYUNRELATED for ENTITYUNRELATED which relies on assigning ENTITYUNRELATED to the ENTITYOTHER within which ENTITY appear , rather than to the ENTITYUNRELATED themselves .
In this paper we describe a morphological analysis method based on a maximum entropy model.	maximum entropy model	morphological analysis method	usage	{'e1': {'word': 'maximum entropy model', 'word_index': [(10, 10)], 'id': 'W01-0512.2'}, 'e2': {'word': 'morphological analysis method', 'word_index': [(6, 6)], 'id': 'W01-0512.1'}}	In this paper we describe a ENTITYOTHER based on a ENTITY .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	model	method	usage	{'e1': {'word': 'model', 'word_index': [(4, 4)], 'id': 'W01-0512.4'}, 'e2': {'word': 'method', 'word_index': [(1, 1)], 'id': 'W01-0512.3'}}	This ENTITYOTHER uses a ENTITY that can not only consult a ENTITYUNRELATED with a large amount of ENTITYUNRELATED but can also identify ENTITYUNRELATED by learning certain ENTITYUNRELATED .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	lexical information	dictionary	part_whole	{'e1': {'word': 'lexical information', 'word_index': [(17, 17)], 'id': 'W01-0512.6'}, 'e2': {'word': 'dictionary', 'word_index': [(11, 11)], 'id': 'W01-0512.5'}}	This ENTITYUNRELATED uses a ENTITYUNRELATED that can not only consult a ENTITYOTHER with a large amount of ENTITY but can also identify ENTITYUNRELATED by learning certain ENTITYUNRELATED .
This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics.	characteristics	unknown words	model-feature	{'e1': {'word': 'characteristics', 'word_index': [(26, 26)], 'id': 'W01-0512.8'}, 'e2': {'word': 'unknown words', 'word_index': [(22, 22)], 'id': 'W01-0512.7'}}	This ENTITYUNRELATED uses a ENTITYUNRELATED that can not only consult a ENTITYUNRELATED with a large amount of ENTITYUNRELATED but can also identify ENTITYOTHER by learning certain ENTITY .
Finally, we present Corporator, an Open Source software which was designed for collecting corpus from RSS feeds.	corpus	RSS feeds	part_whole	{'e1': {'word': 'corpus', 'word_index': [(13, 13)], 'id': 'W06-1707.10'}, 'e2': {'word': 'RSS feeds', 'word_index': [(15, 15)], 'id': 'W06-1707.11'}}	Finally , we present ENTITYUNRELATED , an ENTITYUNRELATED which was designed for collecting ENTITY from ENTITYOTHER .
Several SVMs are trained using information from pyramids of summary content units.	summary content units	SVMs	usage	{'e1': {'word': 'summary content units', 'word_index': [(9, 9)], 'id': 'P07-2015.4'}, 'e2': {'word': 'SVMs', 'word_index': [(1, 1)], 'id': 'P07-2015.3'}}	Several ENTITYOTHER are trained using information from pyramids of ENTITY .
Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.	performance	DUC-2005	compare	{'e1': {'word': 'performance', 'word_index': [(1, 1)], 'id': 'P07-2015.5'}, 'e2': {'word': 'DUC-2005', 'word_index': [(10, 10)], 'id': 'P07-2015.6'}}	Their ENTITY is compared with the best performing systems in ENTITYOTHER , using both ENTITYUNRELATED and ENTITYUNRELATED , an ENTITYUNRELATED for ENTITYUNRELATED .
Their performance is compared with the best performing systems in DUC-2005, using both ROUGE and autoPan, an automatic scoring method for pyramid evaluation.	automatic scoring method	pyramid evaluation	usage	{'e1': {'word': 'automatic scoring method', 'word_index': [(19, 19)], 'id': 'P07-2015.9'}, 'e2': {'word': 'pyramid evaluation', 'word_index': [(21, 21)], 'id': 'P07-2015.10'}}	Their ENTITYUNRELATED is compared with the best performing systems in ENTITYUNRELATED , using both ENTITYUNRELATED and ENTITYUNRELATED , an ENTITY for ENTITYOTHER .
We present a novel unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees.	unsupervised method	sentence compression	usage	{'e1': {'word': 'unsupervised method', 'word_index': [(4, 4)], 'id': 'W08-1105.1'}, 'e2': {'word': 'sentence compression', 'word_index': [(6, 6)], 'id': 'W08-1105.2'}}	We present a novel ENTITY for ENTITYOTHER which relies on a ENTITYUNRELATED and shortens ENTITYUNRELATED by removing ENTITYUNRELATED .
We demonstrate that the choice of the parser affects the performance of the system.	parser	performance	result	{'e1': {'word': 'parser', 'word_index': [(7, 7)], 'id': 'W08-1105.8'}, 'e2': {'word': 'performance', 'word_index': [(10, 10)], 'id': 'W08-1105.9'}}	We demonstrate that the choice of the ENTITY affects the ENTITYOTHER of the ENTITYUNRELATED .
We also apply the method to German and report the results of an evaluation with humans.	method	German	usage	{'e1': {'word': 'method', 'word_index': [(4, 4)], 'id': 'W08-1105.11'}, 'e2': {'word': 'German', 'word_index': [(6, 6)], 'id': 'W08-1105.12'}}	We also apply the ENTITY to ENTITYOTHER and report the results of an evaluation with humans .
Our approach even outperformed the hand coded system on NER in Spanish, and it achieved high accuracies in Portuguese.	NER	Spanish	usage	{'e1': {'word': 'NER', 'word_index': [(9, 9)], 'id': 'P05-2005.5'}, 'e2': {'word': 'Spanish', 'word_index': [(11, 11)], 'id': 'P05-2005.6'}}	Our approach even outperformed the hand coded system on ENTITY in ENTITYOTHER , and it achieved high ENTITYUNRELATED in ENTITYUNRELATED .
A karaka based approach to parsing of Indian languages is described.	parsing	Indian languages	usage	{'e1': {'word': 'parsing', 'word_index': [(3, 3)], 'id': 'C90-3005.2'}, 'e2': {'word': 'Indian languages', 'word_index': [(5, 5)], 'id': 'C90-3005.3'}}	A ENTITYUNRELATED to ENTITY of ENTITYOTHER is described .
It has been used for building a parser of Hindi for a prototype Machine Translation system.	parser	Hindi	usage	{'e1': {'word': 'parser', 'word_index': [(7, 7)], 'id': 'C90-3005.4'}, 'e2': {'word': 'Hindi', 'word_index': [(9, 9)], 'id': 'C90-3005.5'}}	It has been used for building a ENTITY of ENTITYOTHER for a ENTITYUNRELATED .
This paper presents our work on the detection of temporal information in web pages.	detection	web pages	usage	{'e1': {'word': 'detection', 'word_index': [(7, 7)], 'id': 'L08-1559.1'}, 'e2': {'word': 'web pages', 'word_index': [(11, 11)], 'id': 'L08-1559.3'}}	This paper presents our work on the ENTITY of ENTITYUNRELATED in ENTITYOTHER .
The pages examined within the scope of this study were taken from the tourism sector and the temporal information in question is thus particular to this area.	temporal information	pages	part_whole	{'e1': {'word': 'temporal information', 'word_index': [(17, 17)], 'id': 'L08-1559.5'}, 'e2': {'word': 'pages', 'word_index': [(1, 1)], 'id': 'L08-1559.4'}}	The ENTITYOTHER examined within the scope of this study were taken from the tourism sector and the ENTITY in question is thus particular to this area .
The differences that exist between extraction from plain textual data and extraction from the web are brought to light.	extraction	plain textual data	usage	{'e1': {'word': 'extraction', 'word_index': [(5, 5)], 'id': 'L08-1559.6'}, 'e2': {'word': 'plain textual data', 'word_index': [(7, 7)], 'id': 'L08-1559.7'}}	The differences that exist between ENTITY from ENTITYOTHER and ENTITYUNRELATED from the web are brought to light .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	patterns	symbolic approach	usage	{'e1': {'word': 'patterns', 'word_index': [(6, 6)], 'id': 'L08-1559.16'}, 'e2': {'word': 'symbolic approach', 'word_index': [(3, 3)], 'id': 'L08-1559.15'}}	We adopt a ENTITYOTHER relying on ENTITY and ENTITYUNRELATED for the ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED of ENTITYUNRELATED ; our method is based on the use of ENTITYUNRELATED .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	rules	detection	usage	{'e1': {'word': 'rules', 'word_index': [(8, 8)], 'id': 'L08-1559.17'}, 'e2': {'word': 'detection', 'word_index': [(11, 11)], 'id': 'L08-1559.18'}}	We adopt a ENTITYUNRELATED relying on ENTITYUNRELATED and ENTITY for the ENTITYOTHER , ENTITYUNRELATED and ENTITYUNRELATED of ENTITYUNRELATED ; our method is based on the use of ENTITYUNRELATED .
We adopt a symbolic approach relying on patterns and rules for the detection, extraction and annotation of temporal expressions; our method is based on the use of transducers.	annotation	temporal expressions	usage	{'e1': {'word': 'annotation', 'word_index': [(15, 15)], 'id': 'L08-1559.20'}, 'e2': {'word': 'temporal expressions', 'word_index': [(17, 17)], 'id': 'L08-1559.21'}}	We adopt a ENTITYUNRELATED relying on ENTITYUNRELATED and ENTITYUNRELATED for the ENTITYUNRELATED , ENTITYUNRELATED and ENTITY of ENTITYOTHER ; our method is based on the use of ENTITYUNRELATED .
The first release of the German Ph@ttSessionz speech database contains read and spontaneous speech from 864 adolescent speakers and is the largest database of its kind for German.	read and spontaneous speech	German Ph@ttSessionz speech database	part_whole	{'e1': {'word': 'read and spontaneous speech', 'word_index': [(7, 7)], 'id': 'L08-1196.2'}, 'e2': {'word': 'German Ph@ttSessionz speech database', 'word_index': [(5, 5)], 'id': 'L08-1196.1'}}	The first release of the ENTITYOTHER contains ENTITY from 864 ENTITYUNRELATED and is the largest ENTITYUNRELATED of its kind for ENTITYUNRELATED .
In this paper, we present a cross-sectional study of f0 measurements on this database.	cross-sectional study	f0 measurements	topic	{'e1': {'word': 'cross-sectional study', 'word_index': [(7, 7)], 'id': 'L08-1196.7'}, 'e2': {'word': 'f0 measurements', 'word_index': [(9, 9)], 'id': 'L08-1196.8'}}	In this paper , we present a ENTITY of ENTITYOTHER on this ENTITYUNRELATED .
Furthermore, it shows that on a perceptive mel-scale, there is little difference in the relative f0 variability for male and female speakers.	relative f0 variability	male and female speakers	model-feature	{'e1': {'word': 'relative f0 variability', 'word_index': [(16, 16)], 'id': 'L08-1196.11'}, 'e2': {'word': 'male and female speakers', 'word_index': [(18, 18)], 'id': 'L08-1196.12'}}	Furthermore , it shows that on a perceptive mel-scale , there is little difference in the ENTITY for ENTITYOTHER .
The study provides statistically reliable voice parameters of adolescent speakers for German.	voice parameters	adolescent speakers	model-feature	{'e1': {'word': 'voice parameters', 'word_index': [(5, 5)], 'id': 'L08-1196.17'}, 'e2': {'word': 'adolescent speakers', 'word_index': [(7, 7)], 'id': 'L08-1196.18'}}	The ENTITYUNRELATED provides statistically reliable ENTITY of ENTITYOTHER for ENTITYUNRELATED .
The results may contribute to making spoken dialog systems more robust by restricting user input to utterances with low f0 variability.	utterances	user input	part_whole	{'e1': {'word': 'utterances', 'word_index': [(13, 13)], 'id': 'L08-1196.22'}, 'e2': {'word': 'user input', 'word_index': [(11, 11)], 'id': 'L08-1196.21'}}	The results may contribute to making ENTITYUNRELATED more robust by restricting ENTITYOTHER to ENTITY with low ENTITYUNRELATED .
The platform will support researchers and engineers with well-developed and standardized resources and application tools thereby avoiding duplicate activities from scratch and amplifying overall effort on the domain.	standardized resources	platform	usage	{'e1': {'word': 'standardized resources', 'word_index': [(12, 12)], 'id': 'C96-2185.9'}, 'e2': {'word': 'platform', 'word_index': [(1, 1)], 'id': 'C96-2185.8'}}	The ENTITYOTHER will support researchers and engineers with well - developed and ENTITY and ENTITYUNRELATED thereby avoiding duplicate activities from scratch and amplifying overall effort on the domain .
We present in this article, as a part of aspectual operation system, a generation system of iterative expressions using a set of operators called iterative operators.	generation system	aspectual operation system	part_whole	{'e1': {'word': 'generation system', 'word_index': [(13, 13)], 'id': 'E83-1003.2'}, 'e2': {'word': 'aspectual operation system', 'word_index': [(10, 10)], 'id': 'E83-1003.1'}}	We present in this article , as a part of ENTITYOTHER , a ENTITY of ENTITYUNRELATED using a set of ENTITYUNRELATED called ENTITYUNRELATED .
The classification has been carried out especially in consideration of the durative / non-durative character of the denoted events and also in consideration of existence / non-existence of a culmination point (or a boundary) in the events.	durative / non-durative character	events	model-feature	{'e1': {'word': 'durative / non-durative character', 'word_index': [(11, 11)], 'id': 'E83-1003.11'}, 'e2': {'word': 'events', 'word_index': [(15, 15)], 'id': 'E83-1003.12'}}	The classification has been carried out especially in consideration of the ENTITY of the denoted ENTITYOTHER and also in consideration of existence / non-existence of a culmination point ( or a ENTITYUNRELATED ) in the ENTITYUNRELATED .
In this paper, we show that time-synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.	cross-word triphones	time-synchronous one-pass decoding	usage	{'e1': {'word': 'cross-word triphones', 'word_index': [(9, 9)], 'id': 'H94-1080.16'}, 'e2': {'word': 'time-synchronous one-pass decoding', 'word_index': [(7, 7)], 'id': 'H94-1080.15'}}	In this paper , we show that ENTITYOTHER using ENTITY and a ENTITYUNRELATED can be implemented using a dynamically built ENTITYUNRELATED .
In this paper, we show that time-synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.	tree-structured network	trigram language model	usage	{'e1': {'word': 'tree-structured network', 'word_index': [(20, 20)], 'id': 'H94-1080.18'}, 'e2': {'word': 'trigram language model', 'word_index': [(12, 12)], 'id': 'H94-1080.17'}}	In this paper , we show that ENTITYUNRELATED using ENTITYUNRELATED and a ENTITYOTHER can be implemented using a dynamically built ENTITY .
It was included in the HTK large vocabulary speech recognition system used for the 1993 ARPA WSJ evaluation and experimental results are presented for that task.	HTK large vocabulary speech recognition system	1993 ARPA WSJ evaluation	usage	{'e1': {'word': 'HTK large vocabulary speech recognition system', 'word_index': [(5, 5)], 'id': 'H94-1080.20'}, 'e2': {'word': '1993 ARPA WSJ evaluation', 'word_index': [(9, 9)], 'id': 'H94-1080.21'}}	It was included in the ENTITY used for the ENTITYOTHER and experimental results are presented for that task .
Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model.	parallel bi-lingual data	statistical machine translation systems	usage	{'e1': {'word': 'parallel bi-lingual data', 'word_index': [(6, 6)], 'id': 'D08-1090.2'}, 'e2': {'word': 'statistical machine translation systems', 'word_index': [(2, 2)], 'id': 'D08-1090.1'}}	Traditionally , ENTITYOTHER have relied on ENTITY to train a ENTITYUNRELATED .
While bi-lingual parallel data are expensive to generate, monolingual data are relatively common.	bi-lingual parallel data	monolingual data	compare	{'e1': {'word': 'bi-lingual parallel data', 'word_index': [(1, 1)], 'id': 'D08-1090.4'}, 'e2': {'word': 'monolingual data', 'word_index': [(7, 7)], 'id': 'D08-1090.5'}}	While ENTITY are expensive to generate , ENTITYOTHER are relatively common .
Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language.	monolingual data	language model	usage	{'e1': {'word': 'monolingual data', 'word_index': [(1, 1)], 'id': 'D08-1090.6'}, 'e2': {'word': 'language model', 'word_index': [(13, 13)], 'id': 'D08-1090.7'}}	Yet ENTITY have been under-utilized , having been used primarily for training a ENTITYOTHER in the ENTITYUNRELATED .
This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories.	monolingual target data	statistical machine translation system	usage	{'e1': {'word': 'monolingual target data', 'word_index': [(8, 8)], 'id': 'D08-1090.9'}, 'e2': {'word': 'statistical machine translation system', 'word_index': [(15, 15)], 'id': 'D08-1090.10'}}	This paper describes a novel method for utilizing ENTITY to improve the performance of a ENTITYOTHER on news stories .
For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to thesource documents.	documents	source documents	compare	{'e1': {'word': 'documents', 'word_index': [(18, 18)], 'id': 'D08-1090.18'}, 'e2': {'word': 'source documents', 'word_index': [(25, 25)], 'id': 'D08-1090.19'}}	For every ENTITYUNRELATED that is to be translated , a large ENTITYUNRELATED in the ENTITYUNRELATED is searched for ENTITY that might be comparable to the ENTITYOTHER .
These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document.	documents	MT system	usage	{'e1': {'word': 'documents', 'word_index': [(1, 1)], 'id': 'D08-1090.20'}, 'e2': {'word': 'MT system', 'word_index': [(8, 8)], 'id': 'D08-1090.21'}}	These ENTITY are then used to adapt the ENTITYOTHER to increase the probability of generating texts that resemble the ENTITYUNRELATED .
Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system.	language and translation models	baseline system	compare	{'e1': {'word': 'language and translation models', 'word_index': [(7, 7)], 'id': 'D08-1090.23'}, 'e2': {'word': 'baseline system', 'word_index': [(13, 13)], 'id': 'D08-1090.24'}}	Experimental results obtained by adapting both the ENTITY show substantial gains over the ENTITYOTHER .
This paper describes an unsupervised knowledge-lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus.	ambiguous word	corpus	part_whole	{'e1': {'word': 'ambiguous word', 'word_index': [(15, 15)], 'id': 'E06-2007.3'}, 'e2': {'word': 'corpus', 'word_index': [(21, 21)], 'id': 'E06-2007.4'}}	This paper describes an ENTITYUNRELATED for automatically determining the number of ENTITYUNRELATED in which an ENTITY is used in a large ENTITYOTHER .
This paper describes the Unisys MUC-3 text understanding system, a system based upon a three-tiered approach to text processing in which a powerful knowledge-based form of information retrieval plays a central role.	three-tiered approach	system	usage	{'e1': {'word': 'three-tiered approach', 'word_index': [(11, 11)], 'id': 'M91-1032.3'}, 'e2': {'word': 'system', 'word_index': [(7, 7)], 'id': 'M91-1032.2'}}	This paper describes the ENTITYUNRELATED , a ENTITYOTHER based upon a ENTITY to ENTITYUNRELATED in which a powerful ENTITYUNRELATED of ENTITYUNRELATED plays a central role .
A decision was made to focus on the development of a knowledge-based information retrieval component, and this precluded the integration of Pundit into the prototype.	Pundit	prototype	part_whole	{'e1': {'word': 'Pundit', 'word_index': [(19, 19)], 'id': 'M91-1032.23'}, 'e2': {'word': 'prototype', 'word_index': [(22, 22)], 'id': 'M91-1032.24'}}	A decision was made to focus on the development of a ENTITYUNRELATED , and this precluded the integration of ENTITY into the ENTITYOTHER .
ARBITER is a Prolog program that extracts assertions about macromolecular binding relationships from biomedical text.	macromolecular binding relationships	biomedical text	part_whole	{'e1': {'word': 'macromolecular binding relationships', 'word_index': [(8, 8)], 'id': 'A00-1026.3'}, 'e2': {'word': 'biomedical text', 'word_index': [(10, 10)], 'id': 'A00-1026.4'}}	ENTITYUNRELATED is a ENTITYUNRELATED that extracts assertions about ENTITY from ENTITYOTHER .
After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted.	ARBITER	MEDLINE abstracts	usage	{'e1': {'word': 'ARBITER', 'word_index': [(6, 6)], 'id': 'A00-1026.8'}, 'e2': {'word': 'MEDLINE abstracts', 'word_index': [(15, 15)], 'id': 'A00-1026.9'}}	After discussing a formal evaluation of ENTITY , we report on its application to 491,000 ENTITYOTHER , during which almost 25,000 ENTITYUNRELATED suitable for entry into a ENTITYUNRELATED of ENTITYUNRELATED were extracted .
After discussing a formal evaluation of ARBITER, we report on its application to 491,000 MEDLINE abstracts, during which almost 25,000 binding relationships suitable for entry into a database of macro-molecular function were extracted.	macro-molecular function	database	part_whole	{'e1': {'word': 'macro-molecular function', 'word_index': [(29, 29)], 'id': 'A00-1026.12'}, 'e2': {'word': 'database', 'word_index': [(27, 27)], 'id': 'A00-1026.11'}}	After discussing a formal evaluation of ENTITYUNRELATED , we report on its application to 491,000 ENTITYUNRELATED , during which almost 25,000 ENTITYUNRELATED suitable for entry into a ENTITYOTHER of ENTITY were extracted .
The resolution of lexical ambiguity is important for most natural language processing tasks, and a range of computational techniques have been proposed for its solution.	lexical ambiguity	natural language processing tasks	part_whole	{'e1': {'word': 'lexical ambiguity', 'word_index': [(3, 3)], 'id': 'H92-1046.2'}, 'e2': {'word': 'natural language processing tasks', 'word_index': [(8, 8)], 'id': 'H92-1046.3'}}	The ENTITYUNRELATED of ENTITY is important for most ENTITYOTHER , and a range of ENTITYUNRELATED have been proposed for its solution .
In this paper, we describe a method for lexical disambiguation of text using the definitions in a machine-readable dictionary together with the technique of simulated annealing.	definitions	lexical disambiguation	usage	{'e1': {'word': 'definitions', 'word_index': [(14, 14)], 'id': 'H92-1046.7'}, 'e2': {'word': 'lexical disambiguation', 'word_index': [(9, 9)], 'id': 'H92-1046.5'}}	In this paper , we describe a method for ENTITYOTHER of ENTITYUNRELATED using the ENTITY in a ENTITYUNRELATED together with the technique of ENTITYUNRELATED .
Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.	hand coding	lexical entries	usage	{'e1': {'word': 'hand coding', 'word_index': [(24, 24)], 'id': 'H92-1046.23'}, 'e2': {'word': 'lexical entries', 'word_index': [(26, 26)], 'id': 'H92-1046.24'}}	Our initial results on a sample set of 50 ENTITYUNRELATED are comparable to those of other ENTITYUNRELATED , and the fully ENTITYUNRELATED requires no ENTITY of ENTITYOTHER , or ENTITYUNRELATED of ENTITYUNRELATED .
Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.	hand tagging	text	usage	{'e1': {'word': 'hand tagging', 'word_index': [(29, 29)], 'id': 'H92-1046.25'}, 'e2': {'word': 'text', 'word_index': [(31, 31)], 'id': 'H92-1046.26'}}	Our initial results on a sample set of 50 ENTITYUNRELATED are comparable to those of other ENTITYUNRELATED , and the fully ENTITYUNRELATED requires no ENTITYUNRELATED of ENTITYUNRELATED , or ENTITY of ENTITYOTHER .
To date, this array of formal and natural language processing technologies has been used to perform mass changes to legacy textual databases and to facilitate user interfacing to relational databases and software applications.	formal and natural language processing technologies	legacy textual databases	usage	{'e1': {'word': 'formal and natural language processing technologies', 'word_index': [(6, 6)], 'id': 'W97-0909.3'}, 'e2': {'word': 'legacy textual databases', 'word_index': [(15, 15)], 'id': 'W97-0909.4'}}	To date , this array of ENTITY has been used to perform mass changes to ENTITYOTHER and to facilitate ENTITYUNRELATED to ENTITYUNRELATED and ENTITYUNRELATED .
We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations.	Rhetorical Structure trees	parallel corpus	part_whole	{'e1': {'word': 'Rhetorical Structure trees', 'word_index': [(10, 10)], 'id': 'W00-1403.3'}, 'e2': {'word': 'parallel corpus', 'word_index': [(8, 8)], 'id': 'W00-1403.2'}}	We present ENTITYUNRELATED work aimed at constructing a ENTITYOTHER of ENTITY for a collection of ENTITYUNRELATED and their corresponding ENTITYUNRELATED .
The approach to knowledge representation taken in a multi-modal multi-domain dialogue system - SmartKom - is presented.	knowledge representation	multi-modal multi-domain dialogue system - SmartKom -	usage	{'e1': {'word': 'knowledge representation', 'word_index': [(3, 3)], 'id': 'W03-0903.1'}, 'e2': {'word': 'multi-modal multi-domain dialogue system - SmartKom -', 'word_index': [(7, 7)], 'id': 'W03-0903.2'}}	The approach to ENTITY taken in a ENTITYOTHER is presented .
This paper presents intial work on a system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admission Test (LSAT) and the analytic section of the Graduate Record Exam (GRE).	logic puzzles	Law School Admission Test (LSAT)	part_whole	{'e1': {'word': 'logic puzzles', 'word_index': [(20, 20)], 'id': 'W04-0902.5'}, 'e2': {'word': 'Law School Admission Test (LSAT)', 'word_index': [(27, 27)], 'id': 'W04-0902.6'}}	This paper presents intial work on a ENTITYUNRELATED that bridges from ENTITYUNRELATED to ENTITYUNRELATED and ENTITYUNRELATED , focusing on solving ENTITY drawn from sources such as the ENTITYOTHER and the analytic section of the ENTITYUNRELATED .
We highlight key challenges, and discuss the representations and performance of the prototype system.	performance	prototype system	model-feature	{'e1': {'word': 'performance', 'word_index': [(10, 10)], 'id': 'W04-0902.9'}, 'e2': {'word': 'prototype system', 'word_index': [(13, 13)], 'id': 'W04-0902.10'}}	We highlight key challenges , and discuss the ENTITYUNRELATED and ENTITY of the ENTITYOTHER .
In this paper, a new parsing model is proposed to formulate the complete chunking problem as a series of boundary detection subtasks.	chunking problem	parsing model	model-feature	{'e1': {'word': 'chunking problem', 'word_index': [(13, 13)], 'id': 'W06-0113.11'}, 'e2': {'word': 'parsing model', 'word_index': [(6, 6)], 'id': 'W06-0113.10'}}	In this paper , a new ENTITYOTHER is proposed to formulate the complete ENTITY as a series of ENTITYUNRELATED subtasks .
By applying SVM algorithm to these subtasks, we have achieved the best F-Score of 76.56% and 82.26% respectively.	SVM algorithm	F-Score	result	{'e1': {'word': 'SVM algorithm', 'word_index': [(2, 2)], 'id': 'W06-0113.17'}, 'e2': {'word': 'F-Score', 'word_index': [(12, 12)], 'id': 'W06-0113.18'}}	By applying ENTITY to these subtasks , we have achieved the best ENTITYOTHER of 76.56 % and 82.26 % respectively .
In this paper, we introduce a WordNet-based measure of semantic relatedness by combining the structure and content of WordNet with co-occurrence information derived from raw text.	co-occurrence information	raw text	part_whole	{'e1': {'word': 'co-occurrence information', 'word_index': [(19, 19)], 'id': 'W06-2501.4'}, 'e2': {'word': 'raw text', 'word_index': [(22, 22)], 'id': 'W06-2501.5'}}	In this paper , we introduce a ENTITYUNRELATED of ENTITYUNRELATED by combining the structure and content of ENTITYUNRELATED with ENTITY derived from ENTITYOTHER .
We use the co-occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet.	gloss vectors	concept	model-feature	{'e1': {'word': 'gloss vectors', 'word_index': [(10, 10)], 'id': 'W06-2501.8'}, 'e2': {'word': 'concept', 'word_index': [(14, 14)], 'id': 'W06-2501.9'}}	We use the ENTITYUNRELATED along with the ENTITYUNRELATED to build ENTITY corresponding to each ENTITYOTHER in ENTITYUNRELATED .
Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors.	Numeric scores of relatedness	concepts	model-feature	{'e1': {'word': 'Numeric scores of relatedness', 'word_index': [(0, 0)], 'id': 'W06-2501.11'}, 'e2': {'word': 'concepts', 'word_index': [(7, 7)], 'id': 'W06-2501.12'}}	ENTITY are assigned to a pair of ENTITYOTHER by measuring the ENTITYUNRELATED between their respective ENTITYUNRELATED .
We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness.	semantic relatedness	word sense disambiguation algorithm	usage	{'e1': {'word': 'semantic relatedness', 'word_index': [(28, 28)], 'id': 'W06-2501.18'}, 'e2': {'word': 'word sense disambiguation algorithm', 'word_index': [(24, 24)], 'id': 'W06-2501.17'}}	We show that this ENTITYUNRELATED compares favorably to other measures with respect to ENTITYUNRELATED , and that it performs well when used in a ENTITYOTHER that relies on ENTITY .
In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the cooccurrence information.	cooccurrence information	plain text corpus	part_whole	{'e1': {'word': 'cooccurrence information', 'word_index': [(20, 20)], 'id': 'W06-2501.23'}, 'e2': {'word': 'plain text corpus', 'word_index': [(13, 13)], 'id': 'W06-2501.22'}}	In addition , it can be adapted to different domains , since any ENTITYOTHER can be used to derive the ENTITY .
This paper describes our system as used in the RTE3 task.	system	RTE3 task	usage	{'e1': {'word': 'system', 'word_index': [(4, 4)], 'id': 'W07-1403.1'}, 'e2': {'word': 'RTE3 task', 'word_index': [(9, 9)], 'id': 'W07-1403.2'}}	This paper describes our ENTITY as used in the ENTITYOTHER .
The system maps premise and hypothesis pairs into an abstract knowledge representation (AKR) and then performs entailment and contradiction detection (ecd) on the resulting AKRs.	abstract knowledge representation (AKR)	premise and hypothesis pairs	model-feature	{'e1': {'word': 'abstract knowledge representation (AKR)', 'word_index': [(6, 6)], 'id': 'W07-1403.5'}, 'e2': {'word': 'premise and hypothesis pairs', 'word_index': [(3, 3)], 'id': 'W07-1403.4'}}	The ENTITYUNRELATED maps ENTITYOTHER into an ENTITY and then performs ENTITYUNRELATED on the resulting ENTITYUNRELATED .
Two versions of ECD were used in RTE3, one with strict ECD and one with looser ECD.	ECD	RTE3	usage	{'e1': {'word': 'ECD', 'word_index': [(3, 3)], 'id': 'W07-1403.8'}, 'e2': {'word': 'RTE3', 'word_index': [(7, 7)], 'id': 'W07-1403.9'}}	Two versions of ENTITY were used in ENTITYOTHER , one with ENTITYUNRELATED and one with ENTITYUNRELATED .
We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax.	natural language strings	semantics	model-feature	{'e1': {'word': 'natural language strings', 'word_index': [(14, 14)], 'id': 'P07-1105.3'}, 'e2': {'word': 'semantics', 'word_index': [(18, 18)], 'id': 'P07-1105.4'}}	We propose a new ENTITYUNRELATED that learns a ENTITYUNRELATED from a small number of ENTITY annotated with their ENTITYOTHER , along with ENTITYUNRELATED about ENTITYUNRELATED .
In this paper, we propose a novel graph based sentence ranking algorithm, namely PNR2, for update summarization.	graph based sentence ranking algorithm	update summarization	usage	{'e1': {'word': 'graph based sentence ranking algorithm', 'word_index': [(8, 8)], 'id': 'C08-1062.7'}, 'e2': {'word': 'update summarization', 'word_index': [(14, 14)], 'id': 'C08-1062.9'}}	In this paper , we propose a novel ENTITY , namely ENTITYUNRELATED , for ENTITYOTHER .
This paper discusses the application of the Expectation-Maximization (EM) clustering algorithm to the task of Chinese verb sense discrimination.	Expectation-Maximization (EM) clustering algorithm	Chinese verb sense discrimination	usage	{'e1': {'word': 'Expectation-Maximization (EM) clustering algorithm', 'word_index': [(7, 7)], 'id': 'P04-1038.1'}, 'e2': {'word': 'Chinese verb sense discrimination', 'word_index': [(12, 12)], 'id': 'P04-1038.2'}}	This paper discusses the application of the ENTITY to the task of ENTITYOTHER .
The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.	rich linguistic features	model	usage	{'e1': {'word': 'rich linguistic features', 'word_index': [(3, 3)], 'id': 'P04-1038.4'}, 'e2': {'word': 'model', 'word_index': [(1, 1)], 'id': 'P04-1038.3'}}	The ENTITYOTHER utilized ENTITY that capture ENTITYUNRELATED of the ENTITYUNRELATED .
The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.	predicate-argument structure information	target verbs	model-feature	{'e1': {'word': 'predicate-argument structure information', 'word_index': [(6, 6)], 'id': 'P04-1038.5'}, 'e2': {'word': 'target verbs', 'word_index': [(9, 9)], 'id': 'P04-1038.6'}}	The ENTITYUNRELATED utilized ENTITYUNRELATED that capture ENTITY of the ENTITYOTHER .
A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.	semantic taxonomy	Chinese nouns	model-feature	{'e1': {'word': 'semantic taxonomy', 'word_index': [(1, 1)], 'id': 'P04-1038.7'}, 'e2': {'word': 'Chinese nouns', 'word_index': [(3, 3)], 'id': 'P04-1038.8'}}	A ENTITY for ENTITYOTHER , which was built semi-automatically based on two ENTITYUNRELATED , was used to provide ENTITYUNRELATED for the ENTITYUNRELATED .
A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.	semantic features	model	usage	{'e1': {'word': 'semantic features', 'word_index': [(18, 18)], 'id': 'P04-1038.10'}, 'e2': {'word': 'model', 'word_index': [(21, 21)], 'id': 'P04-1038.11'}}	A ENTITYUNRELATED for ENTITYUNRELATED , which was built semi-automatically based on two ENTITYUNRELATED , was used to provide ENTITY for the ENTITYOTHER .
We further enhanced the model with certain fine-grained semantic categories called lexical sets.	fine-grained semantic categories	model	usage	{'e1': {'word': 'fine-grained semantic categories', 'word_index': [(7, 7)], 'id': 'P04-1038.20'}, 'e2': {'word': 'model', 'word_index': [(4, 4)], 'id': 'P04-1038.19'}}	We further enhanced the ENTITYOTHER with certain ENTITY called ENTITYUNRELATED .
Our results indicate that these lexical sets improve the model's performance for the three most challenging verbs chosen from the first set of experiments.	lexical sets	model	result	{'e1': {'word': 'lexical sets', 'word_index': [(5, 5)], 'id': 'P04-1038.22'}, 'e2': {'word': 'model', 'word_index': [(8, 8)], 'id': 'P04-1038.23'}}	Our results indicate that these ENTITY improve the ENTITYOTHER 's performance for the three most challenging ENTITYUNRELATED chosen from the first set of experiments .
This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar.	dependency structure grammar	speech recognition and understanding	usage	{'e1': {'word': 'dependency structure grammar', 'word_index': [(10, 10)], 'id': 'C88-1082.3'}, 'e2': {'word': 'speech recognition and understanding', 'word_index': [(7, 7)], 'id': 'C88-1082.2'}}	This paper proposes an efficient ENTITYUNRELATED for ENTITYOTHER using a ENTITY .
After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships.	phoneme recognition	phrase recognition	usage	{'e1': {'word': 'phoneme recognition', 'word_index': [(6, 6)], 'id': 'C88-1082.9'}, 'e2': {'word': 'phrase recognition', 'word_index': [(3, 3)], 'id': 'C88-1082.8'}}	After ENTITYUNRELATED and ENTITYOTHER based on ENTITY , the ENTITYUNRELATED extracts the ENTITYUNRELATED with the best ENTITYUNRELATED taking account of the ENTITYUNRELATED of ENTITYUNRELATED and the ENTITYUNRELATED of the ENTITYUNRELATED .
A fast parsing algorithm using breadth-first search is also proposed.	breadth-first search	parsing algorithm	usage	{'e1': {'word': 'breadth-first search', 'word_index': [(4, 4)], 'id': 'C88-1082.18'}, 'e2': {'word': 'parsing algorithm', 'word_index': [(2, 2)], 'id': 'C88-1082.17'}}	A fast ENTITYOTHER using ENTITY is also proposed .
The predictor pre-selects the phrase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing.	transition rules	predictor	usage	{'e1': {'word': 'transition rules', 'word_index': [(6, 6)], 'id': 'C88-1082.21'}, 'e2': {'word': 'predictor', 'word_index': [(1, 1)], 'id': 'C88-1082.19'}}	The ENTITYOTHER pre-selects the ENTITYUNRELATED using ENTITY combined with a ENTITYUNRELATED to reduce the amount of ENTITYUNRELATED .
The experimental results show that it greatly increases the accuracy of speech recognitions, and the breadth-first parsing algorithm and predictor increase processing speed.	predictor	processing speed	result	{'e1': {'word': 'predictor', 'word_index': [(17, 17)], 'id': 'C88-1082.29'}, 'e2': {'word': 'processing speed', 'word_index': [(19, 19)], 'id': 'C88-1082.30'}}	The experimental results show that it greatly increases the ENTITYUNRELATED of ENTITYUNRELATED , and the ENTITYUNRELATED and ENTITY increase ENTITYOTHER .
In the EU-funded project, QALL-ME, a domain-specific ontology was developed and applied for question answering in the domain of tourism, along with the assistance of two upper ontologies for concept expansion and reasoning.	domain-specific ontology	question answering	usage	{'e1': {'word': 'domain-specific ontology', 'word_index': [(10, 10)], 'id': 'L08-1178.6'}, 'e2': {'word': 'question answering', 'word_index': [(16, 16)], 'id': 'L08-1178.7'}}	In the EU - funded project , ENTITYUNRELATED , a ENTITY was developed and applied for ENTITYOTHER in the domain of tourism , along with the assistance of two upper ENTITYUNRELATED for ENTITYUNRELATED .
The design of the ontology is presented in the paper, and a semi-automatic alignment procedure is described with some alignment results given as well.	semi-automatic alignment procedure	alignment results	result	{'e1': {'word': 'semi-automatic alignment procedure', 'word_index': [(13, 13)], 'id': 'L08-1178.14'}, 'e2': {'word': 'alignment results', 'word_index': [(18, 18)], 'id': 'L08-1178.15'}}	The design of the ENTITYUNRELATED is presented in the paper , and a ENTITY is described with some ENTITYOTHER given as well .
Furthermore, the aligned ontology was used to semantically annotate original data obtained from the tourism web sites and natural language questions.	natural language questions	data	part_whole	{'e1': {'word': 'natural language questions', 'word_index': [(18, 18)], 'id': 'L08-1178.18'}, 'e2': {'word': 'data', 'word_index': [(10, 10)], 'id': 'L08-1178.17'}}	Furthermore , the ENTITYUNRELATED was used to semantically annotate original ENTITYOTHER obtained from the tourism web sites and ENTITY .
The storage schema of the annotated data and the data access method for retrieving answers from the annotated data are also reported in the paper.	data access method	annotated data	usage	{'e1': {'word': 'data access method', 'word_index': [(7, 7)], 'id': 'L08-1178.21'}, 'e2': {'word': 'annotated data', 'word_index': [(13, 13)], 'id': 'L08-1178.22'}}	The ENTITYUNRELATED of the ENTITYUNRELATED and the ENTITY for retrieving answers from the ENTITYOTHER are also reported in the paper .
In particular we discuss a natural language generation system that is composed of SPoT, a trainable sentence planner, and FERGUS, a stochastic surface realizer.	SPoT	natural language generation system	part_whole	{'e1': {'word': 'SPoT', 'word_index': [(10, 10)], 'id': 'C02-1138.9'}, 'e2': {'word': 'natural language generation system', 'word_index': [(5, 5)], 'id': 'C02-1138.8'}}	In particular we discuss a ENTITYOTHER that is composed of ENTITY , a trainable ENTITYUNRELATED , and ENTITYUNRELATED .
We show how these stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system.	NLG components	real-time dialog system	part_whole	{'e1': {'word': 'NLG components', 'word_index': [(27, 27)], 'id': 'C02-1138.13'}, 'e2': {'word': 'real-time dialog system', 'word_index': [(33, 33)], 'id': 'C02-1138.14'}}	We show how these ENTITYUNRELATED can be made to work together , that they can be ported to new domains with apparent ease , and that such ENTITY can be integrated in a ENTITYOTHER .
In the current work, we produce a manual segmentation of laughter in a large corpus of interactive multi-party seminars, which promises to be a valuable resource for acoustic modeling purposes.	interactive multi-party seminars	corpus	part_whole	{'e1': {'word': 'interactive multi-party seminars', 'word_index': [(16, 16)], 'id': 'L08-1016.10'}, 'e2': {'word': 'corpus', 'word_index': [(14, 14)], 'id': 'L08-1016.9'}}	In the current work , we produce a ENTITYUNRELATED of laughter in a large ENTITYOTHER of ENTITY , which promises to be a valuable resource for ENTITYUNRELATED purposes .
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further.	grammar PCFG*	extended GLR parsing algorithm	usage	{'e1': {'word': 'grammar PCFG*', 'word_index': [(6, 6)], 'id': 'C02-2028.2'}, 'e2': {'word': 'extended GLR parsing algorithm', 'word_index': [(4, 4)], 'id': 'C02-2028.1'}}	This paper presents an ENTITYOTHER with ENTITY that is based on ENTITYUNRELATED and extends it further .
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further.	Tomita's GLR parsing algorithm	grammar PCFG*	usage	"{'e1': {'word': ""Tomita's GLR parsing algorithm"", 'word_index': [(11, 11)], 'id': 'C02-2028.3'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(6, 6)], 'id': 'C02-2028.2'}}"	This paper presents an ENTITYUNRELATED with ENTITYOTHER that is based on ENTITY and extends it further .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	PCFG	grammar PCFG*	usage	{'e1': {'word': 'PCFG', 'word_index': [(10, 10)], 'id': 'C02-2028.5'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(5, 5)], 'id': 'C02-2028.4'}}	We also define a new ENTITYOTHER that is based on ENTITY and assigns not only ENTITYUNRELATED but also ENTITYUNRELATED associated with each ENTITYUNRELATED .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	frequency	rule	model-feature	{'e1': {'word': 'frequency', 'word_index': [(18, 18)], 'id': 'C02-2028.7'}, 'e2': {'word': 'rule', 'word_index': [(22, 22)], 'id': 'C02-2028.8'}}	We also define a new ENTITYUNRELATED that is based on ENTITYUNRELATED and assigns not only ENTITYUNRELATED but also ENTITY associated with each ENTITYOTHER .
So our syntactic parsing system is implemented based on rule-based approach and statistics approach.	rule-based approach	syntactic parsing system	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(7, 7)], 'id': 'C02-2028.10'}, 'e2': {'word': 'syntactic parsing system', 'word_index': [(2, 2)], 'id': 'C02-2028.9'}}	So our ENTITYOTHER is implemented based on ENTITY and ENTITYUNRELATED .
In this paper, we discuss lemma identification in Japanese morphological analysis, which is crucial for a proper formulation of morphological analysis that benefits not only NLP researchers but also corpus linguists.	lemma identification	Japanese morphological analysis	usage	{'e1': {'word': 'lemma identification', 'word_index': [(6, 6)], 'id': 'L08-1535.1'}, 'e2': {'word': 'Japanese morphological analysis', 'word_index': [(8, 8)], 'id': 'L08-1535.2'}}	In this paper , we discuss ENTITY in ENTITYOTHER , which is crucial for a proper formulation of ENTITYUNRELATED that benefits not only NLP researchers but also corpus linguists .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	words	vocabulary	part_whole	{'e1': {'word': 'words', 'word_index': [(14, 14)], 'id': 'L08-1535.8'}, 'e2': {'word': 'vocabulary', 'word_index': [(9, 9)], 'id': 'L08-1535.6'}}	Since ENTITYUNRELATED often have variation in ENTITYUNRELATED and the ENTITYOTHER of ENTITYUNRELATED consists of ENTITY of several different origins , it sometimes happens that more than one ENTITYUNRELATED corresponds to the same ENTITYUNRELATED and that a single ENTITYUNRELATED corresponds to two or more ENTITYUNRELATED with different ENTITYUNRELATED and / or ENTITYUNRELATED .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	writing form	lemma	model-feature	{'e1': {'word': 'writing form', 'word_index': [(27, 27)], 'id': 'L08-1535.9'}, 'e2': {'word': 'lemma', 'word_index': [(32, 32)], 'id': 'L08-1535.10'}}	Since ENTITYUNRELATED often have variation in ENTITYUNRELATED and the ENTITYUNRELATED of ENTITYUNRELATED consists of ENTITYUNRELATED of several different origins , it sometimes happens that more than one ENTITY corresponds to the same ENTITYOTHER and that a single ENTITYUNRELATED corresponds to two or more ENTITYUNRELATED with different ENTITYUNRELATED and / or ENTITYUNRELATED .
Since Japanese words often have variation in orthography and the vocabulary of Japanese consists of words of several different origins, it sometimes happens that more than one writing form corresponds to the same lemma and that a single writing form corresponds to two or more lemmas with different readings and/or meanings.	writing form	lemmas	model-feature	{'e1': {'word': 'writing form', 'word_index': [(37, 37)], 'id': 'L08-1535.11'}, 'e2': {'word': 'lemmas', 'word_index': [(43, 43)], 'id': 'L08-1535.12'}}	Since ENTITYUNRELATED often have variation in ENTITYUNRELATED and the ENTITYUNRELATED of ENTITYUNRELATED consists of ENTITYUNRELATED of several different origins , it sometimes happens that more than one ENTITYUNRELATED corresponds to the same ENTITYUNRELATED and that a single ENTITY corresponds to two or more ENTITYOTHER with different ENTITYUNRELATED and / or ENTITYUNRELATED .
The mapping from a writing form onto a lemma is important in linguistic analysis of corpora.	linguistic analysis	corpora	topic	{'e1': {'word': 'linguistic analysis', 'word_index': [(11, 11)], 'id': 'L08-1535.17'}, 'e2': {'word': 'corpora', 'word_index': [(13, 13)], 'id': 'L08-1535.18'}}	The mapping from a ENTITYUNRELATED onto a ENTITYUNRELATED is important in ENTITY of ENTITYOTHER .
The current study focuses on disambiguation of heteronyms, words with the same writing form but with different word forms.	disambiguation	heteronyms	usage	{'e1': {'word': 'disambiguation', 'word_index': [(5, 5)], 'id': 'L08-1535.19'}, 'e2': {'word': 'heteronyms', 'word_index': [(7, 7)], 'id': 'L08-1535.20'}}	The current study focuses on ENTITY of ENTITYOTHER , ENTITYUNRELATED with the same ENTITYUNRELATED but with different ENTITYUNRELATED .
To resolve heteronym ambiguity, we make use of goshu information, the classification of words based on their origin.	origin	words	model-feature	{'e1': {'word': 'origin', 'word_index': [(17, 17)], 'id': 'L08-1535.28'}, 'e2': {'word': 'words', 'word_index': [(13, 13)], 'id': 'L08-1535.27'}}	To resolve ENTITYUNRELATED , we make use of ENTITYUNRELATED , the ENTITYUNRELATED of ENTITYOTHER based on their ENTITY .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	words	goshu classes	part_whole	{'e1': {'word': 'words', 'word_index': [(5, 5)], 'id': 'L08-1535.29'}, 'e2': {'word': 'goshu classes', 'word_index': [(8, 8)], 'id': 'L08-1535.30'}}	Founded on the fact that ENTITY of some ENTITYOTHER are more likely to combine into ENTITYUNRELATED than ENTITYUNRELATED of other ENTITYUNRELATED , we employ a ENTITYUNRELATED based on ENTITYUNRELATED using ENTITYUNRELATED .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	words	classes	part_whole	{'e1': {'word': 'words', 'word_index': [(17, 17)], 'id': 'L08-1535.32'}, 'e2': {'word': 'classes', 'word_index': [(20, 20)], 'id': 'L08-1535.33'}}	Founded on the fact that ENTITYUNRELATED of some ENTITYUNRELATED are more likely to combine into ENTITYUNRELATED than ENTITY of other ENTITYOTHER , we employ a ENTITYUNRELATED based on ENTITYUNRELATED using ENTITYUNRELATED .
Founded on the fact that words of some goshu classes are more likely to combine into compound words than words of other classes, we employ a statistical model based on CRFs using goshu information.	CRFs	statistical model	usage	{'e1': {'word': 'CRFs', 'word_index': [(28, 28)], 'id': 'L08-1535.35'}, 'e2': {'word': 'statistical model', 'word_index': [(25, 25)], 'id': 'L08-1535.34'}}	Founded on the fact that ENTITYUNRELATED of some ENTITYUNRELATED are more likely to combine into ENTITYUNRELATED than ENTITYUNRELATED of other ENTITYUNRELATED , we employ a ENTITYOTHER based on ENTITY using ENTITYUNRELATED .
Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.	goshu information	performance	result	{'e1': {'word': 'goshu information', 'word_index': [(6, 6)], 'id': 'L08-1535.38'}, 'e2': {'word': 'performance', 'word_index': [(10, 10)], 'id': 'L08-1535.39'}}	ENTITYUNRELATED show that the use of ENTITY considerably improves the ENTITYOTHER of ENTITYUNRELATED and ENTITYUNRELATED , suggesting that goshu ENTITYUNRELATED solves the ENTITYUNRELATED very effectively .
Experimental results show that the use of goshu information considerably improves the performance of heteronym disambiguation and lemma identification, suggesting that goshu information solves the lemma identification task very effectively.	information	lemma identification task	usage	{'e1': {'word': 'information', 'word_index': [(19, 19)], 'id': 'L08-1535.42'}, 'e2': {'word': 'lemma identification task', 'word_index': [(22, 22)], 'id': 'L08-1535.43'}}	ENTITYUNRELATED show that the use of ENTITYUNRELATED considerably improves the ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED , suggesting that goshu ENTITY solves the ENTITYOTHER very effectively .
An event detection algorithm identifies the collocations that may cause an event in a specific timestamp.	event detection algorithm	collocations	usage	{'e1': {'word': 'event detection algorithm', 'word_index': [(1, 1)], 'id': 'L08-1003.6'}, 'e2': {'word': 'collocations', 'word_index': [(4, 4)], 'id': 'L08-1003.7'}}	An ENTITY identifies the ENTITYOTHER that may cause an event in a specific timestamp .
An event summarization algorithm retrieves a set of collocations which describe an event.	event summarization algorithm	collocations	usage	{'e1': {'word': 'event summarization algorithm', 'word_index': [(1, 1)], 'id': 'L08-1003.8'}, 'e2': {'word': 'collocations', 'word_index': [(6, 6)], 'id': 'L08-1003.9'}}	An ENTITY retrieves a set of ENTITYOTHER which describe an event .
We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis (LSA) to predict team performance.	Latent Semantic Analysis (LSA)	tagging	usage	{'e1': {'word': 'Latent Semantic Analysis (LSA)', 'word_index': [(11, 11)], 'id': 'N04-4025.2'}, 'e2': {'word': 'tagging', 'word_index': [(7, 7)], 'id': 'N04-4025.1'}}	We describe two approaches to analyzing and ENTITYOTHER team discourse using ENTITY to predict team performance .
A huge amount of translation work needs to be done when creating and updating technical documentation.	translation work	technical documentation	usage	{'e1': {'word': 'translation work', 'word_index': [(4, 4)], 'id': 'A94-1044.3'}, 'e2': {'word': 'technical documentation', 'word_index': [(13, 13)], 'id': 'A94-1044.4'}}	A huge amount of ENTITY needs to be done when creating and updating ENTITYOTHER .
The objective of this project is a pilot study of several new ideas for the automatic adaptation and improvement of natural language processing (NLP) systems.	pilot study	automatic adaptation	topic	{'e1': {'word': 'pilot study', 'word_index': [(7, 7)], 'id': 'H91-1079.1'}, 'e2': {'word': 'automatic adaptation', 'word_index': [(14, 14)], 'id': 'H91-1079.2'}}	The objective of this project is a ENTITY of several new ideas for the ENTITYOTHER and ENTITYUNRELATED of ENTITYUNRELATED .
The effort focuses particularly on automatically inferring the meaning of new words in context and on developing partial interpretations of language that is either fragmentary or beyond the capability of the NLP system to understand.	meaning	words	model-feature	{'e1': {'word': 'meaning', 'word_index': [(8, 8)], 'id': 'H91-1079.5'}, 'e2': {'word': 'words', 'word_index': [(11, 11)], 'id': 'H91-1079.6'}}	The effort focuses particularly on automatically inferring the ENTITY of new ENTITYOTHER in ENTITYUNRELATED and on developing ENTITYUNRELATED of ENTITYUNRELATED that is either fragmentary or beyond the capability of the ENTITYUNRELATED to understand .
The NLP system uses large annotated corpora, such as those being developed under the DARPA-funded TREE-BANK project at the University of Pennsylvania, to adapt by acquiring syntactic and semantic information from the annotated examples.	large annotated corpora	NLP system	usage	{'e1': {'word': 'large annotated corpora', 'word_index': [(3, 3)], 'id': 'H91-1079.13'}, 'e2': {'word': 'NLP system', 'word_index': [(1, 1)], 'id': 'H91-1079.12'}}	The ENTITYOTHER uses ENTITY , such as those being developed under the ENTITYUNRELATED at the University of Pennsylvania , to adapt by acquiring ENTITYUNRELATED from the ENTITYUNRELATED .
The NLP system uses large annotated corpora, such as those being developed under the DARPA-funded TREE-BANK project at the University of Pennsylvania, to adapt by acquiring syntactic and semantic information from the annotated examples.	syntactic and semantic information	annotated examples	part_whole	{'e1': {'word': 'syntactic and semantic information', 'word_index': [(23, 23)], 'id': 'H91-1079.15'}, 'e2': {'word': 'annotated examples', 'word_index': [(26, 26)], 'id': 'H91-1079.16'}}	The ENTITYUNRELATED uses ENTITYUNRELATED , such as those being developed under the ENTITYUNRELATED at the University of Pennsylvania , to adapt by acquiring ENTITY from the ENTITYOTHER .
Statistical language modeling, based on probability estimates derived from the large corpora, will provide a means of ranking alternative interpretations of fragments.	probability estimates	Statistical language modeling	usage	{'e1': {'word': 'probability estimates', 'word_index': [(4, 4)], 'id': 'H91-1079.18'}, 'e2': {'word': 'Statistical language modeling', 'word_index': [(0, 0)], 'id': 'H91-1079.17'}}	ENTITYOTHER , based on ENTITY derived from the ENTITYUNRELATED , will provide a means of ranking alternative interpretations of fragments .
Lexicalized concepts are organized by semantic relations (synonymy, antonymy, hyponymy, meronymy, etc.)	semantic relations	Lexicalized concepts	model-feature	{'e1': {'word': 'semantic relations', 'word_index': [(4, 4)], 'id': 'H92-1116.6'}, 'e2': {'word': 'Lexicalized concepts', 'word_index': [(0, 0)], 'id': 'H92-1116.5'}}	ENTITYOTHER are organized by ENTITY ( ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , etc. )
Work under this grant is intended to extend and upgrade WordNet, to make it generally available, and to develop it as a tool for use in practical applications.	WordNet	applications	usage	{'e1': {'word': 'WordNet', 'word_index': [(10, 10)], 'id': 'H92-1116.14'}, 'e2': {'word': 'applications', 'word_index': [(29, 29)], 'id': 'H92-1116.15'}}	Work under this grant is intended to extend and upgrade ENTITY , to make it generally available , and to develop it as a tool for use in practical ENTITYOTHER .
In order to make it available for information retrieval and machine translation, a system is being developed English text as input and automatically gives as output the same text augmented by syntactic and semantic anotations that disambiguate all of the substantive words.	syntactic and semantic anotations	substantive words	model-feature	{'e1': {'word': 'syntactic and semantic anotations', 'word_index': [(29, 29)], 'id': 'H92-1116.20'}, 'e2': {'word': 'substantive words', 'word_index': [(35, 35)], 'id': 'H92-1116.21'}}	In order to make it available for ENTITYUNRELATED and ENTITYUNRELATED , a system is being developed ENTITYUNRELATED as input and automatically gives as output the same ENTITYUNRELATED augmented by ENTITY that disambiguate all of the ENTITYOTHER .
Initially, the semantic tagging is being done manually so that we can (1) obtain extensive experience with the tagging process and (2) create a database of correctly tagged text for use in testing proposals for automatic sense disambiguation.	text	database	part_whole	{'e1': {'word': 'text', 'word_index': [(31, 31)], 'id': 'H92-1116.25'}, 'e2': {'word': 'database', 'word_index': [(27, 27)], 'id': 'H92-1116.24'}}	Initially , the ENTITYUNRELATED is being done manually so that we can ( 1 ) obtain extensive experience with the ENTITYUNRELATED and ( 2 ) create a ENTITYOTHER of correctly tagged ENTITY for use in testing proposals for ENTITYUNRELATED .
Review previous designs involving TIPSTER technology, to support you design process.Determine if your application can benefit from upgrading to advanced TIPSTER technology that has been developed since your application was implemented.	TIPSTER technology	application	usage	{'e1': {'word': 'TIPSTER technology', 'word_index': [(22, 22)], 'id': 'X96-1060.8'}, 'e2': {'word': 'application', 'word_index': [(15, 15)], 'id': 'X96-1060.7'}}	Review previous designs involving ENTITYUNRELATED , to support you design process . Determine if your ENTITYOTHER can benefit from upgrading to advanced ENTITY that has been developed since your ENTITYUNRELATED was implemented .
Accurate lemmatization of German nouns mandates the use of a lexicon.	lemmatization	German nouns	usage	{'e1': {'word': 'lemmatization', 'word_index': [(1, 1)], 'id': 'H05-1080.1'}, 'e2': {'word': 'German nouns', 'word_index': [(3, 3)], 'id': 'H05-1080.2'}}	Accurate ENTITY of ENTITYOTHER mandates the use of a ENTITYUNRELATED .
We present a self-learning lemmatizer capable of automatically creating a full-form lexicon by processing German documents.	self-learning lemmatizer	German documents	usage	{'e1': {'word': 'self-learning lemmatizer', 'word_index': [(3, 3)], 'id': 'H05-1080.5'}, 'e2': {'word': 'German documents', 'word_index': [(12, 12)], 'id': 'H05-1080.7'}}	We present a ENTITY capable of automatically creating a ENTITYUNRELATED by processing ENTITYOTHER .
In this paper we describe how Why-Atlas creates and utilizes a proof-based representation of student essays.	proof-based representation	Why-Atlas	usage	{'e1': {'word': 'proof-based representation', 'word_index': [(11, 11)], 'id': 'W02-0211.6'}, 'e2': {'word': 'Why-Atlas', 'word_index': [(6, 6)], 'id': 'W02-0211.5'}}	In this paper we describe how ENTITYOTHER creates and utilizes a ENTITY of student essays .
We describe how it creates the proof given the output of sentence-level understanding, how it uses the proofs to give students feedback, some preliminary runtime measures, and the work we are currently doing to derive additional benefits from a proof-based approach for tutoring applications.	proof-based approach	tutoring applications	usage	{'e1': {'word': 'proof-based approach', 'word_index': [(40, 40)], 'id': 'W02-0211.10'}, 'e2': {'word': 'tutoring applications', 'word_index': [(42, 42)], 'id': 'W02-0211.11'}}	We describe how it creates the proof given the output of ENTITYUNRELATED , how it uses the ENTITYUNRELATED to give students feedback , some preliminary ENTITYUNRELATED , and the work we are currently doing to derive additional benefits from a ENTITY for ENTITYOTHER .
Compared with syntactic knowledge, semantic knowledge is more difficult to annotate, for ambiguity problem is more serious.	syntactic knowledge	semantic knowledge	compare	{'e1': {'word': 'syntactic knowledge', 'word_index': [(2, 2)], 'id': 'W03-1712.13'}, 'e2': {'word': 'semantic knowledge', 'word_index': [(4, 4)], 'id': 'W03-1712.14'}}	Compared with ENTITY , ENTITYOTHER is more difficult to annotate , for ENTITYUNRELATED is more serious .
Finally, we will compare our corpus with other well-known corpora.	corpus	corpora	compare	{'e1': {'word': 'corpus', 'word_index': [(6, 6)], 'id': 'W03-1712.17'}, 'e2': {'word': 'corpora', 'word_index': [(12, 12)], 'id': 'W03-1712.18'}}	Finally , we will compare our ENTITY with other well - known ENTITYOTHER .
These techniques, developed within the Augmented Transition Network (ATN) model, are shown to be adequate to handle many of these cases.	techniques	Augmented Transition Network (ATN) model	usage	{'e1': {'word': 'techniques', 'word_index': [(1, 1)], 'id': 'J81-2002.7'}, 'e2': {'word': 'Augmented Transition Network (ATN) model', 'word_index': [(6, 6)], 'id': 'J81-2002.8'}}	These ENTITY , developed within the ENTITYOTHER , are shown to be adequate to handle many of these cases .
Once identified, reference chains can be extended across segment boundaries, thus enabling the application of CT over the entire discourse.	CT	discourse	usage	{'e1': {'word': 'CT', 'word_index': [(15, 15)], 'id': 'P98-1044.13'}, 'e2': {'word': 'discourse', 'word_index': [(19, 19)], 'id': 'P98-1044.14'}}	Once identified , ENTITYUNRELATED can be extended across ENTITYUNRELATED , thus enabling the application of ENTITY over the entire ENTITYOTHER .
We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains.	CT	global discourse	usage	{'e1': {'word': 'CT', 'word_index': [(13, 13)], 'id': 'P98-1044.17'}, 'e2': {'word': 'global discourse', 'word_index': [(18, 18)], 'id': 'P98-1044.18'}}	We describe the processes by which ENTITYUNRELATED are defined over ENTITYUNRELATED and how ENTITY can be applied to ENTITYOTHER by using these ENTITYUNRELATED .
We also define a discourse smoothness index which can be used to compare different discourse structures and interpretations, and show how VT can be used to abstract a span of text in the context of the whole discourse.	discourse smoothness index	discourse structures and interpretations	usage	{'e1': {'word': 'discourse smoothness index', 'word_index': [(4, 4)], 'id': 'P98-1044.20'}, 'e2': {'word': 'discourse structures and interpretations', 'word_index': [(12, 12)], 'id': 'P98-1044.21'}}	We also define a ENTITY which can be used to compare different ENTITYOTHER , and show how ENTITYUNRELATED can be used to abstract a span of ENTITYUNRELATED in the context of the whole ENTITYUNRELATED .
HITIQA is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports in order to satisfy a given scenario.	natural language	exploratory questions	model-feature	{'e1': {'word': 'natural language', 'word_index': [(13, 13)], 'id': 'W04-2507.7'}, 'e2': {'word': 'exploratory questions', 'word_index': [(11, 11)], 'id': 'W04-2507.6'}}	ENTITYUNRELATED is an ENTITYUNRELATED designed to allow ENTITYUNRELATED to pose complex ENTITYOTHER in ENTITY and obtain relevant ENTITYUNRELATED to prepare their ENTITYUNRELATED in order to satisfy a given ENTITYUNRELATED .
The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space.	data-driven semantics	system	usage	{'e1': {'word': 'data-driven semantics', 'word_index': [(4, 4)], 'id': 'W04-2507.12'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'W04-2507.11'}}	The ENTITYOTHER uses novel ENTITY to conduct a clarification dialogue with the ENTITYUNRELATED that explores the ENTITYUNRELATED and the ENTITYUNRELATED of the desired ENTITYUNRELATED .
One is that it resulted in the first freely distributable corpus of fully anonymized clinical text.	fully anonymized clinical text	corpus	part_whole	{'e1': {'word': 'fully anonymized clinical text', 'word_index': [(12, 12)], 'id': 'W07-1013.4'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'W07-1013.3'}}	One is that it resulted in the first freely distributable ENTITYOTHER of ENTITY .
The other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels.	set of labels	categorization	usage	{'e1': {'word': 'set of labels', 'word_index': [(20, 20)], 'id': 'W07-1013.7'}, 'e2': {'word': 'categorization', 'word_index': [(11, 11)], 'id': 'W07-1013.6'}}	The other key feature of the task is that it required ENTITYOTHER with respect to a large and commercially significant ENTITY .
Many systems performed at levels approaching the inter-coder agreement, suggesting that human-like performance on this task is within the reach of currently available technologies.	human-like performance	currently available technologies	compare	{'e1': {'word': 'human-like performance', 'word_index': [(11, 11)], 'id': 'W07-1013.12'}, 'e2': {'word': 'currently available technologies', 'word_index': [(20, 20)], 'id': 'W07-1013.13'}}	Many systems performed at levels approaching the ENTITYUNRELATED , suggesting that ENTITY on this task is within the reach of ENTITYOTHER .
In this paper we describe automatic information nuggetization and its application to text comparison.	automatic information nuggetization	text comparison	usage	{'e1': {'word': 'automatic information nuggetization', 'word_index': [(5, 5)], 'id': 'N07-2055.1'}, 'e2': {'word': 'text comparison', 'word_index': [(10, 10)], 'id': 'N07-2055.2'}}	In this paper we describe ENTITY and its application to ENTITYOTHER .
More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material.	machine-generated nuggets	evaluation material	usage	{'e1': {'word': 'machine-generated nuggets', 'word_index': [(10, 10)], 'id': 'N07-2055.3'}, 'e2': {'word': 'evaluation material', 'word_index': [(16, 16)], 'id': 'N07-2055.4'}}	More specifically , we take a close look at how ENTITY can be used to create ENTITYOTHER .
A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.	semiautomatic annotation scheme	gold-standard data	usage	{'e1': {'word': 'semiautomatic annotation scheme', 'word_index': [(1, 1)], 'id': 'N07-2055.5'}, 'e2': {'word': 'gold-standard data', 'word_index': [(6, 6)], 'id': 'N07-2055.6'}}	A ENTITY is designed to produce ENTITYOTHER with exceptionally high ENTITYUNRELATED .
This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts.	features	retrieval	usage	{'e1': {'word': 'features', 'word_index': [(14, 14)], 'id': 'P07-3007.1'}, 'e2': {'word': 'retrieval', 'word_index': [(16, 16)], 'id': 'P07-3007.2'}}	This paper presents the results of experiments in which we tested different kinds of ENTITY for ENTITYOTHER of ENTITYUNRELATED .
We assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR, but with some distinct features.	retrieval	IR	part_whole	{'e1': {'word': 'retrieval', 'word_index': [(6, 6)], 'id': 'P07-3007.4'}, 'e2': {'word': 'IR', 'word_index': [(17, 17)], 'id': 'P07-3007.6'}}	We assume that the task of ENTITY of ENTITYUNRELATED can be regarded as a subtask of general ENTITYOTHER , but with some distinct ENTITYUNRELATED .
This paper discusses the supervised learning of morphology using stochastic transducers, trained using the Expectation-Maximization (EM) algorithm.	stochastic transducers	supervised learning	usage	{'e1': {'word': 'stochastic transducers', 'word_index': [(8, 8)], 'id': 'P02-1065.3'}, 'e2': {'word': 'supervised learning', 'word_index': [(4, 4)], 'id': 'P02-1065.1'}}	This paper discusses the ENTITYOTHER of ENTITYUNRELATED using ENTITY , trained using the ENTITYUNRELATED .
These are evaluated and compared ondata sets from English, German, Slovene and Arabic.	data sets	English	part_whole	{'e1': {'word': 'data sets', 'word_index': [(6, 6)], 'id': 'P02-1065.9'}, 'e2': {'word': 'English', 'word_index': [(8, 8)], 'id': 'P02-1065.10'}}	These are evaluated and compared on ENTITY from ENTITYOTHER , ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED .
Speech recognition problems are a reality in current spoken dialogue systems	Speech recognition problems	spoken dialogue systems	part_whole	{'e1': {'word': 'Speech recognition problems', 'word_index': [(0, 0)], 'id': 'P06-1025.1'}, 'e2': {'word': 'spoken dialogue systems', 'word_index': [(6, 6)], 'id': 'P06-1025.2'}}	ENTITY are a reality in current ENTITYOTHER
We apply Chi Square (%2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns.	Chi Square (%2) analysis	corpus of speech-based computer tutoring dialogues	usage	{'e1': {'word': 'Chi Square (%2) analysis', 'word_index': [(2, 2)], 'id': 'P06-1025.5'}, 'e2': {'word': 'corpus of speech-based computer tutoring dialogues', 'word_index': [(5, 5)], 'id': 'P06-1025.6'}}	We apply ENTITY to a ENTITYOTHER to discover these dependencies both within and across turns .
In an interlingual knowledge-based machine translation system, ambiguity arises when the source language analyzer produces more than one interlingua expression for a source sentence.	interlingua expression	source sentence	model-feature	{'e1': {'word': 'interlingua expression', 'word_index': [(13, 13)], 'id': 'C94-1012.4'}, 'e2': {'word': 'source sentence', 'word_index': [(16, 16)], 'id': 'C94-1012.5'}}	In an ENTITYUNRELATED , ENTITYUNRELATED arises when the ENTITYUNRELATED produces more than one ENTITY for a ENTITYOTHER .
We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence.	test sentences	corpus	part_whole	{'e1': {'word': 'test sentences', 'word_index': [(10, 10)], 'id': 'C94-1012.12'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'C94-1012.11'}}	We also test these methods on a large ENTITYOTHER of ENTITY , in order to illustrate how the different ENTITYUNRELATED reduce the average number of ENTITYUNRELATED per ENTITYUNRELATED .
We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods reduce the average number of parses per sentence.	disambiguation methods	parses	result	{'e1': {'word': 'disambiguation methods', 'word_index': [(19, 19)], 'id': 'C94-1012.13'}, 'e2': {'word': 'parses', 'word_index': [(25, 25)], 'id': 'C94-1012.14'}}	We also test these methods on a large ENTITYUNRELATED of ENTITYUNRELATED , in order to illustrate how the different ENTITY reduce the average number of ENTITYOTHER per ENTITYUNRELATED .
The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding.	dynamic programs	machine translation decoding	usage	{'e1': {'word': 'dynamic programs', 'word_index': [(9, 9)], 'id': 'D08-1012.3'}, 'e2': {'word': 'machine translation decoding', 'word_index': [(11, 11)], 'id': 'D08-1012.4'}}	The intersection of ENTITYUNRELATED with ENTITYUNRELATED results in huge ENTITY for ENTITYOTHER .
In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language.	clustered encoding	encoding-based methods	usage	{'e1': {'word': 'clustered encoding', 'word_index': [(14, 14)], 'id': 'D08-1012.10'}, 'e2': {'word': 'encoding-based methods', 'word_index': [(9, 9)], 'id': 'D08-1012.9'}}	In contrast to previous ENTITYUNRELATED , we focus on ENTITYOTHER , which use a ENTITY of the ENTITYUNRELATED .
Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.	decoding cascade for trigram language models	bigram-to-trigram decoder	compare	{'e1': {'word': 'decoding cascade for trigram language models', 'word_index': [(4, 4)], 'id': 'D08-1012.16'}, 'e2': {'word': 'bigram-to-trigram decoder', 'word_index': [(14, 14)], 'id': 'D08-1012.18'}}	Moreover , our entire ENTITY is faster than the corresponding ENTITYUNRELATED alone of a ENTITYOTHER .
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain.	pronoun resolution	biomedical texts	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(10, 10)], 'id': 'L08-1071.1'}, 'e2': {'word': 'biomedical texts', 'word_index': [(12, 12)], 'id': 'L08-1071.2'}}	This paper presents our findings on the feasibility of doing ENTITY for ENTITYOTHER , in comparison with conducting ENTITYUNRELATED for the ENTITYUNRELATED .
This paper presents our findings on the feasibility of doing pronoun resolution for biomedical texts, in comparison with conducting pronoun resolution for the newswire domain.	pronoun resolution	newswire domain	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(18, 18)], 'id': 'L08-1071.3'}, 'e2': {'word': 'newswire domain', 'word_index': [(21, 21)], 'id': 'L08-1071.4'}}	This paper presents our findings on the feasibility of doing ENTITYUNRELATED for ENTITYUNRELATED , in comparison with conducting ENTITY for the ENTITYOTHER .
Sharing portions of grammars across languages greatly reduces the costs of multilingual grammar engineering.	grammars	multilingual grammar engineering	result	{'e1': {'word': 'grammars', 'word_index': [(3, 3)], 'id': 'C00-1005.1'}, 'e2': {'word': 'multilingual grammar engineering', 'word_index': [(11, 11)], 'id': 'C00-1005.3'}}	Sharing portions of ENTITY across ENTITYUNRELATED greatly reduces the costs of ENTITYOTHER .
Taking grammatical relatedness seriously, we are particularly interested in designing linguistically motivated grammatical resources for Slavic languages to be used in applied and theoretical computational linguistics.	linguistically motivated grammatical resources	applied and theoretical computational linguistics	usage	{'e1': {'word': 'linguistically motivated grammatical resources', 'word_index': [(10, 10)], 'id': 'C00-1005.8'}, 'e2': {'word': 'applied and theoretical computational linguistics', 'word_index': [(17, 17)], 'id': 'C00-1005.10'}}	Taking ENTITYUNRELATED seriously , we are particularly interested in designing ENTITY for ENTITYUNRELATED to be used in ENTITYOTHER .
"On the basis of Slavic data, we show how a domain ontology conceptualising morpho-syntactic ""building blocks"" can serve as a basis of a shared grammar of Slavic."	domain ontology	shared grammar of Slavic	usage	{'e1': {'word': 'domain ontology', 'word_index': [(10, 10)], 'id': 'C00-1005.18'}, 'e2': {'word': 'shared grammar of Slavic', 'word_index': [(20, 20)], 'id': 'C00-1005.20'}}	On the basis of ENTITYUNRELATED , we show how a ENTITY conceptualising ENTITYUNRELATED can serve as a basis of a ENTITYOTHER .
We are currently developing MiniSTEx, a spatiotemporal annotation system to handle temporal and/or geospatial information directly and indirectly expressed in texts.	temporal and/or geospatial information	texts	part_whole	{'e1': {'word': 'temporal and/or geospatial information', 'word_index': [(10, 10)], 'id': 'L08-1561.6'}, 'e2': {'word': 'texts', 'word_index': [(16, 16)], 'id': 'L08-1561.7'}}	We are currently developing ENTITYUNRELATED , a ENTITYUNRELATED to handle ENTITY directly and indirectly expressed in ENTITYOTHER .
A first version of MiniSTEx was originally developed for Dutch, keeping in mind that it should also be useful for other European languages, and for multilingual applications.	MiniSTEx	Dutch	usage	{'e1': {'word': 'MiniSTEx', 'word_index': [(4, 4)], 'id': 'L08-1561.11'}, 'e2': {'word': 'Dutch', 'word_index': [(9, 9)], 'id': 'L08-1561.12'}}	A first version of ENTITY was originally developed for ENTITYOTHER , keeping in mind that it should also be useful for other ENTITYUNRELATED , and for ENTITYUNRELATED .
The world knowledge MiniSTEx uses is contained in interconnected tables in a database.	world knowledge	MiniSTEx	part_whole	{'e1': {'word': 'world knowledge', 'word_index': [(1, 1)], 'id': 'L08-1561.20'}, 'e2': {'word': 'MiniSTEx', 'word_index': [(2, 2)], 'id': 'L08-1561.21'}}	The ENTITY ENTITYOTHER uses is contained in interconnected tables in a ENTITYUNRELATED .
In this paper we describe a technique for improving the performance of an information extraction system for speech data by explicitly modeling the errors in the recognizer output.	information extraction system	speech data	usage	{'e1': {'word': 'information extraction system', 'word_index': [(13, 13)], 'id': 'H01-1034.4'}, 'e2': {'word': 'speech data', 'word_index': [(15, 15)], 'id': 'H01-1034.5'}}	In this paper we describe a technique for improving the performance of an ENTITY for ENTITYOTHER by explicitly modeling the errors in the ENTITYUNRELATED .
In this paper, we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list.	QA system	type questions	usage	{'e1': {'word': 'QA system', 'word_index': [(7, 7)], 'id': 'I05-2006.1'}, 'e2': {'word': 'type questions', 'word_index': [(12, 12)], 'id': 'I05-2006.2'}}	In this paper , we report a ENTITY which can answer how ENTITYOTHER based on the ENTITYUNRELATED which was developed by using ENTITYUNRELATED posted to a ENTITYUNRELATED .
In this paper, we report a QA system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list.	confirmed knowledge base	mails	part_whole	{'e1': {'word': 'confirmed knowledge base', 'word_index': [(16, 16)], 'id': 'I05-2006.3'}, 'e2': {'word': 'mails', 'word_index': [(22, 22)], 'id': 'I05-2006.4'}}	In this paper , we report a ENTITYUNRELATED which can answer how ENTITYUNRELATED based on the ENTITY which was developed by using ENTITYOTHER posted to a ENTITYUNRELATED .
We first discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents.	knowledge base	natural language documents	part_whole	{'e1': {'word': 'knowledge base', 'word_index': [(8, 8)], 'id': 'I05-2006.6'}, 'e2': {'word': 'natural language documents', 'word_index': [(11, 11)], 'id': 'I05-2006.7'}}	We first discuss a problem of developing a ENTITY by using ENTITYOTHER : ENTITYUNRELATED in ENTITYUNRELATED .
We first discuss a problem of developing a knowledge base by using natural language documents: wrong information in natural language documents.	wrong information	natural language documents	part_whole	{'e1': {'word': 'wrong information', 'word_index': [(13, 13)], 'id': 'I05-2006.8'}, 'e2': {'word': 'natural language documents', 'word_index': [(15, 15)], 'id': 'I05-2006.9'}}	We first discuss a problem of developing a ENTITYUNRELATED by using ENTITYUNRELATED : ENTITY in ENTITYOTHER .
Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails.	wrong information	mails	part_whole	{'e1': {'word': 'wrong information', 'word_index': [(8, 8)], 'id': 'I05-2006.10'}, 'e2': {'word': 'mails', 'word_index': [(10, 10)], 'id': 'I05-2006.11'}}	Then , we describe a method of detecting ENTITY in ENTITYOTHER posted to a ENTITYUNRELATED and developing a ENTITYUNRELATED by using these ENTITYUNRELATED .
Then, we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails.	knowledge base	mails	part_whole	{'e1': {'word': 'knowledge base', 'word_index': [(18, 18)], 'id': 'I05-2006.13'}, 'e2': {'word': 'mails', 'word_index': [(22, 22)], 'id': 'I05-2006.14'}}	Then , we describe a method of detecting ENTITYUNRELATED in ENTITYUNRELATED posted to a ENTITYUNRELATED and developing a ENTITY by using these ENTITYOTHER .
Finally, we show that question and answer mails posted to a mailing list can be used as a knowledge base for a QA system.	knowledge base	QA system	usage	{'e1': {'word': 'knowledge base', 'word_index': [(15, 15)], 'id': 'I05-2006.17'}, 'e2': {'word': 'QA system', 'word_index': [(18, 18)], 'id': 'I05-2006.18'}}	Finally , we show that ENTITYUNRELATED posted to a ENTITYUNRELATED can be used as a ENTITY for a ENTITYOTHER .
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.	reading difficulty	text passage	model-feature	{'e1': {'word': 'reading difficulty', 'word_index': [(12, 12)], 'id': 'N04-1025.1'}, 'e2': {'word': 'text passage', 'word_index': [(15, 15)], 'id': 'N04-1025.2'}}	We demonstrate a new research approach to the problem of predicting the ENTITY of a ENTITYOTHER , by recasting ENTITYUNRELATED in terms of ENTITYUNRELATED .
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.	statistical language modeling	readability	model-feature	{'e1': {'word': 'statistical language modeling', 'word_index': [(23, 23)], 'id': 'N04-1025.4'}, 'e2': {'word': 'readability', 'word_index': [(19, 19)], 'id': 'N04-1025.3'}}	We demonstrate a new research approach to the problem of predicting the ENTITYUNRELATED of a ENTITYUNRELATED , by recasting ENTITYOTHER in terms of ENTITY .
We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.	language models	multinomial naive Bayes classification	usage	{'e1': {'word': 'language models', 'word_index': [(13, 13)], 'id': 'N04-1025.6'}, 'e2': {'word': 'multinomial naive Bayes classification', 'word_index': [(9, 9)], 'id': 'N04-1025.5'}}	We derive a measure based on an extension of ENTITYOTHER that combines multiple ENTITY to estimate the most likely grade level for a given ENTITYUNRELATED .
We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.	semantic variables	readability measures	part_whole	{'e1': {'word': 'semantic variables', 'word_index': [(16, 16)], 'id': 'N04-1025.10'}, 'e2': {'word': 'readability measures', 'word_index': [(19, 19)], 'id': 'N04-1025.11'}}	We perform predictions for individual ENTITYUNRELATED in English and compare our performance to widely - used ENTITY from traditional ENTITYOTHER .
Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).	language modeling approach	Web documents	usage	{'e1': {'word': 'language modeling approach', 'word_index': [(17, 17)], 'id': 'N04-1025.17'}, 'e2': {'word': 'Web documents', 'word_index': [(22, 22)], 'id': 'N04-1025.18'}}	Some traditional ENTITYUNRELATED such as ENTITYUNRELATED gave the best performance on commercial calibrated ENTITYUNRELATED , while our ENTITY gave better accuracy for ENTITYOTHER and very short ENTITYUNRELATED ( less than 10 ENTITYUNRELATED ) .
Syntactic and semantic information are both represented in the grammar in a uniform manner, similar to HPSG ( Pollard and Sag, 1987 ).	grammar	Syntactic and semantic information	model-feature	{'e1': {'word': 'grammar', 'word_index': [(6, 6)], 'id': 'M93-1024.7'}, 'e2': {'word': 'Syntactic and semantic information', 'word_index': [(0, 0)], 'id': 'M93-1024.6'}}	ENTITYOTHER are both represented in the ENTITY in a uniform manner , similar to ENTITYUNRELATED ( Pollard and Sag , 1987 ) .
LINK has been used in several information extraction applications.	LINK	information extraction applications	usage	{'e1': {'word': 'LINK', 'word_index': [(0, 0)], 'id': 'M93-1024.9'}, 'e2': {'word': 'information extraction applications', 'word_index': [(6, 6)], 'id': 'M93-1024.10'}}	ENTITY has been used in several ENTITYOTHER .
In a project with General Motors, LINK was used to process terse free-form descriptions of symptoms displayed by malfunctioning automobiles, and the repairs which fixed them.	LINK	free-form descriptions	usage	{'e1': {'word': 'LINK', 'word_index': [(7, 7)], 'id': 'M93-1024.11'}, 'e2': {'word': 'free-form descriptions', 'word_index': [(13, 13)], 'id': 'M93-1024.12'}}	In a project with General Motors , ENTITY was used to process terse ENTITYOTHER of symptoms displayed by malfunctioning automobiles , and the repairs which fixed them .
Reduplication, a central instance of prosodic morphology, is particularly challenging for state-of-the-art computational morphology, since it involves copying of some part of a phonological string	Reduplication	prosodic morphology	part_whole	{'e1': {'word': 'Reduplication', 'word_index': [(0, 0)], 'id': 'A00-2039.1'}, 'e2': {'word': 'prosodic morphology', 'word_index': [(6, 6)], 'id': 'A00-2039.2'}}	ENTITY , a central instance of ENTITYOTHER , is particularly challenging for ENTITYUNRELATED , since it involves copying of some part of a ENTITYUNRELATED
In this paper I advocate a finite-state method that combines enriched lexical representations via intersection to implement the copying.	enriched lexical representations	finite-state method	usage	{'e1': {'word': 'enriched lexical representations', 'word_index': [(9, 9)], 'id': 'A00-2039.6'}, 'e2': {'word': 'finite-state method', 'word_index': [(6, 6)], 'id': 'A00-2039.5'}}	In this paper I advocate a ENTITYOTHER that combines ENTITY via intersection to implement the copying .
The proposal includes a resource-conscious variant of automata and can benefit from the existence of lazy algorithms.	lazy algorithms	resource-conscious variant of automata	usage	{'e1': {'word': 'lazy algorithms', 'word_index': [(12, 12)], 'id': 'A00-2039.8'}, 'e2': {'word': 'resource-conscious variant of automata', 'word_index': [(4, 4)], 'id': 'A00-2039.7'}}	The proposal includes a ENTITYOTHER and can benefit from the existence of ENTITY .
"These quick relevancy judgements require two steps: (1) recognizing an expression that is highly relevant to the given domain, e.g. ""were killed"" in the domain of terrorism, and (2) verifying that the context surrounding the expression is consistent with the relevancy guidelines for the domain, e.g. ""5 soldiers were killed by guerrillas"" is not consistent with the terrorism domain since victims of terrorist acts must be civilians."	expression	given domain	model-feature	{'e1': {'word': 'expression', 'word_index': [(13, 13)], 'id': 'H92-1094.3'}, 'e2': {'word': 'given domain', 'word_index': [(20, 20)], 'id': 'H92-1094.4'}}	"These quick relevancy judgements require two steps : ( 1 ) recognizing an ENTITY that is highly relevant to the ENTITYOTHER , e.g. "" were killed "" in the ENTITYUNRELATED , and ( 2 ) verifying that the ENTITYUNRELATED surrounding the ENTITYUNRELATED is consistent with the ENTITYUNRELATED for the ENTITYUNRELATED , e.g. "" 5 soldiers were killed by guerrillas "" is not consistent with the ENTITYUNRELATED since victims of terrorist acts must be civilians ."
The Relevancy Signatures Algorithm attempts to simulate the first step in this process by deriving reliable relevancy cues from a corpus of training texts and using these cues to quickly identify new texts that are highly likely to be relevant.	reliable relevancy cues	corpus of training texts	part_whole	{'e1': {'word': 'reliable relevancy cues', 'word_index': [(13, 13)], 'id': 'H92-1094.12'}, 'e2': {'word': 'corpus of training texts', 'word_index': [(16, 16)], 'id': 'H92-1094.13'}}	The ENTITYUNRELATED attempts to simulate the first step in this process by deriving ENTITY from a ENTITYOTHER and using these cues to quickly identify ENTITYUNRELATED that are highly likely to be relevant .
Our system accepts an entire English news story (text) as query, and retrieves relevant Chinese broadcast news stories (audio) from the document collection.	relevant Chinese broadcast news stories (audio)	document collection	part_whole	{'e1': {'word': 'relevant Chinese broadcast news stories (audio)', 'word_index': [(11, 11)], 'id': 'H01-1050.6'}, 'e2': {'word': 'document collection', 'word_index': [(14, 14)], 'id': 'H01-1050.7'}}	Our system accepts an entire ENTITYUNRELATED as ENTITYUNRELATED , and retrieves ENTITY from the ENTITYOTHER .
The English queries are translated into Chinese by means of a dictionary-based approach, where we have integrated phrase-based translation with word-by-word translation.	phrase-based translation	dictionary-based approach	usage	{'e1': {'word': 'phrase-based translation', 'word_index': [(16, 16)], 'id': 'H01-1050.16'}, 'e2': {'word': 'dictionary-based approach', 'word_index': [(10, 10)], 'id': 'H01-1050.15'}}	The ENTITYUNRELATED are translated into ENTITYUNRELATED by means of a ENTITYOTHER , where we have integrated ENTITY with ENTITYUNRELATED .
Untranslatable named entities are transliterated by a novel subword translation technique.	novel subword translation technique	Untranslatable named entities	usage	{'e1': {'word': 'novel subword translation technique', 'word_index': [(5, 5)], 'id': 'H01-1050.19'}, 'e2': {'word': 'Untranslatable named entities', 'word_index': [(0, 0)], 'id': 'H01-1050.18'}}	ENTITYOTHER are transliterated by a ENTITY .
Experimental results demonstrate that the use of phrase-based translation and subword translation gave performance gains, and multi-scale retrieval outperforms word-based retrieval.	multi-scale retrieval	word-based retrieval	compare	{'e1': {'word': 'multi-scale retrieval', 'word_index': [(15, 15)], 'id': 'H01-1050.26'}, 'e2': {'word': 'word-based retrieval', 'word_index': [(17, 17)], 'id': 'H01-1050.27'}}	Experimental results demonstrate that the use of ENTITYUNRELATED and ENTITYUNRELATED gave performance gains , and ENTITY outperforms ENTITYOTHER .
But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English.	features	English past tense	model-feature	{'e1': {'word': 'features', 'word_index': [(7, 7)], 'id': 'W98-1240.10'}, 'e2': {'word': 'English past tense', 'word_index': [(3, 3)], 'id': 'W98-1240.9'}}	But while the ENTITYOTHER has some interesting ENTITY in its combination of ENTITYUNRELATED with ENTITYUNRELATED , it is in many other respects a very trivial ENTITYUNRELATED - reflecting the generally ENTITYUNRELATED of ENTITYUNRELATED within ENTITYUNRELATED .
But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system - reflecting the generally vestigal nature of inflectional morphology within modern English.	vestigal nature	inflectional morphology	model-feature	{'e1': {'word': 'vestigal nature', 'word_index': [(30, 30)], 'id': 'W98-1240.14'}, 'e2': {'word': 'inflectional morphology', 'word_index': [(32, 32)], 'id': 'W98-1240.15'}}	But while the ENTITYUNRELATED has some interesting ENTITYUNRELATED in its combination of ENTITYUNRELATED with ENTITYUNRELATED , it is in many other respects a very trivial ENTITYUNRELATED - reflecting the generally ENTITY of ENTITYOTHER within ENTITYUNRELATED .
We define, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages.	shallow syntactic analysis	statistical machine translation	usage	{'e1': {'word': 'shallow syntactic analysis', 'word_index': [(16, 16)], 'id': 'W03-1002.2'}, 'e2': {'word': 'statistical machine translation', 'word_index': [(10, 10)], 'id': 'W03-1002.1'}}	We define , implement and evaluate a novel model for ENTITYOTHER , which is based on ENTITY ( ENTITYUNRELATED and ENTITYUNRELATED ) in both the ENTITYUNRELATED .
This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.	machine translation systems	language pairs	usage	{'e1': {'word': 'machine translation systems', 'word_index': [(6, 6)], 'id': 'W08-0309.2'}, 'e2': {'word': 'language pairs', 'word_index': [(9, 9)], 'id': 'W08-0309.3'}}	This paper analyzes the ENTITYUNRELATED of ENTITY for 10 ENTITYOTHER translating between ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED .
We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information.	intra- and inter-annotator agreement	manual evaluation methodology	usage	{'e1': {'word': 'intra- and inter-annotator agreement', 'word_index': [(6, 6)], 'id': 'W08-0309.18'}, 'e2': {'word': 'manual evaluation methodology', 'word_index': [(3, 3)], 'id': 'W08-0309.17'}}	We validate our ENTITYOTHER by measuring ENTITY , and collecting timing information .
In this paper, we describe issues in the translation of proper names from English to Chinese which we have faced in constructing a system for multilingual text generation supporting both languages.	translation	proper names	usage	{'e1': {'word': 'translation', 'word_index': [(9, 9)], 'id': 'P98-2220.1'}, 'e2': {'word': 'proper names', 'word_index': [(11, 11)], 'id': 'P98-2220.2'}}	In this paper , we describe issues in the ENTITY of ENTITYOTHER from ENTITYUNRELATED to ENTITYUNRELATED which we have faced in constructing a system for ENTITYUNRELATED supporting both ENTITYUNRELATED .
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.	backward maximum matching	CWS	usage	{'e1': {'word': 'backward maximum matching', 'word_index': [(5, 5)], 'id': 'W06-0119.6'}, 'e2': {'word': 'CWS', 'word_index': [(1, 1)], 'id': 'W06-0119.5'}}	Our ENTITYOTHER is based on ENTITY with ENTITYUNRELATED and ENTITYUNRELATED .
The Arabic language has far richer systems of inflection and derivation than English which has very little morphology.	systems of inflection and derivation	Arabic language	model-feature	{'e1': {'word': 'systems of inflection and derivation', 'word_index': [(5, 5)], 'id': 'W06-3103.2'}, 'e2': {'word': 'Arabic language', 'word_index': [(1, 1)], 'id': 'W06-3103.1'}}	The ENTITYOTHER has far richer ENTITY than ENTITYUNRELATED which has very little ENTITYUNRELATED .
Segmentation of inflected Arabic words is a way to smooth its highly morphological nature.	Segmentation	inflected Arabic words	usage	{'e1': {'word': 'Segmentation', 'word_index': [(0, 0)], 'id': 'W06-3103.8'}, 'e2': {'word': 'inflected Arabic words', 'word_index': [(2, 2)], 'id': 'W06-3103.9'}}	ENTITY of ENTITYOTHER is a way to smooth its ENTITYUNRELATED .
In this paper, we describe some statistically and linguistically motivated methods for Arabic word segmentation.	statistically and linguistically motivated methods	Arabic word segmentation	usage	{'e1': {'word': 'statistically and linguistically motivated methods', 'word_index': [(7, 7)], 'id': 'W06-3103.11'}, 'e2': {'word': 'Arabic word segmentation', 'word_index': [(9, 9)], 'id': 'W06-3103.12'}}	In this paper , we describe some ENTITY for ENTITYOTHER .
Then, we show the efficiency of proposed methods on the Arabic-English BTEC and NIST tasks.	proposed methods	Arabic-English BTEC and NIST tasks	usage	{'e1': {'word': 'proposed methods', 'word_index': [(7, 7)], 'id': 'W06-3103.13'}, 'e2': {'word': 'Arabic-English BTEC and NIST tasks', 'word_index': [(10, 10)], 'id': 'W06-3103.14'}}	Then , we show the efficiency of ENTITY on the ENTITYOTHER .
Developing a high-quality lexicon is often the first step towards building a POS tagger, which is in turn the front-end to many NLP systems.	high-quality lexicon	POS tagger	usage	{'e1': {'word': 'high-quality lexicon', 'word_index': [(2, 2)], 'id': 'W06-1647.4'}, 'e2': {'word': 'POS tagger', 'word_index': [(11, 11)], 'id': 'W06-1647.5'}}	Developing a ENTITY is often the first step towards building a ENTITYOTHER , which is in turn the front - end to many ENTITYUNRELATED .
We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Transductive SVMs, Spectral Graph Transducers, and a novel Transductive Clustering method.	Transductive SVMs	Spectral Graph Transducers	compare	{'e1': {'word': 'Transductive SVMs', 'word_index': [(15, 15)], 'id': 'W06-1647.10'}, 'e2': {'word': 'Spectral Graph Transducers', 'word_index': [(17, 17)], 'id': 'W06-1647.11'}}	We frame the ENTITYUNRELATED as a ENTITYUNRELATED , and perform comparisons on three ENTITYUNRELATED : ENTITY , ENTITYOTHER , and a ENTITYUNRELATED .
We present an API for computing the semantic relatedness of words in Wikipedia.	API	semantic relatedness	usage	{'e1': {'word': 'API', 'word_index': [(3, 3)], 'id': 'P07-2013.1'}, 'e2': {'word': 'semantic relatedness', 'word_index': [(7, 7)], 'id': 'P07-2013.2'}}	We present an ENTITY for computing the ENTITYOTHER of ENTITYUNRELATED in ENTITYUNRELATED .
This paper presents a general platform, namely synchronous tree sequence substitution grammar (STSSG), for the grammar comparison study in Translational Equivalence Modeling (TEM) and Statistical Machine Translation (SMT).	synchronous tree sequence substitution grammar (STSSG)	grammar comparison study	usage	{'e1': {'word': 'synchronous tree sequence substitution grammar (STSSG)', 'word_index': [(8, 8)], 'id': 'C08-1138.1'}, 'e2': {'word': 'grammar comparison study', 'word_index': [(12, 12)], 'id': 'C08-1138.2'}}	This paper presents a general platform , namely ENTITY , for the ENTITYOTHER in ENTITYUNRELATED and ENTITYUNRELATED .
Experimental results show that the STSSG is able to better explain the data in parallel corpora than other grammars.	STSSG	other grammars	compare	{'e1': {'word': 'STSSG', 'word_index': [(5, 5)], 'id': 'C08-1138.11'}, 'e2': {'word': 'other grammars', 'word_index': [(16, 16)], 'id': 'C08-1138.13'}}	Experimental results show that the ENTITY is able to better explain the data in ENTITYUNRELATED than ENTITYOTHER .
Our study further finds that the complexity of structure divergence is much higher than suggested in literature, which imposes a big challenge to syntactic transformation-based SMT.	structure divergence	syntactic transformation-based SMT	result	{'e1': {'word': 'structure divergence', 'word_index': [(8, 8)], 'id': 'C08-1138.14'}, 'e2': {'word': 'syntactic transformation-based SMT', 'word_index': [(23, 23)], 'id': 'C08-1138.15'}}	Our study further finds that the complexity of ENTITY is much higher than suggested in literature , which imposes a big challenge to ENTITYOTHER .
This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms.	unsupervised method	automatic sentence extraction	usage	{'e1': {'word': 'unsupervised method', 'word_index': [(5, 5)], 'id': 'P04-3020.1'}, 'e2': {'word': 'automatic sentence extraction', 'word_index': [(7, 7)], 'id': 'P04-3020.2'}}	This paper presents an innovative ENTITY for ENTITYOTHER using ENTITYUNRELATED .
We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.	method	text summarization task	usage	{'e1': {'word': 'method', 'word_index': [(3, 3)], 'id': 'P04-3020.4'}, 'e2': {'word': 'text summarization task', 'word_index': [(9, 9)], 'id': 'P04-3020.5'}}	We evaluate the ENTITY in the context of a ENTITYOTHER , and show that the results obtained compare favorably with previously published results on ENTITYUNRELATED .
Preferred antecedents are a subset of the possible antecedents, selected by the application of extralinguistic knowledge.	Preferred antecedents	possible antecedents	part_whole	{'e1': {'word': 'Preferred antecedents', 'word_index': [(0, 0)], 'id': 'C90-2017.7'}, 'e2': {'word': 'possible antecedents', 'word_index': [(6, 6)], 'id': 'C90-2017.8'}}	ENTITY are a subset of the ENTITYOTHER , selected by the application of ENTITYUNRELATED .
This paper presents a syntactic description of a fragment of German that has been worked out within the machine translation project Eurotra.	syntactic description	German	model-feature	{'e1': {'word': 'syntactic description', 'word_index': [(4, 4)], 'id': 'C88-2123.1'}, 'e2': {'word': 'German', 'word_index': [(9, 9)], 'id': 'C88-2123.2'}}	This paper presents a ENTITY of a fragment of ENTITYOTHER that has been worked out within the ENTITYUNRELATED .
It represents the syntactic part of the German module of this multilingual translation system.	syntactic part	German module	part_whole	{'e1': {'word': 'syntactic part', 'word_index': [(3, 3)], 'id': 'C88-2123.4'}, 'e2': {'word': 'German module', 'word_index': [(6, 6)], 'id': 'C88-2123.5'}}	It represents the ENTITY of the ENTITYOTHER of this ENTITYUNRELATED .
We present an approach for querying collections of heterogeneous linguistic corpora that are annotated on multiple layers using arbitrary XML-based markup languages.	multiple layers	heterogeneous linguistic corpora	model-feature	{'e1': {'word': 'multiple layers', 'word_index': [(13, 13)], 'id': 'L08-1190.2'}, 'e2': {'word': 'heterogeneous linguistic corpora', 'word_index': [(8, 8)], 'id': 'L08-1190.1'}}	We present an approach for querying collections of ENTITYOTHER that are annotated on ENTITY using arbitrary ENTITYUNRELATED .
An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion.	OWL ontology	markup languages	usage	{'e1': {'word': 'OWL ontology', 'word_index': [(1, 1)], 'id': 'L08-1190.4'}, 'e2': {'word': 'markup languages', 'word_index': [(10, 10)], 'id': 'L08-1190.5'}}	An ENTITY provides a homogenising view on the conceptually different ENTITYOTHER so that a common ENTITYUNRELATED can be established using the method of ENTITYUNRELATED .
An OWL ontology provides a homogenising view on the conceptually different markup languages so that a common querying framework can be established using the method of ontology-based query expansion.	ontology-based query expansion	querying framework	usage	{'e1': {'word': 'ontology-based query expansion', 'word_index': [(23, 23)], 'id': 'L08-1190.7'}, 'e2': {'word': 'querying framework', 'word_index': [(15, 15)], 'id': 'L08-1190.6'}}	An ENTITYUNRELATED provides a homogenising view on the conceptually different ENTITYUNRELATED so that a common ENTITYOTHER can be established using the method of ENTITY .
This interface can also be used for ontology-based querying of multiple corpora simultaneously.	ontology-based querying	corpora	usage	{'e1': {'word': 'ontology-based querying', 'word_index': [(7, 7)], 'id': 'L08-1190.12'}, 'e2': {'word': 'corpora', 'word_index': [(10, 10)], 'id': 'L08-1190.13'}}	This interface can also be used for ENTITY of multiple ENTITYOTHER simultaneously .
We also evaluate our model on the related role-labelling task, and compare it with a standard role labeller.	model	standard role labeller	compare	{'e1': {'word': 'model', 'word_index': [(4, 4)], 'id': 'E06-1044.5'}, 'e2': {'word': 'standard role labeller', 'word_index': [(15, 15)], 'id': 'E06-1044.7'}}	We also evaluate our ENTITY on the related ENTITYUNRELATED , and compare it with a ENTITYOTHER .
For both tasks, our model benefits from class-based smoothing, which allows it to make correct argument-specific predictions despite a severe sparse data problem.	class-based smoothing	model	result	{'e1': {'word': 'class-based smoothing', 'word_index': [(7, 7)], 'id': 'E06-1044.10'}, 'e2': {'word': 'model', 'word_index': [(4, 4)], 'id': 'E06-1044.9'}}	For ENTITYUNRELATED , our ENTITYOTHER benefits from ENTITY , which allows it to make correct ENTITYUNRELATED despite a severe ENTITYUNRELATED .
The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.	sparse data	standard labeller	result	{'e1': {'word': 'sparse data', 'word_index': [(4, 4)], 'id': 'E06-1044.14'}, 'e2': {'word': 'standard labeller', 'word_index': [(1, 1)], 'id': 'E06-1044.13'}}	The ENTITYOTHER suffers from ENTITY and a strong reliance on ENTITYUNRELATED , especially in the ENTITYUNRELATED .
The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.	syntactic cues	prediction task	usage	{'e1': {'word': 'syntactic cues', 'word_index': [(10, 10)], 'id': 'E06-1044.15'}, 'e2': {'word': 'prediction task', 'word_index': [(15, 15)], 'id': 'E06-1044.16'}}	The ENTITYUNRELATED suffers from ENTITYUNRELATED and a strong reliance on ENTITY , especially in the ENTITYOTHER .
In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step.	'Pendulum Algorithm'	word sets	usage	"{'e1': {'word': ""'Pendulum Algorithm'"", 'word_index': [(8, 8)], 'id': 'C04-1178.6'}, 'e2': {'word': 'word sets', 'word_index': [(11, 11)], 'id': 'C04-1178.7'}}"	In a ENTITYUNRELATED , the so - called ENTITY operates on ENTITYOTHER obtained by ENTITYUNRELATED on a ENTITYUNRELATED and keeps ENTITYUNRELATED low by a ENTITYUNRELATED .
In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step.	co-occurrence statistics	large un-annotated corpus	part_whole	{'e1': {'word': 'co-occurrence statistics', 'word_index': [(14, 14)], 'id': 'C04-1178.8'}, 'e2': {'word': 'large un-annotated corpus', 'word_index': [(17, 17)], 'id': 'C04-1178.9'}}	In a ENTITYUNRELATED , the so - called ENTITYUNRELATED operates on ENTITYUNRELATED obtained by ENTITY on a ENTITYOTHER and keeps ENTITYUNRELATED low by a ENTITYUNRELATED .
The first algorithm, phone-dependent cepstral compensation, is similar in concept to the previously-described MFCDCN method, except that cepstral compensation vectors are selected according to the current phonetic hypothesis, rather than on the basis of SNR or VQ codeword identity.	phone-dependent cepstral compensation	MFCDCN method	compare	{'e1': {'word': 'phone-dependent cepstral compensation', 'word_index': [(4, 4)], 'id': 'H94-1066.4'}, 'e2': {'word': 'MFCDCN method', 'word_index': [(15, 15)], 'id': 'H94-1066.5'}}	The first algorithm , ENTITY , is similar in concept to the previously - described ENTITYOTHER , except that ENTITYUNRELATED are selected according to the current ENTITYUNRELATED , rather than on the basis of ENTITYUNRELATED .
Use of the various compensation algorithms in consort produces a reduction of error rates for SPHINX-II by as much as 40 percent relative to the rate achieved with cepstral mean normalization alone, in both development test sets and in the context of the 1993 ARPA CSR evaluations.	compensation algorithms	reduction of error rates	result	{'e1': {'word': 'compensation algorithms', 'word_index': [(4, 4)], 'id': 'H94-1066.12'}, 'e2': {'word': 'reduction of error rates', 'word_index': [(9, 9)], 'id': 'H94-1066.13'}}	Use of the various ENTITY in consort produces a ENTITYOTHER for ENTITYUNRELATED by as much as 40 percent relative to the rate achieved with ENTITYUNRELATED alone , in both ENTITYUNRELATED and in the context of the ENTITYUNRELATED .
One of the critical components of the CLT is a speech recognition system which is used to track the child's progress during oral reading and to provide sufficient information to detect reading miscues.	reading miscues	oral reading	part_whole	{'e1': {'word': 'reading miscues', 'word_index': [(29, 29)], 'id': 'C04-1182.7'}, 'e2': {'word': 'oral reading', 'word_index': [(21, 21)], 'id': 'C04-1182.6'}}	One of the critical components of the ENTITYUNRELATED is a ENTITYUNRELATED which is used to track the child 's progress during ENTITYOTHER and to provide sufficient information to detect ENTITY .
In this paper, we extend on prior work by examining a novel labeling of children's oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors.	labeling	oral reading audio data	model-feature	{'e1': {'word': 'labeling', 'word_index': [(13, 13)], 'id': 'C04-1182.8'}, 'e2': {'word': 'oral reading audio data', 'word_index': [(17, 17)], 'id': 'C04-1182.9'}}	In this paper , we extend on prior work by examining a novel ENTITY of children 's ENTITYOTHER in order to better understand the factors that contribute most significantly to ENTITYUNRELATED .
Next, we consider the problem of detecting miscues during oral reading.	miscues	oral reading	part_whole	{'e1': {'word': 'miscues', 'word_index': [(8, 8)], 'id': 'C04-1182.14'}, 'e2': {'word': 'oral reading', 'word_index': [(10, 10)], 'id': 'C04-1182.15'}}	Next , we consider the problem of detecting ENTITY during ENTITYOTHER .
This paper introduces a simple mixture language model that attempts to capture long distance constraints in a sentence or paragraph.	long distance constraints	sentence	part_whole	{'e1': {'word': 'long distance constraints', 'word_index': [(10, 10)], 'id': 'H94-1014.2'}, 'e2': {'word': 'sentence', 'word_index': [(13, 13)], 'id': 'H94-1014.3'}}	This paper introduces a simple ENTITYUNRELATED that attempts to capture ENTITY in a ENTITYOTHER or ENTITYUNRELATED .
Using the BU recognition system, experiments show a 7% improvement in recognition accuracy with the mixture digram models as compared to using a Digram model.	BU recognition system	recognition accuracy	result	{'e1': {'word': 'BU recognition system', 'word_index': [(2, 2)], 'id': 'H94-1014.9'}, 'e2': {'word': 'recognition accuracy', 'word_index': [(11, 11)], 'id': 'H94-1014.10'}}	Using the ENTITY , experiments show a 7 % improvement in ENTITYOTHER with the ENTITYUNRELATED as compared to using a ENTITYUNRELATED .
Using the BU recognition system, experiments show a 7% improvement in recognition accuracy with the mixture digram models as compared to using a Digram model.	mixture digram models	Digram model	compare	{'e1': {'word': 'mixture digram models', 'word_index': [(14, 14)], 'id': 'H94-1014.11'}, 'e2': {'word': 'Digram model', 'word_index': [(20, 20)], 'id': 'H94-1014.12'}}	Using the ENTITYUNRELATED , experiments show a 7 % improvement in ENTITYUNRELATED with the ENTITY as compared to using a ENTITYOTHER .
The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text.	proper names	text	part_whole	{'e1': {'word': 'proper names', 'word_index': [(4, 4)], 'id': 'W93-0105.1'}, 'e2': {'word': 'text', 'word_index': [(6, 6)], 'id': 'W93-0105.2'}}	The identification of unknown ENTITY in ENTITYOTHER is a significant challenge for ENTITYUNRELATED operating on ENTITYUNRELATED .
The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text.	NLP systems	unrestricted text	usage	{'e1': {'word': 'NLP systems', 'word_index': [(12, 12)], 'id': 'W93-0105.3'}, 'e2': {'word': 'unrestricted text', 'word_index': [(15, 15)], 'id': 'W93-0105.4'}}	The identification of unknown ENTITYUNRELATED in ENTITYUNRELATED is a significant challenge for ENTITY operating on ENTITYOTHER .
A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction.	name references	documents	model-feature	{'e1': {'word': 'name references', 'word_index': [(7, 7)], 'id': 'W93-0105.6'}, 'e2': {'word': 'documents', 'word_index': [(4, 4)], 'id': 'W93-0105.5'}}	A system which indexes ENTITYOTHER according to ENTITY can be useful for ENTITYUNRELATED or as a ENTITYUNRELATED for more ENTITYUNRELATED such as ENTITYUNRELATED .
This paper describes a system which uses text skimming techniques for deriving proper names and their semantic attributes automatically from newswire text, without relying on any listing of name elements.	semantic attributes	proper names	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(13, 13)], 'id': 'W93-0105.13'}, 'e2': {'word': 'proper names', 'word_index': [(10, 10)], 'id': 'W93-0105.12'}}	This paper describes a system which uses ENTITYUNRELATED for deriving ENTITYOTHER and their ENTITY automatically from ENTITYUNRELATED , without relying on any listing of ENTITYUNRELATED .
In order to identify new names, the system treats proper names as (potentially) context-dependent linguistic expressions.	context-dependent linguistic expressions	proper names	model-feature	{'e1': {'word': 'context-dependent linguistic expressions', 'word_index': [(15, 15)], 'id': 'W93-0105.17'}, 'e2': {'word': 'proper names', 'word_index': [(10, 10)], 'id': 'W93-0105.16'}}	In order to identify new names , the system treats ENTITYOTHER as ( potentially ) ENTITY .
In addition to using information in the local context, the system exploits a computational model of discourse which identifies individuals based on the way they are described in the text, instead of relying on their description in a pre-existing knowledge base.	information	local context	part_whole	{'e1': {'word': 'information', 'word_index': [(4, 4)], 'id': 'W93-0105.18'}, 'e2': {'word': 'local context', 'word_index': [(7, 7)], 'id': 'W93-0105.19'}}	In addition to using ENTITY in the ENTITYOTHER , the system exploits a ENTITYUNRELATED which identifies individuals based on the way they are described in the text , instead of relying on their description in a pre-existing ENTITYUNRELATED .
Some entities belong more or less to a class.	entities	class	part_whole	{'e1': {'word': 'entities', 'word_index': [(1, 1)], 'id': 'L08-1274.2'}, 'e2': {'word': 'class', 'word_index': [(8, 8)], 'id': 'L08-1274.3'}}	Some ENTITY belong more or less to a ENTITYOTHER .
To specify whether an individual entity belonging to a class is typical or not, we borrow the topological concepts of interior, border, closure, and exterior.	entity	class	part_whole	{'e1': {'word': 'entity', 'word_index': [(5, 5)], 'id': 'L08-1274.8'}, 'e2': {'word': 'class', 'word_index': [(9, 9)], 'id': 'L08-1274.9'}}	To specify whether an individual ENTITY belonging to a ENTITYOTHER is typical or not , we borrow the ENTITYUNRELATED of interior , border , closure , and exterior .
It enables to define levels of typicality where individual entities are more or less typical elements of a concept.	entities	concept	model-feature	{'e1': {'word': 'entities', 'word_index': [(9, 9)], 'id': 'L08-1274.16'}, 'e2': {'word': 'concept', 'word_index': [(18, 18)], 'id': 'L08-1274.17'}}	It enables to define levels of ENTITYUNRELATED where individual ENTITY are more or less typical elements of a ENTITYOTHER .
WordNet has been used extensively as a resource for the Word Sense Disambiguation (WSD) task, both as a sense inventory and a repository of semantic relationships.	WordNet	Word Sense Disambiguation (WSD) task	usage	{'e1': {'word': 'WordNet', 'word_index': [(0, 0)], 'id': 'L08-1598.1'}, 'e2': {'word': 'Word Sense Disambiguation (WSD) task', 'word_index': [(10, 10)], 'id': 'L08-1598.2'}}	ENTITY has been used extensively as a resource for the ENTITYOTHER , both as a ENTITYUNRELATED and a ENTITYUNRELATED of ENTITYUNRELATED .
We found that it would be very useful to assign to geographical entities in WordNet their coordinates, especially in order to implement geometric shape-based disambiguation methods.	geographical entities	WordNet	part_whole	{'e1': {'word': 'geographical entities', 'word_index': [(11, 11)], 'id': 'L08-1598.9'}, 'e2': {'word': 'WordNet', 'word_index': [(13, 13)], 'id': 'L08-1598.10'}}	We found that it would be very useful to assign to ENTITY in ENTITYOTHER their coordinates , especially in order to implement ENTITYUNRELATED .
The annotation has been carried out by extracting geographical synsets from WordNet, together with their holonyms and hypernyms, and comparing them to the entries in the Wikipedia-World geographical database.	geographical synsets	WordNet	part_whole	{'e1': {'word': 'geographical synsets', 'word_index': [(8, 8)], 'id': 'L08-1598.17'}, 'e2': {'word': 'WordNet', 'word_index': [(10, 10)], 'id': 'L08-1598.18'}}	The ENTITYUNRELATED has been carried out by extracting ENTITY from ENTITYOTHER , together with their ENTITYUNRELATED and ENTITYUNRELATED , and comparing them to the entries in the ENTITYUNRELATED .
A weight was calculated for each of the candidate annotations, on the basis of matches found between the database entries and synset gloss, holonyms and hypernyms.	weight	candidate annotations	model-feature	{'e1': {'word': 'weight', 'word_index': [(1, 1)], 'id': 'L08-1598.22'}, 'e2': {'word': 'candidate annotations', 'word_index': [(8, 8)], 'id': 'L08-1598.23'}}	A ENTITY was calculated for each of the ENTITYOTHER , on the basis of ENTITYUNRELATED found between the ENTITYUNRELATED and ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED .
"""We present a robust summarisation system developed within the GATE architecture that makes use of robust components for semantic tagging and coreference resolution provided by GATE."	robust components	semantic tagging	usage	{'e1': {'word': 'robust components', 'word_index': [(13, 13)], 'id': 'E03-2013.3'}, 'e2': {'word': 'semantic tagging', 'word_index': [(15, 15)], 'id': 'E03-2013.4'}}	""" We present a ENTITYUNRELATED developed within the ENTITYUNRELATED that makes use of ENTITY for ENTITYOTHER and ENTITYUNRELATED provided by ENTITYUNRELATED ."
Our system combines GATE components with well established statistical techniques developed for the purpose of text summarisation research.	statistical techniques	text summarisation research	usage	{'e1': {'word': 'statistical techniques', 'word_index': [(7, 7)], 'id': 'E03-2013.9'}, 'e2': {'word': 'text summarisation research', 'word_index': [(13, 13)], 'id': 'E03-2013.10'}}	Our ENTITYUNRELATED combines ENTITYUNRELATED with well established ENTITY developed for the purpose of ENTITYOTHER .
Prior to MUC-4, LINK had been used to extract information from free-form texts in two narrow application domains.	information	free-form texts	part_whole	{'e1': {'word': 'information', 'word_index': [(10, 10)], 'id': 'M92-1039.6'}, 'e2': {'word': 'free-form texts', 'word_index': [(12, 12)], 'id': 'M92-1039.7'}}	Prior to ENTITYUNRELATED , ENTITYUNRELATED had been used to extract ENTITY from ENTITYOTHER in two narrow ENTITYUNRELATED .
One application corpus contained terse descriptions of symptoms displayed by malfunctioning automobiles, and the repairs which fixed them.	terse descriptions	application corpus	part_whole	{'e1': {'word': 'terse descriptions', 'word_index': [(3, 3)], 'id': 'M92-1039.10'}, 'e2': {'word': 'application corpus', 'word_index': [(1, 1)], 'id': 'M92-1039.9'}}	One ENTITYOTHER contained ENTITY of symptoms displayed by malfunctioning automobiles , and the repairs which fixed them .
In empirical testing in these two domains, LINK correctly processed 70% of previously unseen descriptions.	LINK	descriptions	usage	{'e1': {'word': 'LINK', 'word_index': [(8, 8)], 'id': 'M92-1039.15'}, 'e2': {'word': 'descriptions', 'word_index': [(16, 16)], 'id': 'M92-1039.16'}}	In empirical testing in these two ENTITYUNRELATED , ENTITY correctly processed 70 % of previously unseen ENTITYOTHER .
A template was counted as correct only if all of the fillers in the template were filled correctly.	fillers	template	part_whole	{'e1': {'word': 'fillers', 'word_index': [(11, 11)], 'id': 'M92-1039.18'}, 'e2': {'word': 'template', 'word_index': [(14, 14)], 'id': 'M92-1039.19'}}	A ENTITYUNRELATED was counted as correct only if all of the ENTITY in the ENTITYOTHER were filled correctly .
These previous domains were much narrower than the MUC-4 terrorism domain.	domains	MUC-4 terrorism domain	compare	{'e1': {'word': 'domains', 'word_index': [(2, 2)], 'id': 'M92-1039.23'}, 'e2': {'word': 'MUC-4 terrorism domain', 'word_index': [(8, 8)], 'id': 'M92-1039.24'}}	These previous ENTITY were much narrower than the ENTITYOTHER .
As a comparison, the lexicons for the previous domains contained only 300-500 words, compared with 6700 words in our MUC-4 test configuration.	words	lexicons	part_whole	{'e1': {'word': 'words', 'word_index': [(15, 15)], 'id': 'M92-1039.27'}, 'e2': {'word': 'lexicons', 'word_index': [(5, 5)], 'id': 'M92-1039.25'}}	As a comparison , the ENTITYOTHER for the previous ENTITYUNRELATED contained only 300 - 500 ENTITY , compared with 6700 ENTITYUNRELATED in our ENTITYUNRELATED .
Previous grammar size ranged from 75-100 rules, compared with over 500 rules in the MUC-4 knowledge base.	grammar size	MUC-4 knowledge base	compare	{'e1': {'word': 'grammar size', 'word_index': [(1, 1)], 'id': 'M92-1039.30'}, 'e2': {'word': 'MUC-4 knowledge base', 'word_index': [(16, 16)], 'id': 'M92-1039.33'}}	Previous ENTITY ranged from 75 - 100 ENTITYUNRELATED , compared with over 500 ENTITYUNRELATED in the ENTITYOTHER .
Thus, the integration of information from multiple sentences was not an issue in our previous work.	information	sentences	part_whole	{'e1': {'word': 'information', 'word_index': [(5, 5)], 'id': 'M92-1039.36'}, 'e2': {'word': 'sentences', 'word_index': [(8, 8)], 'id': 'M92-1039.37'}}	Thus , the integration of ENTITY from multiple ENTITYOTHER was not an issue in our previous work .
Brief Summary of Objectives: There are three objectives of the contract: to perform research and development in parallel parsing, semantic representation, ill-formed input, discourse, and tools for linguistic knowledge acquisition, and to integrate software components from BBN and elsewhere to produce Janus, DARPA's New Generation Natural Language Interface, and to demonstrate state-of-the-art natural language technology in DARPA applications.	language technology	DARPA applications	usage	{'e1': {'word': 'language technology', 'word_index': [(55, 55)], 'id': 'H89-2057.8'}, 'e2': {'word': 'DARPA applications', 'word_index': [(57, 57)], 'id': 'H89-2057.9'}}	Brief Summary of Objectives : There are three objectives of the contract : to perform research and development in ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and tools for ENTITYUNRELATED , and to integrate ENTITYUNRELATED from BBN and elsewhere to produce ENTITYUNRELATED , and to demonstrate state - of - the - art natural ENTITY in ENTITYOTHER .
In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence.	dependency tree	sentence	model-feature	{'e1': {'word': 'dependency tree', 'word_index': [(17, 17)], 'id': 'H01-1009.11'}, 'e2': {'word': 'sentence', 'word_index': [(20, 20)], 'id': 'H01-1009.12'}}	In this paper , we introduce ENTITYUNRELATED where a ENTITYUNRELATED is denoted as a ENTITYUNRELATED in the ENTITY of a ENTITYOTHER .
We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text.	Tree-Based Patterns	un-annotated text	part_whole	{'e1': {'word': 'Tree-Based Patterns', 'word_index': [(6, 6)], 'id': 'H01-1009.13'}, 'e2': {'word': 'un-annotated text', 'word_index': [(10, 10)], 'id': 'H01-1009.15'}}	We outline the procedure to acquire ENTITY in ENTITYUNRELATED from ENTITYOTHER .
The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns.	sentences	training data	part_whole	{'e1': {'word': 'sentences', 'word_index': [(5, 5)], 'id': 'H01-1009.16'}, 'e2': {'word': 'training data', 'word_index': [(8, 8)], 'id': 'H01-1009.17'}}	The system extracts the relevant ENTITY from the ENTITYOTHER based on ENTITYUNRELATED and the common ENTITYUNRELATED in the ENTITYUNRELATED of relevant ENTITYUNRELATED are taken as extracted ENTITYUNRELATED .
The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns.	parse tree	sentences	model-feature	{'e1': {'word': 'parse tree', 'word_index': [(18, 18)], 'id': 'H01-1009.20'}, 'e2': {'word': 'sentences', 'word_index': [(21, 21)], 'id': 'H01-1009.21'}}	The system extracts the relevant ENTITYUNRELATED from the ENTITYUNRELATED based on ENTITYUNRELATED and the common ENTITYUNRELATED in the ENTITY of relevant ENTITYOTHER are taken as extracted ENTITYUNRELATED .
To overcome this problem, we present in this paper a procedure for the automatic extraction of application-tuned consistent subgrammars from proved large-scale generation grammars.	application-tuned consistent subgrammars	large-scale generation grammars	part_whole	{'e1': {'word': 'application-tuned consistent subgrammars', 'word_index': [(16, 16)], 'id': 'W97-1507.6'}, 'e2': {'word': 'large-scale generation grammars', 'word_index': [(19, 19)], 'id': 'W97-1507.7'}}	To overcome this problem , we present in this paper a procedure for the ENTITYUNRELATED of ENTITY from proved ENTITYOTHER .
The procedure has been implemented for large-scale systemic grammars and builds on the formal equivalence between systemic grammars and typed unification based grammars.	systemic grammars	typed unification based grammars	compare	{'e1': {'word': 'systemic grammars', 'word_index': [(13, 13)], 'id': 'W97-1507.10'}, 'e2': {'word': 'typed unification based grammars', 'word_index': [(15, 15)], 'id': 'W97-1507.11'}}	The procedure has been implemented for ENTITYUNRELATED and builds on the ENTITYUNRELATED between ENTITY and ENTITYOTHER .
First, the task has been performed traditionally using heuristics from the domain.	heuristics	task	usage	{'e1': {'word': 'heuristics', 'word_index': [(9, 9)], 'id': 'W00-0719.4'}, 'e2': {'word': 'task', 'word_index': [(3, 3)], 'id': 'W00-0719.3'}}	First , the ENTITYOTHER has been performed traditionally using ENTITY from the ENTITYUNRELATED .
We present a comparative evaluation of several machine learning algorithms applied to spam filtering, considering the text of the messages and a set of heuristics for the task.	machine learning algorithms	spam filtering	usage	{'e1': {'word': 'machine learning algorithms', 'word_index': [(6, 6)], 'id': 'W00-0719.10'}, 'e2': {'word': 'spam filtering', 'word_index': [(9, 9)], 'id': 'W00-0719.11'}}	We present a ENTITYUNRELATED of several ENTITY applied to ENTITYOTHER , considering the ENTITYUNRELATED of the ENTITYUNRELATED and a set of ENTITYUNRELATED for the ENTITYUNRELATED .
We present a comparative evaluation of several machine learning algorithms applied to spam filtering, considering the text of the messages and a set of heuristics for the task.	heuristics	task	usage	{'e1': {'word': 'heuristics', 'word_index': [(21, 21)], 'id': 'W00-0719.14'}, 'e2': {'word': 'task', 'word_index': [(24, 24)], 'id': 'W00-0719.15'}}	We present a ENTITYUNRELATED of several ENTITYUNRELATED applied to ENTITYUNRELATED , considering the ENTITYUNRELATED of the ENTITYUNRELATED and a set of ENTITY for the ENTITYOTHER .
The framework includes a dialog history that tracks input, output, and results.	dialog history	framework	part_whole	{'e1': {'word': 'dialog history', 'word_index': [(4, 4)], 'id': 'W02-0209.11'}, 'e2': {'word': 'framework', 'word_index': [(1, 1)], 'id': 'W02-0209.10'}}	The ENTITYOTHER includes a ENTITY that tracks input , output , and results .
We present the framework and preliminary results in two application domains.	framework	application domains	usage	{'e1': {'word': 'framework', 'word_index': [(3, 3)], 'id': 'W02-0209.12'}, 'e2': {'word': 'application domains', 'word_index': [(9, 9)], 'id': 'W02-0209.13'}}	We present the ENTITY and preliminary results in two ENTITYOTHER .
Near-perfect automatic accent assignment is attainable for citation-style speech, but better computational models are needed to predict accent in extended, spontaneous discourses.	automatic accent assignment	citation-style speech	usage	{'e1': {'word': 'automatic accent assignment', 'word_index': [(1, 1)], 'id': 'P98-2155.1'}, 'e2': {'word': 'citation-style speech', 'word_index': [(5, 5)], 'id': 'P98-2155.2'}}	Near-perfect ENTITY is attainable for ENTITYOTHER , but better ENTITYUNRELATED are needed to predict ENTITYUNRELATED in ENTITYUNRELATED .
Near-perfect automatic accent assignment is attainable for citation-style speech, but better computational models are needed to predict accent in extended, spontaneous discourses.	computational models	extended, spontaneous discourses	usage	{'e1': {'word': 'computational models', 'word_index': [(9, 9)], 'id': 'P98-2155.3'}, 'e2': {'word': 'extended, spontaneous discourses', 'word_index': [(16, 16)], 'id': 'P98-2155.5'}}	Near-perfect ENTITYUNRELATED is attainable for ENTITYUNRELATED , but better ENTITY are needed to predict ENTITYUNRELATED in ENTITYOTHER .
Machine learning experiments on 1031 noun phrases from eighteen spontaneous direction-giving monologues show that accent assignment can be significantly improved by up to 4%-6% relative to a hypothetical baseline system that would produce only citation-form accentuation, giving error rate reductions of 11%-25%.	Machine learning experiments	noun phrases	usage	{'e1': {'word': 'Machine learning experiments', 'word_index': [(0, 0)], 'id': 'P98-2155.16'}, 'e2': {'word': 'noun phrases', 'word_index': [(3, 3)], 'id': 'P98-2155.17'}}	ENTITY on 1031 ENTITYOTHER from eighteen ENTITYUNRELATED show that ENTITYUNRELATED can be significantly improved by up to 4 % - 6 % relative to a hypothetical ENTITYUNRELATED that would produce only ENTITYUNRELATED , giving ENTITYUNRELATED of 11 % - 25 % .
A key task in an extraction system for query-oriented multi-document summarisation, necessary for computing relevance and redundancy, is modelling text semantics.	extraction system	query-oriented multi-document summarisation	usage	{'e1': {'word': 'extraction system', 'word_index': [(5, 5)], 'id': 'W06-0701.1'}, 'e2': {'word': 'query-oriented multi-document summarisation', 'word_index': [(7, 7)], 'id': 'W06-0701.2'}}	A key task in an ENTITY for ENTITYOTHER , necessary for computing ENTITYUNRELATED and ENTITYUNRELATED , is modelling ENTITYUNRELATED .
Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (ccg).	Combinatory Categorial Grammar (ccg)	wide-coverage statistical parsers	usage	{'e1': {'word': 'Combinatory Categorial Grammar (ccg)', 'word_index': [(12, 12)], 'id': 'W04-3215.3'}, 'e2': {'word': 'wide-coverage statistical parsers', 'word_index': [(10, 10)], 'id': 'W04-3215.2'}}	Accurate ENTITYUNRELATED has recently been reported for a number of ENTITYOTHER using ENTITY .
However, overall figures give no indication of a parser's performance on specific constructions, nor how suitable a parser is for specific applications.	parser	applications	usage	{'e1': {'word': 'parser', 'word_index': [(18, 18)], 'id': 'W04-3215.6'}, 'e2': {'word': 'applications', 'word_index': [(22, 22)], 'id': 'W04-3215.7'}}	However , overall figures give no indication of a ENTITYUNRELATED on specific ENTITYUNRELATED , nor how suitable a ENTITY is for specific ENTITYOTHER .
In this paper we give a detailed evaluation of a ccg parser on object extraction dependencies found in wsj text.	object extraction dependencies	wsj text	part_whole	{'e1': {'word': 'object extraction dependencies', 'word_index': [(12, 12)], 'id': 'W04-3215.9'}, 'e2': {'word': 'wsj text', 'word_index': [(15, 15)], 'id': 'W04-3215.10'}}	In this paper we give a detailed evaluation of a ENTITYUNRELATED on ENTITY found in ENTITYOTHER .
We also show how the parser can be used to parse questions for Question Answering.	parser	questions	usage	{'e1': {'word': 'parser', 'word_index': [(5, 5)], 'id': 'W04-3215.11'}, 'e2': {'word': 'questions', 'word_index': [(11, 11)], 'id': 'W04-3215.12'}}	We also show how the ENTITY can be used to parse ENTITYOTHER for ENTITYUNRELATED .
The accuracy of the original parser on questions is very poor, and we propose a novel technique for porting the parser to a new domain, by creating new labelled data at the lexical category level only.	parser	accuracy	result	{'e1': {'word': 'parser', 'word_index': [(5, 5)], 'id': 'W04-3215.15'}, 'e2': {'word': 'accuracy', 'word_index': [(1, 1)], 'id': 'W04-3215.14'}}	The ENTITYOTHER of the original ENTITY on ENTITYUNRELATED is very poor , and we propose a novel technique for porting the ENTITYUNRELATED to a new ENTITYUNRELATED , by creating new ENTITYUNRELATED at the ENTITYUNRELATED only .
Using a supertagger to assign categories to words, trained on the new data, leads to a dramatic increase in question parsing accuracy.	categories	words	model-feature	{'e1': {'word': 'categories', 'word_index': [(5, 5)], 'id': 'W04-3215.22'}, 'e2': {'word': 'words', 'word_index': [(7, 7)], 'id': 'W04-3215.23'}}	Using a ENTITYUNRELATED to assign ENTITY to ENTITYOTHER , trained on the new data , leads to a dramatic increase in ENTITYUNRELATED .
This paper addresses syntax-based paraphrasing methods for Recognizing Textual Entailment (RTE).	syntax-based paraphrasing methods	Recognizing Textual Entailment (RTE)	usage	{'e1': {'word': 'syntax-based paraphrasing methods', 'word_index': [(3, 3)], 'id': 'W07-1414.1'}, 'e2': {'word': 'Recognizing Textual Entailment (RTE)', 'word_index': [(5, 5)], 'id': 'W07-1414.2'}}	This paper addresses ENTITY for ENTITYOTHER .
In particular, we describe a dependency-based paraphrasing algorithm, using the DIRT data set, and its application in the context of a straightforward RTE system based on aligning dependency trees.	DIRT data set	dependency-based paraphrasing algorithm	usage	{'e1': {'word': 'DIRT data set', 'word_index': [(10, 10)], 'id': 'W07-1414.4'}, 'e2': {'word': 'dependency-based paraphrasing algorithm', 'word_index': [(6, 6)], 'id': 'W07-1414.3'}}	In particular , we describe a ENTITYOTHER , using the ENTITY , and its application in the context of a straightforward ENTITYUNRELATED based on aligning ENTITYUNRELATED .
The goal of the project is to provide a quantitative description of Polish preposition-pronoun contractions taking into consideration morphosyntactic properties of their components.	morphosyntactic properties	Polish preposition-pronoun contractions	model-feature	{'e1': {'word': 'morphosyntactic properties', 'word_index': [(16, 16)], 'id': 'W06-2103.6'}, 'e2': {'word': 'Polish preposition-pronoun contractions', 'word_index': [(12, 12)], 'id': 'W06-2103.5'}}	The goal of the project is to provide a quantitative description of ENTITYOTHER taking into consideration ENTITY of their components .
The results of corpus-based investigations of the distribution of prepositions within preposition-pronoun contractions can be used for grammar-theoretical and lexicographic purposes.	distribution	prepositions	model-feature	{'e1': {'word': 'distribution', 'word_index': [(6, 6)], 'id': 'W06-2103.11'}, 'e2': {'word': 'prepositions', 'word_index': [(8, 8)], 'id': 'W06-2103.12'}}	The results of ENTITYUNRELATED of the ENTITY of ENTITYOTHER within ENTITYUNRELATED can be used for grammar-theoretical and lexicographic purposes .
This paper presents a new unsupervised algorithm (WordEnds) for inferring word boundaries from transcribed adult conversations.	unsupervised algorithm	word boundaries	usage	{'e1': {'word': 'unsupervised algorithm', 'word_index': [(5, 5)], 'id': 'P08-1016.1'}, 'e2': {'word': 'word boundaries', 'word_index': [(11, 11)], 'id': 'P08-1016.3'}}	This paper presents a new ENTITY ( ENTITYUNRELATED ) for inferring ENTITYOTHER from ENTITYUNRELATED .
This fast algorithm delivers high performance even on morphologically complex words in English and Arabic, and promising results on accurate phonetic transcriptions with extensive pronunciation variation.	algorithm	performance	result	{'e1': {'word': 'algorithm', 'word_index': [(2, 2)], 'id': 'P08-1016.8'}, 'e2': {'word': 'performance', 'word_index': [(5, 5)], 'id': 'P08-1016.9'}}	This fast ENTITY delivers high ENTITYOTHER even on ENTITYUNRELATED in ENTITYUNRELATED and ENTITYUNRELATED , and promising results on accurate ENTITYUNRELATED with extensive ENTITYUNRELATED .
This suggests that WordEnds is a viable model of child language acquisition and might be useful in speech understanding.	model	speech understanding	usage	{'e1': {'word': 'model', 'word_index': [(7, 7)], 'id': 'P08-1016.19'}, 'e2': {'word': 'speech understanding', 'word_index': [(15, 15)], 'id': 'P08-1016.21'}}	This suggests that ENTITYUNRELATED is a viable ENTITY of ENTITYUNRELATED and might be useful in ENTITYOTHER .
Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disfluencies, or difficulties in the accurate identification of sentence boundaries	output	Speech-to-text summarization systems	usage	{'e1': {'word': 'output', 'word_index': [(6, 6)], 'id': 'W08-1406.3'}, 'e2': {'word': 'Speech-to-text summarization systems', 'word_index': [(0, 0)], 'id': 'W08-1406.1'}}	ENTITYOTHER usually take as ENTITYUNRELATED the ENTITY of an ENTITYUNRELATED that is affected by issues like ENTITYUNRELATED , ENTITYUNRELATED , or difficulties in the accurate identification of ENTITYUNRELATED
We propose the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech-to-text summarization.	multi-document summarization techniques	single document speech-to-text summarization	usage	{'e1': {'word': 'multi-document summarization techniques', 'word_index': [(21, 21)], 'id': 'W08-1406.10'}, 'e2': {'word': 'single document speech-to-text summarization', 'word_index': [(23, 23)], 'id': 'W08-1406.11'}}	We propose the inclusion of related , solid ENTITYUNRELATED to cope with the difficulties of summarizing ENTITYUNRELATED and the use of ENTITY in ENTITYOTHER .
We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences, while avoiding the need for syntactic analysis of the less resource rich language, and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models.	automated word alignment	parallel texts	usage	{'e1': {'word': 'automated word alignment', 'word_index': [(6, 6)], 'id': 'P04-3014.1'}, 'e2': {'word': 'parallel texts', 'word_index': [(8, 8)], 'id': 'P04-3014.2'}}	We present an improved method for ENTITY of ENTITYOTHER which takes advantage of knowledge of ENTITYUNRELATED , while avoiding the need for ENTITYUNRELATED of the ENTITYUNRELATED , and retaining the ENTITYUNRELATED of ENTITYUNRELATED such as the ENTITYUNRELATED .
We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences, while avoiding the need for syntactic analysis of the less resource rich language, and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models.	syntactic analysis	less resource rich language	topic	{'e1': {'word': 'syntactic analysis', 'word_index': [(22, 22)], 'id': 'P04-3014.4'}, 'e2': {'word': 'less resource rich language', 'word_index': [(25, 25)], 'id': 'P04-3014.5'}}	We present an improved method for ENTITYUNRELATED of ENTITYUNRELATED which takes advantage of knowledge of ENTITYUNRELATED , while avoiding the need for ENTITY of the ENTITYOTHER , and retaining the ENTITYUNRELATED of ENTITYUNRELATED such as the ENTITYUNRELATED .
We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences, while avoiding the need for syntactic analysis of the less resource rich language, and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models.	robustness	syntactically agnostic approaches	model-feature	{'e1': {'word': 'robustness', 'word_index': [(30, 30)], 'id': 'P04-3014.6'}, 'e2': {'word': 'syntactically agnostic approaches', 'word_index': [(32, 32)], 'id': 'P04-3014.7'}}	We present an improved method for ENTITYUNRELATED of ENTITYUNRELATED which takes advantage of knowledge of ENTITYUNRELATED , while avoiding the need for ENTITYUNRELATED of the ENTITYUNRELATED , and retaining the ENTITY of ENTITYOTHER such as the ENTITYUNRELATED .
We achieve this by using simple, easily-elicited knowledge to produce syntax-based heuristics which transform the target language (e.g. English) into a form more closely resembling the source language, and then by using standard alignment methods to align the transformed bitext.	syntax-based heuristics	target language	usage	{'e1': {'word': 'syntax-based heuristics', 'word_index': [(13, 13)], 'id': 'P04-3014.9'}, 'e2': {'word': 'target language', 'word_index': [(17, 17)], 'id': 'P04-3014.10'}}	We achieve this by using simple , easily - elicited knowledge to produce ENTITY which transform the ENTITYOTHER ( e.g. ENTITYUNRELATED ) into a form more closely resembling the ENTITYUNRELATED , and then by using ENTITYUNRELATED to align the ENTITYUNRELATED .
The knowledge representation method is introduced to be applied in the ICAI system to teach programming language.	knowledge representation method	ICAI system	usage	{'e1': {'word': 'knowledge representation method', 'word_index': [(1, 1)], 'id': 'C82-1003.1'}, 'e2': {'word': 'ICAI system', 'word_index': [(9, 9)], 'id': 'C82-1003.2'}}	The ENTITY is introduced to be applied in the ENTITYOTHER to teach ENTITYUNRELATED .
The directed graph of concepts is mentioned as a method to represent an instructional structure of the domain knowledge.	directed graph	concepts	model-feature	{'e1': {'word': 'directed graph', 'word_index': [(1, 1)], 'id': 'C82-1003.10'}, 'e2': {'word': 'concepts', 'word_index': [(3, 3)], 'id': 'C82-1003.11'}}	The ENTITY of ENTITYOTHER is mentioned as a method to represent an instructional structure of the ENTITYUNRELATED .
We consider the problem of extracting specified types of information from natural language text.	information	natural language text	part_whole	{'e1': {'word': 'information', 'word_index': [(9, 9)], 'id': 'C90-3071.1'}, 'e2': {'word': 'natural language text', 'word_index': [(11, 11)], 'id': 'C90-3071.2'}}	We consider the problem of extracting specified types of ENTITY from ENTITYOTHER .
We describe a specific information extraction task, and report on the benefits of using preference semantics for this task.	preference semantics	information extraction task	usage	{'e1': {'word': 'preference semantics', 'word_index': [(13, 13)], 'id': 'C90-3071.10'}, 'e2': {'word': 'information extraction task', 'word_index': [(4, 4)], 'id': 'C90-3071.9'}}	We describe a specific ENTITYOTHER , and report on the benefits of using ENTITY for this task .
The studies were the result of semantic annotation of the corpus in this domain.	semantic annotation	corpus	usage	{'e1': {'word': 'semantic annotation', 'word_index': [(6, 6)], 'id': 'L08-1593.3'}, 'e2': {'word': 'corpus', 'word_index': [(9, 9)], 'id': 'L08-1593.4'}}	The studies were the result of ENTITY of the ENTITYOTHER in this ENTITYUNRELATED .
We begin by explaining the criteria involved in the annotation process, not only for the colour categories but also for the colour groups created in order to do finer-grained analyses, presenting also some quantitative data regarding these categories and groups.	quantitative data	categories	model-feature	{'e1': {'word': 'quantitative data', 'word_index': [(34, 34)], 'id': 'L08-1593.11'}, 'e2': {'word': 'categories', 'word_index': [(37, 37)], 'id': 'L08-1593.12'}}	We begin by explaining the criteria involved in the ENTITYUNRELATED , not only for the ENTITYUNRELATED but also for the ENTITYUNRELATED created in order to do finer - grained analyses , presenting also some ENTITY regarding these ENTITYOTHER and ENTITYUNRELATED .
We end by explaining how any user who wants to do serious studies using the corpus can collaborate in enhancing the corpus and making their semantic annotations widely available as well.	semantic annotations	corpus	model-feature	{'e1': {'word': 'semantic annotations', 'word_index': [(25, 25)], 'id': 'L08-1593.20'}, 'e2': {'word': 'corpus', 'word_index': [(21, 21)], 'id': 'L08-1593.19'}}	We end by explaining how any user who wants to do serious studies using the ENTITYUNRELATED can collaborate in enhancing the ENTITYOTHER and making their ENTITY widely available as well .
The processing tasks involved in reconstructing the temporal structure of a narrative ( Webber 's e/s structure) are formulated in terms of these two notions.	temporal structure	narrative	model-feature	{'e1': {'word': 'temporal structure', 'word_index': [(6, 6)], 'id': 'E87-1042.14'}, 'e2': {'word': 'narrative', 'word_index': [(9, 9)], 'id': 'E87-1042.15'}}	The ENTITYUNRELATED involved in reconstructing the ENTITY of a ENTITYOTHER ( Webber 's ENTITYUNRELATED ) are formulated in terms of these two notions .
The remainder of the paper analyzes the durational and aspectual knowledge needed for those tasks.	durational and aspectual knowledge	tasks	usage	{'e1': {'word': 'durational and aspectual knowledge', 'word_index': [(7, 7)], 'id': 'E87-1042.17'}, 'e2': {'word': 'tasks', 'word_index': [(11, 11)], 'id': 'E87-1042.18'}}	The remainder of the paper analyzes the ENTITY needed for those ENTITYOTHER .
In this paper we present a project which aims to standardise the format of a set of bilingual lexicons in order to make them available to potential users, to facilitate the exchange of data (among the resources and with other (monolingual) resources ) and to enable reuse of these lexicons for NLP applications like machine translation and multilingual information retrieval.	lexicons	NLP applications	usage	{'e1': {'word': 'lexicons', 'word_index': [(49, 49)], 'id': 'L08-1432.8'}, 'e2': {'word': 'NLP applications', 'word_index': [(51, 51)], 'id': 'L08-1432.9'}}	In this paper we present a project which aims to standardise the format of a set of ENTITYUNRELATED in order to make them available to potential users , to facilitate the exchange of ENTITYUNRELATED ( among the resources and with other ENTITYUNRELATED ) and to enable reuse of these ENTITY for ENTITYOTHER like ENTITYUNRELATED and ENTITYUNRELATED .
After exploiting the linguistic discrepancy between numbered arguments and ARGMs, we built a semantic role classifier based on a hierarchical feature selection strategy.	linguistic discrepancy	arguments	model-feature	{'e1': {'word': 'linguistic discrepancy', 'word_index': [(3, 3)], 'id': 'D08-1034.10'}, 'e2': {'word': 'arguments', 'word_index': [(6, 6)], 'id': 'D08-1034.11'}}	After exploiting the ENTITY between numbered ENTITYOTHER and ENTITYUNRELATED , we built a ENTITYUNRELATED based on a ENTITYUNRELATED .
After exploiting the linguistic discrepancy between numbered arguments and ARGMs, we built a semantic role classifier based on a hierarchical feature selection strategy.	hierarchical feature selection strategy	semantic role classifier	usage	{'e1': {'word': 'hierarchical feature selection strategy', 'word_index': [(17, 17)], 'id': 'D08-1034.14'}, 'e2': {'word': 'semantic role classifier', 'word_index': [(13, 13)], 'id': 'D08-1034.13'}}	After exploiting the ENTITYUNRELATED between numbered ENTITYUNRELATED and ENTITYUNRELATED , we built a ENTITYOTHER based on a ENTITY .
In recent years tree kernels have been proposed for the automatic learning of natural language applications.	tree kernels	automatic learning	usage	{'e1': {'word': 'tree kernels', 'word_index': [(3, 3)], 'id': 'E06-1015.1'}, 'e2': {'word': 'automatic learning', 'word_index': [(9, 9)], 'id': 'E06-1015.2'}}	In recent years ENTITY have been proposed for the ENTITYOTHER of ENTITYUNRELATED .
In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.	tree kernels	processing of natural language	usage	{'e1': {'word': 'tree kernels', 'word_index': [(7, 7)], 'id': 'E06-1015.7'}, 'e2': {'word': 'processing of natural language', 'word_index': [(13, 13)], 'id': 'E06-1015.8'}}	In this paper , we show that ENTITY are very helpful in the ENTITYOTHER as ( a ) we provide a simple algorithm to compute ENTITYUNRELATED in ENTITYUNRELATED and ( b ) our study on the ENTITYUNRELATED of diverse ENTITYUNRELATED show that ENTITYUNRELATED always improve the traditional methods .
In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.	classification properties	tree kernels	model-feature	{'e1': {'word': 'classification properties', 'word_index': [(36, 36)], 'id': 'E06-1015.11'}, 'e2': {'word': 'tree kernels', 'word_index': [(39, 39)], 'id': 'E06-1015.12'}}	In this paper , we show that ENTITYUNRELATED are very helpful in the ENTITYUNRELATED as ( a ) we provide a simple algorithm to compute ENTITYUNRELATED in ENTITYUNRELATED and ( b ) our study on the ENTITY of diverse ENTITYOTHER show that ENTITYUNRELATED always improve the traditional methods .
Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.	Support Vector Machines	argument classification task	usage	{'e1': {'word': 'Support Vector Machines', 'word_index': [(2, 2)], 'id': 'E06-1015.14'}, 'e2': {'word': 'argument classification task', 'word_index': [(6, 6)], 'id': 'E06-1015.15'}}	Experiments with ENTITY on the predicate ENTITYOTHER provide empirical support to our thesis .
This paper presents a study of military applications of advanced speech processing technology which includes three major elements: (1) review and assessment of current efforts in military applications of speech technology ; (2) identification of opportunities for future military applications of advanced speech technology ; and (3) identification of problem areas where research in speech processing is needed to meet application requirements, and of current research thrusts which appear promising.	speech processing technology	military applications	usage	{'e1': {'word': 'speech processing technology', 'word_index': [(9, 9)], 'id': 'H90-1103.2'}, 'e2': {'word': 'military applications', 'word_index': [(6, 6)], 'id': 'H90-1103.1'}}	This paper presents a study of ENTITYOTHER of advanced ENTITY which includes three major elements : ( 1 ) review and assessment of current efforts in ENTITYUNRELATED of ENTITYUNRELATED ; ( 2 ) identification of opportunities for future ENTITYUNRELATED of advanced ENTITYUNRELATED ; and ( 3 ) identification of problem areas where research in ENTITYUNRELATED is needed to meet ENTITYUNRELATED , and of current research thrusts which appear promising .
This paper presents a study of military applications of advanced speech processing technology which includes three major elements: (1) review and assessment of current efforts in military applications of speech technology ; (2) identification of opportunities for future military applications of advanced speech technology ; and (3) identification of problem areas where research in speech processing is needed to meet application requirements, and of current research thrusts which appear promising.	speech technology	military applications	usage	{'e1': {'word': 'speech technology', 'word_index': [(28, 28)], 'id': 'H90-1103.4'}, 'e2': {'word': 'military applications', 'word_index': [(26, 26)], 'id': 'H90-1103.3'}}	This paper presents a study of ENTITYUNRELATED of advanced ENTITYUNRELATED which includes three major elements : ( 1 ) review and assessment of current efforts in ENTITYOTHER of ENTITY ; ( 2 ) identification of opportunities for future ENTITYUNRELATED of advanced ENTITYUNRELATED ; and ( 3 ) identification of problem areas where research in ENTITYUNRELATED is needed to meet ENTITYUNRELATED , and of current research thrusts which appear promising .
This paper presents a study of military applications of advanced speech processing technology which includes three major elements: (1) review and assessment of current efforts in military applications of speech technology ; (2) identification of opportunities for future military applications of advanced speech technology ; and (3) identification of problem areas where research in speech processing is needed to meet application requirements, and of current research thrusts which appear promising.	speech technology	military applications	usage	{'e1': {'word': 'speech technology', 'word_index': [(41, 41)], 'id': 'H90-1103.6'}, 'e2': {'word': 'military applications', 'word_index': [(38, 38)], 'id': 'H90-1103.5'}}	This paper presents a study of ENTITYUNRELATED of advanced ENTITYUNRELATED which includes three major elements : ( 1 ) review and assessment of current efforts in ENTITYUNRELATED of ENTITYUNRELATED ; ( 2 ) identification of opportunities for future ENTITYOTHER of advanced ENTITY ; and ( 3 ) identification of problem areas where research in ENTITYUNRELATED is needed to meet ENTITYUNRELATED , and of current research thrusts which appear promising .
The relationship of this study to previous assessments of military applications of speech technology is discussed, and substantial recent progress is noted.	speech technology	military applications	usage	{'e1': {'word': 'speech technology', 'word_index': [(11, 11)], 'id': 'H90-1103.10'}, 'e2': {'word': 'military applications', 'word_index': [(9, 9)], 'id': 'H90-1103.9'}}	The relationship of this study to previous assessments of ENTITYOTHER of ENTITY is discussed , and substantial recent progress is noted .
Current efforts in military applications of speech technology which are highlighted include : (1) narrowband (2400 b/s) and very low-rate (50-1200 b/s) secure voice communication ; (2) voice/data integration in computer networks ; (3) speech recognition in fighter aircraft, military helicopters, battle management, and air traffic control training systems ; and (4) noise and interference removal for human listeners.	speech technology	military applications	usage	{'e1': {'word': 'speech technology', 'word_index': [(5, 5)], 'id': 'H90-1103.12'}, 'e2': {'word': 'military applications', 'word_index': [(3, 3)], 'id': 'H90-1103.11'}}	Current efforts in ENTITYOTHER of ENTITY which are highlighted include : ( 1 ) ENTITYUNRELATED ; ( 2 ) ENTITYUNRELATED ; ( 3 ) ENTITYUNRELATED in ENTITYUNRELATED , military helicopters , battle management , and air traffic control ENTITYUNRELATED ; and ( 4 ) ENTITYUNRELATED for ENTITYUNRELATED .
"The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels (""uniform lexico-structural processing "")."	declarative lexico-structural transformations	generation and transfer	usage	{'e1': {'word': 'declarative lexico-structural transformations', 'word_index': [(10, 10)], 'id': 'A00-1009.4'}, 'e2': {'word': 'generation and transfer', 'word_index': [(7, 7)], 'id': 'A00-1009.3'}}	"The framework demonstrates a uniform approach to ENTITYOTHER based on ENTITY of ENTITYUNRELATED of syntactic or ENTITYUNRELATED ( "" ENTITYUNRELATED "" ) ."
 Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text.	Lexical co-occurrence statistics	syntactic analysis	usage	{'e1': {'word': 'Lexical co-occurrence statistics', 'word_index': [(0, 0)], 'id': 'W93-0307.1'}, 'e2': {'word': 'syntactic analysis', 'word_index': [(7, 7)], 'id': 'W93-0307.2'}}	ENTITY are becoming widely used in the ENTITYOTHER of ENTITYUNRELATED .
However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters.	sparseness	data	model-feature	{'e1': {'word': 'sparseness', 'word_index': [(9, 9)], 'id': 'W93-0307.5'}, 'e2': {'word': 'data', 'word_index': [(11, 11)], 'id': 'W93-0307.6'}}	However , analyses based solely on ENTITYUNRELATED suffer from ENTITY of ENTITYOTHER : it is sometimes necessary to use a ENTITYUNRELATED in order to reliably estimate ENTITYUNRELATED .
However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters.	less informed model	statistical parameters	usage	{'e1': {'word': 'less informed model', 'word_index': [(20, 20)], 'id': 'W93-0307.7'}, 'e2': {'word': 'statistical parameters', 'word_index': [(26, 26)], 'id': 'W93-0307.8'}}	However , analyses based solely on ENTITYUNRELATED suffer from ENTITYUNRELATED of ENTITYUNRELATED : it is sometimes necessary to use a ENTITY in order to reliably estimate ENTITYOTHER .
"For example, the ""lexical association"" strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth 1991] takes into account only the attachment site (a verb or its direct object ) and the preposition, ignoring the object of the preposition.We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account."	"""lexical association"""	resolving ambiguous prepositional phrase attachments	usage	"{'e1': {'word': '""lexical association""', 'word_index': [(4, 4)], 'id': 'W93-0307.9'}, 'e2': {'word': 'resolving ambiguous prepositional phrase attachments', 'word_index': [(7, 7)], 'id': 'W93-0307.10'}}"	For example , the ENTITY strategy for ENTITYOTHER [ Hindle and Rooth 1991 ] takes into account only the ENTITYUNRELATED ( a ENTITYUNRELATED or its ENTITYUNRELATED ) and the ENTITYUNRELATED , ignoring the ENTITYUNRELATED of the ENTITYUNRELATED . We investigated an extension of the ENTITYUNRELATED strategy to make use of ENTITYUNRELATED , thus permitting a ENTITYUNRELATED strategy to take more information into account .
"For example, the ""lexical association"" strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth 1991] takes into account only the attachment site (a verb or its direct object ) and the preposition, ignoring the object of the preposition.We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account."	noun class information	disambiguation	usage	{'e1': {'word': 'noun class information', 'word_index': [(50, 50)], 'id': 'W93-0307.18'}, 'e2': {'word': 'disambiguation', 'word_index': [(55, 55)], 'id': 'W93-0307.19'}}	For example , the ENTITYUNRELATED strategy for ENTITYUNRELATED [ Hindle and Rooth 1991 ] takes into account only the ENTITYUNRELATED ( a ENTITYUNRELATED or its ENTITYUNRELATED ) and the ENTITYUNRELATED , ignoring the ENTITYUNRELATED of the ENTITYUNRELATED . We investigated an extension of the ENTITYUNRELATED strategy to make use of ENTITY , thus permitting a ENTITYOTHER strategy to take more information into account .
Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone, a qualitative analysis of results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation.	qualitative analysis	results	topic	{'e1': {'word': 'qualitative analysis', 'word_index': [(17, 17)], 'id': 'W93-0307.21'}, 'e2': {'word': 'results', 'word_index': [(19, 19)], 'id': 'W93-0307.22'}}	Although in preliminary experiments the extended strategy did not yield improved performance over ENTITYUNRELATED alone , a ENTITY of ENTITYOTHER suggests that the problem lies not in the ENTITYUNRELATED , but rather in the ENTITYUNRELATED available for each ENTITYUNRELATED in the absence of ENTITYUNRELATED .
Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone, a qualitative analysis of results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation.	multiplicity of classes	noun	model-feature	{'e1': {'word': 'multiplicity of classes', 'word_index': [(34, 34)], 'id': 'W93-0307.24'}, 'e2': {'word': 'noun', 'word_index': [(38, 38)], 'id': 'W93-0307.25'}}	Although in preliminary experiments the extended strategy did not yield improved performance over ENTITYUNRELATED alone , a ENTITYUNRELATED of ENTITYUNRELATED suggests that the problem lies not in the ENTITYUNRELATED , but rather in the ENTITY available for each ENTITYOTHER in the absence of ENTITYUNRELATED .
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.	context-dependent lexicon	error-driven HMM-based text chunk tagger	part_whole	{'e1': {'word': 'context-dependent lexicon', 'word_index': [(6, 6)], 'id': 'W00-0737.2'}, 'e2': {'word': 'error-driven HMM-based text chunk tagger', 'word_index': [(4, 4)], 'id': 'W00-0737.1'}}	This paper proposes an ENTITYOTHER with ENTITY .
Compared with standard HMM-based tagger, this tagger incorporates more contextual information into a lexical entry.	contextual information	tagger	usage	{'e1': {'word': 'contextual information', 'word_index': [(9, 9)], 'id': 'W00-0737.5'}, 'e2': {'word': 'tagger', 'word_index': [(6, 6)], 'id': 'W00-0737.4'}}	Compared with standard ENTITYUNRELATED , this ENTITYOTHER incorporates more ENTITY into a ENTITYUNRELATED .
Finally, memory-based learning is adopted to further improve the performance of the chunk tagger.	memory-based learning	chunk tagger	usage	{'e1': {'word': 'memory-based learning', 'word_index': [(2, 2)], 'id': 'W00-0737.11'}, 'e2': {'word': 'chunk tagger', 'word_index': [(12, 12)], 'id': 'W00-0737.12'}}	Finally , ENTITY is adopted to further improve the performance of the ENTITYOTHER .
However, recent evaluations of a prototype prediction system showed that it significantly decreased the productivity of most translators who used it.	productivity	translators	model-feature	{'e1': {'word': 'productivity', 'word_index': [(14, 14)], 'id': 'W02-1020.6'}, 'e2': {'word': 'translators', 'word_index': [(17, 17)], 'id': 'W02-1020.7'}}	However , recent evaluations of a prototype ENTITYUNRELATED showed that it significantly decreased the ENTITY of most ENTITYOTHER who used it .
In this paper, we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected benefit to the translator, rather than just trying to anticipate some amount of upcoming text.	predictions	expected benefit	result	{'e1': {'word': 'predictions', 'word_index': [(18, 18)], 'id': 'W02-1020.8'}, 'e2': {'word': 'expected benefit', 'word_index': [(22, 22)], 'id': 'W02-1020.9'}}	In this paper , we analyze the reasons for this and propose a solution which consists in seeking ENTITY that maximize the ENTITYOTHER to the ENTITYUNRELATED , rather than just trying to anticipate some amount of ENTITYUNRELATED .
Using a model of a typical translator constructed from data collected in the evaluations of the prediction prototype, we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator.	data	prediction prototype	part_whole	{'e1': {'word': 'data', 'word_index': [(8, 8)], 'id': 'W02-1020.13'}, 'e2': {'word': 'prediction prototype', 'word_index': [(15, 15)], 'id': 'W02-1020.14'}}	Using a model of a ENTITYUNRELATED constructed from ENTITY collected in the evaluations of the ENTITYOTHER , we show that this approach has the potential to turn ENTITYUNRELATED into a help rather than a hindrance to a ENTITYUNRELATED .
"""We describe an experimental text-to-speech system that uses information about syntactic constituency, adjacency to a verb, and constituent length to determine prosodic phrasing for synthetic speech."	syntactic constituency	text-to-speech system	usage	{'e1': {'word': 'syntactic constituency', 'word_index': [(10, 10)], 'id': 'J90-3003.2'}, 'e2': {'word': 'text-to-speech system', 'word_index': [(5, 5)], 'id': 'J90-3003.1'}}	""" We describe an experimental ENTITYOTHER that uses information about ENTITY , ENTITYUNRELATED , and ENTITYUNRELATED to determine ENTITYUNRELATED for ENTITYUNRELATED ."
"Results so far indicate that the current system performs well when measured against a corpus of judgments of prosodic phrasing."""	judgments of prosodic phrasing	corpus	part_whole	{'e1': {'word': 'judgments of prosodic phrasing', 'word_index': [(16, 16)], 'id': 'J90-3003.13'}, 'e2': {'word': 'corpus', 'word_index': [(14, 14)], 'id': 'J90-3003.12'}}	"Results so far indicate that the current system performs well when measured against a ENTITYOTHER of ENTITY . """
In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars.	potential entries	on-line corpus	part_whole	{'e1': {'word': 'potential entries', 'word_index': [(10, 10)], 'id': 'P98-2182.2'}, 'e2': {'word': 'on-line corpus', 'word_index': [(16, 16)], 'id': 'P98-2182.3'}}	In this paper , we present an algorithm for extracting ENTITY for a category from an ENTITYOTHER , based upon a small set of ENTITYUNRELATED .
In this paper, we describe our experiments on statistical word sense disambiguation (WSD) using two systems based on different approaches : Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features.	word tokens	Naive Bayes	usage	{'e1': {'word': 'word tokens', 'word_index': [(20, 20)], 'id': 'W04-0833.3'}, 'e2': {'word': 'Naive Bayes', 'word_index': [(18, 18)], 'id': 'W04-0833.2'}}	In this paper , we describe our experiments on ENTITYUNRELATED using two systems based on different approaches : ENTITYOTHER on ENTITY and ENTITYUNRELATED on ENTITYUNRELATED .
In this paper, we describe our experiments on statistical word sense disambiguation (WSD) using two systems based on different approaches : Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features.	local syntactic and semantic features	Maximum Entropy	usage	{'e1': {'word': 'local syntactic and semantic features', 'word_index': [(24, 24)], 'id': 'W04-0833.5'}, 'e2': {'word': 'Maximum Entropy', 'word_index': [(22, 22)], 'id': 'W04-0833.4'}}	In this paper , we describe our experiments on ENTITYUNRELATED using two systems based on different approaches : ENTITYUNRELATED on ENTITYUNRELATED and ENTITYOTHER on ENTITY .
In the first approach, we consider a context window and a sub-window within it around the word to disambiguate.	sub-window	context window	part_whole	{'e1': {'word': 'sub-window', 'word_index': [(11, 11)], 'id': 'W04-0833.7'}, 'e2': {'word': 'context window', 'word_index': [(8, 8)], 'id': 'W04-0833.6'}}	In the first approach , we consider a ENTITYOTHER and a ENTITY within it around the ENTITYUNRELATED to disambiguate .
In the second system, sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner.	syntactic structure	sense resolution	usage	{'e1': {'word': 'syntactic structure', 'word_index': [(11, 11)], 'id': 'W04-0833.18'}, 'e2': {'word': 'sense resolution', 'word_index': [(5, 5)], 'id': 'W04-0833.17'}}	In the second system , ENTITYOTHER is done using an approximate ENTITY as well as ENTITYUNRELATED of ENTITYUNRELATED as ENTITYUNRELATED to a ENTITYUNRELATED .
In the second system, sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner.	semantics	neighboring nouns	model-feature	{'e1': {'word': 'semantics', 'word_index': [(15, 15)], 'id': 'W04-0833.19'}, 'e2': {'word': 'neighboring nouns', 'word_index': [(17, 17)], 'id': 'W04-0833.20'}}	In the second system , ENTITYUNRELATED is done using an approximate ENTITYUNRELATED as well as ENTITY of ENTITYOTHER as ENTITYUNRELATED to a ENTITYUNRELATED .
The Document Understanding Conference (DUC) 2005 evaluation had a single user-oriented, question-focused summarization task, which was to synthesize from a set of 25-50 documents a well-organized, fluent answer to a complex question.	user-oriented, question-focused summarization task	documents	usage	{'e1': {'word': 'user-oriented, question-focused summarization task', 'word_index': [(5, 5)], 'id': 'W06-0707.2'}, 'e2': {'word': 'documents', 'word_index': [(18, 18)], 'id': 'W06-0707.3'}}	The ENTITYUNRELATED had a single ENTITY , which was to synthesize from a set of 25 - 50 ENTITYOTHER a well - organized , ENTITYUNRELATED to a ENTITYUNRELATED .
The evaluation shows that the best summarization systems have difficulty extracting relevant sentences in response to complex questions (as opposed to representative sentences that might be appropriate to a generic summary ).	summarization systems	extracting relevant sentences	usage	{'e1': {'word': 'summarization systems', 'word_index': [(6, 6)], 'id': 'W06-0707.6'}, 'e2': {'word': 'extracting relevant sentences', 'word_index': [(9, 9)], 'id': 'W06-0707.7'}}	The evaluation shows that the best ENTITY have difficulty ENTITYOTHER in response to ENTITYUNRELATED ( as opposed to ENTITYUNRELATED that might be appropriate to a ENTITYUNRELATED ) .
This paper presents two systems for textual entailment, both employing decision trees as a supervised learning algorithm.	decision trees	textual entailment	usage	{'e1': {'word': 'decision trees', 'word_index': [(10, 10)], 'id': 'W07-1420.2'}, 'e2': {'word': 'textual entailment', 'word_index': [(6, 6)], 'id': 'W07-1420.1'}}	This paper presents two systems for ENTITYOTHER , both employing ENTITY as a ENTITYUNRELATED .
The first one is based primarily on the concept of lexical overlap, considering a bag of words similarity overlap measure to form a mapping of terms in the hypothesis to the source text.	bag of words similarity overlap measure	mapping	usage	{'e1': {'word': 'bag of words similarity overlap measure', 'word_index': [(14, 14)], 'id': 'W07-1420.5'}, 'e2': {'word': 'mapping', 'word_index': [(18, 18)], 'id': 'W07-1420.6'}}	The first one is based primarily on the concept of ENTITYUNRELATED , considering a ENTITY to form a ENTITYOTHER of ENTITYUNRELATED in the ENTITYUNRELATED to the ENTITYUNRELATED .
We contrast the MT condition, for both text and audio data types, with high quality human reference Gold Standard (GS) translations.	MT condition	human reference Gold Standard (GS) translations	compare	{'e1': {'word': 'MT condition', 'word_index': [(3, 3)], 'id': 'N07-2020.9'}, 'e2': {'word': 'human reference Gold Standard (GS) translations', 'word_index': [(15, 15)], 'id': 'N07-2020.12'}}	We contrast the ENTITY , for both ENTITYUNRELATED and ENTITYUNRELATED types , with high quality ENTITYOTHER .
Overall, subjects achieved 95% comprehension for GS and 74% for MT, across 4 genres and 3 difficulty levels.	GS	MT	compare	{'e1': {'word': 'GS', 'word_index': [(6, 6)], 'id': 'N07-2020.14'}, 'e2': {'word': 'MT', 'word_index': [(10, 10)], 'id': 'N07-2020.16'}}	Overall , subjects achieved ENTITYUNRELATED for ENTITY and ENTITYUNRELATED for ENTITYOTHER , across 4 ENTITYUNRELATED and 3 ENTITYUNRELATED .
This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations.	features	event-matching strategy	usage	{'e1': {'word': 'features', 'word_index': [(7, 7)], 'id': 'P08-2037.2'}, 'e2': {'word': 'event-matching strategy', 'word_index': [(5, 5)], 'id': 'P08-2037.1'}}	This paper describes a novel ENTITYOTHER using ENTITY obtained from the ENTITYUNRELATED of ENTITYUNRELATED .
This paper describes a novel event-matching strategy using features obtained from the transitive closure of dependency relations.	transitive closure	dependency relations	usage	{'e1': {'word': 'transitive closure', 'word_index': [(11, 11)], 'id': 'P08-2037.3'}, 'e2': {'word': 'dependency relations', 'word_index': [(13, 13)], 'id': 'P08-2037.4'}}	This paper describes a novel ENTITYUNRELATED using ENTITYUNRELATED obtained from the ENTITY of ENTITYOTHER .
The method yields a model capable of matching events with an F-measure of 66.5%.	model	matching events	usage	{'e1': {'word': 'model', 'word_index': [(4, 4)], 'id': 'P08-2037.5'}, 'e2': {'word': 'matching events', 'word_index': [(7, 7)], 'id': 'P08-2037.6'}}	The method yields a ENTITY capable of ENTITYOTHER with an ENTITYUNRELATED of 66.5 %.
The objective of this paper is to demonstrate how an existing web application can be modified using VoiceXML to enable non-visual access from any phone.	VoiceXML	web application	usage	{'e1': {'word': 'VoiceXML', 'word_index': [(16, 16)], 'id': 'W08-1504.7'}, 'e2': {'word': 'web application', 'word_index': [(11, 11)], 'id': 'W08-1504.6'}}	The objective of this paper is to demonstrate how an existing ENTITYOTHER can be modified using ENTITY to enable ENTITYUNRELATED from any ENTITYUNRELATED .
In order to elucidate the entire process, we present a sample Package Tracking System application, which is based on an existing website and provides the same functionality as the website does.	website	Package Tracking System application	usage	{'e1': {'word': 'website', 'word_index': [(20, 20)], 'id': 'W08-1504.19'}, 'e2': {'word': 'Package Tracking System application', 'word_index': [(12, 12)], 'id': 'W08-1504.18'}}	In order to elucidate the entire process , we present a sample ENTITYOTHER , which is based on an existing ENTITY and provides the same functionality as the ENTITYUNRELATED does .
This paper reports the development of log-linear models for the disambiguation in wide-coverage HPSG parsing.	log-linear models	disambiguation in wide-coverage HPSG parsing	usage	{'e1': {'word': 'log-linear models', 'word_index': [(6, 6)], 'id': 'P05-1011.1'}, 'e2': {'word': 'disambiguation in wide-coverage HPSG parsing', 'word_index': [(9, 9)], 'id': 'P05-1011.2'}}	This paper reports the development of ENTITY for the ENTITYOTHER .
The estimation of log-linear models requires high computational cost, especially with wide-coverage grammars.	computational cost	log-linear models	model-feature	{'e1': {'word': 'computational cost', 'word_index': [(6, 6)], 'id': 'P05-1011.4'}, 'e2': {'word': 'log-linear models', 'word_index': [(3, 3)], 'id': 'P05-1011.3'}}	The estimation of ENTITYOTHER requires high ENTITY , especially with ENTITYUNRELATED .
A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences.	disambiguation models	parsing	usage	{'e1': {'word': 'disambiguation models', 'word_index': [(16, 16)], 'id': 'P05-1011.9'}, 'e2': {'word': 'parsing', 'word_index': [(19, 19)], 'id': 'P05-1011.10'}}	A series of experiments empirically evaluated the ENTITYUNRELATED , and also examined the performance of the ENTITY on the ENTITYOTHER of real - world ENTITYUNRELATED .
 Text normalization is an important aspect of successful information retrieval from medical documents such as clinical notes, radiology reports and discharge summaries	information retrieval	medical documents	usage	{'e1': {'word': 'information retrieval', 'word_index': [(7, 7)], 'id': 'P02-1021.2'}, 'e2': {'word': 'medical documents', 'word_index': [(9, 9)], 'id': 'P02-1021.3'}}	ENTITYUNRELATED is an important aspect of successful ENTITY from ENTITYOTHER such as ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED
In the medical domain, a significant part of the general problem of text normalization is abbreviation and acronym disambiguation.	abbreviation and acronym disambiguation	text normalization	part_whole	{'e1': {'word': 'abbreviation and acronym disambiguation', 'word_index': [(14, 14)], 'id': 'P02-1021.9'}, 'e2': {'word': 'text normalization', 'word_index': [(12, 12)], 'id': 'P02-1021.8'}}	In the ENTITYUNRELATED , a significant part of the general problem of ENTITYOTHER is ENTITY .
Numerous abbreviations are used routinely throughout such texts and knowing their meaning is critical to data retrieval from the document.	abbreviations	texts	part_whole	{'e1': {'word': 'abbreviations', 'word_index': [(1, 1)], 'id': 'P02-1021.10'}, 'e2': {'word': 'texts', 'word_index': [(7, 7)], 'id': 'P02-1021.11'}}	Numerous ENTITY are used routinely throughout such ENTITYOTHER and knowing their meaning is critical to ENTITYUNRELATED from the ENTITYUNRELATED .
Numerous abbreviations are used routinely throughout such texts and knowing their meaning is critical to data retrieval from the document.	data retrieval	document	usage	{'e1': {'word': 'data retrieval', 'word_index': [(15, 15)], 'id': 'P02-1021.12'}, 'e2': {'word': 'document', 'word_index': [(18, 18)], 'id': 'P02-1021.13'}}	Numerous ENTITYUNRELATED are used routinely throughout such ENTITYUNRELATED and knowing their meaning is critical to ENTITY from the ENTITYOTHER .
In this paper I will demonstrate a method of automatically generating training data for Maximum Entropy (ME) modeling of abbreviations and acronyms and will show that using ME modeling is a promising technique for abbreviation and acronym normalization.	automatically generating training data	Maximum Entropy (ME) modeling	usage	{'e1': {'word': 'automatically generating training data', 'word_index': [(9, 9)], 'id': 'P02-1021.14'}, 'e2': {'word': 'Maximum Entropy (ME) modeling', 'word_index': [(11, 11)], 'id': 'P02-1021.15'}}	In this paper I will demonstrate a method of ENTITY for ENTITYOTHER of ENTITYUNRELATED and ENTITYUNRELATED and will show that using ENTITYUNRELATED is a promising technique for ENTITYUNRELATED .
In this paper I will demonstrate a method of automatically generating training data for Maximum Entropy (ME) modeling of abbreviations and acronyms and will show that using ME modeling is a promising technique for abbreviation and acronym normalization.	ME modeling	abbreviation and acronym normalization	usage	{'e1': {'word': 'ME modeling', 'word_index': [(21, 21)], 'id': 'P02-1021.18'}, 'e2': {'word': 'abbreviation and acronym normalization', 'word_index': [(27, 27)], 'id': 'P02-1021.19'}}	In this paper I will demonstrate a method of ENTITYUNRELATED for ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED and will show that using ENTITY is a promising technique for ENTITYOTHER .
I report on the results of an experiment involving training a number of ME models used to normalize abbreviations and acronyms on a sample of 10,000 rheumatology notes with ~89% accuracy.	ME models	abbreviations	usage	{'e1': {'word': 'ME models', 'word_index': [(13, 13)], 'id': 'P02-1021.20'}, 'e2': {'word': 'abbreviations', 'word_index': [(17, 17)], 'id': 'P02-1021.21'}}	I report on the results of an experiment involving training a number of ENTITY used to normalize ENTITYOTHER and ENTITYUNRELATED on a sample of 10,000 ENTITYUNRELATED with ~ 89 % ENTITYUNRELATED .
In this paper we describe the roots of Controlled English (CE), the analysis of several existing CE grammars, the development of a well-founded 150-rule CE grammar (COGRAM), the elaboration of an algorithmic variant (ALCOGRAM) as a basis for NLP applications, the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively, and the preparatory study into a Controlled English grammar and style checker within a desktop publishing (DTP) environment.	algorithmic variant (ALCOGRAM)	NLP applications	usage	{'e1': {'word': 'algorithmic variant (ALCOGRAM)', 'word_index': [(30, 30)], 'id': 'C92-2090.4'}, 'e2': {'word': 'NLP applications', 'word_index': [(35, 35)], 'id': 'C92-2090.5'}}	In this paper we describe the roots of ENTITYUNRELATED , the analysis of several existing ENTITYUNRELATED , the development of a well - founded ENTITYUNRELATED , the elaboration of an ENTITY as a basis for ENTITYOTHER , the use of ENTITYUNRELATED in a ENTITYUNRELATED teaching ENTITYUNRELATED how to use it effectively , and the preparatory study into a ENTITYUNRELATED and ENTITYUNRELATED within a ENTITYUNRELATED .
In this paper we describe the roots of Controlled English (CE), the analysis of several existing CE grammars, the development of a well-founded 150-rule CE grammar (COGRAM), the elaboration of an algorithmic variant (ALCOGRAM) as a basis for NLP applications, the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively, and the preparatory study into a Controlled English grammar and style checker within a desktop publishing (DTP) environment.	ALCOGRAM	CA1 program	usage	{'e1': {'word': 'ALCOGRAM', 'word_index': [(40, 40)], 'id': 'C92-2090.6'}, 'e2': {'word': 'CA1 program', 'word_index': [(43, 43)], 'id': 'C92-2090.7'}}	In this paper we describe the roots of ENTITYUNRELATED , the analysis of several existing ENTITYUNRELATED , the development of a well - founded ENTITYUNRELATED , the elaboration of an ENTITYUNRELATED as a basis for ENTITYUNRELATED , the use of ENTITY in a ENTITYOTHER teaching ENTITYUNRELATED how to use it effectively , and the preparatory study into a ENTITYUNRELATED and ENTITYUNRELATED within a ENTITYUNRELATED .
In this paper we describe the roots of Controlled English (CE), the analysis of several existing CE grammars, the development of a well-founded 150-rule CE grammar (COGRAM), the elaboration of an algorithmic variant (ALCOGRAM) as a basis for NLP applications, the use of ALCOGRAM in a CA1 program teaching writers how to use it effectively, and the preparatory study into a Controlled English grammar and style checker within a desktop publishing (DTP) environment.	style checker	desktop publishing (DTP) environment	part_whole	{'e1': {'word': 'style checker', 'word_index': [(60, 60)], 'id': 'C92-2090.10'}, 'e2': {'word': 'desktop publishing (DTP) environment', 'word_index': [(63, 63)], 'id': 'C92-2090.11'}}	In this paper we describe the roots of ENTITYUNRELATED , the analysis of several existing ENTITYUNRELATED , the development of a well - founded ENTITYUNRELATED , the elaboration of an ENTITYUNRELATED as a basis for ENTITYUNRELATED , the use of ENTITYUNRELATED in a ENTITYUNRELATED teaching ENTITYUNRELATED how to use it effectively , and the preparatory study into a ENTITYUNRELATED and ENTITY within a ENTITYOTHER .
We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement.	discriminative, latent variable approach	syntactic parsing	usage	{'e1': {'word': 'discriminative, latent variable approach', 'word_index': [(3, 3)], 'id': 'D08-1091.1'}, 'e2': {'word': 'syntactic parsing', 'word_index': [(5, 5)], 'id': 'D08-1091.2'}}	We present a ENTITY to ENTITYOTHER in which ENTITYUNRELATED exist at multiple scales of ENTITYUNRELATED .
We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement.	refinement	rules	model-feature	{'e1': {'word': 'refinement', 'word_index': [(14, 14)], 'id': 'D08-1091.4'}, 'e2': {'word': 'rules', 'word_index': [(8, 8)], 'id': 'D08-1091.3'}}	We present a ENTITYUNRELATED to ENTITYUNRELATED in which ENTITYOTHER exist at multiple scales of ENTITY .
The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories ).	grammar productions	latent variable CRF grammar	usage	{'e1': {'word': 'grammar productions', 'word_index': [(13, 13)], 'id': 'D08-1091.7'}, 'e2': {'word': 'latent variable CRF grammar', 'word_index': [(5, 5)], 'id': 'D08-1091.5'}}	The model is formally a ENTITYOTHER over ENTITYUNRELATED , learned by iteratively splitting ENTITY ( not ENTITYUNRELATED ) .
On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.	method	parsing accuracies	result	{'e1': {'word': 'method', 'word_index': [(9, 9)], 'id': 'D08-1091.19'}, 'e2': {'word': 'parsing accuracies', 'word_index': [(14, 14)], 'id': 'D08-1091.20'}}	On a variety of ENTITYUNRELATED and ENTITYUNRELATED , this ENTITY produces the best published ENTITYOTHER with the smallest reported ENTITYUNRELATED .
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further.	grammar PCFG*	extended GLR parsing algorithm	usage	{'e1': {'word': 'grammar PCFG*', 'word_index': [(6, 6)], 'id': 'C02-2028.2'}, 'e2': {'word': 'extended GLR parsing algorithm', 'word_index': [(4, 4)], 'id': 'C02-2028.1'}}	This paper presents an ENTITYOTHER with ENTITY that is based on ENTITYUNRELATED and extends it further .
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further.	Tomita's GLR parsing algorithm	grammar PCFG*	usage	"{'e1': {'word': ""Tomita's GLR parsing algorithm"", 'word_index': [(11, 11)], 'id': 'C02-2028.3'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(6, 6)], 'id': 'C02-2028.2'}}"	This paper presents an ENTITYUNRELATED with ENTITYOTHER that is based on ENTITY and extends it further .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	PCFG	grammar PCFG*	usage	{'e1': {'word': 'PCFG', 'word_index': [(10, 10)], 'id': 'C02-2028.5'}, 'e2': {'word': 'grammar PCFG*', 'word_index': [(5, 5)], 'id': 'C02-2028.4'}}	We also define a new ENTITYOTHER that is based on ENTITY and assigns not only ENTITYUNRELATED but also ENTITYUNRELATED associated with each ENTITYUNRELATED .
We also define a new grammar PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule.	frequency	rule	model-feature	{'e1': {'word': 'frequency', 'word_index': [(18, 18)], 'id': 'C02-2028.7'}, 'e2': {'word': 'rule', 'word_index': [(22, 22)], 'id': 'C02-2028.8'}}	We also define a new ENTITYUNRELATED that is based on ENTITYUNRELATED and assigns not only ENTITYUNRELATED but also ENTITY associated with each ENTITYOTHER .
So our syntactic parsing system is implemented based on rule-based approach and statistics approach.	rule-based approach	syntactic parsing system	usage	{'e1': {'word': 'rule-based approach', 'word_index': [(7, 7)], 'id': 'C02-2028.10'}, 'e2': {'word': 'syntactic parsing system', 'word_index': [(2, 2)], 'id': 'C02-2028.9'}}	So our ENTITYOTHER is implemented based on ENTITY and ENTITYUNRELATED .
In this paper a method for controlling the dialog in a natural language (NL) system is presented.	controlling the dialog	natural language (NL) system	part_whole	{'e1': {'word': 'controlling the dialog', 'word_index': [(6, 6)], 'id': 'E89-1004.1'}, 'e2': {'word': 'natural language (NL) system', 'word_index': [(9, 9)], 'id': 'E89-1004.2'}}	In this paper a method for ENTITY in a ENTITYOTHER is presented .
It provides a deep modeling of information processing based on time dependent propositional attitudes of the interacting agents.	time dependent propositional attitudes	modeling	usage	{'e1': {'word': 'time dependent propositional attitudes', 'word_index': [(9, 9)], 'id': 'E89-1004.5'}, 'e2': {'word': 'modeling', 'word_index': [(4, 4)], 'id': 'E89-1004.3'}}	It provides a deep ENTITYOTHER of ENTITYUNRELATED based on ENTITY of the ENTITYUNRELATED .
Knowledge about the state of the dialog is represented in a dedicated language and changes of this state are described by a compact set of rules.	state	dialog	model-feature	{'e1': {'word': 'state', 'word_index': [(3, 3)], 'id': 'E89-1004.8'}, 'e2': {'word': 'dialog', 'word_index': [(6, 6)], 'id': 'E89-1004.9'}}	ENTITYUNRELATED about the ENTITY of the ENTITYOTHER is represented in a dedicated ENTITYUNRELATED and changes of this ENTITYUNRELATED are described by a compact set of ENTITYUNRELATED .
An appropriate organization of rule application is introduced including the initiation of an adequate system reaction.	initiation	rule application	part_whole	{'e1': {'word': 'initiation', 'word_index': [(9, 9)], 'id': 'E89-1004.14'}, 'e2': {'word': 'rule application', 'word_index': [(4, 4)], 'id': 'E89-1004.13'}}	An appropriate organization of ENTITYOTHER is introduced including the ENTITY of an ENTITYUNRELATED .
This paper reports the principles behind designing a tagset to cover Russian morphosyntactic phenomena, modifications of the core tagset, and its evaluation.	tagset	Russian morphosyntactic phenomena	model-feature	{'e1': {'word': 'tagset', 'word_index': [(8, 8)], 'id': 'L08-1539.1'}, 'e2': {'word': 'Russian morphosyntactic phenomena', 'word_index': [(11, 11)], 'id': 'L08-1539.2'}}	This paper reports the principles behind designing a ENTITY to cover ENTITYOTHER , modifications of the ENTITYUNRELATED , and its evaluation .
The final tagset contains about 600 tags and achieves about 95% accuracy on the disambiguated portion of the Russian National Corpus.	tags	tagset	part_whole	{'e1': {'word': 'tags', 'word_index': [(6, 6)], 'id': 'L08-1539.9'}, 'e2': {'word': 'tagset', 'word_index': [(2, 2)], 'id': 'L08-1539.8'}}	The final ENTITYOTHER contains about 600 ENTITY and achieves about ENTITYUNRELATED on the disambiguated portion of the ENTITYUNRELATED .
Within the framework of translation knowledge acquisition from WWW news sites, this paper studies issues on the effect of cross-language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora.	translation knowledge acquisition	WWW news sites	usage	{'e1': {'word': 'translation knowledge acquisition', 'word_index': [(4, 4)], 'id': 'E03-1023.1'}, 'e2': {'word': 'WWW news sites', 'word_index': [(6, 6)], 'id': 'E03-1023.2'}}	Within the framework of ENTITY from ENTITYOTHER , this paper studies issues on the effect of ENTITYUNRELATED of relevant ENTITYUNRELATED in ENTITYUNRELATED from ENTITYUNRELATED .
Within the framework of translation knowledge acquisition from WWW news sites, this paper studies issues on the effect of cross-language retrieval of relevant texts in bilingual lexicon acquisition from comparable corpora.	texts	comparable corpora	part_whole	{'e1': {'word': 'texts', 'word_index': [(19, 19)], 'id': 'E03-1023.4'}, 'e2': {'word': 'comparable corpora', 'word_index': [(23, 23)], 'id': 'E03-1023.6'}}	Within the framework of ENTITYUNRELATED from ENTITYUNRELATED , this paper studies issues on the effect of ENTITYUNRELATED of relevant ENTITY in ENTITYUNRELATED from ENTITYOTHER .
We experimentally show that it is quite effective to reduce the candidate bilingual term pairs against which bilingual term correspondences are estimated, in terms of both computational complexity and the performance of precise estimation of bilingual term correspondences.	estimation	bilingual term correspondences	model-feature	{'e1': {'word': 'estimation', 'word_index': [(28, 28)], 'id': 'E03-1023.10'}, 'e2': {'word': 'bilingual term correspondences', 'word_index': [(30, 30)], 'id': 'E03-1023.11'}}	We experimentally show that it is quite effective to reduce the ENTITYUNRELATED against which ENTITYUNRELATED are estimated , in terms of both ENTITYUNRELATED and the performance of precise ENTITY of ENTITYOTHER .
We present an approach to building a test collection of research papers.	research papers	test collection	part_whole	{'e1': {'word': 'research papers', 'word_index': [(9, 9)], 'id': 'N06-1050.2'}, 'e2': {'word': 'test collection', 'word_index': [(7, 7)], 'id': 'N06-1050.1'}}	We present an approach to building a ENTITYOTHER of ENTITY .
The resultant test collection is different from TREC's in that it comprises scientific articles rather than newspaper text and, thus, allows for IR experiments that include citation information.	scientific articles	test collection	part_whole	{'e1': {'word': 'scientific articles', 'word_index': [(11, 11)], 'id': 'N06-1050.9'}, 'e2': {'word': 'test collection', 'word_index': [(2, 2)], 'id': 'N06-1050.7'}}	The resultant ENTITYOTHER is different from ENTITYUNRELATED in that it comprises ENTITY rather than ENTITYUNRELATED and , thus , allows for ENTITYUNRELATED that include ENTITYUNRELATED .
The test collection currently consists of 170 queries with relevance judgements; the document collection is the ACL Anthology.	queries	test collection	part_whole	{'e1': {'word': 'queries', 'word_index': [(6, 6)], 'id': 'N06-1050.14'}, 'e2': {'word': 'test collection', 'word_index': [(1, 1)], 'id': 'N06-1050.13'}}	The ENTITYOTHER currently consists of 170 ENTITY with ENTITYUNRELATED ; the ENTITYUNRELATED is the ENTITYUNRELATED .
This paper presents a model for generating prosodically appropriate synthesized responses to database queries using Combinatory Categorial Grammar (CCG - cf.	Combinatory Categorial Grammar	prosodically appropriate synthesized responses	usage	{'e1': {'word': 'Combinatory Categorial Grammar', 'word_index': [(11, 11)], 'id': 'H94-1035.3'}, 'e2': {'word': 'prosodically appropriate synthesized responses', 'word_index': [(7, 7)], 'id': 'H94-1035.1'}}	This paper presents a model for generating ENTITYOTHER to ENTITYUNRELATED using ENTITY ( CCG - cf.
As part of our TIPSTER III research program, we have continued our research into strategies to resolve coreferences within a free text document; this research was begun during our TIPSTER II research program.	coreferences	free text document	part_whole	{'e1': {'word': 'coreferences', 'word_index': [(15, 15)], 'id': 'X98-1010.2'}, 'e2': {'word': 'free text document', 'word_index': [(18, 18)], 'id': 'X98-1010.3'}}	As part of our ENTITYUNRELATED , we have continued our research into strategies to resolve ENTITY within a ENTITYOTHER ; this research was begun during our ENTITYUNRELATED .
It also has raised the importance of understanding the structure of a document in order to guide the coreference resolution process.	structure	document	model-feature	{'e1': {'word': 'structure', 'word_index': [(9, 9)], 'id': 'X98-1010.19'}, 'e2': {'word': 'document', 'word_index': [(12, 12)], 'id': 'X98-1010.20'}}	It also has raised the importance of understanding the ENTITY of a ENTITYOTHER in order to guide the ENTITYUNRELATED .
We describe the design and implementation of the dialogue management module in a voice operated car-driver information system.	dialogue management module	voice operated car-driver information system	part_whole	{'e1': {'word': 'dialogue management module', 'word_index': [(8, 8)], 'id': 'W97-0616.1'}, 'e2': {'word': 'voice operated car-driver information system', 'word_index': [(11, 11)], 'id': 'W97-0616.2'}}	We describe the design and implementation of the ENTITY in a ENTITYOTHER .
In this paper, we show how these constraints influence the design and subsequent implementation of the Dialogue Manager module, and how the additional requirements fit in with the 7 commandments.	constraints	Dialogue Manager module	result	{'e1': {'word': 'constraints', 'word_index': [(8, 8)], 'id': 'W97-0616.13'}, 'e2': {'word': 'Dialogue Manager module', 'word_index': [(17, 17)], 'id': 'W97-0616.14'}}	In this paper , we show how these ENTITY influence the design and subsequent implementation of the ENTITYOTHER , and how the additional requirements fit in with the 7 ENTITYUNRELATED .
HMM-based models are developed for the alignment of words and phrases in bitext	HMM-based models	alignment	usage	{'e1': {'word': 'HMM-based models', 'word_index': [(0, 0)], 'id': 'H05-1022.1'}, 'e2': {'word': 'alignment', 'word_index': [(5, 5)], 'id': 'H05-1022.2'}}	ENTITY are developed for the ENTITYOTHER of ENTITYUNRELATED and ENTITYUNRELATED in ENTITYUNRELATED
We find that Chinese-English word alignment performance is comparable to that of IBM Model-4 even over large training bitexts.	Chinese-English word alignment performance	IBM Model-4	compare	{'e1': {'word': 'Chinese-English word alignment performance', 'word_index': [(3, 3)], 'id': 'H05-1022.9'}, 'e2': {'word': 'IBM Model-4', 'word_index': [(9, 9)], 'id': 'H05-1022.10'}}	We find that ENTITY is comparable to that of ENTITYOTHER even over large ENTITYUNRELATED .
Phrase pairs extracted from word alignments generated under the model can also be used for phrase-based translation, and in Chinese to English and Arabic to English translation, performance is comparable to systems based on Model-4 alignments.	Phrase pairs	word alignments	part_whole	{'e1': {'word': 'Phrase pairs', 'word_index': [(0, 0)], 'id': 'H05-1022.12'}, 'e2': {'word': 'word alignments', 'word_index': [(3, 3)], 'id': 'H05-1022.13'}}	ENTITY extracted from ENTITYOTHER generated under the ENTITYUNRELATED can also be used for ENTITYUNRELATED , and in ENTITYUNRELATED , ENTITYUNRELATED is comparable to ENTITYUNRELATED based on ENTITYUNRELATED .
Phrase pairs extracted from word alignments generated under the model can also be used for phrase-based translation, and in Chinese to English and Arabic to English translation, performance is comparable to systems based on Model-4 alignments.	Model-4 alignments	systems	usage	{'e1': {'word': 'Model-4 alignments', 'word_index': [(26, 26)], 'id': 'H05-1022.19'}, 'e2': {'word': 'systems', 'word_index': [(23, 23)], 'id': 'H05-1022.18'}}	ENTITYUNRELATED extracted from ENTITYUNRELATED generated under the ENTITYUNRELATED can also be used for ENTITYUNRELATED , and in ENTITYUNRELATED , ENTITYUNRELATED is comparable to ENTITYOTHER based on ENTITY .
This paper proposed a new query translation method based on the mutual information matrices of terms in the Chinese and English corpora.	mutual information matrices	query translation method	usage	{'e1': {'word': 'mutual information matrices', 'word_index': [(9, 9)], 'id': 'W00-1313.2'}, 'e2': {'word': 'query translation method', 'word_index': [(5, 5)], 'id': 'W00-1313.1'}}	This paper proposed a new ENTITYOTHER based on the ENTITY of ENTITYUNRELATED in the ENTITYUNRELATED .
A novel selection method for translations of query terms is also presented in detail.	selection method	translations	usage	{'e1': {'word': 'selection method', 'word_index': [(2, 2)], 'id': 'W00-1313.13'}, 'e2': {'word': 'translations', 'word_index': [(4, 4)], 'id': 'W00-1313.14'}}	A novel ENTITY for ENTITYOTHER of ENTITYUNRELATED is also presented in detail .
The evaluation results show that the retrieval performance achieved by our query translation method is about 73% of monolingual information retrieval and is about 28% higher than that of simple word-by-word translation way.	query translation method	retrieval performance	result	{'e1': {'word': 'query translation method', 'word_index': [(10, 10)], 'id': 'W00-1313.21'}, 'e2': {'word': 'retrieval performance', 'word_index': [(6, 6)], 'id': 'W00-1313.20'}}	The evaluation results show that the ENTITYOTHER achieved by our ENTITY is about 73 % of ENTITYUNRELATED and is about 28 % higher than that of simple ENTITYUNRELATED way .
First, we describe a method of classifying facts (information) into categories or levels; where each level signifies a different degree of difficulty of extracting a fact from a piece of text containing it.	level	degree of difficulty	model-feature	{'e1': {'word': 'level', 'word_index': [(19, 19)], 'id': 'W00-0106.4'}, 'e2': {'word': 'degree of difficulty', 'word_index': [(23, 23)], 'id': 'W00-0106.5'}}	First , we describe a method of classifying ENTITYUNRELATED ( information ) into categories or ENTITYUNRELATED ; where each ENTITY signifies a different ENTITYOTHER of extracting a ENTITYUNRELATED from a piece of ENTITYUNRELATED containing it .
First, we describe a method of classifying facts (information) into categories or levels; where each level signifies a different degree of difficulty of extracting a fact from a piece of text containing it.	fact	text	part_whole	{'e1': {'word': 'fact', 'word_index': [(27, 27)], 'id': 'W00-0106.6'}, 'e2': {'word': 'text', 'word_index': [(32, 32)], 'id': 'W00-0106.7'}}	First , we describe a method of classifying ENTITYUNRELATED ( information ) into categories or ENTITYUNRELATED ; where each ENTITYUNRELATED signifies a different ENTITYUNRELATED of extracting a ENTITY from a piece of ENTITYOTHER containing it .
The two main factors that characterize a text are its content and its style, and both can be used as a means of categorization.	categorization	text	usage	{'e1': {'word': 'categorization', 'word_index': [(24, 24)], 'id': 'J00-4001.4'}, 'e2': {'word': 'text', 'word_index': [(7, 7)], 'id': 'J00-4001.1'}}	The two main factors that characterize a ENTITYOTHER are its ENTITYUNRELATED and its ENTITYUNRELATED , and both can be used as a means of ENTITY .
In this paper we present an approach to text categorization in terms of genre and author for Modern Greek.	genre	text categorization	usage	{'e1': {'word': 'genre', 'word_index': [(12, 12)], 'id': 'J00-4001.6'}, 'e2': {'word': 'text categorization', 'word_index': [(8, 8)], 'id': 'J00-4001.5'}}	In this paper we present an approach to ENTITYOTHER in terms of ENTITY and author for ENTITYUNRELATED .
To this end, we propose a set of style markers including analysis-level measures that represent the way in which the input text has been analyzed and capture useful stylistic information without additional cost.	analysis-level measures	style markers	part_whole	{'e1': {'word': 'analysis-level measures', 'word_index': [(11, 11)], 'id': 'J00-4001.11'}, 'e2': {'word': 'style markers', 'word_index': [(9, 9)], 'id': 'J00-4001.10'}}	To this end , we propose a set of ENTITYOTHER including ENTITY that represent the way in which the ENTITYUNRELATED has been analyzed and capture useful ENTITYUNRELATED without additional cost .
We present a set of small-scale but reasonable experiments in text genre detection, author identification, and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence of the most frequent words.	proposed method	distributional lexical measures	compare	{'e1': {'word': 'proposed method', 'word_index': [(23, 23)], 'id': 'J00-4001.17'}, 'e2': {'word': 'distributional lexical measures', 'word_index': [(30, 30)], 'id': 'J00-4001.18'}}	We present a set of small - scale but reasonable experiments in ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED tasks and show that the ENTITY performs better than the most popular ENTITYOTHER , i.e. , functions of ENTITYUNRELATED and ENTITYUNRELATED of the most frequent ENTITYUNRELATED .
We present a set of small-scale but reasonable experiments in text genre detection, author identification, and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence of the most frequent words.	frequencies of occurrence	words	model-feature	{'e1': {'word': 'frequencies of occurrence', 'word_index': [(38, 38)], 'id': 'J00-4001.20'}, 'e2': {'word': 'words', 'word_index': [(43, 43)], 'id': 'J00-4001.21'}}	We present a set of small - scale but reasonable experiments in ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED tasks and show that the ENTITYUNRELATED performs better than the most popular ENTITYUNRELATED , i.e. , functions of ENTITYUNRELATED and ENTITY of the most frequent ENTITYOTHER .
All the presented experiments are based on unrestricted text downloaded from the World Wide Web without any manual text preprocessing or text sampling.	unrestricted text	World Wide Web	part_whole	{'e1': {'word': 'unrestricted text', 'word_index': [(7, 7)], 'id': 'J00-4001.22'}, 'e2': {'word': 'World Wide Web', 'word_index': [(11, 11)], 'id': 'J00-4001.23'}}	All the presented experiments are based on ENTITY downloaded from the ENTITYOTHER without any ENTITYUNRELATED or ENTITYUNRELATED .
Our system can be used in any application that requires fast and easily adaptable text categorization in terms of stylistically homogeneous categories.	system	application	usage	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'J00-4001.28'}, 'e2': {'word': 'application', 'word_index': [(7, 7)], 'id': 'J00-4001.29'}}	Our ENTITY can be used in any ENTITYOTHER that requires fast and easily adaptable ENTITYUNRELATED in terms of ENTITYUNRELATED .
We first split a dataset consisting of pairs of sentences into clusters according to their similarities, and then construct a classifier for each cluster to identify equivalence relations.	sentences	dataset	part_whole	{'e1': {'word': 'sentences', 'word_index': [(9, 9)], 'id': 'I08-1019.19'}, 'e2': {'word': 'dataset', 'word_index': [(4, 4)], 'id': 'I08-1019.18'}}	We first split a ENTITYOTHER consisting of pairs of ENTITY into ENTITYUNRELATED according to their similarities , and then construct a ENTITYUNRELATED for each ENTITYUNRELATED to identify ENTITYUNRELATED .
In this paper, we propose a new learning method to solve the sparse data problem in automatic extraction of bilingual word pairs from parallel corpora with various languages.	learning method	sparse data problem	usage	{'e1': {'word': 'learning method', 'word_index': [(8, 8)], 'id': 'W05-1010.1'}, 'e2': {'word': 'sparse data problem', 'word_index': [(12, 12)], 'id': 'W05-1010.2'}}	In this paper , we propose a new ENTITY to solve the ENTITYOTHER in ENTITYUNRELATED of ENTITYUNRELATED from ENTITYUNRELATED with various ENTITYUNRELATED .
In this paper, we propose a new learning method to solve the sparse data problem in automatic extraction of bilingual word pairs from parallel corpora with various languages.	bilingual word pairs	parallel corpora	part_whole	{'e1': {'word': 'bilingual word pairs', 'word_index': [(16, 16)], 'id': 'W05-1010.4'}, 'e2': {'word': 'parallel corpora', 'word_index': [(18, 18)], 'id': 'W05-1010.5'}}	In this paper , we propose a new ENTITYUNRELATED to solve the ENTITYUNRELATED in ENTITYUNRELATED of ENTITY from ENTITYOTHER with various ENTITYUNRELATED .
Our learning method automatically acquires rules, which are effective to solve the sparse data problem, only from parallel corpora without any bilingual resource (e.g., a bilingual dictionary, machine translation systems) beforehand.	rules	sparse data problem	usage	{'e1': {'word': 'rules', 'word_index': [(4, 4)], 'id': 'W05-1010.8'}, 'e2': {'word': 'sparse data problem', 'word_index': [(12, 12)], 'id': 'W05-1010.9'}}	Our ENTITYUNRELATED automatically acquires ENTITY , which are effective to solve the ENTITYOTHER , only from ENTITYUNRELATED without any ENTITYUNRELATED ( e.g. , a ENTITYUNRELATED , ENTITYUNRELATED ) beforehand .
Using ICL, the recall in three systems based on similarity measures improved respectively 8.0, 6.1 and 6.0 percentage points.	similarity measures	systems	usage	{'e1': {'word': 'similarity measures', 'word_index': [(10, 10)], 'id': 'W05-1010.22'}, 'e2': {'word': 'systems', 'word_index': [(7, 7)], 'id': 'W05-1010.21'}}	Using ENTITYUNRELATED , the ENTITYUNRELATED in three ENTITYOTHER based on ENTITY improved respectively 8.0 , 6.1 and 6.0 percentage points .
NLP systems for tasks such as question answering and information extraction typically rely on statistical parsers	statistical parsers	NLP systems	usage	{'e1': {'word': 'statistical parsers', 'word_index': [(11, 11)], 'id': 'W06-1604.4'}, 'e2': {'word': 'NLP systems', 'word_index': [(0, 0)], 'id': 'W06-1604.1'}}	ENTITYOTHER for tasks such as ENTITYUNRELATED and ENTITYUNRELATED typically rely on ENTITY
But the efficacy of such parsers can be surprisingly low, particularly for sentences drawn from heterogeneous corpora such as the Web.	sentences	heterogeneous corpora	part_whole	{'e1': {'word': 'sentences', 'word_index': [(13, 13)], 'id': 'W06-1604.6'}, 'e2': {'word': 'heterogeneous corpora', 'word_index': [(16, 16)], 'id': 'W06-1604.7'}}	But the efficacy of such ENTITYUNRELATED can be surprisingly low , particularly for ENTITY drawn from ENTITYOTHER such as the ENTITYUNRELATED .
We have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences, which can be detected automatically using semantic information obtained from the Web.	semantic interpretations	sentences	model-feature	{'e1': {'word': 'semantic interpretations', 'word_index': [(10, 10)], 'id': 'W06-1604.10'}, 'e2': {'word': 'sentences', 'word_index': [(12, 12)], 'id': 'W06-1604.11'}}	We have observed that ENTITYUNRELATED often result in wildly implausible ENTITY of ENTITYOTHER , which can be detected automatically using ENTITYUNRELATED obtained from the ENTITYUNRELATED .
We have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences, which can be detected automatically using semantic information obtained from the Web.	semantic information	Web	part_whole	{'e1': {'word': 'semantic information', 'word_index': [(20, 20)], 'id': 'W06-1604.12'}, 'e2': {'word': 'Web', 'word_index': [(24, 24)], 'id': 'W06-1604.13'}}	We have observed that ENTITYUNRELATED often result in wildly implausible ENTITYUNRELATED of ENTITYUNRELATED , which can be detected automatically using ENTITY obtained from the ENTITYOTHER .
We demonstrate that a previously defined formal algebra applies to grammar engineering across a much greater range of frameworks than was originally envisaged.	formal algebra	grammar engineering	usage	{'e1': {'word': 'formal algebra', 'word_index': [(6, 6)], 'id': 'W07-1210.4'}, 'e2': {'word': 'grammar engineering', 'word_index': [(9, 9)], 'id': 'W07-1210.5'}}	We demonstrate that a previously defined ENTITY applies to ENTITYOTHER across a much greater range of ENTITYUNRELATED than was originally envisaged .
We show how this algebra can be adapted to composition in grammar frameworks where a lexicon is not assumed, and how this underlies a practical implementation of semantic construction for the RASP system.	semantic construction	RASP system	usage	{'e1': {'word': 'semantic construction', 'word_index': [(27, 27)], 'id': 'W07-1210.11'}, 'e2': {'word': 'RASP system', 'word_index': [(30, 30)], 'id': 'W07-1210.12'}}	We show how this ENTITYUNRELATED can be adapted to ENTITYUNRELATED in ENTITYUNRELATED where a ENTITYUNRELATED is not assumed , and how this underlies a practical implementation of ENTITY for the ENTITYOTHER .
We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER).	Wikipedia	named entity recognition (NER)	usage	{'e1': {'word': 'Wikipedia', 'word_index': [(5, 5)], 'id': 'D07-1073.1'}, 'e2': {'word': 'named entity recognition (NER)', 'word_index': [(10, 10)], 'id': 'D07-1073.3'}}	We explore the use of ENTITY as ENTITYUNRELATED to improve ENTITYOTHER .
Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry, which can be thought of as a definition part.	category label	sentence	part_whole	{'e1': {'word': 'category label', 'word_index': [(12, 12)], 'id': 'D07-1073.6'}, 'e2': {'word': 'sentence', 'word_index': [(16, 16)], 'id': 'D07-1073.7'}}	Our method retrieves the corresponding ENTITYUNRELATED for each ENTITYUNRELATED and extracts a ENTITY from the first ENTITYOTHER of the ENTITYUNRELATED , which can be thought of as a ENTITYUNRELATED .
These category labels are used as features in a CRF-based NE tagger.	features	CRF-based NE tagger	usage	{'e1': {'word': 'features', 'word_index': [(5, 5)], 'id': 'D07-1073.11'}, 'e2': {'word': 'CRF-based NE tagger', 'word_index': [(8, 8)], 'id': 'D07-1073.12'}}	These ENTITYUNRELATED are used as ENTITY in a ENTITYOTHER .
We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT's peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages.	POS tagging	languages	usage	{'e1': {'word': 'POS tagging', 'word_index': [(47, 47)], 'id': 'P07-2053.7'}, 'e2': {'word': 'languages', 'word_index': [(53, 53)], 'id': 'P07-2053.8'}}	We argue here that some of the criticism aimed at ENTITYUNRELATED on ENTITYUNRELATED should more properly be directed at ENTITYUNRELATED , free but not open source , since it is those details of the implementation which are hidden from the user that hold the key for improved ENTITY across a wider variety of ENTITYOTHER .
We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT.	syntactic structural knowledge	word reordering	usage	{'e1': {'word': 'syntactic structural knowledge', 'word_index': [(10, 10)], 'id': 'C08-1027.2'}, 'e2': {'word': 'word reordering', 'word_index': [(6, 6)], 'id': 'C08-1027.1'}}	We present a novel approach to ENTITYOTHER which successfully integrates ENTITY with ENTITYUNRELATED .
This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules.	probabilistic syntactic rules	lattice of alternatives	usage	{'e1': {'word': 'probabilistic syntactic rules', 'word_index': [(11, 11)], 'id': 'C08-1027.5'}, 'e2': {'word': 'lattice of alternatives', 'word_index': [(6, 6)], 'id': 'C08-1027.4'}}	This is done by constructing a ENTITYOTHER based on automatically learned ENTITY .
In decoding, the alternatives are scored based on the output word order, not the order of the input.	output word order	alternatives	model-feature	{'e1': {'word': 'output word order', 'word_index': [(10, 10)], 'id': 'C08-1027.8'}, 'e2': {'word': 'alternatives', 'word_index': [(4, 4)], 'id': 'C08-1027.7'}}	In ENTITYUNRELATED , the ENTITYOTHER are scored based on the ENTITY , not the ENTITYUNRELATED .
Manual evaluation supports the claim that the present approach is significantly superior to previous approaches.	approach	approaches	compare	{'e1': {'word': 'approach', 'word_index': [(8, 8)], 'id': 'C08-1027.15'}, 'e2': {'word': 'approaches', 'word_index': [(14, 14)], 'id': 'C08-1027.16'}}	Manual evaluation supports the claim that the present ENTITY is significantly superior to previous ENTITYOTHER .
Most of the previous Korean noun extraction systems use a morphological analyzer or a Part-of- Speech (POS) tagger.	morphological analyzer	Korean noun extraction systems	usage	{'e1': {'word': 'morphological analyzer', 'word_index': [(7, 7)], 'id': 'P03-1060.7'}, 'e2': {'word': 'Korean noun extraction systems', 'word_index': [(4, 4)], 'id': 'P03-1060.6'}}	Most of the previous ENTITYOTHER use a ENTITY or a ENTITYUNRELATED .
This paper proposes a new noun extraction method that uses the syllable based word recognition model.	syllable based word recognition model	noun extraction method	usage	{'e1': {'word': 'syllable based word recognition model', 'word_index': [(9, 9)], 'id': 'P03-1060.14'}, 'e2': {'word': 'noun extraction method', 'word_index': [(5, 5)], 'id': 'P03-1060.13'}}	This paper proposes a new ENTITYOTHER that uses the ENTITY .
It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries.	syllable-tag sequence	input sentence	model-feature	{'e1': {'word': 'syllable-tag sequence', 'word_index': [(5, 5)], 'id': 'P03-1060.15'}, 'e2': {'word': 'input sentence', 'word_index': [(8, 8)], 'id': 'P03-1060.16'}}	It finds the most probable ENTITY of the ENTITYOTHER by using ENTITYUNRELATED from the ENTITYUNRELATED and extracts ENTITYUNRELATED by detecting ENTITYUNRELATED .
It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries.	automatically acquired statistical information	POS tagged corpus	part_whole	{'e1': {'word': 'automatically acquired statistical information', 'word_index': [(11, 11)], 'id': 'P03-1060.17'}, 'e2': {'word': 'POS tagged corpus', 'word_index': [(14, 14)], 'id': 'P03-1060.18'}}	It finds the most probable ENTITYUNRELATED of the ENTITYUNRELATED by using ENTITY from the ENTITYOTHER and extracts ENTITYUNRELATED by detecting ENTITYUNRELATED .
The experimental results show that without morphological analysis or POS tagging, the proposed method achieves comparable performance with the previous methods.	proposed method	performance	result	{'e1': {'word': 'proposed method', 'word_index': [(11, 11)], 'id': 'P03-1060.25'}, 'e2': {'word': 'performance', 'word_index': [(14, 14)], 'id': 'P03-1060.26'}}	The experimental results show that without ENTITYUNRELATED or ENTITYUNRELATED , the ENTITY achieves comparable ENTITYOTHER with the previous ENTITYUNRELATED .
This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs.	word alignment	languages with scarce resources	usage	{'e1': {'word': 'word alignment', 'word_index': [(7, 7)], 'id': 'P06-2112.1'}, 'e2': {'word': 'languages with scarce resources', 'word_index': [(9, 9)], 'id': 'P06-2112.2'}}	This paper proposes an approach to improve ENTITY for ENTITYOTHER using ENTITYUNRELATED of other ENTITYUNRELATED .
Based on these two additional corpora and with L3 as the pivot language, we build a word alignment model for L1 and L2.	word alignment model	L1 and L2	usage	{'e1': {'word': 'word alignment model', 'word_index': [(16, 16)], 'id': 'P06-2112.16'}, 'e2': {'word': 'L1 and L2', 'word_index': [(18, 18)], 'id': 'P06-2112.17'}}	Based on these two additional ENTITYUNRELATED and with ENTITYUNRELATED as the ENTITYUNRELATED , we build a ENTITY for ENTITYOTHER .
This approach can build a word alignment model for two languages even if no bilingual corpus is available in this language pair.	word alignment model	languages	usage	{'e1': {'word': 'word alignment model', 'word_index': [(5, 5)], 'id': 'P06-2112.19'}, 'e2': {'word': 'languages', 'word_index': [(8, 8)], 'id': 'P06-2112.20'}}	This ENTITYUNRELATED can build a ENTITY for two ENTITYOTHER even if no ENTITYUNRELATED is available in this ENTITYUNRELATED .
In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus.	L1-L2 bilingual corpus	word alignment model	usage	{'e1': {'word': 'L1-L2 bilingual corpus', 'word_index': [(12, 12)], 'id': 'P06-2112.25'}, 'e2': {'word': 'word alignment model', 'word_index': [(6, 6)], 'id': 'P06-2112.23'}}	In addition , we build another ENTITYOTHER for ENTITYUNRELATED using the small ENTITY .
This paper provides an approach to the semi-automatic extraction of collocations from corpora using statistics.	collocations	corpora	part_whole	{'e1': {'word': 'collocations', 'word_index': [(9, 9)], 'id': 'C96-1009.2'}, 'e2': {'word': 'corpora', 'word_index': [(11, 11)], 'id': 'C96-1009.3'}}	This paper provides an approach to the ENTITYUNRELATED of ENTITY from ENTITYOTHER using statistics .
In this paper, we address the problem of nested collocations; that is, those being part of longer collocations.	nested collocations	collocations	part_whole	{'e1': {'word': 'nested collocations', 'word_index': [(9, 9)], 'id': 'C96-1009.6'}, 'e2': {'word': 'collocations', 'word_index': [(19, 19)], 'id': 'C96-1009.7'}}	In this paper , we address the problem of ENTITY ; that is , those being part of longer ENTITYOTHER .
Most approaches till now, treated substring of collocations as collocations only if they appeared frequently enough by themselves in the corpus.	collocations	corpus	part_whole	{'e1': {'word': 'collocations', 'word_index': [(10, 10)], 'id': 'C96-1009.10'}, 'e2': {'word': 'corpus', 'word_index': [(21, 21)], 'id': 'C96-1009.11'}}	Most approaches till now , treated ENTITYUNRELATED of ENTITYUNRELATED as ENTITY only if they appeared frequently enough by themselves in the ENTITYOTHER .
Surprisingly, students had initiative more of the time in the didactic dialogues (21% of the turns) than in the Socratic dialogues (10% of the turns), and there was no direct relationship between student initiative and learning.	initiative	didactic dialogues	model-feature	{'e1': {'word': 'initiative', 'word_index': [(4, 4)], 'id': 'E03-1072.13'}, 'e2': {'word': 'didactic dialogues', 'word_index': [(11, 11)], 'id': 'E03-1072.14'}}	Surprisingly , students had ENTITY more of the time in the ENTITYOTHER ( 21 % of the turns ) than in the ENTITYUNRELATED ( 10 % of the turns ) , and there was no direct relationship between ENTITYUNRELATED and ENTITYUNRELATED .
However, Socratic dialogues were more interactive than didactic dialogues as measured by percentage of tutor utterances that were questions and percentage of words in the dialogue uttered by the student, and interactivity had a positive correlation with learning.	Socratic dialogues	didactic dialogues	compare	{'e1': {'word': 'Socratic dialogues', 'word_index': [(2, 2)], 'id': 'E03-1072.18'}, 'e2': {'word': 'didactic dialogues', 'word_index': [(7, 7)], 'id': 'E03-1072.19'}}	However , ENTITY were more interactive than ENTITYOTHER as measured by percentage of ENTITYUNRELATED that were questions and percentage of ENTITYUNRELATED in the ENTITYUNRELATED uttered by the student , and ENTITYUNRELATED had a positive correlation with ENTITYUNRELATED .
We present several statistical models of syntactic constituent order for sentence realization.	statistical models	syntactic constituent order	model-feature	{'e1': {'word': 'statistical models', 'word_index': [(3, 3)], 'id': 'C04-1097.1'}, 'e2': {'word': 'syntactic constituent order', 'word_index': [(5, 5)], 'id': 'C04-1097.2'}}	We present several ENTITY of ENTITYOTHER for ENTITYUNRELATED .
We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models.	joint models	conditional models	compare	{'e1': {'word': 'joint models', 'word_index': [(7, 7)], 'id': 'C04-1097.4'}, 'e2': {'word': 'conditional models', 'word_index': [(16, 16)], 'id': 'C04-1097.6'}}	We compare several models , including simple ENTITY inspired by existing ENTITYUNRELATED , and several novel ENTITYOTHER .
We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank.	model	unordered trees	usage	{'e1': {'word': 'model', 'word_index': [(6, 6)], 'id': 'C04-1097.14'}, 'e2': {'word': 'unordered trees', 'word_index': [(11, 11)], 'id': 'C04-1097.15'}}	We employ a version of that ENTITY in an evaluation on ENTITYOTHER from the ENTITYUNRELATED .
A system would accomplish speech reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and content-preserving English, the message that the speaker intended to convey.	speech reconstruction	spontaneous speech input	usage	{'e1': {'word': 'speech reconstruction', 'word_index': [(4, 4)], 'id': 'L08-1530.4'}, 'e2': {'word': 'spontaneous speech input', 'word_index': [(7, 7)], 'id': 'L08-1530.5'}}	A system would accomplish ENTITY of its ENTITYOTHER if its output were to represent , in flawless , fluent , and ENTITYUNRELATED , the ENTITYUNRELATED that the ENTITYUNRELATED intended to convey .
These cleaner speech transcripts would allow for more accurate language processing as needed for NLP tasks such as machine translation and conversation summarization, which often rely on grammatical input.	grammatical input	conversation summarization	usage	{'e1': {'word': 'grammatical input', 'word_index': [(23, 23)], 'id': 'L08-1530.14'}, 'e2': {'word': 'conversation summarization', 'word_index': [(17, 17)], 'id': 'L08-1530.13'}}	These cleaner ENTITYUNRELATED would allow for more accurate ENTITYUNRELATED as needed for ENTITYUNRELATED such as ENTITYUNRELATED and ENTITYOTHER , which often rely on ENTITY .
This small corpus of reconstructed and aligned conversational telephone speech transcriptions for the Fisher conversational telephone speech corpus ( Strassel and Walker, 2004 ) was annotated on several levels including string transformations and predicate-argument structure, and will be shared with the linguistic research community.	reconstructed and aligned conversational telephone speech transcriptions	corpus	part_whole	{'e1': {'word': 'reconstructed and aligned conversational telephone speech transcriptions', 'word_index': [(4, 4)], 'id': 'L08-1530.20'}, 'e2': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'L08-1530.19'}}	This small ENTITYOTHER of ENTITY for the ENTITYUNRELATED ( Strassel and Walker , 2004 ) was annotated on several levels including ENTITYUNRELATED and ENTITYUNRELATED , and will be shared with the linguistic research community .
The corpus contains recordings of approximately 77 hours of broadcast news shows from the Norwegian broadcasting company NRK.	recordings	corpus	part_whole	{'e1': {'word': 'recordings', 'word_index': [(3, 3)], 'id': 'L08-1122.3'}, 'e2': {'word': 'corpus', 'word_index': [(1, 1)], 'id': 'L08-1122.2'}}	The ENTITYOTHER contains ENTITY of approximately 77 hours of ENTITYUNRELATED from the Norwegian broadcasting company NRK .
The corpus covers both read and spontaneous speech as well as spontaneous dialogues and multipart discussions, including frequent occurrences of non-speech material (e.g. music, jingles).	read and spontaneous speech	corpus	part_whole	{'e1': {'word': 'read and spontaneous speech', 'word_index': [(4, 4)], 'id': 'L08-1122.6'}, 'e2': {'word': 'corpus', 'word_index': [(1, 1)], 'id': 'L08-1122.5'}}	The ENTITYOTHER covers both ENTITY as well as ENTITYUNRELATED and ENTITYUNRELATED , including frequent occurrences of ENTITYUNRELATED ( e.g. music , jingles ) .
The RUNDKAST corpus is planned to be included in a future national Norwegian language resource bank.	RUNDKAST corpus	Norwegian language resource bank	part_whole	{'e1': {'word': 'RUNDKAST corpus', 'word_index': [(1, 1)], 'id': 'L08-1122.24'}, 'e2': {'word': 'Norwegian language resource bank', 'word_index': [(11, 11)], 'id': 'L08-1122.25'}}	The ENTITY is planned to be included in a future national ENTITYOTHER .
Statistical measures of word similarity have application in many areas of natural language processing, such as language modeling and information retrieval	Statistical measures	word similarity	model-feature	{'e1': {'word': 'Statistical measures', 'word_index': [(0, 0)], 'id': 'N03-1032.1'}, 'e2': {'word': 'word similarity', 'word_index': [(2, 2)], 'id': 'N03-1032.2'}}	ENTITY of ENTITYOTHER have application in many areas of ENTITYUNRELATED , such as ENTITYUNRELATED and ENTITYUNRELATED
Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures.	Web data	corpus	part_whole	{'e1': {'word': 'Web data', 'word_index': [(11, 11)], 'id': 'N03-1032.10'}, 'e2': {'word': 'corpus', 'word_index': [(9, 9)], 'id': 'N03-1032.9'}}	Our ENTITYUNRELATED are generated from a terabyte - sized ENTITYOTHER of ENTITY , and we study the impact of ENTITYUNRELATED on the effectiveness of the ENTITYUNRELATED .
Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures.	corpus size	measures	result	{'e1': {'word': 'corpus size', 'word_index': [(19, 19)], 'id': 'N03-1032.11'}, 'e2': {'word': 'measures', 'word_index': [(25, 25)], 'id': 'N03-1032.12'}}	Our ENTITYUNRELATED are generated from a terabyte - sized ENTITYUNRELATED of ENTITYUNRELATED , and we study the impact of ENTITY on the effectiveness of the ENTITYOTHER .
We base the evaluation on one TOEFL question set and two practice questions sets, each consisting of a number of multiple choice questions seeking the best synonym for a given target word.	multiple choice questions	practice questions sets	part_whole	{'e1': {'word': 'multiple choice questions', 'word_index': [(17, 17)], 'id': 'N03-1032.15'}, 'e2': {'word': 'practice questions sets', 'word_index': [(9, 9)], 'id': 'N03-1032.14'}}	We base the evaluation on one ENTITYUNRELATED and two ENTITYOTHER , each consisting of a number of ENTITY seeking the best ENTITYUNRELATED for a given ENTITYUNRELATED .
The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer.	stack decoder	acoustic and language model	usage	{'e1': {'word': 'stack decoder', 'word_index': [(1, 1)], 'id': 'H92-1082.1'}, 'e2': {'word': 'acoustic and language model', 'word_index': [(9, 9)], 'id': 'H92-1082.2'}}	The ENTITY is an attractive algorithm for controlling the ENTITYOTHER matching in a ENTITYUNRELATED .
A previous paper described a near-optimal admissible Viterbi A* search algorithm for use with non-cross-word acoustic models and no-grammar language models [16].	non-cross-word acoustic models	Viterbi A* search algorithm	usage	{'e1': {'word': 'non-cross-word acoustic models', 'word_index': [(11, 11)], 'id': 'H92-1082.5'}, 'e2': {'word': 'Viterbi A* search algorithm', 'word_index': [(7, 7)], 'id': 'H92-1082.4'}}	A previous paper described a near-optimal admissible ENTITYOTHER for use with ENTITY and ENTITYUNRELATED [ 16 ] .
In addition, we make a proposal for organising intermodule communication in an NLG system by having a central server for this information.	NLG system	intermodule communication	usage	{'e1': {'word': 'NLG system', 'word_index': [(12, 12)], 'id': 'A00-1017.6'}, 'e2': {'word': 'intermodule communication', 'word_index': [(9, 9)], 'id': 'A00-1017.5'}}	In addition , we make a proposal for organising ENTITYOTHER in an ENTITY by having a ENTITYUNRELATED for this ENTITYUNRELATED .
This paper describes a sense tagging technique for the automatic sense tagging of running Chinese text.	automatic sense tagging	running Chinese text	usage	{'e1': {'word': 'automatic sense tagging', 'word_index': [(7, 7)], 'id': 'W93-0312.2'}, 'e2': {'word': 'running Chinese text', 'word_index': [(9, 9)], 'id': 'W93-0312.3'}}	This paper describes a ENTITYUNRELATED for the ENTITY of ENTITYOTHER .
Whereas previous work (Yarowsky, 1992; Gale et al., 1992, 1993) relies heavily on the role of statistics, the present system makes use of Machine Readable/Tractable Dictionaries (Wilks et al., 1990; Guo, in press) and an example-based reasoning technique (Nagao, 1984; Sumita et al., 1990) to treat novel words, compound words, and phrases found in the input text.	phrases	input text	part_whole	{'e1': {'word': 'phrases', 'word_index': [(64, 64)], 'id': 'W93-0312.10'}, 'e2': {'word': 'input text', 'word_index': [(68, 68)], 'id': 'W93-0312.11'}}	Whereas previous work ( Yarowsky , 1992 ; Gale et al. , 1992 , 1993 ) relies heavily on the role of statistics , the present system makes use of ENTITYUNRELATED ( Wilks et al. , 1990 ; Guo , in press ) and an ENTITYUNRELATED ( Nagao , 1984 ; Sumita et al. , 1990 ) to treat ENTITYUNRELATED , ENTITYUNRELATED , and ENTITY found in the ENTITYOTHER .
A syntactic description is autonomous in the sense that it has certain explicit formal properties.	formal properties	syntactic description	model-feature	{'e1': {'word': 'formal properties', 'word_index': [(12, 12)], 'id': 'W98-0501.3'}, 'e2': {'word': 'syntactic description', 'word_index': [(1, 1)], 'id': 'W98-0501.2'}}	A ENTITYOTHER is autonomous in the sense that it has certain explicit ENTITY .
Such a description relates to the semantic interpretation of the sentences, and to the surface text.	semantic interpretation	sentences	model-feature	{'e1': {'word': 'semantic interpretation', 'word_index': [(6, 6)], 'id': 'W98-0501.4'}, 'e2': {'word': 'sentences', 'word_index': [(9, 9)], 'id': 'W98-0501.5'}}	Such a description relates to the ENTITY of the ENTITYOTHER , and to the ENTITYUNRELATED .
As the formalism is implemented in a broad-coverage syntactic parser, we concentrate on issues that must be resolved by any practical system that uses such models.	formalism	broad-coverage syntactic parser	usage	{'e1': {'word': 'formalism', 'word_index': [(2, 2)], 'id': 'W98-0501.7'}, 'e2': {'word': 'broad-coverage syntactic parser', 'word_index': [(7, 7)], 'id': 'W98-0501.8'}}	As the ENTITY is implemented in a ENTITYOTHER , we concentrate on issues that must be resolved by any practical system that uses such models .
We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text.	argumentative structure	text	model-feature	{'e1': {'word': 'argumentative structure', 'word_index': [(23, 23)], 'id': 'W00-1302.7'}, 'e2': {'word': 'text', 'word_index': [(26, 26)], 'id': 'W00-1302.8'}}	We present the algorithm and a systematic evaluation of a system which can recognize the most salient ENTITYUNRELATED that contribute to the global ENTITY of a ENTITYOTHER .
"RE operates either interactively, allowing word-by-word evaluation of hypothesized sound changes and semantic shifts, or in a ""batch"" mode, processing entire multilingual lexicons."	word-by-word evaluation	hypothesized sound changes	usage	{'e1': {'word': 'word-by-word evaluation', 'word_index': [(6, 6)], 'id': 'J94-3004.22'}, 'e2': {'word': 'hypothesized sound changes', 'word_index': [(8, 8)], 'id': 'J94-3004.23'}}	"ENTITYUNRELATED operates either interactively , allowing ENTITY of ENTITYOTHER and ENTITYUNRELATED , or in a "" batch "" mode , processing entire ENTITYUNRELATED ."
We describe the algorithms implemented in RE, specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time, the procedures for creating and consolidating cognate sets based on these projections, and the ad hoc techniques developed for handling the semantic component of the comparative method.	combinatorial techniques	projections	usage	{'e1': {'word': 'combinatorial techniques', 'word_index': [(12, 12)], 'id': 'J94-3004.28'}, 'e2': {'word': 'projections', 'word_index': [(16, 16)], 'id': 'J94-3004.29'}}	We describe the algorithms implemented in ENTITYUNRELATED , specifically the ENTITYUNRELATED and ENTITY used to make ENTITYOTHER upstream or downstream in the sense of time , the procedures for creating and consolidating ENTITYUNRELATED based on these ENTITYUNRELATED , and the ad hoc techniques developed for handling the ENTITYUNRELATED of the ENTITYUNRELATED .
We describe the algorithms implemented in RE, specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time, the procedures for creating and consolidating cognate sets based on these projections, and the ad hoc techniques developed for handling the semantic component of the comparative method.	projections	cognate sets	usage	{'e1': {'word': 'projections', 'word_index': [(36, 36)], 'id': 'J94-3004.31'}, 'e2': {'word': 'cognate sets', 'word_index': [(32, 32)], 'id': 'J94-3004.30'}}	We describe the algorithms implemented in ENTITYUNRELATED , specifically the ENTITYUNRELATED and ENTITYUNRELATED used to make ENTITYUNRELATED upstream or downstream in the sense of time , the procedures for creating and consolidating ENTITYOTHER based on these ENTITY , and the ad hoc techniques developed for handling the ENTITYUNRELATED of the ENTITYUNRELATED .
We describe the algorithms implemented in RE, specifically the parsing and combinatorial techniques used to make projections upstream or downstream in the sense of time, the procedures for creating and consolidating cognate sets based on these projections, and the ad hoc techniques developed for handling the semantic component of the comparative method.	semantic component	comparative method	part_whole	{'e1': {'word': 'semantic component', 'word_index': [(47, 47)], 'id': 'J94-3004.32'}, 'e2': {'word': 'comparative method', 'word_index': [(50, 50)], 'id': 'J94-3004.33'}}	We describe the algorithms implemented in ENTITYUNRELATED , specifically the ENTITYUNRELATED and ENTITYUNRELATED used to make ENTITYUNRELATED upstream or downstream in the sense of time , the procedures for creating and consolidating ENTITYUNRELATED based on these ENTITYUNRELATED , and the ad hoc techniques developed for handling the ENTITY of the ENTITYOTHER .
Finally, we discuss features of RE that make it possible to handle the complex and sometimes imprecise representations of lexical items, and speculate on possible directions for future research.	representations	lexical items	model-feature	{'e1': {'word': 'representations', 'word_index': [(18, 18)], 'id': 'J94-3004.40'}, 'e2': {'word': 'lexical items', 'word_index': [(20, 20)], 'id': 'J94-3004.41'}}	Finally , we discuss features of ENTITYUNRELATED that make it possible to handle the complex and sometimes imprecise ENTITY of ENTITYOTHER , and speculate on possible directions for future research .
In this paper, we present a chunk based partial parsing system for spontaneous, conversational speech in unrestricted domains.	chunk based partial parsing system	spontaneous, conversational speech	usage	{'e1': {'word': 'chunk based partial parsing system', 'word_index': [(7, 7)], 'id': 'P98-2237.1'}, 'e2': {'word': 'spontaneous, conversational speech', 'word_index': [(9, 9)], 'id': 'P98-2237.2'}}	In this paper , we present a ENTITY for ENTITYOTHER in ENTITYUNRELATED .
The input for the system is N-best lists generated from speech recognizer lattices.	speech recognizer lattices	N-best lists	usage	{'e1': {'word': 'speech recognizer lattices', 'word_index': [(9, 9)], 'id': 'P98-2237.11'}, 'e2': {'word': 'N-best lists', 'word_index': [(6, 6)], 'id': 'P98-2237.10'}}	The input for the system is ENTITYOTHER generated from ENTITY .
"The hypotheses from the N-best lists are tagged for part of speech, ""cleaned up"" by a preprocessing pipe, parsed by a part of speech based chunk parser, and rescored using a backpropagation neural net trained on the chunk based scores."	chunk based scores	backpropagation neural net	usage	{'e1': {'word': 'chunk based scores', 'word_index': [(31, 31)], 'id': 'P98-2237.17'}, 'e2': {'word': 'backpropagation neural net', 'word_index': [(27, 27)], 'id': 'P98-2237.16'}}	"The hypotheses from the ENTITYUNRELATED are tagged for ENTITYUNRELATED , "" cleaned up "" by a ENTITYUNRELATED , parsed by a ENTITYUNRELATED , and rescored using a ENTITYOTHER trained on the ENTITY ."
In this paper, we study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction.	features	data representations	part_whole	{'e1': {'word': 'features', 'word_index': [(9, 9)], 'id': 'W05-0409.1'}, 'e2': {'word': 'data representations', 'word_index': [(12, 12)], 'id': 'W05-0409.2'}}	In this paper , we study how to generate ENTITY from various ENTITYOTHER , such as ENTITYUNRELATED and ENTITYUNRELATED , for ENTITYUNRELATED .
Besides the featuresgenerated from the surface texts, we mainly discuss the feature generation in the parse trees.	features	surface texts	part_whole	{'e1': {'word': 'features', 'word_index': [(2, 2)], 'id': 'W05-0409.6'}, 'e2': {'word': 'surface texts', 'word_index': [(6, 6)], 'id': 'W05-0409.7'}}	Besides the ENTITY generated from the ENTITYOTHER , we mainly discuss the ENTITYUNRELATED in the ENTITYUNRELATED .
We propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines.	syntactic features	Support Vector Machines	usage	{'e1': {'word': 'syntactic features', 'word_index': [(17, 17)], 'id': 'W05-0409.13'}, 'e2': {'word': 'Support Vector Machines', 'word_index': [(19, 19)], 'id': 'W05-0409.14'}}	We propose and compare three methods , including ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED , to represent the ENTITY in ENTITYOTHER .
The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts.	features	data representations	part_whole	{'e1': {'word': 'features', 'word_index': [(8, 8)], 'id': 'W05-0409.16'}, 'e2': {'word': 'data representations', 'word_index': [(14, 14)], 'id': 'W05-0409.17'}}	The experiment on the ENTITYUNRELATED shows that the ENTITY generated from the more structured ENTITYOTHER significantly improve the performance based on the ENTITYUNRELATED generated from the ENTITYUNRELATED .
The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts.	features	surface texts	part_whole	{'e1': {'word': 'features', 'word_index': [(22, 22)], 'id': 'W05-0409.18'}, 'e2': {'word': 'surface texts', 'word_index': [(26, 26)], 'id': 'W05-0409.19'}}	The experiment on the ENTITYUNRELATED shows that the ENTITYUNRELATED generated from the more structured ENTITYUNRELATED significantly improve the performance based on the ENTITY generated from the ENTITYOTHER .
In this paper, we explore the use of structured content as semantic constraints for enhancing the performance of traditional term-based document retrieval in special domains.	semantic constraints	term-based document retrieval	usage	{'e1': {'word': 'semantic constraints', 'word_index': [(11, 11)], 'id': 'W06-0805.2'}, 'e2': {'word': 'term-based document retrieval', 'word_index': [(18, 18)], 'id': 'W06-0805.3'}}	In this paper , we explore the use of ENTITYUNRELATED as ENTITY for enhancing the performance of traditional ENTITYOTHER in special domains .
First, we describe a method for automatic extraction of semantic content in the form of attribute-value (AV) pairs from natural language texts based on domain modelsconstructed from a semi-structured web resource.	attribute-value (AV) pairs	semantic content	model-feature	{'e1': {'word': 'attribute-value (AV) pairs', 'word_index': [(14, 14)], 'id': 'W06-0805.6'}, 'e2': {'word': 'semantic content', 'word_index': [(9, 9)], 'id': 'W06-0805.5'}}	First , we describe a method for ENTITYUNRELATED of ENTITYOTHER in the form of ENTITY from ENTITYUNRELATED based on ENTITYUNRELATED constructed from a ENTITYUNRELATED .
First, we describe a method for automatic extraction of semantic content in the form of attribute-value (AV) pairs from natural language texts based on domain modelsconstructed from a semi-structured web resource.	domain models	semi-structured web resource	part_whole	{'e1': {'word': 'domain models', 'word_index': [(19, 19)], 'id': 'W06-0805.8'}, 'e2': {'word': 'semi-structured web resource', 'word_index': [(23, 23)], 'id': 'W06-0805.9'}}	First , we describe a method for ENTITYUNRELATED of ENTITYUNRELATED in the form of ENTITYUNRELATED from ENTITYUNRELATED based on ENTITY constructed from a ENTITYOTHER .
Then, we explore the effect of combining a state-of-the-art term-based IR system and a simple constraint-based search system that uses the extracted AV pairs.	AV pairs	constraint-based search system	usage	{'e1': {'word': 'AV pairs', 'word_index': [(18, 18)], 'id': 'W06-0805.12'}, 'e2': {'word': 'constraint-based search system', 'word_index': [(13, 13)], 'id': 'W06-0805.11'}}	Then , we explore the effect of combining a ENTITYUNRELATED and a simple ENTITYOTHER that uses the extracted ENTITY .
Our evaluation results have shown that such combination produces some improvement in IR performance over the term-based IR system on our test collection.	IR performance	term-based IR system	compare	{'e1': {'word': 'IR performance', 'word_index': [(12, 12)], 'id': 'W06-0805.13'}, 'e2': {'word': 'term-based IR system', 'word_index': [(15, 15)], 'id': 'W06-0805.14'}}	Our evaluation results have shown that such combination produces some improvement in ENTITY over the ENTITYOTHER on our test collection .
These networks are induced from treebanks: their vertices denote word forms which occur as nuclei of dependency trees.	networks	treebanks	part_whole	{'e1': {'word': 'networks', 'word_index': [(1, 1)], 'id': 'W07-0210.4'}, 'e2': {'word': 'treebanks', 'word_index': [(5, 5)], 'id': 'W07-0210.5'}}	These ENTITY are induced from ENTITYOTHER : their ENTITYUNRELATED denote ENTITYUNRELATED which occur as ENTITYUNRELATED of ENTITYUNRELATED .
These networks are induced from treebanks: their vertices denote word forms which occur as nuclei of dependency trees.	vertices	word forms	model-feature	{'e1': {'word': 'vertices', 'word_index': [(8, 8)], 'id': 'W07-0210.6'}, 'e2': {'word': 'word forms', 'word_index': [(10, 10)], 'id': 'W07-0210.7'}}	These ENTITYUNRELATED are induced from ENTITYUNRELATED : their ENTITY denote ENTITYOTHER which occur as ENTITYUNRELATED of ENTITYUNRELATED .
Their edges connect pairs of vertices if at least two instance nuclei of these vertices are linked in the dependency structure of a sentence.	dependency structure	sentence	model-feature	{'e1': {'word': 'dependency structure', 'word_index': [(18, 18)], 'id': 'W07-0210.14'}, 'e2': {'word': 'sentence', 'word_index': [(21, 21)], 'id': 'W07-0210.15'}}	Their ENTITYUNRELATED connect pairs of ENTITYUNRELATED if at least two ENTITYUNRELATED of these ENTITYUNRELATED are linked in the ENTITY of a ENTITYOTHER .
We examine the syntactic dependency networks of seven languages.	syntactic dependency networks	languages	model-feature	{'e1': {'word': 'syntactic dependency networks', 'word_index': [(3, 3)], 'id': 'W07-0210.16'}, 'e2': {'word': 'languages', 'word_index': [(6, 6)], 'id': 'W07-0210.17'}}	We examine the ENTITY of seven ENTITYOTHER .
Secondly, the mean clustering of vertices decreases with their degree - this finding suggests the presence of a hierarchical network organization.	mean clustering	vertices	model-feature	{'e1': {'word': 'mean clustering', 'word_index': [(3, 3)], 'id': 'W07-0210.21'}, 'e2': {'word': 'vertices', 'word_index': [(5, 5)], 'id': 'W07-0210.22'}}	Secondly , the ENTITY of ENTITYOTHER decreases with their ENTITYUNRELATED - this finding suggests the presence of a ENTITYUNRELATED .
Thirdly, the mean degree of the nearest neighbors of a vertex x tends to decrease as the degree of x grows - this finding indicates disassortative mixing in the sense that links tend to connect vertices of dissimilar degrees.	mean degree	nearest neighbors	model-feature	{'e1': {'word': 'mean degree', 'word_index': [(3, 3)], 'id': 'W07-0210.25'}, 'e2': {'word': 'nearest neighbors', 'word_index': [(6, 6)], 'id': 'W07-0210.26'}}	Thirdly , the ENTITY of the ENTITYOTHER of a ENTITYUNRELATED x tends to decrease as the ENTITYUNRELATED of x grows - this finding indicates ENTITYUNRELATED in the sense that ENTITYUNRELATED tend to connect ENTITYUNRELATED of dissimilar ENTITYUNRELATED .
Dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity	Dependency-based representations	natural language syntax	model-feature	{'e1': {'word': 'Dependency-based representations', 'word_index': [(0, 0)], 'id': 'P07-1021.1'}, 'e2': {'word': 'natural language syntax', 'word_index': [(2, 2)], 'id': 'P07-1021.2'}}	ENTITY of ENTITYOTHER require a fine balance between structural flexibility and ENTITYUNRELATED
Most constraints are formulated on fully specified structures, which makes them hard to integrate into models where structures are composed from lexical information.	lexical information	structures	usage	{'e1': {'word': 'lexical information', 'word_index': [(20, 20)], 'id': 'P07-1021.10'}, 'e2': {'word': 'structures', 'word_index': [(16, 16)], 'id': 'P07-1021.9'}}	Most ENTITYUNRELATED are formulated on ENTITYUNRELATED , which makes them hard to integrate into models where ENTITYOTHER are composed from ENTITY .
We show that morphological decomposition of the Arabic source is beneficial, especially for smaller-size corpora, and investigate different recombination techniques.	morphological decomposition	Arabic	usage	{'e1': {'word': 'morphological decomposition', 'word_index': [(3, 3)], 'id': 'P08-2039.2'}, 'e2': {'word': 'Arabic', 'word_index': [(6, 6)], 'id': 'P08-2039.3'}}	We show that ENTITY of the ENTITYOTHER source is beneficial , especially for ENTITYUNRELATED , and investigate different ENTITYUNRELATED .
We also report on the use of Factored Translation Models for English-to-Arabic translation.	Factored Translation Models	English-to-Arabic translation	usage	{'e1': {'word': 'Factored Translation Models', 'word_index': [(7, 7)], 'id': 'P08-2039.6'}, 'e2': {'word': 'English-to-Arabic translation', 'word_index': [(9, 9)], 'id': 'P08-2039.7'}}	We also report on the use of ENTITY for ENTITYOTHER .
The first algorithm uses machine learning methods to identify the semantic dependencies in four stages: identification and labeling of predicates, identification and labeling of arguments.	labeling	predicates	usage	{'e1': {'word': 'labeling', 'word_index': [(15, 15)], 'id': 'W08-2131.8'}, 'e2': {'word': 'predicates', 'word_index': [(17, 17)], 'id': 'W08-2131.9'}}	The first algorithm uses ENTITYUNRELATED to identify the ENTITYUNRELATED in four stages : ENTITYUNRELATED and ENTITY of ENTITYOTHER , ENTITYUNRELATED and ENTITYUNRELATED of ENTITYUNRELATED .
The first algorithm uses machine learning methods to identify the semantic dependencies in four stages: identification and labeling of predicates, identification and labeling of arguments.	labeling	arguments	usage	{'e1': {'word': 'labeling', 'word_index': [(21, 21)], 'id': 'W08-2131.11'}, 'e2': {'word': 'arguments', 'word_index': [(23, 23)], 'id': 'W08-2131.12'}}	The first algorithm uses ENTITYUNRELATED to identify the ENTITYUNRELATED in four stages : ENTITYUNRELATED and ENTITYUNRELATED of ENTITYUNRELATED , ENTITYUNRELATED and ENTITY of ENTITYOTHER .
A hybrid algorithm combining the best stages of the two algorithms attains 86.62% labeled syntactic attachment accuracy, 73.24% labeled semantic dependency F1 and 79.93% labeled macro Fl score for the combined WSJ and Brown test sets.	hybrid algorithm	labeled syntactic attachment accuracy	result	{'e1': {'word': 'hybrid algorithm', 'word_index': [(1, 1)], 'id': 'W08-2131.15'}, 'e2': {'word': 'labeled syntactic attachment accuracy', 'word_index': [(13, 13)], 'id': 'W08-2131.16'}}	A ENTITY combining the best stages of the two algorithms attains 86.62 % ENTITYOTHER , 73.24 % ENTITYUNRELATED and 79.93 % ENTITYUNRELATED for the combined ENTITYUNRELATED .
This paper presents experiments into modelling the substitutability of discourse connectives.	substitutability	discourse connectives	model-feature	{'e1': {'word': 'substitutability', 'word_index': [(7, 7)], 'id': 'P05-1019.6'}, 'e2': {'word': 'discourse connectives', 'word_index': [(9, 9)], 'id': 'P05-1019.7'}}	This paper presents experiments into modelling the ENTITY of ENTITYOTHER .
A new Theory of Names and Descriptions that offers a uniform treatment for many types of non-singular concepts found in natural language discourse is presented.	non-singular concepts	natural language discourse	part_whole	{'e1': {'word': 'non-singular concepts', 'word_index': [(12, 12)], 'id': 'C86-1086.2'}, 'e2': {'word': 'natural language discourse', 'word_index': [(15, 15)], 'id': 'C86-1086.3'}}	A new ENTITYUNRELATED that offers a uniform treatment for many types of ENTITY found in ENTITYOTHER is presented .
This paper proposes a novel, corpus-based method for producing mappings between lexical resources.	corpus-based method	mappings	usage	{'e1': {'word': 'corpus-based method', 'word_index': [(6, 6)], 'id': 'E99-1050.1'}, 'e2': {'word': 'mappings', 'word_index': [(9, 9)], 'id': 'E99-1050.2'}}	This paper proposes a novel , ENTITY for producing ENTITYOTHER between ENTITYUNRELATED .
We propose a novel method to predict the inter-paragraph discourse structure of text, i.e.	inter-paragraph discourse structure	text	model-feature	{'e1': {'word': 'inter-paragraph discourse structure', 'word_index': [(8, 8)], 'id': 'C04-1007.1'}, 'e2': {'word': 'text', 'word_index': [(10, 10)], 'id': 'C04-1007.2'}}	We propose a novel method to predict the ENTITY of ENTITYOTHER , i.e.
"Our method combines a clustering algorithm with a model of segment ""relatedness"" acquired in a machine learning step."	model	"segment ""relatedness"""	model-feature	"{'e1': {'word': 'model', 'word_index': [(7, 7)], 'id': 'C04-1007.5'}, 'e2': {'word': 'segment ""relatedness""', 'word_index': [(9, 9)], 'id': 'C04-1007.6'}}"	Our method combines a ENTITYUNRELATED with a ENTITY of ENTITYOTHER acquired in a ENTITYUNRELATED .
It improves on other recent analyses in the computational linguistics literature in three respects: (i) it uses no tree- or logical-form rewriting devices in building meaning representations (ii) this results in a fully reversible linguistic description, equally suited for analysis or generation (iii) the analysis extends to types of elliptical comparative not elsewhere treated.	tree- or logical-form rewriting devices	meaning representations	usage	{'e1': {'word': 'tree- or logical-form rewriting devices', 'word_index': [(20, 20)], 'id': 'E91-1002.6'}, 'e2': {'word': 'meaning representations', 'word_index': [(23, 23)], 'id': 'E91-1002.7'}}	It improves on other recent analyses in the ENTITYUNRELATED literature in three respects : ( i ) it uses no ENTITY in building ENTITYOTHER ( ii ) this results in a ENTITYUNRELATED , equally suited for analysis or ENTITYUNRELATED ( iii ) the analysis extends to types of ENTITYUNRELATED not elsewhere treated .
In contrast to earlier dialectology, we seek a comprehensive characterization of (potentially gradual) differences between dialects, rather than a geographic delineation of (discrete) features of individual words or pronunciations.	(discrete) features	individual words	model-feature	{'e1': {'word': '(discrete) features', 'word_index': [(26, 26)], 'id': 'E99-1048.4'}, 'e2': {'word': 'individual words', 'word_index': [(28, 28)], 'id': 'E99-1048.5'}}	In contrast to earlier ENTITYUNRELATED , we seek a comprehensive characterization of ( potentially gradual ) differences between ENTITYUNRELATED , rather than a geographic delineation of ENTITY of ENTITYOTHER or ENTITYUNRELATED .
We measure phonetic (un)relatedness between dialects using Levenshtein distance, and classify by clustering distances but also by analysis through multidimensional scaling.	Levenshtein distance	phonetic (un)relatedness	usage	{'e1': {'word': 'Levenshtein distance', 'word_index': [(6, 6)], 'id': 'E99-1048.10'}, 'e2': {'word': 'phonetic (un)relatedness', 'word_index': [(2, 2)], 'id': 'E99-1048.8'}}	We measure ENTITYOTHER between ENTITYUNRELATED using ENTITY , and classify by ENTITYUNRELATED but also by analysis through multidimensional scaling .
Experimental results using a threaded discussion corpus from an undergraduate class show that it achieves significant performance improvements compared with the baseline system.	performance improvements	baseline system	compare	{'e1': {'word': 'performance improvements', 'word_index': [(14, 14)], 'id': 'N06-1027.16'}, 'e2': {'word': 'baseline system', 'word_index': [(18, 18)], 'id': 'N06-1027.17'}}	Experimental results using a ENTITYUNRELATED from an undergraduate class show that it achieves significant ENTITY compared with the ENTITYOTHER .
A method of anaphoral resolution of zero pronouns in Japanese language texts using the verbal semantic attributes is suggested.	anaphoral resolution	Japanese language texts	usage	{'e1': {'word': 'anaphoral resolution', 'word_index': [(3, 3)], 'id': 'A92-1028.1'}, 'e2': {'word': 'Japanese language texts', 'word_index': [(7, 7)], 'id': 'A92-1028.3'}}	A method of ENTITY of ENTITYUNRELATED in ENTITYOTHER using the ENTITYUNRELATED is suggested .
This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents.	semantic attributes	verbs	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(6, 6)], 'id': 'A92-1028.5'}, 'e2': {'word': 'verbs', 'word_index': [(8, 8)], 'id': 'A92-1028.6'}}	This method focuses attention on the ENTITY of ENTITYOTHER and examines the context from the relationship between the ENTITYUNRELATED of ENTITYUNRELATED governing ENTITYUNRELATED and the ENTITYUNRELATED of ENTITYUNRELATED governing their ENTITYUNRELATED .
This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents.	semantic attributes	verbs	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(18, 18)], 'id': 'A92-1028.7'}, 'e2': {'word': 'verbs', 'word_index': [(20, 20)], 'id': 'A92-1028.8'}}	This method focuses attention on the ENTITYUNRELATED of ENTITYUNRELATED and examines the context from the relationship between the ENTITY of ENTITYOTHER governing ENTITYUNRELATED and the ENTITYUNRELATED of ENTITYUNRELATED governing their ENTITYUNRELATED .
This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents.	semantic attributes	verbs	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(25, 25)], 'id': 'A92-1028.10'}, 'e2': {'word': 'verbs', 'word_index': [(27, 27)], 'id': 'A92-1028.11'}}	This method focuses attention on the ENTITYUNRELATED of ENTITYUNRELATED and examines the context from the relationship between the ENTITYUNRELATED of ENTITYUNRELATED governing ENTITYUNRELATED and the ENTITY of ENTITYOTHER governing their ENTITYUNRELATED .
The semantic attributes of verbs are created using 2 different viewpoints: dynamic characteristics of verbs and the relationship of verbs to cases.	semantic attributes	verbs	model-feature	{'e1': {'word': 'semantic attributes', 'word_index': [(1, 1)], 'id': 'A92-1028.13'}, 'e2': {'word': 'verbs', 'word_index': [(3, 3)], 'id': 'A92-1028.14'}}	The ENTITY of ENTITYOTHER are created using 2 different viewpoints : ENTITYUNRELATED of ENTITYUNRELATED and the relationship of ENTITYUNRELATED to ENTITYUNRELATED .
The semantic attributes of verbs are created using 2 different viewpoints: dynamic characteristics of verbs and the relationship of verbs to cases.	dynamic characteristics	verbs	model-feature	{'e1': {'word': 'dynamic characteristics', 'word_index': [(11, 11)], 'id': 'A92-1028.15'}, 'e2': {'word': 'verbs', 'word_index': [(13, 13)], 'id': 'A92-1028.16'}}	The ENTITYUNRELATED of ENTITYUNRELATED are created using 2 different viewpoints : ENTITY of ENTITYOTHER and the relationship of ENTITYUNRELATED to ENTITYUNRELATED .
By using this method, it is shown that, in the case of translating newspaper articles, the major portion (93%) of anaphoral resolution of zero pronouns necessary for machine translation can be achieved by using only linguistic knowledge.Factors to be given special attention when incorporating this method into a machine translation system are examined, together with suggested conditions for the detection of zero pronouns and methods for their conversion.	linguistic knowledge	anaphoral resolution	usage	{'e1': {'word': 'linguistic knowledge', 'word_index': [(36, 36)], 'id': 'A92-1028.23'}, 'e2': {'word': 'anaphoral resolution', 'word_index': [(24, 24)], 'id': 'A92-1028.20'}}	By using this method , it is shown that , in the case of ENTITYUNRELATED , the major portion ( 93 % ) of ENTITYOTHER of ENTITYUNRELATED necessary for ENTITYUNRELATED can be achieved by using only ENTITY . Factors to be given special attention when incorporating this method into a ENTITYUNRELATED are examined , together with suggested conditions for the detection of ENTITYUNRELATED and methods for their conversion .
Implementation of the proposed method with due consideration of these points leads to a viable method for anaphoral resolution of zero pronouns in a practical machine translation system.	anaphoral resolution	machine translation system	usage	{'e1': {'word': 'anaphoral resolution', 'word_index': [(17, 17)], 'id': 'A92-1028.33'}, 'e2': {'word': 'machine translation system', 'word_index': [(24, 24)], 'id': 'A92-1028.34'}}	Implementation of the proposed method with due consideration of these points leads to a viable method for ENTITY of zero pronouns in a practical ENTITYOTHER .
The primary objective of this project is to develop a robust, high-performance parser for English by automatically extracting a grammar from an annotated corpus of bracketed sentences, called the Treebank.	high-performance parser	English	usage	{'e1': {'word': 'high-performance parser', 'word_index': [(12, 12)], 'id': 'H93-1092.1'}, 'e2': {'word': 'English', 'word_index': [(14, 14)], 'id': 'H93-1092.2'}}	The primary objective of this project is to develop a robust , ENTITY for ENTITYOTHER by automatically extracting a ENTITYUNRELATED from an ENTITYUNRELATED of ENTITYUNRELATED , called the ENTITYUNRELATED .
The primary objective of this project is to develop a robust, high-performance parser for English by automatically extracting a grammar from an annotated corpus of bracketed sentences, called the Treebank.	grammar	annotated corpus	part_whole	{'e1': {'word': 'grammar', 'word_index': [(19, 19)], 'id': 'H93-1092.3'}, 'e2': {'word': 'annotated corpus', 'word_index': [(22, 22)], 'id': 'H93-1092.4'}}	The primary objective of this project is to develop a robust , ENTITYUNRELATED for ENTITYUNRELATED by automatically extracting a ENTITY from an ENTITYOTHER of ENTITYUNRELATED , called the ENTITYUNRELATED .
"The test collection consists of over 1 million documents from diverse full-text sources, 250 topics, and the set of relevant documents or ""right answers"" to those topics."	documents	test collection	part_whole	{'e1': {'word': 'documents', 'word_index': [(7, 7)], 'id': 'X96-1007.19'}, 'e2': {'word': 'test collection', 'word_index': [(1, 1)], 'id': 'X96-1007.18'}}	"The ENTITYOTHER consists of over 1 million ENTITY from diverse ENTITYUNRELATED , 250 ENTITYUNRELATED , and the set of relevant ENTITYUNRELATED or "" right answers "" to those ENTITYUNRELATED ."
The results from TREC-2 showed significant improvements over the TREC-1 results, and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection.	TREC-2	TREC-1 results	compare	{'e1': {'word': 'TREC-2', 'word_index': [(3, 3)], 'id': 'X96-1007.32'}, 'e2': {'word': 'TREC-1 results', 'word_index': [(9, 9)], 'id': 'X96-1007.33'}}	The results from ENTITY showed significant improvements over the ENTITYOTHER , and should be viewed as the appropriate baseline representing state - of - the - art ENTITYUNRELATED as scaled up to handling a 2 gigabyte collection .
In this paper we outline a lexical organization for Turkish that makes use of lexical rules for inflections, derivations, and lexical category changes to control the proliferation of lexical entries.	lexical rules	inflections	usage	{'e1': {'word': 'lexical rules', 'word_index': [(14, 14)], 'id': 'W96-0311.2'}, 'e2': {'word': 'inflections', 'word_index': [(16, 16)], 'id': 'W96-0311.3'}}	In this paper we outline a lexical organization for ENTITYUNRELATED that makes use of ENTITY for ENTITYOTHER , ENTITYUNRELATED , and ENTITYUNRELATED changes to control the proliferation of ENTITYUNRELATED .
A lexical inheritance hierarchy facilitates the enforcement of type constraints.	lexical inheritance hierarchy	type constraints	usage	{'e1': {'word': 'lexical inheritance hierarchy', 'word_index': [(1, 1)], 'id': 'W96-0311.12'}, 'e2': {'word': 'type constraints', 'word_index': [(6, 6)], 'id': 'W96-0311.13'}}	A ENTITY facilitates the enforcement of ENTITYOTHER .
Semantic compositions  in inflections and derivations are constrained by the properties of the terms and predicates.The design has been tested as part of a HPSG grammar for Turkish.	Semantic compositions 	inflections	model-feature	{'e1': {'word': 'Semantic compositions ', 'word_index': [(0, 0)], 'id': 'W96-0311.14'}, 'e2': {'word': 'inflections', 'word_index': [(2, 2)], 'id': 'W96-0311.15'}}	ENTITY in ENTITYOTHER and ENTITYUNRELATED are constrained by the properties of the ENTITYUNRELATED and ENTITYUNRELATED . The design has been tested as part of a ENTITYUNRELATED for ENTITYUNRELATED .
The bigram language models are popular, in much language processing applications, in both Indo-European and Asian languages.	bigram language models	language processing applications	usage	{'e1': {'word': 'bigram language models', 'word_index': [(1, 1)], 'id': 'W00-1311.1'}, 'e2': {'word': 'language processing applications', 'word_index': [(7, 7)], 'id': 'W00-1311.2'}}	The ENTITY are popular , in much ENTITYOTHER , in both ENTITYUNRELATED and ENTITYUNRELATED .
However, when the language model for Chinese is applied in a novel domain, the accuracy is reduced significantly, from 96% to 78% in our evaluation.	language model	domain	usage	{'e1': {'word': 'language model', 'word_index': [(4, 4)], 'id': 'W00-1311.5'}, 'e2': {'word': 'domain', 'word_index': [(12, 12)], 'id': 'W00-1311.7'}}	However , when the ENTITY for ENTITYUNRELATED is applied in a novel ENTITYOTHER , the ENTITYUNRELATED is reduced significantly , from 96 % to 78 % in our evaluation .
In our evaluation, Bayesian classifiers produce the best recall performance of 80% but the precision is low (60%).	Bayesian classifiers	recall performance	result	{'e1': {'word': 'Bayesian classifiers', 'word_index': [(4, 4)], 'id': 'W00-1311.15'}, 'e2': {'word': 'recall performance', 'word_index': [(8, 8)], 'id': 'W00-1311.16'}}	In our evaluation , ENTITY produce the best ENTITYOTHER of 80 % but the ENTITYUNRELATED is low ( 60 % ) .
Neural network produced good recall (75%) and precision (80%) but both Bayesian and Neural network have low skip ratio (65%).	Neural network	recall	result	{'e1': {'word': 'Neural network', 'word_index': [(0, 0)], 'id': 'W00-1311.18'}, 'e2': {'word': 'recall', 'word_index': [(3, 3)], 'id': 'W00-1311.19'}}	ENTITY produced good ENTITYOTHER ( 75 % ) and ENTITYUNRELATED ( 80 % ) but both ENTITYUNRELATED have low skip ratio ( 65 % ) .
The decision tree classifier produced the best precision (81%) and skip ratio (76%) but its recall is the lowest (73%).	decision tree classifier	precision	result	{'e1': {'word': 'decision tree classifier', 'word_index': [(1, 1)], 'id': 'W00-1311.22'}, 'e2': {'word': 'precision', 'word_index': [(5, 5)], 'id': 'W00-1311.23'}}	The ENTITY produced the best ENTITYOTHER ( 81 % ) and skip ratio ( 76 % ) but its ENTITYUNRELATED is the lowest ( 73 % ) .
Tokenization is the process of mapping sentences from character strings into strings of words.	mapping sentences	character strings	usage	{'e1': {'word': 'mapping sentences', 'word_index': [(5, 5)], 'id': 'J97-4004.1'}, 'e2': {'word': 'character strings', 'word_index': [(7, 7)], 'id': 'J97-4004.2'}}	Tokenization is the process of ENTITY from ENTITYOTHER into ENTITYUNRELATED .
This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework.	goodness measures	unsupervised segmentation	usage	{'e1': {'word': 'goodness measures', 'word_index': [(11, 11)], 'id': 'I08-1002.1'}, 'e2': {'word': 'unsupervised segmentation', 'word_index': [(13, 13)], 'id': 'I08-1002.2'}}	This paper reports our empirical evaluation and comparison of several popular ENTITY for ENTITYOTHER of ENTITYUNRELATED using ENTITYUNRELATED with a unified framework .
Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores.	word candidates	unlabeled texts	part_whole	{'e1': {'word': 'word candidates', 'word_index': [(15, 15)], 'id': 'I08-1002.8'}, 'e2': {'word': 'unlabeled texts', 'word_index': [(17, 17)], 'id': 'I08-1002.9'}}	Assuming no prior ENTITYUNRELATED about ENTITYUNRELATED , this framework relies on a ENTITYUNRELATED to identify ENTITY from ENTITYOTHER and then applies a generalized ENTITYUNRELATED to find the optimal ENTITYUNRELATED of a ENTITYUNRELATED into such ENTITYUNRELATED with the greatest sum of ENTITYUNRELATED .
Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores.	decoding algorithm	segmentation	usage	{'e1': {'word': 'decoding algorithm', 'word_index': [(23, 23)], 'id': 'I08-1002.10'}, 'e2': {'word': 'segmentation', 'word_index': [(28, 28)], 'id': 'I08-1002.11'}}	Assuming no prior ENTITYUNRELATED about ENTITYUNRELATED , this framework relies on a ENTITYUNRELATED to identify ENTITYUNRELATED from ENTITYUNRELATED and then applies a generalized ENTITY to find the optimal ENTITYOTHER of a ENTITYUNRELATED into such ENTITYUNRELATED with the greatest sum of ENTITYUNRELATED .
Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores.	goodness scores	candidates	model-feature	{'e1': {'word': 'goodness scores', 'word_index': [(40, 40)], 'id': 'I08-1002.14'}, 'e2': {'word': 'candidates', 'word_index': [(34, 34)], 'id': 'I08-1002.13'}}	Assuming no prior ENTITYUNRELATED about ENTITYUNRELATED , this framework relies on a ENTITYUNRELATED to identify ENTITYUNRELATED from ENTITYUNRELATED and then applies a generalized ENTITYUNRELATED to find the optimal ENTITYUNRELATED of a ENTITYUNRELATED into such ENTITYOTHER with the greatest sum of ENTITY .
Building on a state-of-the-art set of features, a binary classifier for each label is trained using AdaBoost with fixed depth decision trees.	decision trees	AdaBoost	usage	{'e1': {'word': 'decision trees', 'word_index': [(24, 24)], 'id': 'W05-0628.10'}, 'e2': {'word': 'AdaBoost', 'word_index': [(20, 20)], 'id': 'W05-0628.9'}}	Building on a state - of - the - art ENTITYUNRELATED , a ENTITYUNRELATED for each ENTITYUNRELATED is trained using ENTITYOTHER with fixed depth ENTITY .
In this paper we propose two metrics to be used in various fields of computational linguistics area.	metrics	computational linguistics	usage	{'e1': {'word': 'metrics', 'word_index': [(6, 6)], 'id': 'W06-1114.1'}, 'e2': {'word': 'computational linguistics', 'word_index': [(14, 14)], 'id': 'W06-1114.2'}}	In this paper we propose two ENTITY to be used in various fields of ENTITYOTHER area .
Finally, a short application is presented: we investigate the similarity of Romance languages by computing the scaled total rank distance between the digram rankings of each language.	scaled total rank distance	similarity	usage	{'e1': {'word': 'scaled total rank distance', 'word_index': [(17, 17)], 'id': 'W06-1114.9'}, 'e2': {'word': 'similarity', 'word_index': [(11, 11)], 'id': 'W06-1114.7'}}	Finally , a short application is presented : we investigate the ENTITYOTHER of ENTITYUNRELATED by computing the ENTITY between the digram rankings of each ENTITYUNRELATED .
First, we formalize the task of identifying Japanese compound functional expressions in a text as a machine learning based chunking problem.	Japanese compound functional expressions	text	part_whole	{'e1': {'word': 'Japanese compound functional expressions', 'word_index': [(8, 8)], 'id': 'W07-1109.4'}, 'e2': {'word': 'text', 'word_index': [(11, 11)], 'id': 'W07-1109.5'}}	First , we formalize the task of identifying ENTITY in a ENTITYOTHER as a ENTITYUNRELATED .
Next, against the results of identifying compound functional expressions, we apply the method of dependency analysis based on the cascaded chunking model.	cascaded chunking model	dependency analysis	usage	{'e1': {'word': 'cascaded chunking model', 'word_index': [(19, 19)], 'id': 'W07-1109.9'}, 'e2': {'word': 'dependency analysis', 'word_index': [(15, 15)], 'id': 'W07-1109.8'}}	Next , against the results of identifying compound ENTITYUNRELATED , we apply the method of ENTITYOTHER based on the ENTITY .
We introduce a relation extraction method to identify the sentences in biomedical text that indicate an interaction among the protein names mentioned.	relation extraction method	biomedical text	usage	{'e1': {'word': 'relation extraction method', 'word_index': [(3, 3)], 'id': 'D07-1024.1'}, 'e2': {'word': 'biomedical text', 'word_index': [(9, 9)], 'id': 'D07-1024.3'}}	We introduce a ENTITY to identify the ENTITYUNRELATED in ENTITYOTHER that indicate an ENTITYUNRELATED among the ENTITYUNRELATED mentioned .
Our approach is based on the analysis of the paths between two protein names in the dependency parse trees of the sentences.	dependency parse trees	sentences	model-feature	{'e1': {'word': 'dependency parse trees', 'word_index': [(15, 15)], 'id': 'D07-1024.7'}, 'e2': {'word': 'sentences', 'word_index': [(18, 18)], 'id': 'D07-1024.8'}}	Our approach is based on the analysis of the paths between two ENTITYUNRELATED in the ENTITY of the ENTITYOTHER .
Given two dependency trees, we define two separate similarity functions (kernels) based on cosine similarity and edit distance among the paths between the protein names.	cosine similarity	similarity functions (kernels)	usage	{'e1': {'word': 'cosine similarity', 'word_index': [(11, 11)], 'id': 'D07-1024.11'}, 'e2': {'word': 'similarity functions (kernels)', 'word_index': [(8, 8)], 'id': 'D07-1024.10'}}	Given two ENTITYUNRELATED , we define two separate ENTITYOTHER based on ENTITY and ENTITYUNRELATED among the paths between the ENTITYUNRELATED .
Semi-supervised algorithms perform better than their supervised version by a wide margin especially when the amount of labeled data is limited.	Semi-supervised algorithms	labeled data	usage	{'e1': {'word': 'Semi-supervised algorithms', 'word_index': [(0, 0)], 'id': 'D07-1024.19'}, 'e2': {'word': 'labeled data', 'word_index': [(16, 16)], 'id': 'D07-1024.20'}}	ENTITY perform better than their supervised version by a wide margin especially when the amount of ENTITYOTHER is limited .
Language model (LM) adaptation is important for both speech and language processing	Language model (LM) adaptation	speech and language processing	usage	{'e1': {'word': 'Language model (LM) adaptation', 'word_index': [(0, 0)], 'id': 'P07-1085.1'}, 'e2': {'word': 'speech and language processing', 'word_index': [(5, 5)], 'id': 'P07-1085.2'}}	ENTITY is important for both ENTITYOTHER
In addition, a new dynamically adapted weighting scheme for topic mixture models is proposed based on LDA topic analysis.	LDA topic analysis	weighting scheme	usage	{'e1': {'word': 'LDA topic analysis', 'word_index': [(14, 14)], 'id': 'P07-1085.15'}, 'e2': {'word': 'weighting scheme', 'word_index': [(7, 7)], 'id': 'P07-1085.13'}}	In addition , a new dynamically adapted ENTITYOTHER for ENTITYUNRELATED is proposed based on ENTITY .
Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline generic LM.	NE-driven LM adaptation	LM	compare	{'e1': {'word': 'NE-driven LM adaptation', 'word_index': [(6, 6)], 'id': 'P07-1085.16'}, 'e2': {'word': 'LM', 'word_index': [(12, 12)], 'id': 'P07-1085.17'}}	Our experimental results show that the ENTITY framework outperforms the baseline generic ENTITYOTHER .
The state-of-the-art system combination method for machine translation (MT) is the word-based combination using confusion networks.	combination method	machine translation (MT)	usage	{'e1': {'word': 'combination method', 'word_index': [(9, 9)], 'id': 'C08-1005.1'}, 'e2': {'word': 'machine translation (MT)', 'word_index': [(11, 11)], 'id': 'C08-1005.2'}}	The state - of - the - art system ENTITY for ENTITYOTHER is the ENTITYUNRELATED using ENTITYUNRELATED .
The state-of-the-art system combination method for machine translation (MT) is the word-based combination using confusion networks.	confusion networks	word-based combination	usage	{'e1': {'word': 'confusion networks', 'word_index': [(16, 16)], 'id': 'C08-1005.4'}, 'e2': {'word': 'word-based combination', 'word_index': [(14, 14)], 'id': 'C08-1005.3'}}	The state - of - the - art system ENTITYUNRELATED for ENTITYUNRELATED is the ENTITYOTHER using ENTITY .
In this paper, we present new methods to improve alignment of hypotheses using word synonyms and a two-pass alignment strategy.	word synonyms	alignment	usage	{'e1': {'word': 'word synonyms', 'word_index': [(14, 14)], 'id': 'C08-1005.8'}, 'e2': {'word': 'alignment', 'word_index': [(10, 10)], 'id': 'C08-1005.7'}}	In this paper , we present new methods to improve ENTITYOTHER of hypotheses using ENTITY and a two - pass ENTITYUNRELATED strategy .
We present a novel method for creating A* estimates for structured search problems.	A* estimates	structured search problems	usage	{'e1': {'word': 'A* estimates', 'word_index': [(7, 7)], 'id': 'N07-1052.1'}, 'e2': {'word': 'structured search problems', 'word_index': [(9, 9)], 'id': 'N07-1052.2'}}	We present a novel method for creating ENTITY for ENTITYOTHER .
The DPA algorithm works on the assumption of Direct Correspondence which simply means	assumption of Direct Correspondence	DPA algorithm	usage	{'e1': {'word': 'assumption of Direct Correspondence', 'word_index': [(5, 5)], 'id': 'P06-2039.11'}, 'e2': {'word': 'DPA algorithm', 'word_index': [(1, 1)], 'id': 'P06-2039.10'}}	The ENTITYOTHER works on the ENTITY which simply means
This leads to wrong parsed structure of the target language sentence.	parsed structure 	target language sentence	model-feature	{'e1': {'word': 'parsed structure ', 'word_index': [(4, 4)], 'id': 'P06-2039.16'}, 'e2': {'word': 'target language sentence', 'word_index': [(7, 7)], 'id': 'P06-2039.17'}}	This leads to wrong ENTITY of the ENTITYOTHER .
A parser is an algorithm that assigns a structural description to a string according to a grammar.	structural description	string	model-feature	{'e1': {'word': 'structural description', 'word_index': [(8, 8)], 'id': 'C88-1049.2'}, 'e2': {'word': 'string', 'word_index': [(11, 11)], 'id': 'C88-1049.3'}}	A ENTITYUNRELATED is an algorithm that assigns a ENTITY to a ENTITYOTHER according to a ENTITYUNRELATED .
Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition.	phrase structure descriptions	parsers	usage	{'e1': {'word': 'phrase structure descriptions', 'word_index': [(3, 3)], 'id': 'C88-1049.9'}, 'e2': {'word': 'parsers', 'word_index': [(1, 1)], 'id': 'C88-1049.8'}}	Common ENTITYOTHER employ ENTITY , ENTITYUNRELATED , and ENTITYUNRELATED or ENTITYUNRELATED .
the syntactical relationships are stated as part of the lexical descriptions of the elements of the language.	syntactical relationships	lexical descriptions	part_whole	{'e1': {'word': 'syntactical relationships', 'word_index': [(1, 1)], 'id': 'C88-1049.20'}, 'e2': {'word': 'lexical descriptions', 'word_index': [(8, 8)], 'id': 'C88-1049.21'}}	the ENTITY are stated as part of the ENTITYOTHER of the elements of the ENTITYUNRELATED .
We are going to describe the design and implementation of a communication system for large AI projects, capable of supporting various software components in a heterogeneous hardware and programming-language environment.	communication system	AI	usage	{'e1': {'word': 'communication system', 'word_index': [(11, 11)], 'id': 'C96-1008.1'}, 'e2': {'word': 'AI', 'word_index': [(14, 14)], 'id': 'C96-1008.2'}}	We are going to describe the design and implementation of a ENTITY for large ENTITYOTHER projects , capable of supporting various software components in a heterogeneous hardware and programming - language environment .
We present some preliminary results of a Czech-English translation system based on dependency trees.	dependency trees	Czech-English translation system	usage	{'e1': {'word': 'dependency trees', 'word_index': [(10, 10)], 'id': 'E03-1004.2'}, 'e2': {'word': 'Czech-English translation system', 'word_index': [(7, 7)], 'id': 'E03-1004.1'}}	We present some preliminary results of a ENTITYOTHER based on ENTITY .
The fully automated process includes: morphological tagging, analytical and tectogrammatical parsing of Czech, tectogrammatical transfer based on lexical substitution using word-to-word translation dictionaries enhanced by the information from the English-Czech parallel corpus of WSJ, and a simple rule-based system for generation from English tectogrammatical representation.	word-to-word translation dictionaries	lexical substitution	usage	{'e1': {'word': 'word-to-word translation dictionaries', 'word_index': [(17, 17)], 'id': 'E03-1004.8'}, 'e2': {'word': 'lexical substitution', 'word_index': [(15, 15)], 'id': 'E03-1004.7'}}	The fully automated process includes : ENTITYUNRELATED , ENTITYUNRELATED of ENTITYUNRELATED , ENTITYUNRELATED based on ENTITYOTHER using ENTITY enhanced by the information from the ENTITYUNRELATED of WSJ , and a ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED from ENTITYUNRELATED .
The fully automated process includes: morphological tagging, analytical and tectogrammatical parsing of Czech, tectogrammatical transfer based on lexical substitution using word-to-word translation dictionaries enhanced by the information from the English-Czech parallel corpus of WSJ, and a simple rule-based system for generation from English tectogrammatical representation.	rule-based system	generation	usage	{'e1': {'word': 'rule-based system', 'word_index': [(31, 31)], 'id': 'E03-1004.11'}, 'e2': {'word': 'generation', 'word_index': [(33, 33)], 'id': 'E03-1004.12'}}	The fully automated process includes : ENTITYUNRELATED , ENTITYUNRELATED of ENTITYUNRELATED , ENTITYUNRELATED based on ENTITYUNRELATED using ENTITYUNRELATED enhanced by the information from the ENTITYUNRELATED of WSJ , and a ENTITYUNRELATED ENTITY for ENTITYOTHER from ENTITYUNRELATED .
ADOMIT is an algorithm for Automatic Detection of OMIssions in Translations.	Automatic Detection	Translations	usage	{'e1': {'word': 'Automatic Detection', 'word_index': [(5, 5)], 'id': 'C96-2129.2'}, 'e2': {'word': 'Translations', 'word_index': [(9, 9)], 'id': 'C96-2129.4'}}	ENTITYUNRELATED is an algorithm for ENTITY of ENTITYUNRELATED in ENTITYOTHER .
The algorithm relies solely on geometric analysis of bitext maps and uses no linguistic information.	geometric analysis	bitext maps	topic	{'e1': {'word': 'geometric analysis', 'word_index': [(5, 5)], 'id': 'C96-2129.5'}, 'e2': {'word': 'bitext maps', 'word_index': [(7, 7)], 'id': 'C96-2129.6'}}	The algorithm relies solely on ENTITY of ENTITYOTHER and uses no ENTITYUNRELATED .
In the end, the aim is to locate all eventualities in a text on a time axis and/or a map to ensure an optimal base for automatic temporal and geospatial reasoning.	optimal base	automatic temporal and geospatial reasoning	part_whole	{'e1': {'word': 'optimal base', 'word_index': [(26, 26)], 'id': 'L08-1561.6'}, 'e2': {'word': 'automatic temporal and geospatial reasoning', 'word_index': [(28, 28)], 'id': 'L08-1561.7'}}	In the end , the aim is to locate all eventualities in a ENTITYUNRELATED on a time axis and / or a map to ensure an ENTITY for ENTITYOTHER .
The world knowledge MiniSTEx uses is contained in interconnected tables in a database.	interconnected tables	database	part_whole	{'e1': {'word': 'interconnected tables', 'word_index': [(7, 7)], 'id': 'L08-1561.20'}, 'e2': {'word': 'database', 'word_index': [(10, 10)], 'id': 'L08-1561.21'}}	The ENTITYUNRELATED ENTITYUNRELATED uses is contained in ENTITY in a ENTITYOTHER .
We propose a semantic construction method for Feature-Based Tree Adjoining Grammar which is based on the derived tree, compare it with related proposals and briefly discuss some implementation possibilities.	derived tree	semantic construction method	usage	{'e1': {'word': 'derived tree', 'word_index': [(11, 11)], 'id': 'E03-1030.3'}, 'e2': {'word': 'semantic construction method', 'word_index': [(3, 3)], 'id': 'E03-1030.1'}}	We propose a ENTITYOTHER for ENTITYUNRELATED which is based on the ENTITY , compare it with related proposals and briefly discuss some ENTITYUNRELATED .
Traditional approaches to the problem of extracting data from texts have emphasized handcrafted linguistic knowledge	data	texts	part_whole	{'e1': {'word': 'data', 'word_index': [(7, 7)], 'id': 'M91-1021.1'}, 'e2': {'word': 'texts', 'word_index': [(9, 9)], 'id': 'M91-1021.2'}}	Traditional approaches to the problem of extracting ENTITY from ENTITYOTHER have emphasized ENTITYUNRELATED
We have previously performed experiments on components of the system with texts from the Wall Street Journal, however, the MUC-3 task is the first end-to-end application of plum.	texts	Wall Street Journal	part_whole	{'e1': {'word': 'texts', 'word_index': [(11, 11)], 'id': 'M91-1021.14'}, 'e2': {'word': 'Wall Street Journal', 'word_index': [(14, 14)], 'id': 'M91-1021.15'}}	We have previously performed experiments on components of the system with ENTITY from the ENTITYOTHER , however , the ENTITYUNRELATED is the first ENTITYUNRELATED of ENTITYUNRELATED .
A central assumption of our approach is that in processing unrestricted text for data extraction, a non-trivial amount of the text will not be understood.	data extraction	unrestricted text	usage	{'e1': {'word': 'data extraction', 'word_index': [(12, 12)], 'id': 'M91-1021.24'}, 'e2': {'word': 'unrestricted text', 'word_index': [(10, 10)], 'id': 'M91-1021.23'}}	A central assumption of our approach is that in processing ENTITYOTHER for ENTITY , a non-trivial amount of the ENTITYUNRELATED will not be understood .
We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose.	sentences	document	part_whole	{'e1': {'word': 'sentences', 'word_index': [(10, 10)], 'id': 'A00-1043.3'}, 'e2': {'word': 'document', 'word_index': [(16, 16)], 'id': 'A00-1043.4'}}	We present a novel ENTITYUNRELATED for automatically removing ENTITYUNRELATED from ENTITY that are extracted from a ENTITYOTHER for ENTITYUNRELATED .
The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed, including syntactic knowledge, context information, and statistics computed from a corpus which consists of examples written by human professionals.	statistics	corpus	part_whole	{'e1': {'word': 'statistics', 'word_index': [(23, 23)], 'id': 'A00-1043.11'}, 'e2': {'word': 'corpus', 'word_index': [(27, 27)], 'id': 'A00-1043.12'}}	The system uses multiple ENTITYUNRELATED to decide which ENTITYUNRELATED in an extracted ENTITYUNRELATED can be removed , including ENTITYUNRELATED , ENTITYUNRELATED , and ENTITY computed from a ENTITYOTHER which consists of examples written by human professionals .
Reduction can significantly improve the conciseness of automatic summaries.	conciseness	automatic summaries	model-feature	{'e1': {'word': 'conciseness', 'word_index': [(5, 5)], 'id': 'A00-1043.14'}, 'e2': {'word': 'automatic summaries', 'word_index': [(7, 7)], 'id': 'A00-1043.15'}}	ENTITYUNRELATED can significantly improve the ENTITY of ENTITYOTHER .
The method is automatically trainable, acquiring information from both positive and negative examples.	information	positive and negative examples	part_whole	{'e1': {'word': 'information', 'word_index': [(7, 7)], 'id': 'H94-1040.5'}, 'e2': {'word': 'positive and negative examples', 'word_index': [(10, 10)], 'id': 'H94-1040.6'}}	The method is automatically trainable , acquiring ENTITY from both ENTITYOTHER .
This paper introduces an algorithm for automatically acquiring the conceptual structure of each word from corpus.	conceptual structure	word	model-feature	{'e1': {'word': 'conceptual structure', 'word_index': [(9, 9)], 'id': 'W93-0103.1'}, 'e2': {'word': 'word', 'word_index': [(12, 12)], 'id': 'W93-0103.2'}}	This paper introduces an algorithm for automatically acquiring the ENTITY of each ENTITYOTHER from ENTITYUNRELATED .
The lexical concept obtained from the Collocation Map best reflects the subdomain of language usage.	lexical concept	Collocation Map	part_whole	{'e1': {'word': 'lexical concept', 'word_index': [(1, 1)], 'id': 'W93-0103.24'}, 'e2': {'word': 'Collocation Map', 'word_index': [(5, 5)], 'id': 'W93-0103.25'}}	The ENTITY obtained from the ENTITYOTHER best reflects the ENTITYUNRELATED of ENTITYUNRELATED .
The potential application of conditional probabilities the Collocation Map provides may extend to cover very diverse areas of language processing such as sense disambiguation, thesaurus construction, automatic indexing, and document classification.	conditional probabilities	language processing	usage	{'e1': {'word': 'conditional probabilities', 'word_index': [(4, 4)], 'id': 'W93-0103.28'}, 'e2': {'word': 'language processing', 'word_index': [(16, 16)], 'id': 'W93-0103.30'}}	The potential application of ENTITY the ENTITYUNRELATED provides may extend to cover very diverse areas of ENTITYOTHER such as ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED .
"We discuss such ""strapping"" methods in general, and exhibit a particular method for strapping word-sense classifiers for ambiguous words."	"""strapping"" methods"	word-sense classifiers	usage	"{'e1': {'word': '""strapping"" methods', 'word_index': [(3, 3)], 'id': 'H05-1050.8'}, 'e2': {'word': 'word-sense classifiers', 'word_index': [(14, 14)], 'id': 'H05-1050.9'}}"	We discuss such ENTITY in general , and exhibit a particular method for strapping ENTITYOTHER for ENTITYUNRELATED .
"Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand (Yarowsky, 1995), which in turn is known to rival supervised methods. """	unsupervised technique	supervised methods	compare	{'e1': {'word': 'unsupervised technique', 'word_index': [(8, 8)], 'id': 'H05-1050.12'}, 'e2': {'word': 'supervised methods', 'word_index': [(31, 31)], 'id': 'H05-1050.14'}}	"Our experiments on the ENTITYUNRELATED show that our ENTITY is significantly more effective than picking ENTITYUNRELATED by hand ( Yarowsky , 1995 ) , which in turn is known to rival ENTITYOTHER . """
This word association information is added to the system at the time of the automatic creation of our translation pattern database, thereby making this database more domain specific.	word association information	system	usage	{'e1': {'word': 'word association information', 'word_index': [(1, 1)], 'id': 'W01-1414.6'}, 'e2': {'word': 'system', 'word_index': [(6, 6)], 'id': 'W01-1414.7'}}	This ENTITY is added to the ENTITYOTHER at the time of the automatic creation of our ENTITYUNRELATED , thereby making this ENTITYUNRELATED more domain specific .
This technique significantly improves the overall quality of translation, as measured in an independent blind evaluation.	technique	translation	result	{'e1': {'word': 'technique', 'word_index': [(1, 1)], 'id': 'W01-1414.10'}, 'e2': {'word': 'translation', 'word_index': [(8, 8)], 'id': 'W01-1414.11'}}	This ENTITY significantly improves the overall quality of ENTITYOTHER , as measured in an ENTITYUNRELATED .
We describe the experiments of the UC Berkeley team on improving English-Spanish machine translation of news text, as part of the WMT'08 Shared Translation Task.	English-Spanish machine translation	news text	usage	{'e1': {'word': 'English-Spanish machine translation', 'word_index': [(11, 11)], 'id': 'W08-0320.1'}, 'e2': {'word': 'news text', 'word_index': [(13, 13)], 'id': 'W08-0320.2'}}	We describe the experiments of the UC Berkeley team on improving ENTITY of ENTITYOTHER , as part of the ENTITYUNRELATED .
We further add a third phrase translation model trained on a version of the news bi-text augmented with monolingual sentence-level syntactic paraphrases on the source-language side, and we combine all models in a log-linear model using minimum error rate training.	monolingual sentence-level syntactic paraphrases	news bi-text	part_whole	{'e1': {'word': 'monolingual sentence-level syntactic paraphrases', 'word_index': [(14, 14)], 'id': 'W08-0320.11'}, 'e2': {'word': 'news bi-text', 'word_index': [(11, 11)], 'id': 'W08-0320.10'}}	We further add a ENTITYUNRELATED trained on a version of the ENTITYOTHER augmented with ENTITY on the source - language side , and we combine all ENTITYUNRELATED in a ENTITYUNRELATED using ENTITYUNRELATED .
Several recently reported techniques for the automatic acquisition of Information Extraction (IE) systems have used dependency trees as the basis of their extraction pattern representation.	dependency trees	Information Extraction (IE) systems	usage	{'e1': {'word': 'dependency trees', 'word_index': [(11, 11)], 'id': 'W06-0202.3'}, 'e2': {'word': 'Information Extraction (IE) systems', 'word_index': [(8, 8)], 'id': 'W06-0202.2'}}	Several recently reported techniques for the ENTITYUNRELATED of ENTITYOTHER have used ENTITY as the basis of their ENTITYUNRELATED .
An appropriate model should be expressive enough to represent the information which is to be extracted from text without being overly complicated.	information	text	part_whole	{'e1': {'word': 'information', 'word_index': [(10, 10)], 'id': 'W06-0202.9'}, 'e2': {'word': 'text', 'word_index': [(17, 17)], 'id': 'W06-0202.10'}}	An appropriate model should be expressive enough to represent the ENTITY which is to be extracted from ENTITYOTHER without being overly complicated .
The number of control actions needed to switch languages was decreased over 93% when using TypeAny rather than a conventional method.	TypeAny	conventional method	compare	{'e1': {'word': 'TypeAny', 'word_index': [(15, 15)], 'id': 'I08-1058.23'}, 'e2': {'word': 'conventional method', 'word_index': [(19, 19)], 'id': 'I08-1058.24'}}	The number of ENTITYUNRELATED needed to switch ENTITYUNRELATED was decreased over 93 % when using ENTITY rather than a ENTITYOTHER .
It builds on an earlier system based on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.	word overlap	shallow component	usage	{'e1': {'word': 'word overlap', 'word_index': [(20, 20)], 'id': 'W07-1402.5'}, 'e2': {'word': 'shallow component', 'word_index': [(17, 17)], 'id': 'W07-1402.4'}}	It builds on an earlier system based on a relatively ENTITYUNRELATED , which we complement with a ENTITYOTHER based on ENTITY .
However, earlier observations that the combination of features improves the overall accuracy could be replicated only partly.	features	overall accuracy	result	{'e1': {'word': 'features', 'word_index': [(8, 8)], 'id': 'W07-1402.7'}, 'e2': {'word': 'overall accuracy', 'word_index': [(11, 11)], 'id': 'W07-1402.8'}}	However , earlier observations that the combination of ENTITY improves the ENTITYOTHER could be replicated only partly .
In this paper, we address the problem of extracting data records and their attributes from unstructured biomedical full text.	data records	unstructured biomedical full text	part_whole	{'e1': {'word': 'data records', 'word_index': [(10, 10)], 'id': 'D07-1088.1'}, 'e2': {'word': 'unstructured biomedical full text', 'word_index': [(15, 15)], 'id': 'D07-1088.3'}}	In this paper , we address the problem of extracting ENTITY and their ENTITYUNRELATED from ENTITYOTHER .
We evaluate the approach from the perspective of Information Extraction and achieve significant improvements on system performance compared with other baseline systems.	system performance	baseline systems	compare	{'e1': {'word': 'system performance', 'word_index': [(14, 14)], 'id': 'D07-1088.12'}, 'e2': {'word': 'baseline systems', 'word_index': [(18, 18)], 'id': 'D07-1088.13'}}	We evaluate the approach from the perspective of ENTITYUNRELATED and achieve significant improvements on ENTITY compared with other ENTITYOTHER .
In this paper, we describe a system by which the multilingual characteristics of Wikipedia can be utilized to annotate a large corpus of text with Named Entity Recognition (NER) tags requiring minimal human intervention and no linguistic expertise.	multilingual characteristics	Wikipedia	model-feature	{'e1': {'word': 'multilingual characteristics', 'word_index': [(11, 11)], 'id': 'P08-1001.1'}, 'e2': {'word': 'Wikipedia', 'word_index': [(13, 13)], 'id': 'P08-1001.2'}}	In this paper , we describe a system by which the ENTITY of ENTITYOTHER can be utilized to annotate a ENTITYUNRELATED of ENTITYUNRELATED with ENTITYUNRELATED requiring minimal human intervention and no ENTITYUNRELATED .
In this paper, we describe a system by which the multilingual characteristics of Wikipedia can be utilized to annotate a large corpus of text with Named Entity Recognition (NER) tags requiring minimal human intervention and no linguistic expertise.	text	large corpus	part_whole	{'e1': {'word': 'text', 'word_index': [(22, 22)], 'id': 'P08-1001.4'}, 'e2': {'word': 'large corpus', 'word_index': [(20, 20)], 'id': 'P08-1001.3'}}	In this paper , we describe a system by which the ENTITYUNRELATED of ENTITYUNRELATED can be utilized to annotate a ENTITYOTHER of ENTITY with ENTITYUNRELATED requiring minimal human intervention and no ENTITYUNRELATED .
We further describe the methods by which English language data can be used to bootstrap the NER process in other languages.	English language data	NER process	usage	{'e1': {'word': 'English language data', 'word_index': [(7, 7)], 'id': 'P08-1001.17'}, 'e2': {'word': 'NER process', 'word_index': [(14, 14)], 'id': 'P08-1001.18'}}	We further describe the methods by which ENTITY can be used to bootstrap the ENTITYOTHER in other ENTITYUNRELATED .
Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text.	entity-grid representation	discourse	model-feature	{'e1': {'word': 'entity-grid representation', 'word_index': [(6, 6)], 'id': 'J08-1001.2'}, 'e2': {'word': 'discourse', 'word_index': [(8, 8)], 'id': 'J08-1001.3'}}	Central to this approach is the ENTITY of ENTITYOTHER , which captures ENTITYUNRELATED of ENTITYUNRELATED in a ENTITYUNRELATED .
The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities.	entity transition sequences	text	model-feature	{'e1': {'word': 'entity transition sequences', 'word_index': [(14, 14)], 'id': 'J08-1001.8'}, 'e2': {'word': 'text', 'word_index': [(9, 9)], 'id': 'J08-1001.7'}}	The algorithm introduced in the article automatically abstracts a ENTITYOTHER into a set of ENTITY and records ENTITYUNRELATED about ENTITYUNRELATED .
This paper describes discriminative language modeling for a large vocabulary speech recognition task.	discriminative language modeling	vocabulary speech recognition task	usage	{'e1': {'word': 'discriminative language modeling', 'word_index': [(3, 3)], 'id': 'P04-1007.1'}, 'e2': {'word': 'vocabulary speech recognition task', 'word_index': [(7, 7)], 'id': 'P04-1007.2'}}	This paper describes ENTITY for a large ENTITYOTHER .
We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random fields (CRFs).	perceptron algorithm	conditional random fields (CRFs)	compare	{'e1': {'word': 'perceptron algorithm', 'word_index': [(6, 6)], 'id': 'P04-1007.4'}, 'e2': {'word': 'conditional random fields (CRFs)', 'word_index': [(13, 13)], 'id': 'P04-1007.5'}}	We contrast two ENTITYUNRELATED : the ENTITY , and a method based on ENTITYOTHER .
The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data.	feature set	training data	part_whole	{'e1': {'word': 'feature set', 'word_index': [(11, 11)], 'id': 'P04-1007.12'}, 'e2': {'word': 'training data', 'word_index': [(20, 20)], 'id': 'P04-1007.13'}}	The ENTITYUNRELATED has the benefit of automatically selecting a relatively small ENTITY in just a couple of passes over the ENTITYOTHER .
However, using the feature set output from the perceptron algorithm (initialized with their weights), CRF training provides an additional 0.5% reduction in word error rate, for a total 1.8% absolute reduction from the baseline of 39.2%.	CRF training	word error rate	result	{'e1': {'word': 'CRF training', 'word_index': [(15, 15)], 'id': 'P04-1007.17'}, 'e2': {'word': 'word error rate', 'word_index': [(23, 23)], 'id': 'P04-1007.18'}}	However , using the ENTITYUNRELATED from the ENTITYUNRELATED ( initialized with their ENTITYUNRELATED ) , ENTITY provides an additional 0.5 % reduction in ENTITYOTHER , for a total 1.8 % absolute reduction from the ENTITYUNRELATED of 39.2 %.
We briefly describe the design and implementation status of the system, and then focus on how this system is used to elicit useful data for supporting hypotheses about multimodal interaction in the domain of meeting retrieval and for developing NLP modules for this specific domain.	system	useful data	usage	{'e1': {'word': 'system', 'word_index': [(17, 17)], 'id': 'P06-4013.6'}, 'e2': {'word': 'useful data', 'word_index': [(22, 22)], 'id': 'P06-4013.7'}}	We briefly describe the design and ENTITYUNRELATED of the ENTITYUNRELATED , and then focus on how this ENTITY is used to elicit ENTITYOTHER for supporting ENTITYUNRELATED about ENTITYUNRELATED in the domain of ENTITYUNRELATED and for developing ENTITYUNRELATED for this specific domain .
We show that the possibility to label distinctions with names has major advantages both for the use of feature logic in computational linguistics and its implementation.	feature logic	computational linguistics	usage	{'e1': {'word': 'feature logic', 'word_index': [(18, 18)], 'id': 'C90-2018.7'}, 'e2': {'word': 'computational linguistics', 'word_index': [(20, 20)], 'id': 'C90-2018.8'}}	We show that the possibility to label distinctions with ENTITYUNRELATED has major advantages both for the use of ENTITY in ENTITYOTHER and its implementation .
We give an open world semantics for feature terms, where the denotation of a term is determined in dependence on the disjunctive context, i.e.	denotation	term	model-feature	{'e1': {'word': 'denotation', 'word_index': [(9, 9)], 'id': 'C90-2018.11'}, 'e2': {'word': 'term', 'word_index': [(12, 12)], 'id': 'C90-2018.12'}}	We give an ENTITYUNRELATED for ENTITYUNRELATED , where the ENTITY of a ENTITYOTHER is determined in dependence on the ENTITYUNRELATED , i.e.
Acquiring source language documents for testing, creating training datasets for customized MT lexicons, and building parallel corpora for MT evaluation require translators and non-native speaking analysts to handle large document collections.	source language documents	training datasets	usage	{'e1': {'word': 'source language documents', 'word_index': [(1, 1)], 'id': 'L08-1588.4'}, 'e2': {'word': 'training datasets', 'word_index': [(6, 6)], 'id': 'L08-1588.5'}}	Acquiring ENTITY for testing , creating ENTITYOTHER for ENTITYUNRELATED , and building ENTITYUNRELATED for ENTITYUNRELATED require ENTITYUNRELATED and ENTITYUNRELATED to handle large ENTITYUNRELATED .
In particular, we will discuss the development and use of MTriage, an application environment that enables the translator to markup documents with metadata for MT parameterization and routing.	metadata	documents	model-feature	{'e1': {'word': 'metadata', 'word_index': [(24, 24)], 'id': 'L08-1588.24'}, 'e2': {'word': 'documents', 'word_index': [(22, 22)], 'id': 'L08-1588.23'}}	In particular , we will discuss the development and use of ENTITYUNRELATED , an application environment that enables the ENTITYUNRELATED to markup ENTITYOTHER with ENTITY for ENTITYUNRELATED .
The use of MTriage as a web-enabled front end to multiple MT engines has leveraged the capabilities of our human translators for creating lexicons from NFW (Not-Found-Word) lists, writing reference translations, and creating parallel corpora for MT development and evaluation.	lexicons	NFW (Not-Found-Word) lists	part_whole	{'e1': {'word': 'lexicons', 'word_index': [(19, 19)], 'id': 'L08-1588.30'}, 'e2': {'word': 'NFW (Not-Found-Word) lists', 'word_index': [(21, 21)], 'id': 'L08-1588.31'}}	The use of ENTITYUNRELATED as a ENTITYUNRELATED to multiple ENTITYUNRELATED has leveraged the capabilities of our ENTITYUNRELATED for creating ENTITY from ENTITYOTHER , writing ENTITYUNRELATED , and creating ENTITYUNRELATED for ENTITYUNRELATED development and evaluation .
The use of MTriage as a web-enabled front end to multiple MT engines has leveraged the capabilities of our human translators for creating lexicons from NFW (Not-Found-Word) lists, writing reference translations, and creating parallel corpora for MT development and evaluation.	parallel corpora	MT	usage	{'e1': {'word': 'parallel corpora', 'word_index': [(28, 28)], 'id': 'L08-1588.33'}, 'e2': {'word': 'MT', 'word_index': [(30, 30)], 'id': 'L08-1588.34'}}	The use of ENTITYUNRELATED as a ENTITYUNRELATED to multiple ENTITYUNRELATED has leveraged the capabilities of our ENTITYUNRELATED for creating ENTITYUNRELATED from ENTITYUNRELATED , writing ENTITYUNRELATED , and creating ENTITY for ENTITYOTHER development and evaluation .
This paper describes a method for analyzing Japanese double-subject construction having an adjective predicate based on the valency structure.	adjective predicate	Japanese double-subject construction	model-feature	{'e1': {'word': 'adjective predicate', 'word_index': [(10, 10)], 'id': 'C96-2146.2'}, 'e2': {'word': 'Japanese double-subject construction', 'word_index': [(7, 7)], 'id': 'C96-2146.1'}}	This paper describes a method for analyzing ENTITYOTHER having an ENTITY based on the ENTITYUNRELATED .
A simple sentence usually has only one subjective case in most languages.	subjective case	simple sentence	model-feature	{'e1': {'word': 'subjective case', 'word_index': [(6, 6)], 'id': 'C96-2146.5'}, 'e2': {'word': 'simple sentence', 'word_index': [(1, 1)], 'id': 'C96-2146.4'}}	A ENTITYOTHER usually has only one ENTITY in most ENTITYUNRELATED .
This paper proposes a method for analyzing a Japanese double-subject construction having an adjective predicate in order to overcome thee problems described.	adjective predicate	Japanese double-subject construction	model-feature	{'e1': {'word': 'adjective predicate', 'word_index': [(11, 11)], 'id': 'C96-2146.16'}, 'e2': {'word': 'Japanese double-subject construction', 'word_index': [(8, 8)], 'id': 'C96-2146.15'}}	This paper proposes a method for analyzing a ENTITYOTHER having an ENTITY in order to overcome thee problems described .
Classification Hierarchies (CHs) are widely used to organize documents in a way that makes their retrieval easier	Classification Hierarchies (CHs)	retrieval	usage	{'e1': {'word': 'Classification Hierarchies (CHs)', 'word_index': [(0, 0)], 'id': 'C04-1163.1'}, 'e2': {'word': 'retrieval', 'word_index': [(13, 13)], 'id': 'C04-1163.3'}}	ENTITY are widely used to organize ENTITYUNRELATED in a way that makes their ENTITYOTHER easier
In this paper we discuss and evaluate CtxMatch, an approach to interoperability that discovers mappings among CHs considering the semantic interpretation of their nodes.	semantic interpretation	nodes	model-feature	{'e1': {'word': 'semantic interpretation', 'word_index': [(20, 20)], 'id': 'C04-1163.10'}, 'e2': {'word': 'nodes', 'word_index': [(23, 23)], 'id': 'C04-1163.11'}}	In this paper we discuss and evaluate ENTITYUNRELATED , an approach to interoperability that discovers ENTITYUNRELATED among ENTITYUNRELATED considering the ENTITY of their ENTITYOTHER .
CtxMatch performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation.	linguistic processing	labels	usage	{'e1': {'word': 'linguistic processing', 'word_index': [(3, 3)], 'id': 'C04-1163.13'}, 'e2': {'word': 'labels', 'word_index': [(6, 6)], 'id': 'C04-1163.14'}}	ENTITYUNRELATED performs a ENTITY of the ENTITYOTHER attached to the ENTITYUNRELATED , including ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED .
A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient.	clustering	retrieval	usage	{'e1': {'word': 'clustering', 'word_index': [(7, 7)], 'id': 'C94-1014.7'}, 'e2': {'word': 'retrieval', 'word_index': [(10, 10)], 'id': 'C94-1014.8'}}	A second contribution is the use of ENTITY to make ENTITYOTHER of the ENTITYUNRELATED from the ENTITYUNRELATED more efficient .
A second contribution is the use of clustering to make retrieval of the best matching example from the database more efficient.	best matching example	database	part_whole	{'e1': {'word': 'best matching example', 'word_index': [(13, 13)], 'id': 'C94-1014.9'}, 'e2': {'word': 'database', 'word_index': [(16, 16)], 'id': 'C94-1014.10'}}	A second contribution is the use of ENTITYUNRELATED to make ENTITYUNRELATED of the ENTITY from the ENTITYOTHER more efficient .
At the same time, higher level collective knowledge is often published using a graphical notation representing all the entities in a pathway and their interactions.	graphical notation	entities	model-feature	{'e1': {'word': 'graphical notation', 'word_index': [(11, 11)], 'id': 'L08-1302.5'}, 'e2': {'word': 'entities', 'word_index': [(15, 15)], 'id': 'L08-1302.6'}}	At the same time , ENTITYUNRELATED is often published using a ENTITY representing all the ENTITYOTHER in a ENTITYUNRELATED and their ENTITYUNRELATED .
