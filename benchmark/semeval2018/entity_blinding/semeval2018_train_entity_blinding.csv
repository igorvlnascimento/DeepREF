original_sentence	e1	e2	relation_type	metadata	preprocessed_sentence
Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance.	keywords	information retrieval techniques	usage	{'e1': {'word': 'keywords', 'word_index': [(6, 6)], 'id': 'H01-1001.7'}, 'e2': {'word': 'information retrieval techniques', 'word_index': [(1, 1)], 'id': 'H01-1001.5'}}	Traditional ENTITYOTHER use a ENTITYUNRELATED of ENTITY as the ENTITYUNRELATED but ENTITYUNRELATED may offer additional ENTITYUNRELATED such as the time and place of the rejoinder and the attendance .
Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance.	oral communication	indices	usage	{'e1': {'word': 'oral communication', 'word_index': [(11, 11)], 'id': 'H01-1001.9'}, 'e2': {'word': 'indices', 'word_index': [(15, 15)], 'id': 'H01-1001.10'}}	Traditional ENTITYUNRELATED use a ENTITYUNRELATED of ENTITYUNRELATED as the ENTITYUNRELATED but ENTITY may offer additional ENTITYOTHER such as the time and place of the rejoinder and the attendance .
Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger database and detect those automatically which is shown on a large database of TV shows .	TV shows	database	part_whole	{'e1': {'word': 'TV shows', 'word_index': [(36, 36)], 'id': 'H01-1001.15'}, 'e2': {'word': 'database', 'word_index': [(34, 34)], 'id': 'H01-1001.14'}}	Several extensions of this basic idea are being discussed and / or evaluated : Similar to activities one can define subsets of larger ENTITYUNRELATED and detect those automatically which is shown on a large ENTITYOTHER of ENTITY .
 To support engaging human users in robust, mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems , the DARPA Communicator program [1] is funding the development of a distributed message-passing infrastructure for dialogue systems which all Communicator participants are using	distributed message-passing infrastructure	dialogue systems	model-feature	{'e1': {'word': 'distributed message-passing infrastructure', 'word_index': [(28, 28)], 'id': 'H01-1017.4'}, 'e2': {'word': 'dialogue systems', 'word_index': [(30, 30)], 'id': 'H01-1017.5'}}	To support engaging human users in robust , ENTITYUNRELATED which reach beyond current capabilities in ENTITYUNRELATED , the ENTITYUNRELATED [ 1 ] is funding the development of a ENTITY for ENTITYOTHER which all ENTITYUNRELATED participants are using
The CCLINC Korean-to-English translation system consists of two core modules , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame .	core modules	CCLINC Korean-to-English translation system	part_whole	{'e1': {'word': 'core modules', 'word_index': [(5, 5)], 'id': 'H01-1041.4'}, 'e2': {'word': 'CCLINC Korean-to-English translation system', 'word_index': [(1, 1)], 'id': 'H01-1041.3'}}	The ENTITYOTHER consists of two ENTITY , ENTITYUNRELATED mediated by a ENTITYUNRELATED called a ENTITYUNRELATED .
The key features of the system include: (i) Robust efficient parsing of Korean (a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ).	parsing	Korean	usage	{'e1': {'word': 'parsing', 'word_index': [(12, 12)], 'id': 'H01-1041.8'}, 'e2': {'word': 'Korean', 'word_index': [(14, 14)], 'id': 'H01-1041.9'}}	The key features of the system include : ( i) Robust efficient ENTITY of ENTITYOTHER ( a ENTITYUNRELATED with ENTITYUNRELATED , relatively ENTITYUNRELATED , and frequent omissions of ENTITYUNRELATED ) .
The key features of the system include: (i) Robust efficient parsing of Korean (a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ).	overt case markers	verb final language	model-feature	{'e1': {'word': 'overt case markers', 'word_index': [(19, 19)], 'id': 'H01-1041.11'}, 'e2': {'word': 'verb final language', 'word_index': [(17, 17)], 'id': 'H01-1041.10'}}	The key features of the system include : ( i) Robust efficient ENTITYUNRELATED of ENTITYUNRELATED ( a ENTITYOTHER with ENTITY , relatively ENTITYUNRELATED , and frequent omissions of ENTITYUNRELATED ) .
(ii) High quality translation via word sense disambiguation and accurate word order generation of the target language .	word sense disambiguation	translation	usage	{'e1': {'word': 'word sense disambiguation', 'word_index': [(7, 7)], 'id': 'H01-1041.15'}, 'e2': {'word': 'translation', 'word_index': [(5, 5)], 'id': 'H01-1041.14'}}	( ii ) High quality ENTITYOTHER via ENTITY and accurate ENTITYUNRELATED of the ENTITYUNRELATED .
 The purpose of this research is to test the efficacy of applying automated evaluation techniques , originally devised for the evaluation of human language learners , to the output of machine translation (MT) systems 	automated evaluation techniques	output	usage	{'e1': {'word': 'automated evaluation techniques', 'word_index': [(12, 12)], 'id': 'H01-1042.1'}, 'e2': {'word': 'output', 'word_index': [(24, 24)], 'id': 'H01-1042.3'}}	The purpose of this research is to test the efficacy of applying ENTITY , originally devised for the evaluation of ENTITYUNRELATED , to the ENTITYOTHER of ENTITYUNRELATED
This, the first experiment in a series of experiments, looks at the intelligibility of MT output .	intelligibility	MT output	model-feature	{'e1': {'word': 'intelligibility', 'word_index': [(14, 14)], 'id': 'H01-1042.10'}, 'e2': {'word': 'MT output', 'word_index': [(16, 16)], 'id': 'H01-1042.11'}}	This , the first experiment in a series of experiments , looks at the ENTITY of ENTITYOTHER .
We integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources .	intelligent mobile agents	spoken language understanding system	part_whole	{'e1': {'word': 'intelligent mobile agents', 'word_index': [(5, 5)], 'id': 'H01-1049.4'}, 'e2': {'word': 'spoken language understanding system', 'word_index': [(3, 3)], 'id': 'H01-1049.3'}}	We integrate a ENTITYOTHER with ENTITY that mediate between ENTITYUNRELATED and ENTITYUNRELATED .
We find that simple interpolation methods , like log-linear and linear interpolation , improve the performance but fall short of the performance of an oracle .	interpolation methods	performance	result	{'e1': {'word': 'interpolation methods', 'word_index': [(4, 4)], 'id': 'H01-1058.2'}, 'e2': {'word': 'performance', 'word_index': [(11, 11)], 'id': 'H01-1058.4'}}	We find that simple ENTITY , like ENTITYUNRELATED , improve the ENTITYOTHER but fall short of the ENTITYUNRELATED of an ENTITYUNRELATED .
The oracle knows the reference word string and selects the word string with the best performance (typically, word or semantic error rate ) from a list of word strings , where each word string has been obtained by using a different LM .	word string	performance	result	{'e1': {'word': 'word string', 'word_index': [(8, 8)], 'id': 'H01-1058.9'}, 'e2': {'word': 'performance', 'word_index': [(12, 12)], 'id': 'H01-1058.10'}}	The ENTITYUNRELATED knows the ENTITYUNRELATED and selects the ENTITY with the best ENTITYOTHER ( typically , ENTITYUNRELATED ) from a list of ENTITYUNRELATED , where each ENTITYUNRELATED has been obtained by using a different ENTITYUNRELATED .
The oracle knows the reference word string and selects the word string with the best performance (typically, word or semantic error rate ) from a list of word strings , where each word string has been obtained by using a different LM .	LM	word string	result	{'e1': {'word': 'LM', 'word_index': [(34, 34)], 'id': 'H01-1058.14'}, 'e2': {'word': 'word string', 'word_index': [(26, 26)], 'id': 'H01-1058.13'}}	The ENTITYUNRELATED knows the ENTITYUNRELATED and selects the ENTITYUNRELATED with the best ENTITYUNRELATED ( typically , ENTITYUNRELATED ) from a list of ENTITYUNRELATED , where each ENTITYOTHER has been obtained by using a different ENTITY .
Actually, the oracle acts like a dynamic combiner with hard decisions using the reference .	reference	dynamic combiner	usage	{'e1': {'word': 'reference', 'word_index': [(12, 12)], 'id': 'H01-1058.18'}, 'e2': {'word': 'dynamic combiner', 'word_index': [(7, 7)], 'id': 'H01-1058.16'}}	Actually , the ENTITYUNRELATED acts like a ENTITYOTHER with ENTITYUNRELATED using the ENTITY .
We provide experimental results that clearly show the need for a dynamic language model combination to improve the performance further .	dynamic language model combination	performance	result	{'e1': {'word': 'dynamic language model combination', 'word_index': [(11, 11)], 'id': 'H01-1058.19'}, 'e2': {'word': 'performance', 'word_index': [(15, 15)], 'id': 'H01-1058.20'}}	We provide experimental results that clearly show the need for a ENTITY to improve the ENTITYOTHER further .
The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence .	confidence measures	LMs	model-feature	{'e1': {'word': 'confidence measures', 'word_index': [(7, 7)], 'id': 'H01-1058.25'}, 'e2': {'word': 'LMs', 'word_index': [(5, 5)], 'id': 'H01-1058.24'}}	The method amounts to tagging ENTITYOTHER with ENTITY and picking the best ENTITYUNRELATED corresponding to the ENTITYUNRELATED with the best ENTITYUNRELATED .
The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence .	confidence	LM	model-feature	{'e1': {'word': 'confidence', 'word_index': [(20, 20)], 'id': 'H01-1058.28'}, 'e2': {'word': 'LM', 'word_index': [(16, 16)], 'id': 'H01-1058.27'}}	The method amounts to tagging ENTITYUNRELATED with ENTITYUNRELATED and picking the best ENTITYUNRELATED corresponding to the ENTITYOTHER with the best ENTITY .
 This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai-English language identification 	error-correction rules	Thai key prediction	usage	{'e1': {'word': 'error-correction rules', 'word_index': [(9, 9)], 'id': 'H01-1070.2'}, 'e2': {'word': 'Thai key prediction', 'word_index': [(11, 11)], 'id': 'H01-1070.3'}}	This paper proposes a practical approach employing ENTITYUNRELATED and ENTITY for ENTITYOTHER and ENTITYUNRELATED
The paper also proposes rule-reduction algorithm applying mutual information to reduce the error-correction rules .	mutual information	error-correction rules	usage	{'e1': {'word': 'mutual information', 'word_index': [(6, 6)], 'id': 'H01-1070.6'}, 'e2': {'word': 'error-correction rules', 'word_index': [(10, 10)], 'id': 'H01-1070.7'}}	The paper also proposes ENTITYUNRELATED applying ENTITY to reduce the ENTITYOTHER .
Our algorithm reported more than 99% accuracy in both language identification and key prediction .	language identification	accuracy	result	{'e1': {'word': 'language identification', 'word_index': [(10, 10)], 'id': 'H01-1070.9'}, 'e2': {'word': 'accuracy', 'word_index': [(7, 7)], 'id': 'H01-1070.8'}}	Our algorithm reported more than 99 % ENTITYOTHER in both ENTITY and ENTITYUNRELATED .
First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of possible sentence plans for a given text-plan input .	sentence plans	text-plan input	model-feature	{'e1': {'word': 'sentence plans', 'word_index': [(14, 14)], 'id': 'N01-1003.12'}, 'e2': {'word': 'text-plan input', 'word_index': [(18, 18)], 'id': 'N01-1003.13'}}	First , a very simple , ENTITYUNRELATED generates a potentially large list of possible ENTITY for a given ENTITYOTHER .
The SPR uses ranking rules automatically learned from training data .	training data	ranking rules	usage	{'e1': {'word': 'training data', 'word_index': [(7, 7)], 'id': 'N01-1003.19'}, 'e2': {'word': 'ranking rules', 'word_index': [(3, 3)], 'id': 'N01-1003.18'}}	The ENTITYUNRELATED uses ENTITYOTHER automatically learned from ENTITY .
We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan .	sentence plan	top human-ranked sentence plan	compare	{'e1': {'word': 'sentence plan', 'word_index': [(10, 10)], 'id': 'N01-1003.21'}, 'e2': {'word': 'top human-ranked sentence plan', 'word_index': [(22, 22)], 'id': 'N01-1003.22'}}	We show that the trained ENTITYUNRELATED learns to select a ENTITY whose rating on average is only 5 % worse than the ENTITYOTHER .
 In this paper, we compare the relative effects of segment order , segmentation and segment contiguity on the retrieval performance of a translation memory system 	translation memory system	retrieval performance	result	{'e1': {'word': 'translation memory system', 'word_index': [(20, 20)], 'id': 'P01-1004.5'}, 'e2': {'word': 'retrieval performance', 'word_index': [(17, 17)], 'id': 'P01-1004.4'}}	In this paper , we compare the relative effects of ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED on the ENTITYOTHER of a ENTITY
We take a selection of both bag-of-words and segment order-sensitive string comparison methods , and run each over both character- and word-segmented data , in combination with a range of local segment contiguity models (in the form of N-grams ).	bag-of-words and segment order-sensitive string comparison methods	character- and word-segmented data	usage	{'e1': {'word': 'bag-of-words and segment order-sensitive string comparison methods', 'word_index': [(6, 6)], 'id': 'P01-1004.6'}, 'e2': {'word': 'character- and word-segmented data', 'word_index': [(13, 13)], 'id': 'P01-1004.7'}}	We take a selection of both ENTITY , and run each over both ENTITYOTHER , in combination with a range of ENTITYUNRELATED ( in the form of ENTITYUNRELATED ) .
Over two distinct datasets , we find that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word N-gram models .	character bigrams	indexing	usage	{'e1': {'word': 'character bigrams', 'word_index': [(12, 12)], 'id': 'P01-1004.12'}, 'e2': {'word': 'indexing', 'word_index': [(8, 8)], 'id': 'P01-1004.11'}}	Over two distinct ENTITYUNRELATED , we find that ENTITYOTHER according to simple ENTITY produces a ENTITYUNRELATED superior to any of the tested ENTITYUNRELATED .
Further,in their optimum configuration , bag-of-words methods are shown to be equivalent to segment order-sensitive methods in terms of retrieval accuracy , but much faster.	bag-of-words methods	segment order-sensitive methods	compare	{'e1': {'word': 'bag-of-words methods', 'word_index': [(7, 7)], 'id': 'P01-1004.16'}, 'e2': {'word': 'segment order-sensitive methods', 'word_index': [(14, 14)], 'id': 'P01-1004.17'}}	Further , in their optimum ENTITYUNRELATED , ENTITY are shown to be equivalent to ENTITYOTHER in terms of ENTITYUNRELATED , but much faster .
 The theoretical study of the range concatenation grammar [RCG] formalism has revealed many attractive properties which may be used in NLP 	range concatenation grammar [RCG] formalism	NLP	usage	{'e1': {'word': 'range concatenation grammar [RCG] formalism', 'word_index': [(5, 5)], 'id': 'P01-1007.1'}, 'e2': {'word': 'NLP', 'word_index': [(16, 16)], 'id': 'P01-1007.2'}}	The theoretical study of the ENTITY has revealed many attractive properties which may be used in ENTITYOTHER
In particular, range concatenation languages [RCL] can be parsed in polynomial time and many classical grammatical formalisms can be translated into equivalent RCGs without increasing their worst-case parsing time complexity .	polynomial time	range concatenation languages [RCL]	model-feature	{'e1': {'word': 'polynomial time', 'word_index': [(8, 8)], 'id': 'P01-1007.4'}, 'e2': {'word': 'range concatenation languages [RCL]', 'word_index': [(3, 3)], 'id': 'P01-1007.3'}}	In particular , ENTITYOTHER can be parsed in ENTITY and many classical ENTITYUNRELATED can be translated into equivalent ENTITYUNRELATED without increasing their ENTITYUNRELATED .
For example, after translation into an equivalent RCG , any tree adjoining grammar can be parsed in O(n6) time .	O(n6) time	tree adjoining grammar	model-feature	{'e1': {'word': 'O(n6) time', 'word_index': [(16, 16)], 'id': 'P01-1007.11'}, 'e2': {'word': 'tree adjoining grammar', 'word_index': [(11, 11)], 'id': 'P01-1007.10'}}	For example , after ENTITYUNRELATED into an equivalent ENTITYUNRELATED , any ENTITYOTHER can be parsed in ENTITY .
In this paper, we study a parsing technique whose purpose is to improve the practical efficiency of RCL parsers .	parsing technique	RCL parsers	usage	{'e1': {'word': 'parsing technique', 'word_index': [(7, 7)], 'id': 'P01-1007.12'}, 'e2': {'word': 'RCL parsers', 'word_index': [(17, 17)], 'id': 'P01-1007.13'}}	In this paper , we study a ENTITY whose purpose is to improve the practical efficiency of ENTITYOTHER .
The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L .	shared derivation forest	guide	usage	{'e1': {'word': 'shared derivation forest', 'word_index': [(16, 16)], 'id': 'P01-1007.18'}, 'e2': {'word': 'guide', 'word_index': [(12, 12)], 'id': 'P01-1007.17'}}	The ENTITYUNRELATED of the ENTITYUNRELATED for a ENTITYUNRELATED are directed by a ENTITYOTHER which uses the ENTITY output by a prior ENTITYUNRELATED for a suitable ENTITYUNRELATED .
The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L .	RCL parser	superset of L	usage	{'e1': {'word': 'RCL parser', 'word_index': [(21, 21)], 'id': 'P01-1007.19'}, 'e2': {'word': 'superset of L', 'word_index': [(25, 25)], 'id': 'P01-1007.20'}}	The ENTITYUNRELATED of the ENTITYUNRELATED for a ENTITYUNRELATED are directed by a ENTITYUNRELATED which uses the ENTITYUNRELATED output by a prior ENTITY for a suitable ENTITYOTHER .
 While paraphrasing is critical both for interpretation and generation of natural language , current systems use manual or semi-automatic methods to collect paraphrases 	paraphrasing	interpretation and generation of natural language	usage	{'e1': {'word': 'paraphrasing', 'word_index': [(1, 1)], 'id': 'P01-1008.1'}, 'e2': {'word': 'interpretation and generation of natural language', 'word_index': [(6, 6)], 'id': 'P01-1008.2'}}	While ENTITY is critical both for ENTITYOTHER , current systems use manual or semi-automatic methods to collect ENTITYUNRELATED
We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text .	unsupervised learning algorithm	identification of paraphrases	usage	{'e1': {'word': 'unsupervised learning algorithm', 'word_index': [(3, 3)], 'id': 'P01-1008.4'}, 'e2': {'word': 'identification of paraphrases', 'word_index': [(5, 5)], 'id': 'P01-1008.5'}}	We present an ENTITY for ENTITYOTHER from a ENTITYUNRELATED of the same ENTITYUNRELATED .
 This paper presents a formal analysis for a large class of words called alternative markers , which includes other (than) , such (as) , and besides 	formal analysis	alternative markers	topic	{'e1': {'word': 'formal analysis', 'word_index': [(4, 4)], 'id': 'P01-1009.1'}, 'e2': {'word': 'alternative markers', 'word_index': [(12, 12)], 'id': 'P01-1009.3'}}	This paper presents a ENTITY for a large class of ENTITYUNRELATED called ENTITYOTHER , which includes ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED
I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine &apos;s operational semantics .	formal analysis	performance	result	{'e1': {'word': 'formal analysis', 'word_index': [(18, 18)], 'id': 'P01-1009.14'}, 'e2': {'word': 'performance', 'word_index': [(4, 4)], 'id': 'P01-1009.12'}}	I show that the ENTITYOTHER of a ENTITYUNRELATED can be improved dramatically by incorporating an approximation of the ENTITY that is compatible with the ENTITYUNRELATED &apos ;s ENTITYUNRELATED .
The value of this approach is that as the operational semantics of natural language applications improve, even larger improvements are possible.	operational semantics	natural language applications	part_whole	{'e1': {'word': 'operational semantics', 'word_index': [(9, 9)], 'id': 'P01-1009.17'}, 'e2': {'word': 'natural language applications', 'word_index': [(11, 11)], 'id': 'P01-1009.18'}}	The value of this approach is that as the ENTITY of ENTITYOTHER improve , even larger improvements are possible .
Our logical definition leads to a neat relation to categorial grammar , (yielding a treatment of Montague semantics ), a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data (based on a typing-algorithm and type-unification ).	structured data	learning algorithm	usage	{'e1': {'word': 'structured data', 'word_index': [(28, 28)], 'id': 'P01-1047.11'}, 'e2': {'word': 'learning algorithm', 'word_index': [(26, 26)], 'id': 'P01-1047.10'}}	Our ENTITYUNRELATED leads to a neat relation to ENTITYUNRELATED , ( yielding a treatment of ENTITYUNRELATED ) , a ENTITYUNRELATED in a ENTITYUNRELATED , and a ENTITYOTHER from ENTITY ( based on a ENTITYUNRELATED and ENTITYUNRELATED ) .
 Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches 	quality	utterances	model-feature	{'e1': {'word': 'quality', 'word_index': [(17, 17)], 'id': 'P01-1056.3'}, 'e2': {'word': 'utterances', 'word_index': [(19, 19)], 'id': 'P01-1056.4'}}	ENTITYUNRELATED modules of a ENTITYUNRELATED have recently been proposed , but a fundamental concern is whether the ENTITY of ENTITYOTHER produced with ENTITYUNRELATED can compete with ENTITYUNRELATED
 Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches 	trainable components	hand-crafted template-based or rule-based approaches	compare	{'e1': {'word': 'trainable components', 'word_index': [(22, 22)], 'id': 'P01-1056.5'}, 'e2': {'word': 'hand-crafted template-based or rule-based approaches', 'word_index': [(26, 26)], 'id': 'P01-1056.6'}}	ENTITYUNRELATED modules of a ENTITYUNRELATED have recently been proposed , but a fundamental concern is whether the ENTITYUNRELATED of ENTITYUNRELATED produced with ENTITY can compete with ENTITYOTHER
In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments .	trainable sentence planner	spoken dialogue system	usage	{'e1': {'word': 'trainable sentence planner', 'word_index': [(7, 7)], 'id': 'P01-1056.7'}, 'e2': {'word': 'spoken dialogue system', 'word_index': [(10, 10)], 'id': 'P01-1056.8'}}	In this paper We experimentally evaluate a ENTITY for a ENTITYOTHER by eliciting ENTITYUNRELATED .
We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .	trainable sentence planner	rule-based systems	compare	{'e1': {'word': 'trainable sentence planner', 'word_index': [(4, 4)], 'id': 'P01-1056.13'}, 'e2': {'word': 'rule-based systems', 'word_index': [(9, 9)], 'id': 'P01-1056.14'}}	We show that the ENTITY performs better than the ENTITYOTHER and the ENTITYUNRELATED , and as well as the ENTITYUNRELATED .
 We describe a set of supervised machine learning experiments centering on the construction of statistical models of WH-questions 	statistical models	WH-questions	model-feature	{'e1': {'word': 'statistical models', 'word_index': [(12, 12)], 'id': 'P01-1070.2'}, 'e2': {'word': 'WH-questions', 'word_index': [(14, 14)], 'id': 'P01-1070.3'}}	We describe a set of ENTITYUNRELATED experiments centering on the construction of ENTITY of ENTITYOTHER
These models , which are built from shallow linguistic features of questions , are employed to predict target variables which represent a user&apos;s informational goals .	shallow linguistic features	questions	model-feature	{'e1': {'word': 'shallow linguistic features', 'word_index': [(7, 7)], 'id': 'P01-1070.5'}, 'e2': {'word': 'questions', 'word_index': [(9, 9)], 'id': 'P01-1070.6'}}	These ENTITYUNRELATED , which are built from ENTITY of ENTITYOTHER , are employed to predict target variables which represent a ENTITYUNRELATED .
We report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables.	models	predictive performance	result	{'e1': {'word': 'models', 'word_index': [(10, 10)], 'id': 'P01-1070.9'}, 'e2': {'word': 'predictive performance', 'word_index': [(7, 7)], 'id': 'P01-1070.8'}}	We report on different aspects of the ENTITYOTHER of our ENTITY , including the influence of various ENTITYUNRELATED on ENTITYUNRELATED , and examine the relationships among the target variables .
We report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables.	training and testing factors	predictive performance	result	{'e1': {'word': 'training and testing factors', 'word_index': [(17, 17)], 'id': 'P01-1070.10'}, 'e2': {'word': 'predictive performance', 'word_index': [(19, 19)], 'id': 'P01-1070.11'}}	We report on different aspects of the ENTITYUNRELATED of our ENTITYUNRELATED , including the influence of various ENTITY on ENTITYOTHER , and examine the relationships among the target variables .
 This paper describes a method for utterance classification that does not require manual transcription of training data 	manual transcription	utterance classification	usage	{'e1': {'word': 'manual transcription', 'word_index': [(11, 11)], 'id': 'N03-1001.2'}, 'e2': {'word': 'utterance classification', 'word_index': [(6, 6)], 'id': 'N03-1001.1'}}	This paper describes a method for ENTITYOTHER that does not require ENTITY of ENTITYUNRELATED
The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .	manual transcription	word-trigram recognition	usage	{'e1': {'word': 'manual transcription', 'word_index': [(27, 27)], 'id': 'N03-1001.8'}, 'e2': {'word': 'word-trigram recognition', 'word_index': [(25, 25)], 'id': 'N03-1001.7'}}	The method combines ENTITYUNRELATED with off - the - shelf ENTITYUNRELATED to give ENTITYUNRELATED that is surprisingly close to what can be achieved using conventional ENTITYOTHER requiring ENTITY .
In our method, unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .	unsupervised training	phone n-gram model	usage	{'e1': {'word': 'unsupervised training', 'word_index': [(4, 4)], 'id': 'N03-1001.9'}, 'e2': {'word': 'phone n-gram model', 'word_index': [(11, 11)], 'id': 'N03-1001.10'}}	In our method , ENTITY is first used to train a ENTITYOTHER for a particular ENTITYUNRELATED ; the ENTITYUNRELATED of ENTITYUNRELATED with this ENTITYUNRELATED is then passed to a ENTITYUNRELATED .
In our method, unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .	phone-string classifier	output	usage	{'e1': {'word': 'phone-string classifier', 'word_index': [(29, 29)], 'id': 'N03-1001.15'}, 'e2': {'word': 'output', 'word_index': [(18, 18)], 'id': 'N03-1001.12'}}	In our method , ENTITYUNRELATED is first used to train a ENTITYUNRELATED for a particular ENTITYUNRELATED ; the ENTITYOTHER of ENTITYUNRELATED with this ENTITYUNRELATED is then passed to a ENTITY .
 Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora 	answering agents	multi-strategy and multi-source approach to question answering	usage	{'e1': {'word': 'answering agents', 'word_index': [(27, 27)], 'id': 'N03-1004.5'}, 'e2': {'word': 'multi-strategy and multi-source approach to question answering', 'word_index': [(17, 17)], 'id': 'N03-1004.4'}}	Motivated by the success of ENTITYUNRELATED in ENTITYUNRELATED and other areas of ENTITYUNRELATED , we developed a ENTITYOTHER which is based on combining the results from different ENTITY searching for ENTITYUNRELATED in multiple ENTITYUNRELATED
 Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora 	answers	corpora	part_whole	{'e1': {'word': 'answers', 'word_index': [(30, 30)], 'id': 'N03-1004.6'}, 'e2': {'word': 'corpora', 'word_index': [(33, 33)], 'id': 'N03-1004.7'}}	Motivated by the success of ENTITYUNRELATED in ENTITYUNRELATED and other areas of ENTITYUNRELATED , we developed a ENTITYUNRELATED which is based on combining the results from different ENTITYUNRELATED searching for ENTITY in multiple ENTITYOTHER
The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques .	knowledge-based mechanisms	answering agents	usage	{'e1': {'word': 'knowledge-based mechanisms', 'word_index': [(10, 10)], 'id': 'N03-1004.9'}, 'e2': {'word': 'answering agents', 'word_index': [(1, 1)], 'id': 'N03-1004.8'}}	The ENTITYOTHER adopt fundamentally different strategies , one utilizing primarily ENTITY and the other adopting ENTITYUNRELATED .
Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered , and a 32.8% improvement according to the average precision metric .	answer resolution algorithm	baseline system	compare	{'e1': {'word': 'answer resolution algorithm', 'word_index': [(6, 6)], 'id': 'N03-1004.14'}, 'e2': {'word': 'baseline system', 'word_index': [(15, 15)], 'id': 'N03-1004.15'}}	Experiments evaluating the effectiveness of our ENTITY show a 35.0 % relative improvement over our ENTITYOTHER in the number of ENTITYUNRELATED , and a 32.8 % improvement according to the ENTITYUNRELATED .
We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence .	scoring	speech recognition hypotheses (SRH)	usage	{'e1': {'word': 'scoring', 'word_index': [(8, 8)], 'id': 'N03-1012.4'}, 'e2': {'word': 'speech recognition hypotheses (SRH)', 'word_index': [(10, 10)], 'id': 'N03-1012.5'}}	We apply our system to the task of ENTITY alternative ENTITYOTHER in terms of their ENTITYUNRELATED .
An evaluation of our system against the annotated data shows that, it successfully classifies 73.2% in a German corpus of 2.284 SRHs as either coherent or incoherent (given a baseline of 54.55%).	SRHs	German corpus	part_whole	{'e1': {'word': 'SRHs', 'word_index': [(21, 21)], 'id': 'N03-1012.12'}, 'e2': {'word': 'German corpus', 'word_index': [(18, 18)], 'id': 'N03-1012.11'}}	An evaluation of our system against the ENTITYUNRELATED shows that , it successfully classifies 73.2 % in a ENTITYOTHER of 2.284 ENTITY as either coherent or incoherent ( given a ENTITYUNRELATED of 54.55 % ) .
Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models .	phrase-based models	word-based models	compare	{'e1': {'word': 'phrase-based models', 'word_index': [(18, 18)], 'id': 'N03-1017.4'}, 'e2': {'word': 'word-based models', 'word_index': [(20, 20)], 'id': 'N03-1017.5'}}	Within our framework , we carry out a large number of experiments to understand better and explain why ENTITY outperform ENTITYOTHER .
Our empirical results, which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations .	word-based alignments	heuristic learning	usage	{'e1': {'word': 'word-based alignments', 'word_index': [(30, 30)], 'id': 'N03-1017.9'}, 'e2': {'word': 'heuristic learning', 'word_index': [(26, 26)], 'id': 'N03-1017.7'}}	Our empirical results , which hold for all examined ENTITYUNRELATED , suggest that the highest levels of performance can be obtained through relatively simple means : ENTITYOTHER of ENTITYUNRELATED from ENTITY and ENTITYUNRELATED of ENTITYUNRELATED .
Our empirical results, which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations .	lexical weighting	phrase translations	model-feature	{'e1': {'word': 'lexical weighting', 'word_index': [(32, 32)], 'id': 'N03-1017.10'}, 'e2': {'word': 'phrase translations', 'word_index': [(34, 34)], 'id': 'N03-1017.11'}}	Our empirical results , which hold for all examined ENTITYUNRELATED , suggest that the highest levels of performance can be obtained through relatively simple means : ENTITYUNRELATED of ENTITYUNRELATED from ENTITYUNRELATED and ENTITY of ENTITYOTHER .
Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance.	phrases	high-accuracy word-level alignment models	part_whole	{'e1': {'word': 'phrases', 'word_index': [(10, 10)], 'id': 'N03-1017.14'}, 'e2': {'word': 'high-accuracy word-level alignment models', 'word_index': [(12, 12)], 'id': 'N03-1017.15'}}	Surprisingly , learning ENTITYUNRELATED longer than three ENTITYUNRELATED and learning ENTITY from ENTITYOTHER does not have a strong impact on performance .
The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks .	model	error correction	usage	{'e1': {'word': 'model', 'word_index': [(1, 1)], 'id': 'N03-1018.6'}, 'e2': {'word': 'error correction', 'word_index': [(7, 7)], 'id': 'N03-1018.7'}}	The ENTITY is designed for use in ENTITYOTHER , with a focus on ENTITYUNRELATED the ENTITYUNRELATED of black - box ENTITYUNRELATED in order to make it more useful for ENTITYUNRELATED .
The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks .	post-processing	output	usage	{'e1': {'word': 'post-processing', 'word_index': [(13, 13)], 'id': 'N03-1018.8'}, 'e2': {'word': 'output', 'word_index': [(15, 15)], 'id': 'N03-1018.9'}}	The ENTITYUNRELATED is designed for use in ENTITYUNRELATED , with a focus on ENTITY the ENTITYOTHER of black - box ENTITYUNRELATED in order to make it more useful for ENTITYUNRELATED .
We present an implementation of the model based on finite-state models , demonstrate the model &apos;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .	finite-state models	model	usage	{'e1': {'word': 'finite-state models', 'word_index': [(9, 9)], 'id': 'N03-1018.13'}, 'e2': {'word': 'model', 'word_index': [(6, 6)], 'id': 'N03-1018.12'}}	We present an implementation of the ENTITYOTHER based on ENTITY , demonstrate the ENTITYUNRELATED &apos ;s ability to significantly reduce ENTITYUNRELATED , and provide evaluation results involving ENTITYUNRELATED of ENTITYUNRELATED from ENTITYUNRELATED .
We present an implementation of the model based on finite-state models , demonstrate the model &apos;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .	model	character and word error rate	result	{'e1': {'word': 'model', 'word_index': [(13, 13)], 'id': 'N03-1018.14'}, 'e2': {'word': 'character and word error rate', 'word_index': [(20, 20)], 'id': 'N03-1018.15'}}	We present an implementation of the ENTITYUNRELATED based on ENTITYUNRELATED , demonstrate the ENTITY &apos ;s ability to significantly reduce ENTITYOTHER , and provide evaluation results involving ENTITYUNRELATED of ENTITYUNRELATED from ENTITYUNRELATED .
We present an implementation of the model based on finite-state models , demonstrate the model &apos;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .	automatic extraction	printed text	usage	{'e1': {'word': 'automatic extraction', 'word_index': [(27, 27)], 'id': 'N03-1018.16'}, 'e2': {'word': 'printed text', 'word_index': [(31, 31)], 'id': 'N03-1018.18'}}	We present an implementation of the ENTITYUNRELATED based on ENTITYUNRELATED , demonstrate the ENTITYUNRELATED &apos ;s ability to significantly reduce ENTITYUNRELATED , and provide evaluation results involving ENTITY of ENTITYUNRELATED from ENTITYOTHER .
 We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation 	ambiguity packing and stochastic disambiguation techniques	Lexical-Functional Grammars (LFG)	usage	{'e1': {'word': 'ambiguity packing and stochastic disambiguation techniques', 'word_index': [(5, 5)], 'id': 'N03-1026.1'}, 'e2': {'word': 'Lexical-Functional Grammars (LFG)', 'word_index': [(7, 7)], 'id': 'N03-1026.2'}}	We present an application of ENTITY for ENTITYOTHER to the domain of ENTITYUNRELATED
Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	linguistic parser/generator	LFG	usage	{'e1': {'word': 'linguistic parser/generator', 'word_index': [(4, 4)], 'id': 'N03-1026.4'}, 'e2': {'word': 'LFG', 'word_index': [(6, 6)], 'id': 'N03-1026.5'}}	Our system incorporates a ENTITY for ENTITYOTHER , a ENTITYUNRELATED for ENTITYUNRELATED operating on ENTITYUNRELATED , and a ENTITYUNRELATED for ENTITYUNRELATED .
Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	transfer component	parse reduction	usage	{'e1': {'word': 'transfer component', 'word_index': [(9, 9)], 'id': 'N03-1026.6'}, 'e2': {'word': 'parse reduction', 'word_index': [(11, 11)], 'id': 'N03-1026.7'}}	Our system incorporates a ENTITYUNRELATED for ENTITYUNRELATED , a ENTITY for ENTITYOTHER operating on ENTITYUNRELATED , and a ENTITYUNRELATED for ENTITYUNRELATED .
Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .	stochastic output selection	maximum-entropy model	usage	{'e1': {'word': 'stochastic output selection', 'word_index': [(20, 20)], 'id': 'N03-1026.10'}, 'e2': {'word': 'maximum-entropy model', 'word_index': [(18, 18)], 'id': 'N03-1026.9'}}	Our system incorporates a ENTITYUNRELATED for ENTITYUNRELATED , a ENTITYUNRELATED for ENTITYUNRELATED operating on ENTITYUNRELATED , and a ENTITYOTHER for ENTITY .
An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings .	experimental evaluation	summarization	topic	{'e1': {'word': 'experimental evaluation', 'word_index': [(1, 1)], 'id': 'N03-1026.14'}, 'e2': {'word': 'summarization', 'word_index': [(3, 3)], 'id': 'N03-1026.15'}}	An ENTITY of ENTITYOTHER quality shows a close correlation between the ENTITYUNRELATED and a ENTITYUNRELATED of generated ENTITYUNRELATED .
Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator .	grammaticality	system output	model-feature	{'e1': {'word': 'grammaticality', 'word_index': [(18, 18)], 'id': 'N03-1026.20'}, 'e2': {'word': 'system output', 'word_index': [(21, 21)], 'id': 'N03-1026.21'}}	Overall ENTITYUNRELATED quality of the proposed system is state - of - the - art , with guaranteed ENTITY of the ENTITYOTHER due to the use of a ENTITYUNRELATED .
 We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation , (ii) broad use of lexical features , including jointly conditioning on multiple consecutive words , (iii) effective use of priors in conditional loglinear models , and (iv) fine-grained modeling of unknown word features 	priors	conditional loglinear models	usage	{'e1': {'word': 'priors', 'word_index': [(43, 43)], 'id': 'N03-1033.6'}, 'e2': {'word': 'conditional loglinear models', 'word_index': [(45, 45)], 'id': 'N03-1033.7'}}	We present a new ENTITYUNRELATED that demonstrates the following ideas : ( i ) explicit use of both preceding and following ENTITYUNRELATED via a ENTITYUNRELATED , ( ii ) broad use of ENTITYUNRELATED , including ENTITYUNRELATED , ( iii ) effective use of ENTITY in ENTITYOTHER , and ( iv ) fine - grained modeling of ENTITYUNRELATED
Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ , an error reduction of 4.4% on the best previous single automatically learned tagging result.	tagger	accuracy	result	{'e1': {'word': 'tagger', 'word_index': [(7, 7)], 'id': 'N03-1033.9'}, 'e2': {'word': 'accuracy', 'word_index': [(12, 12)], 'id': 'N03-1033.10'}}	Using these ideas together , the resulting ENTITY gives a 97.24 % ENTITYOTHER on the ENTITYUNRELATED , an ENTITYUNRELATED of 4.4 % on the best previous single automatically learned ENTITYUNRELATED result .
 Sources of training data suitable for language modeling of conversational speech are limited	training data	language modeling	usage	{'e1': {'word': 'training data', 'word_index': [(2, 2)], 'id': 'N03-2003.1'}, 'e2': {'word': 'language modeling', 'word_index': [(5, 5)], 'id': 'N03-2003.2'}}	Sources of ENTITY suitable for ENTITYOTHER of ENTITYUNRELATED are limited
In this paper, we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams .	text	web	part_whole	{'e1': {'word': 'text', 'word_index': [(12, 12)], 'id': 'N03-2003.5'}, 'e2': {'word': 'web', 'word_index': [(15, 15)], 'id': 'N03-2003.6'}}	In this paper , we show how ENTITYUNRELATED can be supplemented with ENTITY from the ENTITYOTHER filtered to match the ENTITYUNRELATED and / or ENTITYUNRELATED of the target ENTITYUNRELATED , but also that it is possible to get bigger performance gains from the ENTITYUNRELATED by using ENTITYUNRELATED of ENTITYUNRELATED .
 In order to boost the translation quality of EBMT based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and, in addition, the language model of an in-domain monolingual corpus 	EBMT	translation quality	result	{'e1': {'word': 'EBMT', 'word_index': [(7, 7)], 'id': 'N03-2006.2'}, 'e2': {'word': 'translation quality', 'word_index': [(5, 5)], 'id': 'N03-2006.1'}}	In order to boost the ENTITYOTHER of ENTITY based on a small - sized ENTITYUNRELATED , we use an out - of - domain ENTITYUNRELATED and , in addition , the ENTITYUNRELATED of an in - domain ENTITYUNRELATED
 We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton 	hubs	automaton	part_whole	{'e1': {'word': 'hubs', 'word_index': [(10, 10)], 'id': 'N03-2015.3'}, 'e2': {'word': 'automaton', 'word_index': [(13, 13)], 'id': 'N03-2015.4'}}	We describe a simple ENTITYUNRELATED for learning ENTITYUNRELATED by identifying ENTITY in an ENTITYOTHER
 We present a syntax-based constraint for word alignment , known as the cohesion constraint 	syntax-based constraint	word alignment	usage	{'e1': {'word': 'syntax-based constraint', 'word_index': [(3, 3)], 'id': 'N03-2017.1'}, 'e2': {'word': 'word alignment', 'word_index': [(5, 5)], 'id': 'N03-2017.2'}}	We present a ENTITY for ENTITYOTHER , known as the ENTITYUNRELATED
 We present a syntax-based constraint for word alignment , known as the cohesion constraint 	cohesion constraint	syntax-based constraint	usage	{'e1': {'word': 'cohesion constraint', 'word_index': [(10, 10)], 'id': 'N03-2017.3'}, 'e2': {'word': 'syntax-based constraint', 'word_index': [(3, 3)], 'id': 'N03-2017.1'}}	We present a ENTITYOTHER for ENTITYUNRELATED , known as the ENTITY
 A novel bootstrapping approach to Named Entity (NE) tagging using concept-based seeds and successive learners is presented	bootstrapping approach	Named Entity (NE) tagging	usage	{'e1': {'word': 'bootstrapping approach', 'word_index': [(2, 2)], 'id': 'N03-2025.1'}, 'e2': {'word': 'Named Entity (NE) tagging', 'word_index': [(4, 4)], 'id': 'N03-2025.2'}}	A novel ENTITY to ENTITYOTHER using ENTITYUNRELATED and ENTITYUNRELATED is presented
This approach only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE , e.g. he/she/man/woman for PERSON NE .	seeds	concept	model-feature	{'e1': {'word': 'seeds', 'word_index': [(9, 9)], 'id': 'N03-2025.7'}, 'e2': {'word': 'concept', 'word_index': [(14, 14)], 'id': 'N03-2025.8'}}	This approach only requires a few ENTITYUNRELATED or ENTITYUNRELATED ENTITY that correspond to the ENTITYOTHER for the targeted ENTITYUNRELATED , e.g. he / she / man / woman for ENTITYUNRELATED .
First, decision list is used to learn the parsing-based NE rules .	parsing-based NE rules	decision list	usage	{'e1': {'word': 'parsing-based NE rules', 'word_index': [(8, 8)], 'id': 'N03-2025.14'}, 'e2': {'word': 'decision list', 'word_index': [(2, 2)], 'id': 'N03-2025.13'}}	First , ENTITYOTHER is used to learn the ENTITY .
Then, a Hidden Markov Model is trained on a corpus automatically tagged by the first learner .	Hidden Markov Model	corpus	usage	{'e1': {'word': 'Hidden Markov Model', 'word_index': [(3, 3)], 'id': 'N03-2025.15'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'N03-2025.16'}}	Then , a ENTITY is trained on a ENTITYOTHER automatically tagged by the first ENTITYUNRELATED .
 In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models 	phrase-based unigram model	statistical machine translation	usage	{'e1': {'word': 'phrase-based unigram model', 'word_index': [(7, 7)], 'id': 'N03-2036.1'}, 'e2': {'word': 'statistical machine translation', 'word_index': [(9, 9)], 'id': 'N03-2036.2'}}	In this paper , we describe a ENTITY for ENTITYOTHER that uses a much simpler set of ENTITYUNRELATED than similar ENTITYUNRELATED
During training , the blocks are learned from source interval projections using an underlying word alignment .	blocks	source interval projections	part_whole	{'e1': {'word': 'blocks', 'word_index': [(4, 4)], 'id': 'N03-2036.12'}, 'e2': {'word': 'source interval projections', 'word_index': [(8, 8)], 'id': 'N03-2036.13'}}	During ENTITYUNRELATED , the ENTITY are learned from ENTITYOTHER using an underlying ENTITYUNRELATED .
We show experimental results on block selection criteria based on unigram counts and phrase length.	unigram	block selection criteria	usage	{'e1': {'word': 'unigram', 'word_index': [(8, 8)], 'id': 'N03-2036.16'}, 'e2': {'word': 'block selection criteria', 'word_index': [(5, 5)], 'id': 'N03-2036.15'}}	We show experimental results on ENTITYOTHER based on ENTITY counts and ENTITYUNRELATED length .
 In this paper, we propose a novel Cooperative Model for natural language understanding in a dialogue system 	Cooperative Model	natural language understanding	usage	{'e1': {'word': 'Cooperative Model', 'word_index': [(8, 8)], 'id': 'N03-3010.1'}, 'e2': {'word': 'natural language understanding', 'word_index': [(10, 10)], 'id': 'N03-3010.2'}}	In this paper , we propose a novel ENTITY for ENTITYOTHER in a ENTITYUNRELATED
 The JAVELIN system integrates a flexible, planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text 	planning-based architecture	JAVELIN system	part_whole	{'e1': {'word': 'planning-based architecture', 'word_index': [(6, 6)], 'id': 'N03-4010.2'}, 'e2': {'word': 'JAVELIN system', 'word_index': [(1, 1)], 'id': 'N03-4010.1'}}	The ENTITYOTHER integrates a flexible , ENTITY with a variety of ENTITYUNRELATED to provide an ENTITYUNRELATED on ENTITYUNRELATED
The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus .	JAVELIN	questions	usage	{'e1': {'word': 'JAVELIN', 'word_index': [(6, 6)], 'id': 'N03-4010.6'}, 'e2': {'word': 'questions', 'word_index': [(8, 8)], 'id': 'N03-4010.7'}}	The demonstration will focus on how ENTITY processes ENTITYOTHER and retrieves the most likely ENTITYUNRELATED from the given ENTITYUNRELATED .
The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus .	answer candidates	text corpus	part_whole	{'e1': {'word': 'answer candidates', 'word_index': [(14, 14)], 'id': 'N03-4010.8'}, 'e2': {'word': 'text corpus', 'word_index': [(18, 18)], 'id': 'N03-4010.9'}}	The demonstration will focus on how ENTITYUNRELATED processes ENTITYUNRELATED and retrieves the most likely ENTITY from the given ENTITYOTHER .
The operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session .	data objects	repository	part_whole	{'e1': {'word': 'data objects', 'word_index': [(15, 15)], 'id': 'N03-4010.11'}, 'e2': {'word': 'repository', 'word_index': [(13, 13)], 'id': 'N03-4010.10'}}	The operation of the system will be explained in depth through browsing the ENTITYOTHER of ENTITY created by the system during each ENTITYUNRELATED .
 In this paper we present a novel, customizable : IE paradigm that takes advantage of predicate-argument structures 	predicate-argument structures	IE paradigm	usage	{'e1': {'word': 'predicate-argument structures', 'word_index': [(15, 15)], 'id': 'P03-1002.2'}, 'e2': {'word': 'IE paradigm', 'word_index': [(10, 10)], 'id': 'P03-1002.1'}}	In this paper we present a novel , customizable : ENTITYOTHER that takes advantage of ENTITY
 This paper proposes the Hierarchical Directed Acyclic Graph (HDAG) Kernel for structured natural language data 	Hierarchical Directed Acyclic Graph (HDAG) Kernel	structured natural language data	usage	{'e1': {'word': 'Hierarchical Directed Acyclic Graph (HDAG) Kernel', 'word_index': [(4, 4)], 'id': 'P03-1005.1'}, 'e2': {'word': 'structured natural language data', 'word_index': [(6, 6)], 'id': 'P03-1005.2'}}	This paper proposes the ENTITY for ENTITYOTHER
The HDAG Kernel directly accepts several levels of both chunks and their relations , and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs .	attribute sequences	HDAGs	model-feature	{'e1': {'word': 'attribute sequences', 'word_index': [(24, 24)], 'id': 'P03-1005.7'}, 'e2': {'word': 'HDAGs', 'word_index': [(27, 27)], 'id': 'P03-1005.8'}}	The ENTITYUNRELATED directly accepts several levels of both ENTITYUNRELATED and their ENTITYUNRELATED , and then efficiently computes the ENTITYUNRELATED of the number of common ENTITY of the ENTITYOTHER .
The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods .	HDAG Kernel	kernel functions	compare	{'e1': {'word': 'HDAG Kernel', 'word_index': [(8, 8)], 'id': 'P03-1005.13'}, 'e2': {'word': 'kernel functions', 'word_index': [(13, 13)], 'id': 'P03-1005.14'}}	The results of the experiments demonstrate that the ENTITY is superior to other ENTITYOTHER and ENTITYUNRELATED .
 Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data 	semantic verb classes	corpus data	model-feature	{'e1': {'word': 'semantic verb classes', 'word_index': [(10, 10)], 'id': 'P03-1009.2'}, 'e2': {'word': 'corpus data', 'word_index': [(13, 13)], 'id': 'P03-1009.3'}}	Previous research has demonstrated the utility of ENTITYUNRELATED in inducing ENTITY from undisambiguated ENTITYOTHER
We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods.	Information Bottleneck	subcategorization frame (SCF)	usage	{'e1': {'word': 'Information Bottleneck', 'word_index': [(12, 12)], 'id': 'P03-1009.5'}, 'e2': {'word': 'subcategorization frame (SCF)', 'word_index': [(8, 8)], 'id': 'P03-1009.4'}}	We describe a new approach which involves clustering ENTITYOTHER distributions using the ENTITY and ENTITYUNRELATED methods .
A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters , offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data .	polysemy	clusters	model-feature	{'e1': {'word': 'polysemy', 'word_index': [(11, 11)], 'id': 'P03-1009.9'}, 'e2': {'word': 'clusters', 'word_index': [(14, 14)], 'id': 'P03-1009.10'}}	A novel ENTITYUNRELATED is proposed which accounts for the effect of ENTITY on the ENTITYOTHER , offering us a good insight into the potential and limitations of ENTITYUNRELATED ENTITYUNRELATED .
 We apply a decision tree based approach to pronoun resolution in spoken dialogue 	decision tree based approach	pronoun resolution	usage	{'e1': {'word': 'decision tree based approach', 'word_index': [(3, 3)], 'id': 'P03-1022.1'}, 'e2': {'word': 'pronoun resolution', 'word_index': [(5, 5)], 'id': 'P03-1022.2'}}	We apply a ENTITY to ENTITYOTHER in ENTITYUNRELATED
Our system deals with pronouns with NP- and non-NP-antecedents .	NP- and non-NP-antecedents	pronouns	model-feature	{'e1': {'word': 'NP- and non-NP-antecedents', 'word_index': [(6, 6)], 'id': 'P03-1022.5'}, 'e2': {'word': 'pronouns', 'word_index': [(4, 4)], 'id': 'P03-1022.4'}}	Our system deals with ENTITYOTHER with ENTITY .
We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features .	pronoun resolution	spoken dialogue	usage	{'e1': {'word': 'pronoun resolution', 'word_index': [(8, 8)], 'id': 'P03-1022.7'}, 'e2': {'word': 'spoken dialogue', 'word_index': [(10, 10)], 'id': 'P03-1022.8'}}	We present a set of ENTITYUNRELATED designed for ENTITY in ENTITYOTHER and determine the most promising ENTITYUNRELATED .
 Link detection has been regarded as a core technology for the Topic Detection and Tracking tasks of new event detection 	Topic Detection and Tracking tasks	new event detection	part_whole	{'e1': {'word': 'Topic Detection and Tracking tasks', 'word_index': [(10, 10)], 'id': 'P03-1030.2'}, 'e2': {'word': 'new event detection', 'word_index': [(12, 12)], 'id': 'P03-1030.3'}}	ENTITYUNRELATED has been regarded as a core technology for the ENTITY of ENTITYOTHER
 This paper concerns the discourse understanding process in spoken dialogue systems 	discourse understanding process	spoken dialogue systems	usage	{'e1': {'word': 'discourse understanding process', 'word_index': [(4, 4)], 'id': 'P03-1031.1'}, 'e2': {'word': 'spoken dialogue systems', 'word_index': [(6, 6)], 'id': 'P03-1031.2'}}	This paper concerns the ENTITY in ENTITYOTHER
This process enables the system to understand user utterances based on the context of a dialogue .	context	dialogue	model-feature	{'e1': {'word': 'context', 'word_index': [(11, 11)], 'id': 'P03-1031.4'}, 'e2': {'word': 'dialogue', 'word_index': [(14, 14)], 'id': 'P03-1031.5'}}	This process enables the system to understand ENTITYUNRELATED based on the ENTITY of a ENTITYOTHER .
Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance .	candidates	understanding	usage	{'e1': {'word': 'candidates', 'word_index': [(2, 2)], 'id': 'P03-1031.6'}, 'e2': {'word': 'understanding', 'word_index': [(5, 5)], 'id': 'P03-1031.7'}}	Since multiple ENTITY for the ENTITYOTHER result can be obtained for a ENTITYUNRELATED due to the ENTITYUNRELATED of ENTITYUNRELATED , it is not appropriate to decide on a single ENTITYUNRELATED result after each ENTITYUNRELATED .
Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance .	ambiguity	speech understanding	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(16, 16)], 'id': 'P03-1031.9'}, 'e2': {'word': 'speech understanding', 'word_index': [(18, 18)], 'id': 'P03-1031.10'}}	Since multiple ENTITYUNRELATED for the ENTITYUNRELATED result can be obtained for a ENTITYUNRELATED due to the ENTITY of ENTITYOTHER , it is not appropriate to decide on a single ENTITYUNRELATED result after each ENTITYUNRELATED .
By holding multiple candidates for understanding results and resolving the ambiguity as the dialogue progresses, the discourse understanding accuracy can be improved.	candidates	understanding	usage	{'e1': {'word': 'candidates', 'word_index': [(3, 3)], 'id': 'P03-1031.13'}, 'e2': {'word': 'understanding', 'word_index': [(5, 5)], 'id': 'P03-1031.14'}}	By holding multiple ENTITY for ENTITYOTHER results and resolving the ENTITYUNRELATED as the ENTITYUNRELATED progresses , the ENTITYUNRELATED can be improved .
This paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpora .	statistical information	dialogue corpora	model-feature	{'e1': {'word': 'statistical information', 'word_index': [(11, 11)], 'id': 'P03-1031.19'}, 'e2': {'word': 'dialogue corpora', 'word_index': [(14, 14)], 'id': 'P03-1031.20'}}	This paper proposes a method for resolving this ENTITYUNRELATED based on ENTITY obtained from ENTITYOTHER .
Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple candidates for understanding results is effective.	candidates	understanding	usage	{'e1': {'word': 'candidates', 'word_index': [(18, 18)], 'id': 'P03-1031.23'}, 'e2': {'word': 'understanding', 'word_index': [(20, 20)], 'id': 'P03-1031.24'}}	Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple ENTITY for ENTITYOTHER results is effective .
 We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems 	user modeling	cooperative responses	usage	{'e1': {'word': 'user modeling', 'word_index': [(3, 3)], 'id': 'P03-1033.1'}, 'e2': {'word': 'cooperative responses', 'word_index': [(8, 8)], 'id': 'P03-1033.2'}}	We address appropriate ENTITY in order to generate ENTITYOTHER to each ENTITYUNRELATED in ENTITYUNRELATED
Moreover, the models are automatically derived by decision tree learning using real dialogue data collected by the system.	decision tree learning	models	model-feature	{'e1': {'word': 'decision tree learning', 'word_index': [(8, 8)], 'id': 'P03-1033.15'}, 'e2': {'word': 'models', 'word_index': [(3, 3)], 'id': 'P03-1033.14'}}	Moreover , the ENTITYOTHER are automatically derived by ENTITY using real ENTITYUNRELATED collected by the system .
Dialogue strategies based on the user modeling are implemented in Kyoto city bus information system that has been developed at our laboratory.	user modeling	Dialogue strategies	usage	{'e1': {'word': 'user modeling', 'word_index': [(4, 4)], 'id': 'P03-1033.19'}, 'e2': {'word': 'Dialogue strategies', 'word_index': [(0, 0)], 'id': 'P03-1033.18'}}	ENTITYOTHER based on the ENTITY are implemented in ENTITYUNRELATED that has been developed at our laboratory .
 This paper presents an unsupervised learning approach to building a non-English (Arabic) stemmer 	unsupervised learning approach	non-English (Arabic) stemmer	usage	{'e1': {'word': 'unsupervised learning approach', 'word_index': [(4, 4)], 'id': 'P03-1050.1'}, 'e2': {'word': 'non-English (Arabic) stemmer', 'word_index': [(8, 8)], 'id': 'P03-1050.2'}}	This paper presents an ENTITY to building a ENTITYOTHER
The stemming model is based on statistical machine translation and it uses an English stemmer and a small (10K sentences) parallel corpus as its sole training resources .	statistical machine translation	stemming model	usage	{'e1': {'word': 'statistical machine translation', 'word_index': [(5, 5)], 'id': 'P03-1050.4'}, 'e2': {'word': 'stemming model', 'word_index': [(1, 1)], 'id': 'P03-1050.3'}}	The ENTITYOTHER is based on ENTITY and it uses an ENTITYUNRELATED and a small ( 10 K sentences ) ENTITYUNRELATED as its sole ENTITYUNRELATED .
Examples and results will be given for Arabic , but the approach is applicable to any language that needs affix removal .	affix removal	language	model-feature	{'e1': {'word': 'affix removal', 'word_index': [(19, 19)], 'id': 'P03-1050.16'}, 'e2': {'word': 'language', 'word_index': [(16, 16)], 'id': 'P03-1050.15'}}	Examples and results will be given for ENTITYUNRELATED , but the approach is applicable to any ENTITYOTHER that needs ENTITY .
Our resource-frugal approach results in 87.5% agreement with a state of the art, proprietary Arabic stemmer built using rules , affix lists , and human annotated text , in addition to an unsupervised component .	resource-frugal approach	agreement	result	{'e1': {'word': 'resource-frugal approach', 'word_index': [(1, 1)], 'id': 'P03-1050.17'}, 'e2': {'word': 'agreement', 'word_index': [(6, 6)], 'id': 'P03-1050.18'}}	Our ENTITY results in 87.5 % ENTITYOTHER with a state of the art , proprietary ENTITYUNRELATED built using ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED , in addition to an ENTITYUNRELATED .
Task-based evaluation using Arabic information retrieval indicates an improvement of 22-38% in average precision over unstemmed text , and 96% of the performance of the proprietary stemmer above.	Arabic information retrieval	Task-based evaluation	usage	{'e1': {'word': 'Arabic information retrieval', 'word_index': [(2, 2)], 'id': 'P03-1050.25'}, 'e2': {'word': 'Task-based evaluation', 'word_index': [(0, 0)], 'id': 'P03-1050.24'}}	ENTITYOTHER using ENTITY indicates an improvement of 22- 38 % in ENTITYUNRELATED over ENTITYUNRELATED , and 96 % of the performance of the proprietary ENTITYUNRELATED above .
Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus .	manually segmented Arabic corpus	unsupervised algorithm	usage	{'e1': {'word': 'manually segmented Arabic corpus', 'word_index': [(7, 7)], 'id': 'P03-1051.8'}, 'e2': {'word': 'unsupervised algorithm', 'word_index': [(14, 14)], 'id': 'P03-1051.9'}}	Our method is seeded by a small ENTITY and uses it to bootstrap an ENTITYOTHER to build the ENTITYUNRELATED from a large ENTITYUNRELATED .
Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus .	unsegmented Arabic corpus	Arabic word segmenter	usage	{'e1': {'word': 'unsegmented Arabic corpus', 'word_index': [(22, 22)], 'id': 'P03-1051.11'}, 'e2': {'word': 'Arabic word segmenter', 'word_index': [(18, 18)], 'id': 'P03-1051.10'}}	Our method is seeded by a small ENTITYUNRELATED and uses it to bootstrap an ENTITYUNRELATED to build the ENTITYOTHER from a large ENTITY .
The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input .	morpheme sequence	input	model-feature	{'e1': {'word': 'morpheme sequence', 'word_index': [(10, 10)], 'id': 'P03-1051.13'}, 'e2': {'word': 'input', 'word_index': [(14, 14)], 'id': 'P03-1051.14'}}	The algorithm uses a ENTITYUNRELATED to determine the most probable ENTITY for a given ENTITYOTHER .
The language model is initially estimated from a small manually segmented corpus of about 110,000 words .	language model	manually segmented corpus	model-feature	{'e1': {'word': 'language model', 'word_index': [(1, 1)], 'id': 'P03-1051.15'}, 'e2': {'word': 'manually segmented corpus', 'word_index': [(8, 8)], 'id': 'P03-1051.16'}}	The ENTITY is initially estimated from a small ENTITYOTHER of about 110,000 ENTITYUNRELATED .
To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus , and re-estimate the model parameters with the expanded vocabulary and training corpus .	stems	unsegmented corpus	part_whole	{'e1': {'word': 'stems', 'word_index': [(14, 14)], 'id': 'P03-1051.21'}, 'e2': {'word': 'unsegmented corpus', 'word_index': [(20, 20)], 'id': 'P03-1051.23'}}	To improve the ENTITYUNRELATED ENTITYUNRELATED , we use an ENTITYUNRELATED for automatically acquiring new ENTITY from a 155 million ENTITYUNRELATED ENTITYOTHER , and re-estimate the ENTITYUNRELATED with the expanded ENTITYUNRELATED and ENTITYUNRELATED .
To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus , and re-estimate the model parameters with the expanded vocabulary and training corpus .	model parameters	vocabulary	usage	{'e1': {'word': 'model parameters', 'word_index': [(25, 25)], 'id': 'P03-1051.24'}, 'e2': {'word': 'vocabulary', 'word_index': [(29, 29)], 'id': 'P03-1051.25'}}	To improve the ENTITYUNRELATED ENTITYUNRELATED , we use an ENTITYUNRELATED for automatically acquiring new ENTITYUNRELATED from a 155 million ENTITYUNRELATED ENTITYUNRELATED , and re-estimate the ENTITY with the expanded ENTITYOTHER and ENTITYUNRELATED .
The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens .	Arabic word segmentation system	exact match accuracy	result	{'e1': {'word': 'Arabic word segmentation system', 'word_index': [(2, 2)], 'id': 'P03-1051.27'}, 'e2': {'word': 'exact match accuracy', 'word_index': [(7, 7)], 'id': 'P03-1051.28'}}	The resulting ENTITY achieves around 97 % ENTITYOTHER on a ENTITYUNRELATED containing 28,449 ENTITYUNRELATED .
The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens .	word tokens	test corpus	part_whole	{'e1': {'word': 'word tokens', 'word_index': [(13, 13)], 'id': 'P03-1051.30'}, 'e2': {'word': 'test corpus', 'word_index': [(10, 10)], 'id': 'P03-1051.29'}}	The resulting ENTITYUNRELATED achieves around 97 % ENTITYUNRELATED on a ENTITYOTHER containing 28,449 ENTITY .
We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest.	manually segmented corpus	language	model-feature	{'e1': {'word': 'manually segmented corpus', 'word_index': [(29, 29)], 'id': 'P03-1051.32'}, 'e2': {'word': 'language', 'word_index': [(32, 32)], 'id': 'P03-1051.33'}}	We believe this is a state - of - the - art performance and the algorithm can be used for many ENTITYUNRELATED provided that one can create a small ENTITY of the ENTITYOTHER of interest .
A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning .	manually sense-tagged data	word sense disambiguation (WSD)	usage	{'e1': {'word': 'manually sense-tagged data', 'word_index': [(9, 9)], 'id': 'P03-1058.2'}, 'e2': {'word': 'word sense disambiguation (WSD)', 'word_index': [(4, 4)], 'id': 'P03-1058.1'}}	A central problem of ENTITYOTHER is the lack of ENTITY required for ENTITYUNRELATED .
In this paper, we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora , which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task .	sense-tagged training data	English-Chinese parallel corpora	model-feature	{'e1': {'word': 'sense-tagged training data', 'word_index': [(11, 11)], 'id': 'P03-1058.4'}, 'e2': {'word': 'English-Chinese parallel corpora', 'word_index': [(13, 13)], 'id': 'P03-1058.5'}}	In this paper , we evaluate an approach to automatically acquire ENTITY from ENTITYOTHER , which are then used for disambiguating the ENTITYUNRELATED in the ENTITYUNRELATED .
Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs .	domain dependence	WSD programs	model-feature	{'e1': {'word': 'domain dependence', 'word_index': [(10, 10)], 'id': 'P03-1058.13'}, 'e2': {'word': 'WSD programs', 'word_index': [(13, 13)], 'id': 'P03-1058.14'}}	Our analysis also highlights the importance of the issue of ENTITY in evaluating ENTITYOTHER .
 We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic information , e.g. the construction of domain-independent lexica 	semantically annotated corpus	acquisition of word-semantic information	usage	{'e1': {'word': 'semantically annotated corpus', 'word_index': [(9, 9)], 'id': 'P03-1068.1'}, 'e2': {'word': 'acquisition of word-semantic information', 'word_index': [(19, 19)], 'id': 'P03-1068.2'}}	We describe the ongoing construction of a large , ENTITY resource as reliable basis for the large - scale ENTITYOTHER , e.g. the construction of ENTITYUNRELATED
On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation .	ambiguity	semantic annotation	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(11, 11)], 'id': 'P03-1068.9'}, 'e2': {'word': 'semantic annotation', 'word_index': [(13, 13)], 'id': 'P03-1068.10'}}	On this basis , we discuss the problems of ENTITYUNRELATED and ENTITY in ENTITYOTHER .
We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task .	attentional focus	direction-giving task	model-feature	{'e1': {'word': 'attentional focus', 'word_index': [(6, 6)], 'id': 'P03-1070.9'}, 'e2': {'word': 'direction-giving task', 'word_index': [(12, 12)], 'id': 'P03-1070.10'}}	We analyzed ENTITYUNRELATED , ENTITYUNRELATED and ENTITY in the context of a ENTITYOTHER .
Based on these results, we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state .	verbal and nonverbal grounding acts	ECA	usage	{'e1': {'word': 'verbal and nonverbal grounding acts', 'word_index': [(11, 11)], 'id': 'P03-1070.15'}, 'e2': {'word': 'ECA', 'word_index': [(8, 8)], 'id': 'P03-1070.14'}}	Based on these results , we present an ENTITYOTHER that uses ENTITY to update ENTITYUNRELATED .
We demonstrate that an approximation of HPSG produces a more effective CFG filter than that of LTAG .	HPSG	LTAG	compare	{'e1': {'word': 'HPSG', 'word_index': [(6, 6)], 'id': 'P03-2036.4'}, 'e2': {'word': 'LTAG', 'word_index': [(15, 15)], 'id': 'P03-2036.6'}}	We demonstrate that an approximation of ENTITY produces a more effective ENTITYUNRELATED than that of ENTITYOTHER .
We report experiments conducted on a multilingual corpus to estimate the number of analogies among the sentences that it contains.	analogies	sentences	model-feature	{'e1': {'word': 'analogies', 'word_index': [(12, 12)], 'id': 'C04-1106.5'}, 'e2': {'word': 'sentences', 'word_index': [(15, 15)], 'id': 'C04-1106.6'}}	We report experiments conducted on a ENTITYUNRELATED to estimate the number of ENTITY among the ENTITYOTHER that it contains .
 CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e.g., thesis statements )	CriterionSM Online Essay Evaluation Service	writing	usage	{'e1': {'word': 'CriterionSM Online Essay Evaluation Service', 'word_index': [(0, 0)], 'id': 'N04-1024.1'}, 'e2': {'word': 'writing', 'word_index': [(9, 9)], 'id': 'N04-1024.3'}}	ENTITY includes a capability that labels ENTITYUNRELATED in student ENTITYOTHER with ENTITYUNRELATED ( e.g. , ENTITYUNRELATED )
We describe a new system that enhances Criterion &apos;s capability, by evaluating multiple aspects of coherence in essays .	coherence	essays	model-feature	{'e1': {'word': 'coherence', 'word_index': [(17, 17)], 'id': 'N04-1024.7'}, 'e2': {'word': 'essays', 'word_index': [(19, 19)], 'id': 'N04-1024.8'}}	We describe a new system that enhances ENTITYUNRELATED &apos ;s capability , by evaluating multiple aspects of ENTITY in ENTITYOTHER .
This system identifies features of sentences based on semantic similarity measures and discourse structure .	features	sentences	model-feature	{'e1': {'word': 'features', 'word_index': [(3, 3)], 'id': 'N04-1024.9'}, 'e2': {'word': 'sentences', 'word_index': [(5, 5)], 'id': 'N04-1024.10'}}	This system identifies ENTITY of ENTITYOTHER based on ENTITYUNRELATED and ENTITYUNRELATED .
A support vector machine uses these features to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements .	features	support vector machine	usage	{'e1': {'word': 'features', 'word_index': [(4, 4)], 'id': 'N04-1024.14'}, 'e2': {'word': 'support vector machine', 'word_index': [(1, 1)], 'id': 'N04-1024.13'}}	A ENTITYOTHER uses these ENTITY to capture ENTITYUNRELATED due to relatedness to the ENTITYUNRELATED and relatedness between ENTITYUNRELATED .
Intra-sentential quality is evaluated with rule-based heuristics .	rule-based heuristics	Intra-sentential quality	topic	{'e1': {'word': 'rule-based heuristics', 'word_index': [(4, 4)], 'id': 'N04-1024.19'}, 'e2': {'word': 'Intra-sentential quality', 'word_index': [(0, 0)], 'id': 'N04-1024.18'}}	ENTITYOTHER is evaluated with ENTITY .
 In this paper, we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries 	information redundancy	multilingual input	model-feature	{'e1': {'word': 'information redundancy', 'word_index': [(7, 7)], 'id': 'H05-1005.1'}, 'e2': {'word': 'multilingual input', 'word_index': [(9, 9)], 'id': 'H05-1005.2'}}	In this paper , we use the ENTITY in ENTITYOTHER to correct errors in ENTITYUNRELATED and thus improve the quality of ENTITYUNRELATED
We consider the case of multi-document summarization , where the input documents are in Arabic , and the output summary is in English .	Arabic	documents	model-feature	{'e1': {'word': 'Arabic', 'word_index': [(13, 13)], 'id': 'H05-1005.7'}, 'e2': {'word': 'documents', 'word_index': [(10, 10)], 'id': 'H05-1005.6'}}	We consider the case of ENTITYUNRELATED , where the input ENTITYOTHER are in ENTITY , and the output ENTITYUNRELATED is in ENTITYUNRELATED .
We consider the case of multi-document summarization , where the input documents are in Arabic , and the output summary is in English .	English	summary	model-feature	{'e1': {'word': 'English', 'word_index': [(21, 21)], 'id': 'H05-1005.9'}, 'e2': {'word': 'summary', 'word_index': [(18, 18)], 'id': 'H05-1005.8'}}	We consider the case of ENTITYUNRELATED , where the input ENTITYUNRELATED are in ENTITYUNRELATED , and the output ENTITYOTHER is in ENTITY .
Further, the use of multiple machine translation systems provides yet more redundancy , yielding different ways to realize that information in English .	English	information	model-feature	{'e1': {'word': 'English', 'word_index': [(20, 20)], 'id': 'H05-1005.16'}, 'e2': {'word': 'information', 'word_index': [(18, 18)], 'id': 'H05-1005.15'}}	Further , the use of multiple ENTITYUNRELATED provides yet more ENTITYUNRELATED , yielding different ways to realize that ENTITYOTHER in ENTITY .
 This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data 	supervised training data	maximum entropy word alignment algorithm	usage	{'e1': {'word': 'supervised training data', 'word_index': [(9, 9)], 'id': 'H05-1012.3'}, 'e2': {'word': 'maximum entropy word alignment algorithm', 'word_index': [(4, 4)], 'id': 'H05-1012.1'}}	This paper presents a ENTITYOTHER for ENTITYUNRELATED based on ENTITY
We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance .	supervised and unsupervised methods	performance	result	{'e1': {'word': 'supervised and unsupervised methods', 'word_index': [(18, 18)], 'id': 'H05-1012.6'}, 'e2': {'word': 'performance', 'word_index': [(21, 21)], 'id': 'H05-1012.7'}}	We demonstrate that it is feasible to create ENTITYUNRELATED for problems in ENTITYUNRELATED and that a mixture of ENTITY yields superior ENTITYOTHER .
The probabilistic model used in the alignment directly models the link decisions .	probabilistic model	alignment	usage	{'e1': {'word': 'probabilistic model', 'word_index': [(1, 1)], 'id': 'H05-1012.8'}, 'e2': {'word': 'alignment', 'word_index': [(5, 5)], 'id': 'H05-1012.9'}}	The ENTITY used in the ENTITYOTHER directly models the ENTITYUNRELATED .
 This paper presents a phrase-based statistical machine translation method , based on non-contiguous phrases , i.e	non-contiguous phrases	phrase-based statistical machine translation method	usage	{'e1': {'word': 'non-contiguous phrases', 'word_index': [(8, 8)], 'id': 'H05-1095.2'}, 'e2': {'word': 'phrase-based statistical machine translation method', 'word_index': [(4, 4)], 'id': 'H05-1095.1'}}	This paper presents a ENTITYOTHER , based on ENTITY , i.e
A method for producing such phrases from a word-aligned corpora is proposed.	phrases	word-aligned corpora	part_whole	{'e1': {'word': 'phrases', 'word_index': [(5, 5)], 'id': 'H05-1095.4'}, 'e2': {'word': 'word-aligned corpora', 'word_index': [(8, 8)], 'id': 'H05-1095.5'}}	A method for producing such ENTITY from a ENTITYOTHER is proposed .
A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .	statistical translation model	phrases	usage	{'e1': {'word': 'statistical translation model', 'word_index': [(1, 1)], 'id': 'H05-1095.6'}, 'e2': {'word': 'phrases', 'word_index': [(8, 8)], 'id': 'H05-1095.7'}}	A ENTITY is also presented that deals such ENTITYOTHER , as well as a ENTITYUNRELATED based on the maximization of ENTITYUNRELATED , as measured with the ENTITYUNRELATED .
A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .	translation accuracy	training method	usage	{'e1': {'word': 'translation accuracy', 'word_index': [(20, 20)], 'id': 'H05-1095.9'}, 'e2': {'word': 'training method', 'word_index': [(14, 14)], 'id': 'H05-1095.8'}}	A ENTITYUNRELATED is also presented that deals such ENTITYUNRELATED , as well as a ENTITYOTHER based on the maximization of ENTITY , as measured with the ENTITYUNRELATED .
Translations are produced by means of a beam-search decoder .	beam-search decoder	Translations	usage	{'e1': {'word': 'beam-search decoder', 'word_index': [(7, 7)], 'id': 'H05-1095.12'}, 'e2': {'word': 'Translations', 'word_index': [(0, 0)], 'id': 'H05-1095.11'}}	ENTITYOTHER are produced by means of a ENTITY .
 Following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar approach, implemented in a measure called POURPRE , for automatically evaluating answers to definition questions 	automatic evaluation	machine translation	usage	{'e1': {'word': 'automatic evaluation', 'word_index': [(5, 5)], 'id': 'H05-1117.1'}, 'e2': {'word': 'machine translation', 'word_index': [(7, 7)], 'id': 'H05-1117.2'}}	Following recent developments in the ENTITY of ENTITYOTHER and ENTITYUNRELATED , we present a similar approach , implemented in a measure called ENTITYUNRELATED , for ENTITYUNRELATED
Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings , and that POURPRE outperforms direct application of existing metrics.	rankings	official rankings	compare	{'e1': {'word': 'rankings', 'word_index': [(6, 6)], 'id': 'H05-1117.8'}, 'e2': {'word': 'official rankings', 'word_index': [(14, 14)], 'id': 'H05-1117.9'}}	Experiments with the ENTITYUNRELATED indicate that ENTITY produced by our metric correlate highly with ENTITYOTHER , and that ENTITYUNRELATED outperforms direct application of existing metrics .
 We describe a method for identifying systematic patterns in translation data using part-of-speech tag sequences 	patterns	translation data	part_whole	{'e1': {'word': 'patterns', 'word_index': [(7, 7)], 'id': 'H05-2007.1'}, 'e2': {'word': 'translation data', 'word_index': [(9, 9)], 'id': 'H05-2007.2'}}	We describe a method for identifying systematic ENTITY in ENTITYOTHER using ENTITYUNRELATED
We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore patterns in machine translation output .	patterns	machine translation output	part_whole	{'e1': {'word': 'patterns', 'word_index': [(25, 25)], 'id': 'H05-2007.8'}, 'e2': {'word': 'machine translation output', 'word_index': [(27, 27)], 'id': 'H05-2007.9'}}	We incorporate this analysis into a ENTITYUNRELATED intended for ENTITYUNRELATED of ENTITYUNRELATED , and demonstrate how our application can be used by ENTITYUNRELATED to explore ENTITY in ENTITYOTHER .
At the same time, the recent improvements in the BLEU scores of statistical machine translation (SMT) suggests that SMT models are good at predicting the right translation of the words in source language sentences .	translation	words	model-feature	{'e1': {'word': 'translation', 'word_index': [(22, 22)], 'id': 'I05-2021.12'}, 'e2': {'word': 'words', 'word_index': [(25, 25)], 'id': 'I05-2021.13'}}	At the same time , the recent improvements in the ENTITYUNRELATED of ENTITYUNRELATED suggests that ENTITYUNRELATED are good at predicting the right ENTITY of the ENTITYOTHER in ENTITYUNRELATED .
Surprisingly however, the WSD accuracy of SMT models has never been evaluated and compared with that of the dedicated WSD models .	SMT models	accuracy	result	{'e1': {'word': 'SMT models', 'word_index': [(7, 7)], 'id': 'I05-2021.17'}, 'e2': {'word': 'accuracy', 'word_index': [(5, 5)], 'id': 'I05-2021.16'}}	Surprisingly however , the ENTITYUNRELATED ENTITYOTHER of ENTITY has never been evaluated and compared with that of the dedicated ENTITYUNRELATED .
This tends to support the view that despite recent speculative claims to the contrary, current SMT models do have limitations in comparison with dedicated WSD models , and that SMT should benefit from the better predictions made by the WSD models .	SMT models	WSD models	compare	{'e1': {'word': 'SMT models', 'word_index': [(16, 16)], 'id': 'I05-2021.23'}, 'e2': {'word': 'WSD models', 'word_index': [(24, 24)], 'id': 'I05-2021.24'}}	This tends to support the view that despite recent speculative claims to the contrary , current ENTITY do have limitations in comparison with dedicated ENTITYOTHER , and that ENTITYUNRELATED should benefit from the better predictions made by the ENTITYUNRELATED .
Over the last few years dramatic improvements have been made, and a number of comparative evaluations have shown, that SMT gives competitive results to rule-based translation systems , requiring significantly less development time.	SMT	rule-based translation systems	compare	{'e1': {'word': 'SMT', 'word_index': [(21, 21)], 'id': 'I05-2048.3'}, 'e2': {'word': 'rule-based translation systems', 'word_index': [(26, 26)], 'id': 'I05-2048.4'}}	Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that ENTITY gives competitive results to ENTITYOTHER , requiring significantly less development time .
 In this paper we present our recent work on harvesting English-Chinese bitexts of the laws of Hong Kong from the Web and aligning them to the subparagraph level via utilizing the numbering system in the legal text hierarchy 	English-Chinese bitexts	Web	part_whole	{'e1': {'word': 'English-Chinese bitexts', 'word_index': [(10, 10)], 'id': 'I05-4010.1'}, 'e2': {'word': 'Web', 'word_index': [(19, 19)], 'id': 'I05-4010.2'}}	In this paper we present our recent work on harvesting ENTITY of the laws of Hong Kong from the ENTITYOTHER and aligning them to the ENTITYUNRELATED level via utilizing the ENTITYUNRELATED in the ENTITYUNRELATED
This piece of work has also laid a foundation for exploring and harvesting English-Chinese bitexts in a larger volume from the Web .	English-Chinese bitexts	Web	part_whole	{'e1': {'word': 'English-Chinese bitexts', 'word_index': [(13, 13)], 'id': 'I05-4010.11'}, 'e2': {'word': 'Web', 'word_index': [(20, 20)], 'id': 'I05-4010.12'}}	This piece of work has also laid a foundation for exploring and harvesting ENTITY in a larger volume from the ENTITYOTHER .
 The task of machine translation (MT) evaluation is closely related to the task of sentence-level semantic equivalence classification 	machine translation (MT) evaluation	sentence-level semantic equivalence classification	compare	{'e1': {'word': 'machine translation (MT) evaluation', 'word_index': [(3, 3)], 'id': 'I05-5003.1'}, 'e2': {'word': 'sentence-level semantic equivalence classification', 'word_index': [(11, 11)], 'id': 'I05-5003.2'}}	The task of ENTITY is closely related to the task of ENTITYOTHER
This paper investigates the utility of applying standard MT evaluation methods (BLEU, NIST, WER and PER) to building classifiers to predict semantic equivalence and entailment .	MT evaluation methods (BLEU, NIST, WER and PER)	classifiers	usage	{'e1': {'word': 'MT evaluation methods (BLEU, NIST, WER and PER)', 'word_index': [(8, 8)], 'id': 'I05-5003.3'}, 'e2': {'word': 'classifiers', 'word_index': [(11, 11)], 'id': 'I05-5003.4'}}	This paper investigates the utility of applying standard ENTITY to building ENTITYOTHER to predict ENTITYUNRELATED and ENTITYUNRELATED .
We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence .	PER	classification method	usage	{'e1': {'word': 'PER', 'word_index': [(8, 8)], 'id': 'I05-5003.8'}, 'e2': {'word': 'classification method', 'word_index': [(5, 5)], 'id': 'I05-5003.7'}}	We also introduce a novel ENTITYOTHER based on ENTITY which leverages ENTITYUNRELATED of the ENTITYUNRELATED contributing to the ENTITYUNRELATED in the ENTITYUNRELATED .
We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence .	part of speech information	words	model-feature	{'e1': {'word': 'part of speech information', 'word_index': [(11, 11)], 'id': 'I05-5003.9'}, 'e2': {'word': 'words', 'word_index': [(14, 14)], 'id': 'I05-5003.10'}}	We also introduce a novel ENTITYUNRELATED based on ENTITYUNRELATED which leverages ENTITY of the ENTITYOTHER contributing to the ENTITYUNRELATED in the ENTITYUNRELATED .
Our results show that MT evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment .	MT evaluation techniques	features	result	{'e1': {'word': 'MT evaluation techniques', 'word_index': [(4, 4)], 'id': 'I05-5003.13'}, 'e2': {'word': 'features', 'word_index': [(10, 10)], 'id': 'I05-5003.14'}}	Our results show that ENTITY are able to produce useful ENTITYOTHER for ENTITYUNRELATED and to a lesser extent ENTITYUNRELATED .
Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments.	technique	paraphrase classification accuracy	result	{'e1': {'word': 'technique', 'word_index': [(1, 1)], 'id': 'I05-5003.17'}, 'e2': {'word': 'paraphrase classification accuracy', 'word_index': [(7, 7)], 'id': 'I05-5003.18'}}	Our ENTITY gives a substantial improvement in ENTITYOTHER over all of the other ENTITYUNRELATED used in the experiments .
 We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like BLEU and NIST 	paraphrase	seed sentences	part_whole	{'e1': {'word': 'paraphrase', 'word_index': [(7, 7)], 'id': 'I05-5008.1'}, 'e2': {'word': 'seed sentences', 'word_index': [(10, 10)], 'id': 'I05-5008.2'}}	We propose a method that automatically generates ENTITY sets from ENTITYOTHER to be used as ENTITYUNRELATED in objective ENTITYUNRELATED like ENTITYUNRELATED and ENTITYUNRELATED
We measured the quality of the paraphrases produced in an experiment, i.e., (i) their grammaticality : at least 99% correct sentences ; (ii) their equivalence in meaning : at least 96% correct paraphrases either by meaning equivalence or entailment ; and, (iii) the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .	lexical and syntactical variation	paraphrases	model-feature	{'e1': {'word': 'lexical and syntactical variation', 'word_index': [(54, 54)], 'id': 'I05-5008.14'}, 'e2': {'word': 'paraphrases', 'word_index': [(59, 59)], 'id': 'I05-5008.15'}}	We measured the quality of the ENTITYUNRELATED produced in an experiment , i.e. , ( i ) their ENTITYUNRELATED : at least 99 % correct ENTITYUNRELATED ; ( ii ) their ENTITYUNRELATED : at least 96 % correct ENTITYUNRELATED either by ENTITYUNRELATED or ENTITYUNRELATED ; and , ( iii ) the amount of internal ENTITY in a set of ENTITYOTHER : slightly superior to that of ENTITYUNRELATED .
The paraphrase sets produced by this method thus seem adequate as reference sets to be used for MT evaluation .	reference sets	MT evaluation	usage	{'e1': {'word': 'reference sets', 'word_index': [(11, 11)], 'id': 'I05-5008.18'}, 'e2': {'word': 'MT evaluation', 'word_index': [(16, 16)], 'id': 'I05-5008.19'}}	The ENTITYUNRELATED sets produced by this method thus seem adequate as ENTITY to be used for ENTITYOTHER .
 This paper proposes an annotating scheme that encodes honorifics (respectful words)	annotating scheme	honorifics	model-feature	{'e1': {'word': 'annotating scheme', 'word_index': [(4, 4)], 'id': 'I05-6011.1'}, 'e2': {'word': 'honorifics', 'word_index': [(7, 7)], 'id': 'I05-6011.2'}}	This paper proposes an ENTITY that encodes ENTITYOTHER ( respectful words )
This referential information is vital for resolving zero pronouns and improving machine translation outputs .	referential information	machine translation outputs	result	{'e1': {'word': 'referential information', 'word_index': [(1, 1)], 'id': 'I05-6011.6'}, 'e2': {'word': 'machine translation outputs', 'word_index': [(9, 9)], 'id': 'I05-6011.8'}}	This ENTITY is vital for resolving ENTITYUNRELATED and improving ENTITYOTHER .
Annotating honorifics is a complex task that involves identifying a predicate with honorifics , assigning ranks to referents of the predicate , calibrating the ranks , and connecting referents with their predicates .	ranks	referents	model-feature	{'e1': {'word': 'ranks', 'word_index': [(15, 15)], 'id': 'I05-6011.12'}, 'e2': {'word': 'referents', 'word_index': [(17, 17)], 'id': 'I05-6011.13'}}	Annotating ENTITYUNRELATED is a complex task that involves identifying a ENTITYUNRELATED with ENTITYUNRELATED , assigning ENTITY to ENTITYOTHER of the ENTITYUNRELATED , calibrating the ENTITYUNRELATED , and connecting ENTITYUNRELATED with their ENTITYUNRELATED .
The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .	candidate parses	sentence	model-feature	{'e1': {'word': 'candidate parses', 'word_index': [(7, 7)], 'id': 'J05-1003.3'}, 'e2': {'word': 'sentence', 'word_index': [(11, 11)], 'id': 'J05-1003.4'}}	The base ENTITYUNRELATED produces a set of ENTITY for each input ENTITYOTHER , with associated ENTITYUNRELATED that define an initial ENTITYUNRELATED of these ENTITYUNRELATED .
A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence.	features	tree	model-feature	{'e1': {'word': 'features', 'word_index': [(14, 14)], 'id': 'J05-1003.10'}, 'e2': {'word': 'tree', 'word_index': [(17, 17)], 'id': 'J05-1003.11'}}	A second ENTITYUNRELATED then attempts to improve upon this initial ENTITYUNRELATED , using additional ENTITY of the ENTITYOTHER as evidence .
The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .	features	generative model	usage	{'e1': {'word': 'features', 'word_index': [(44, 44)], 'id': 'J05-1003.17'}, 'e2': {'word': 'generative model', 'word_index': [(40, 40)], 'id': 'J05-1003.16'}}	The strength of our approach is that it allows a ENTITYUNRELATED to be represented as an arbitrary set of ENTITYUNRELATED , without concerns about how these ENTITYUNRELATED interact or overlap and without the need to define a ENTITYUNRELATED or a ENTITYOTHER which takes these ENTITY into account .
We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al. (1998).	boosting approach	ranking problems	usage	{'e1': {'word': 'boosting approach', 'word_index': [(12, 12)], 'id': 'J05-1003.19'}, 'e2': {'word': 'ranking problems', 'word_index': [(14, 14)], 'id': 'J05-1003.20'}}	We introduce a new method for the ENTITYUNRELATED , based on the ENTITY to ENTITYOTHER described in Freund et al. ( 1998 ) .
We apply the boosting method to parsing the Wall Street Journal treebank .	parsing	Wall Street Journal treebank	usage	{'e1': {'word': 'parsing', 'word_index': [(5, 5)], 'id': 'J05-1003.22'}, 'e2': {'word': 'Wall Street Journal treebank', 'word_index': [(7, 7)], 'id': 'J05-1003.23'}}	We apply the ENTITYUNRELATED to ENTITY the ENTITYOTHER .
The new model achieved 89.75% F-measure , a 13% relative decrease in F-measure error over the baseline model's score of 88.2%.	model	F-measure	result	{'e1': {'word': 'model', 'word_index': [(2, 2)], 'id': 'J05-1003.29'}, 'e2': {'word': 'F-measure', 'word_index': [(6, 6)], 'id': 'J05-1003.30'}}	The new ENTITY achieved 89.75 % ENTITYOTHER , a 13 % relative decrease in ENTITYUNRELATED error over the ENTITYUNRELATED of 88.2 %.
The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .	sparsity of the feature space	parsing data	model-feature	{'e1': {'word': 'sparsity of the feature space', 'word_index': [(15, 15)], 'id': 'J05-1003.34'}, 'e2': {'word': 'parsing data', 'word_index': [(18, 18)], 'id': 'J05-1003.35'}}	The article also introduces a new algorithm for the ENTITYUNRELATED which takes advantage of the ENTITY in the ENTITYOTHER .
We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on feature selection methods within log-linear (maximum-entropy) models .	feature selection methods	log-linear (maximum-entropy) models	part_whole	{'e1': {'word': 'feature selection methods', 'word_index': [(21, 21)], 'id': 'J05-1003.38'}, 'e2': {'word': 'log-linear (maximum-entropy) models', 'word_index': [(23, 23)], 'id': 'J05-1003.39'}}	We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on ENTITY within ENTITYOTHER .
Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora .	parallel data	Chinese, Arabic, and English non-parallel newspaper corpora	part_whole	{'e1': {'word': 'parallel data', 'word_index': [(6, 6)], 'id': 'J05-4003.6'}, 'e2': {'word': 'Chinese, Arabic, and English non-parallel newspaper corpora', 'word_index': [(9, 9)], 'id': 'J05-4003.7'}}	Using this approach , we extract ENTITY from large ENTITYOTHER .
We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words ) and exploiting a large non-parallel corpus .	parallel corpus	MT system	usage	{'e1': {'word': 'parallel corpus', 'word_index': [(20, 20)], 'id': 'J05-4003.11'}, 'e2': {'word': 'MT system', 'word_index': [(8, 8)], 'id': 'J05-4003.10'}}	We also show that a good - quality ENTITYOTHER can be built from scratch by starting with a very small ENTITY ( 100,000 ENTITYUNRELATED ) and exploiting a large ENTITYUNRELATED .
We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure .	phrase translations	suffix array-based data structure	part_whole	{'e1': {'word': 'phrase translations', 'word_index': [(9, 9)], 'id': 'P05-1032.9'}, 'e2': {'word': 'suffix array-based data structure', 'word_index': [(12, 12)], 'id': 'P05-1032.10'}}	We detail the ENTITYUNRELATED and ENTITYUNRELATED for looking up ENTITY in our ENTITYOTHER .
 We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation 	syntactic information	source language	model-feature	{'e1': {'word': 'syntactic information', 'word_index': [(9, 9)], 'id': 'P05-1034.2'}, 'e2': {'word': 'source language', 'word_index': [(12, 12)], 'id': 'P05-1034.3'}}	We describe a novel approach to ENTITYUNRELATED that combines ENTITY in the ENTITYOTHER with recent advances in ENTITYUNRELATED
We align a parallel corpus , project the source dependency parse onto the target sentence , extract dependency treelet translation pairs , and train a tree-based ordering model .	source dependency parse	sentence	model-feature	{'e1': {'word': 'source dependency parse', 'word_index': [(7, 7)], 'id': 'P05-1034.11'}, 'e2': {'word': 'sentence', 'word_index': [(11, 11)], 'id': 'P05-1034.12'}}	We align a ENTITYUNRELATED , project the ENTITY onto the target ENTITYOTHER , extract ENTITYUNRELATED , and train a ENTITYUNRELATED .
Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone.	Chinese word sense disambiguation model	translation candidates	usage	{'e1': {'word': 'Chinese word sense disambiguation model', 'word_index': [(9, 9)], 'id': 'P05-1048.4'}, 'e2': {'word': 'translation candidates', 'word_index': [(12, 12)], 'id': 'P05-1048.5'}}	Using a state - of - the - art ENTITY to choose ENTITYOTHER for a typical ENTITYUNRELATED , we find that ENTITYUNRELATED does not yield significantly better ENTITYUNRELATED than the ENTITYUNRELATED alone .
Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone.	word sense disambiguation	translation quality	result	{'e1': {'word': 'word sense disambiguation', 'word_index': [(21, 21)], 'id': 'P05-1048.7'}, 'e2': {'word': 'translation quality', 'word_index': [(27, 27)], 'id': 'P05-1048.8'}}	Using a state - of - the - art ENTITYUNRELATED to choose ENTITYUNRELATED for a typical ENTITYUNRELATED , we find that ENTITY does not yield significantly better ENTITYOTHER than the ENTITYUNRELATED alone .
 Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data 	statistical models	structured data	usage	{'e1': {'word': 'statistical models', 'word_index': [(4, 4)], 'id': 'P05-1067.2'}, 'e2': {'word': 'structured data', 'word_index': [(6, 6)], 'id': 'P05-1067.3'}}	ENTITYUNRELATED aims at applying ENTITY to ENTITYOTHER
In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar .	probabilistic synchronous dependency insertion grammar	syntax-based statistical machine translation system	usage	{'e1': {'word': 'probabilistic synchronous dependency insertion grammar', 'word_index': [(11, 11)], 'id': 'P05-1067.5'}, 'e2': {'word': 'syntax-based statistical machine translation system', 'word_index': [(7, 7)], 'id': 'P05-1067.4'}}	In this paper , we present a ENTITYOTHER based on a ENTITY .
We first introduce our approach to inducing such a grammar from parallel corpora .	grammar	parallel corpora	part_whole	{'e1': {'word': 'grammar', 'word_index': [(9, 9)], 'id': 'P05-1067.9'}, 'e2': {'word': 'parallel corpora', 'word_index': [(11, 11)], 'id': 'P05-1067.10'}}	We first introduce our approach to inducing such a ENTITY from ENTITYOTHER .
Second, we describe the graphical model for the machine translation task , which can also be viewed as a stochastic tree-to-tree transducer .	graphical model	machine translation task	usage	{'e1': {'word': 'graphical model', 'word_index': [(5, 5)], 'id': 'P05-1067.11'}, 'e2': {'word': 'machine translation task', 'word_index': [(8, 8)], 'id': 'P05-1067.12'}}	Second , we describe the ENTITY for the ENTITYOTHER , which can also be viewed as a ENTITYUNRELATED .
The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality .	IBM models	baseline system	usage	{'e1': {'word': 'IBM models', 'word_index': [(12, 12)], 'id': 'P05-1067.19'}, 'e2': {'word': 'baseline system', 'word_index': [(8, 8)], 'id': 'P05-1067.18'}}	The result shows that our system outperforms the ENTITYOTHER based on the ENTITY in both ENTITYUNRELATED .
 In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT) 	localized phrase-based prediction model	statistical machine translation (SMT)	usage	{'e1': {'word': 'localized phrase-based prediction model', 'word_index': [(11, 11)], 'id': 'P05-1069.2'}, 'e2': {'word': 'statistical machine translation (SMT)', 'word_index': [(13, 13)], 'id': 'P05-1069.3'}}	In this paper , we present a novel ENTITYUNRELATED for a ENTITY for ENTITYOTHER
We use a maximum likelihood criterion to train a log-linear block bigram model which uses real-valued features (e.g. a language model score ) as well as binary features based on the block identities themselves, e.g. block bigram features.	real-valued features	log-linear block bigram model	usage	{'e1': {'word': 'real-valued features', 'word_index': [(10, 10)], 'id': 'P05-1069.9'}, 'e2': {'word': 'log-linear block bigram model', 'word_index': [(7, 7)], 'id': 'P05-1069.8'}}	We use a ENTITYUNRELATED to train a ENTITYOTHER which uses ENTITY ( e.g. a ENTITYUNRELATED ) as well as ENTITYUNRELATED based on the ENTITYUNRELATED identities themselves , e.g. block bigram features .
 Previous work has used monolingual parallel corpora to extract and generate paraphrases 	paraphrases	monolingual parallel corpora	part_whole	{'e1': {'word': 'paraphrases', 'word_index': [(9, 9)], 'id': 'P05-1074.2'}, 'e2': {'word': 'monolingual parallel corpora', 'word_index': [(4, 4)], 'id': 'P05-1074.1'}}	Previous work has used ENTITYOTHER to extract and generate ENTITY
We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account.	paraphrases	bilingual parallel corpus	part_whole	{'e1': {'word': 'paraphrases', 'word_index': [(6, 6)], 'id': 'P05-1074.11'}, 'e2': {'word': 'bilingual parallel corpus', 'word_index': [(10, 10)], 'id': 'P05-1074.12'}}	We define a ENTITYUNRELATED that allows ENTITY extracted from a ENTITYOTHER to be ranked using ENTITYUNRELATED , and show how it can be refined to take ENTITYUNRELATED into account .
We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments .	paraphrases	automatic alignments	part_whole	{'e1': {'word': 'paraphrases', 'word_index': [(15, 15)], 'id': 'P05-1074.18'}, 'e2': {'word': 'automatic alignments', 'word_index': [(18, 18)], 'id': 'P05-1074.19'}}	We evaluate our ENTITYUNRELATED using a set of ENTITYUNRELATED , and contrast the ENTITYUNRELATED with ENTITY extracted from ENTITYOTHER .
 We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures 	Czech-English statistical machine translation system	tree-to-tree translation	usage	{'e1': {'word': 'Czech-English statistical machine translation system', 'word_index': [(3, 3)], 'id': 'P05-2016.1'}, 'e2': {'word': 'tree-to-tree translation', 'word_index': [(6, 6)], 'id': 'P05-2016.2'}}	We present a ENTITY which performs ENTITYOTHER of ENTITYUNRELATED
We also refer to an evaluation method and plan to compare our system&apos;s output with a benchmark system .	system&apos;s output	benchmark system	compare	{'e1': {'word': 'system&apos;s output', 'word_index': [(11, 11)], 'id': 'P05-2016.9'}, 'e2': {'word': 'benchmark system', 'word_index': [(14, 14)], 'id': 'P05-2016.10'}}	We also refer to an ENTITYUNRELATED and plan to compare our ENTITY with a ENTITYOTHER .
The combination with a two-step clustering process using sentence co-occurrences as features allows for accurate results.	sentence co-occurrences	two-step clustering process	usage	{'e1': {'word': 'sentence co-occurrences', 'word_index': [(6, 6)], 'id': 'E06-1018.8'}, 'e2': {'word': 'two-step clustering process', 'word_index': [(4, 4)], 'id': 'E06-1018.7'}}	The combination with a ENTITYOTHER using ENTITY as ENTITYUNRELATED allows for accurate results .
 We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers 	addressee identification	four-participants face-to-face meetings	usage	{'e1': {'word': 'addressee identification', 'word_index': [(4, 4)], 'id': 'E06-1022.1'}, 'e2': {'word': 'four-participants face-to-face meetings', 'word_index': [(6, 6)], 'id': 'E06-1022.2'}}	We present results on ENTITY in ENTITYOTHER using ENTITYUNRELATED and ENTITYUNRELATED
The classifiers show little gain from information about meeting context .	classifiers	gain	result	{'e1': {'word': 'classifiers', 'word_index': [(1, 1)], 'id': 'E06-1022.17'}, 'e2': {'word': 'gain', 'word_index': [(4, 4)], 'id': 'E06-1022.18'}}	The ENTITY show little ENTITYOTHER from information about ENTITYUNRELATED .
 Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks	evaluation measures	machine translation	usage	{'e1': {'word': 'evaluation measures', 'word_index': [(7, 7)], 'id': 'E06-1031.1'}, 'e2': {'word': 'machine translation', 'word_index': [(9, 9)], 'id': 'E06-1031.2'}}	Most state - of - the- art ENTITY for ENTITYOTHER assign high ENTITYUNRELATED to movements of ENTITYUNRELATED blocks
In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation .	evaluation measure	block reordering	usage	{'e1': {'word': 'evaluation measure', 'word_index': [(9, 9)], 'id': 'E06-1031.6'}, 'e2': {'word': 'block reordering', 'word_index': [(13, 13)], 'id': 'E06-1031.7'}}	In this paper , we will present a new ENTITY which explicitly models ENTITYOTHER as an ENTITYUNRELATED .
Furthermore, we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs .	word-dependent substitution costs	evaluation measures	result	{'e1': {'word': 'word-dependent substitution costs', 'word_index': [(15, 15)], 'id': 'E06-1031.12'}, 'e2': {'word': 'evaluation measures', 'word_index': [(7, 7)], 'id': 'E06-1031.11'}}	Furthermore , we will show how some ENTITYOTHER can be improved by the introduction of ENTITY .
The correlation of the new measure with human judgment has been investigated systematically on two different language pairs .	measure	human judgment	compare	{'e1': {'word': 'measure', 'word_index': [(5, 5)], 'id': 'E06-1031.13'}, 'e2': {'word': 'human judgment', 'word_index': [(7, 7)], 'id': 'E06-1031.14'}}	The correlation of the new ENTITY with ENTITYOTHER has been investigated systematically on two different ENTITYUNRELATED .
Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment .	automatic evaluation measures	human judgment	compare	{'e1': {'word': 'automatic evaluation measures', 'word_index': [(13, 13)], 'id': 'E06-1031.18'}, 'e2': {'word': 'human judgment', 'word_index': [(15, 15)], 'id': 'E06-1031.19'}}	Results from experiments with ENTITYUNRELATED will demonstrate an additional increase of correlation between ENTITY and ENTITYOTHER .
 In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue 	segment boundaries	spoken multiparty dialogue	part_whole	{'e1': {'word': 'segment boundaries', 'word_index': [(11, 11)], 'id': 'E06-1035.1'}, 'e2': {'word': 'spoken multiparty dialogue', 'word_index': [(13, 13)], 'id': 'E06-1035.2'}}	In this paper , we investigate the problem of automatically predicting ENTITY in ENTITYOTHER
We then explore the impact on performance of using ASR output as opposed to human transcription .	ASR output	human transcription	compare	{'e1': {'word': 'ASR output', 'word_index': [(9, 9)], 'id': 'E06-1035.6'}, 'e2': {'word': 'human transcription', 'word_index': [(13, 13)], 'id': 'E06-1035.7'}}	We then explore the impact on ENTITYUNRELATED of using ENTITY as opposed to ENTITYOTHER .
Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task.	machine learning approach	predicting top-level boundaries	usage	{'e1': {'word': 'machine learning approach', 'word_index': [(36, 36)], 'id': 'E06-1035.13'}, 'e2': {'word': 'predicting top-level boundaries', 'word_index': [(33, 33)], 'id': 'E06-1035.12'}}	Examination of the effect of ENTITYUNRELATED shows that ENTITYUNRELATED are two distinct tasks : ( 1 ) for predicting ENTITYUNRELATED , the ENTITYUNRELATED alone can achieve competitive results , ( 2 ) for ENTITYOTHER , the ENTITY that combines ENTITYUNRELATED performs best , and ( 3 ) ENTITYUNRELATED , such as ENTITYUNRELATED and ENTITYUNRELATED , are better indicators for the top-level prediction task .
We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features , but do not change the general preference of approach for the two tasks.	transcription errors	ASR output	model-feature	{'e1': {'word': 'transcription errors', 'word_index': [(5, 5)], 'id': 'E06-1035.18'}, 'e2': {'word': 'ASR output', 'word_index': [(8, 8)], 'id': 'E06-1035.19'}}	We also find that the ENTITY inevitable in ENTITYOTHER have a negative impact on models that combine ENTITYUNRELATED , but do not change the general preference of approach for the two tasks .
 Combination methods are an effective way of improving system performance 	Combination methods	system performance	result	{'e1': {'word': 'Combination methods', 'word_index': [(0, 0)], 'id': 'P06-1013.1'}, 'e2': {'word': 'system performance', 'word_index': [(7, 7)], 'id': 'P06-1013.2'}}	ENTITY are an effective way of improving ENTITYOTHER
Our combination methods rely on predominant senses which are derived automatically from raw text .	predominant senses	combination methods	usage	{'e1': {'word': 'predominant senses', 'word_index': [(4, 4)], 'id': 'P06-1013.8'}, 'e2': {'word': 'combination methods', 'word_index': [(1, 1)], 'id': 'P06-1013.7'}}	Our ENTITYOTHER rely on ENTITY which are derived automatically from ENTITYUNRELATED .
 We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation (USR) of a scope ambiguity , compute an USR with fewer mutually equivalent readings 	underspecified semantic representation (USR)	scope ambiguity	model-feature	{'e1': {'word': 'underspecified semantic representation (USR)', 'word_index': [(11, 11)], 'id': 'P06-1052.2'}, 'e2': {'word': 'scope ambiguity', 'word_index': [(14, 14)], 'id': 'P06-1052.3'}}	We present an efficient algorithm for the ENTITYUNRELATED : Given an ENTITY of a ENTITYOTHER , compute an ENTITYUNRELATED with fewer mutually ENTITYUNRELATED
 In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque 	machine learning techniques	comma checker	usage	{'e1': {'word': 'machine learning techniques', 'word_index': [(9, 9)], 'id': 'P06-2001.1'}, 'e2': {'word': 'comma checker', 'word_index': [(13, 13)], 'id': 'P06-2001.2'}}	In this paper , we describe the research using ENTITY to build a ENTITYOTHER to be integrated in a ENTITYUNRELATED for ENTITYUNRELATED
 In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque 	grammar checker	Basque	usage	{'e1': {'word': 'grammar checker', 'word_index': [(19, 19)], 'id': 'P06-2001.3'}, 'e2': {'word': 'Basque', 'word_index': [(21, 21)], 'id': 'P06-2001.4'}}	In this paper , we describe the research using ENTITYUNRELATED to build a ENTITYUNRELATED to be integrated in a ENTITY for ENTITYOTHER
After several experiments, and trained with a little corpus of 100,000 words , the system guesses correctly not placing commas with a precision of 96% and a recall of 98%.	words	corpus	part_whole	{'e1': {'word': 'words', 'word_index': [(12, 12)], 'id': 'P06-2001.6'}, 'e2': {'word': 'corpus', 'word_index': [(9, 9)], 'id': 'P06-2001.5'}}	After several experiments , and trained with a little ENTITYOTHER of 100,000 ENTITY , the system guesses correctly not placing ENTITYUNRELATED with a ENTITYUNRELATED of 96 % and a ENTITYUNRELATED of 98 %.
Finally, we have shown that these results can be improved using a bigger and a more homogeneous corpus to train, that is, a bigger corpus written by one unique author .	author	corpus	model-feature	{'e1': {'word': 'author', 'word_index': [(32, 32)], 'id': 'P06-2001.15'}, 'e2': {'word': 'corpus', 'word_index': [(27, 27)], 'id': 'P06-2001.14'}}	Finally , we have shown that these results can be improved using a bigger and a more homogeneous ENTITYUNRELATED to train , that is , a bigger ENTITYOTHER written by one unique ENTITY .
 This paper presents an unsupervised learning approach to disambiguate various relations between named entities by use of various lexical and syntactic features from the contexts 	lexical and syntactic features	unsupervised learning approach	usage	{'e1': {'word': 'lexical and syntactic features', 'word_index': [(15, 15)], 'id': 'P06-2012.3'}, 'e2': {'word': 'unsupervised learning approach', 'word_index': [(4, 4)], 'id': 'P06-2012.1'}}	This paper presents an ENTITYOTHER to disambiguate various relations between ENTITYUNRELATED by use of various ENTITY from the ENTITYUNRELATED
Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .	spectral clustering based approach	clustering methods	compare	{'e1': {'word': 'spectral clustering based approach', 'word_index': [(7, 7)], 'id': 'P06-2012.13'}, 'e2': {'word': 'clustering methods', 'word_index': [(11, 11)], 'id': 'P06-2012.14'}}	Experiment results on ENTITYUNRELATED show that this ENTITY outperforms the other ENTITYOTHER .
 This paper proposes a novel method of building polarity-tagged corpus from HTML documents 	polarity-tagged corpus	HTML documents	part_whole	{'e1': {'word': 'polarity-tagged corpus', 'word_index': [(8, 8)], 'id': 'P06-2059.1'}, 'e2': {'word': 'HTML documents', 'word_index': [(10, 10)], 'id': 'P06-2059.2'}}	This paper proposes a novel method of building ENTITY from ENTITYOTHER
In our experiment, the method could construct a corpus consisting of 126,610 sentences .	sentences	corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(13, 13)], 'id': 'P06-2059.8'}, 'e2': {'word': 'corpus', 'word_index': [(9, 9)], 'id': 'P06-2059.7'}}	In our experiment , the method could construct a ENTITYOTHER consisting of 126,610 ENTITY .
 In this paper we show how two standard outputs from information extraction (IE) systems - named entity annotations and scenario templates - can be used to enhance access to text collections via a standard text browser .	scenario templates	text browser	usage	{'e1': {'word': 'scenario templates', 'word_index': [(14, 14)], 'id': 'H01-1040.3'}, 'e2': {'word': 'text browser', 'word_index': [(27, 27)], 'id': 'H01-1040.5'}}	In this paper we show how two standard outputs from ENTITYUNRELATED - ENTITYUNRELATED and ENTITY - can be used to enhance access to ENTITYUNRELATED via a standard ENTITYOTHER .
We describe how this information is used in a prototype system designed to support information workers &apos; access to a pharmaceutical news archive as part of their industry watch function.	prototype system	industry watch	usage	{'e1': {'word': 'prototype system', 'word_index': [(9, 9)], 'id': 'H01-1040.6'}, 'e2': {'word': 'industry watch', 'word_index': [(24, 24)], 'id': 'H01-1040.9'}}	We describe how this information is used in a ENTITY designed to support ENTITYUNRELATED &apos ; access to a ENTITYUNRELATED as part of their ENTITYOTHER function .
We also report results of a preliminary, qualitative user evaluation of the system, which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of IE-enhanced text browsers .	interface	IE-enhanced text browsers	part_whole	{'e1': {'word': 'interface', 'word_index': [(26, 26)], 'id': 'H01-1040.11'}, 'e2': {'word': 'IE-enhanced text browsers', 'word_index': [(36, 36)], 'id': 'H01-1040.13'}}	We also report results of a preliminary , ENTITYUNRELATED of the system , which while broadly positive indicates further work needs to be done on the ENTITY to make ENTITYUNRELATED aware of the increased potential of ENTITYOTHER .
 Recent advances in Automatic Speech Recognition technology have put the goal of naturally sounding dialog systems within reach.	Automatic Speech Recognition technology	dialog systems	usage	{'e1': {'word': 'Automatic Speech Recognition technology', 'word_index': [(3, 3)], 'id': 'H01-1055.1'}, 'e2': {'word': 'dialog systems', 'word_index': [(11, 11)], 'id': 'H01-1055.2'}}	Recent advances in ENTITY have put the goal of naturally sounding ENTITYOTHER within reach .
The issue of system response to users has been extensively studied by the natural language generation community , though rarely in the context of dialog systems .	natural language generation community	system response	topic	{'e1': {'word': 'natural language generation community', 'word_index': [(12, 12)], 'id': 'H01-1055.9'}, 'e2': {'word': 'system response', 'word_index': [(3, 3)], 'id': 'H01-1055.7'}}	The issue of ENTITYOTHER to ENTITYUNRELATED has been extensively studied by the ENTITY , though rarely in the context of ENTITYUNRELATED .
We show how research in generation can be adapted to dialog systems , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques .	generation	dialog systems	usage	{'e1': {'word': 'generation', 'word_index': [(5, 5)], 'id': 'H01-1055.11'}, 'e2': {'word': 'dialog systems', 'word_index': [(10, 10)], 'id': 'H01-1055.12'}}	We show how research in ENTITY can be adapted to ENTITYOTHER , and how the high cost of hand - crafting ENTITYUNRELATED can be overcome by employing ENTITYUNRELATED .
We show how research in generation can be adapted to dialog systems , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques .	machine learning techniques	knowledge-based generation systems	usage	{'e1': {'word': 'machine learning techniques', 'word_index': [(27, 27)], 'id': 'H01-1055.14'}, 'e2': {'word': 'knowledge-based generation systems', 'word_index': [(21, 21)], 'id': 'H01-1055.13'}}	We show how research in ENTITYUNRELATED can be adapted to ENTITYUNRELATED , and how the high cost of hand - crafting ENTITYOTHER can be overcome by employing ENTITY .
 The TAP-XL Automated Analyst&apos;s Assistant is an application designed to help an English -speaking analyst write a topical report , culling information from a large inflow of multilingual, multimedia data .	multilingual, multimedia data	TAP-XL Automated Analyst&apos;s Assistant	usage	{'e1': {'word': 'multilingual, multimedia data', 'word_index': [(24, 24)], 'id': 'N03-4004.4'}, 'e2': {'word': 'TAP-XL Automated Analyst&apos;s Assistant', 'word_index': [(1, 1)], 'id': 'N03-4004.1'}}	The ENTITYOTHER is an application designed to help an ENTITYUNRELATED - speaking analyst write a ENTITYUNRELATED , culling information from a large inflow of ENTITY .
 This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation .	computational problems	probabilistic translation models	model-feature	{'e1': {'word': 'computational problems', 'word_index': [(4, 4)], 'id': 'H05-1101.1'}, 'e2': {'word': 'probabilistic translation models', 'word_index': [(7, 7)], 'id': 'H05-1101.2'}}	This paper investigates some ENTITY associated with ENTITYOTHER that have recently been adopted in the literature on ENTITYUNRELATED .
These models can be viewed as pairs of probabilistic context-free grammars working in a &apos;synchronous&apos; way.	probabilistic context-free grammars	models	model-feature	{'e1': {'word': 'probabilistic context-free grammars', 'word_index': [(8, 8)], 'id': 'H05-1101.5'}, 'e2': {'word': 'models', 'word_index': [(1, 1)], 'id': 'H05-1101.4'}}	These ENTITYOTHER can be viewed as pairs of ENTITY working in a &apos ; synchronous&apos ; way .
Two hardness results for the class NP are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature.	NP	hardness	model-feature	{'e1': {'word': 'NP', 'word_index': [(6, 6)], 'id': 'H05-1101.7'}, 'e2': {'word': 'hardness', 'word_index': [(1, 1)], 'id': 'H05-1101.6'}}	Two ENTITYOTHER results for the class ENTITY are reported , along with an ENTITYUNRELATED for certain classes of algorithms that are currently used in the literature .
 Automatic evaluation metrics for Machine Translation (MT) systems , such as BLEU or NIST , are now well established.	evaluation metrics	Machine Translation (MT) systems	usage	{'e1': {'word': 'evaluation metrics', 'word_index': [(1, 1)], 'id': 'I05-2014.1'}, 'e2': {'word': 'Machine Translation (MT) systems', 'word_index': [(3, 3)], 'id': 'I05-2014.2'}}	Automatic ENTITY for ENTITYOTHER , such as ENTITYUNRELATED or ENTITYUNRELATED , are now well established .
Yet, they are scarcely used for the assessment of language pairs like English-Chinese or English-Japanese , because of the word segmentation problem .	word segmentation problem	English-Japanese	model-feature	{'e1': {'word': 'word segmentation problem', 'word_index': [(19, 19)], 'id': 'I05-2014.8'}, 'e2': {'word': 'English-Japanese', 'word_index': [(14, 14)], 'id': 'I05-2014.7'}}	Yet , they are scarcely used for the assessment of ENTITYUNRELATED like ENTITYUNRELATED or ENTITYOTHER , because of the ENTITY .
This study establishes the equivalence between the standard use of BLEU in word n-grams and its application at the character level.	BLEU	word n-grams	usage	{'e1': {'word': 'BLEU', 'word_index': [(10, 10)], 'id': 'I05-2014.9'}, 'e2': {'word': 'word n-grams', 'word_index': [(12, 12)], 'id': 'I05-2014.10'}}	This study establishes the equivalence between the standard use of ENTITY in ENTITYOTHER and its application at the ENTITYUNRELATED level .
The use of BLEU at the character level eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with, for instance, statistical MT systems which usually segment their outputs .	BLEU	character	usage	{'e1': {'word': 'BLEU', 'word_index': [(3, 3)], 'id': 'I05-2014.12'}, 'e2': {'word': 'character', 'word_index': [(6, 6)], 'id': 'I05-2014.13'}}	The use of ENTITY at the ENTITYOTHER level eliminates the ENTITYUNRELATED : it makes it possible to directly compare commercial systems outputting ENTITYUNRELATED with , for instance , ENTITYUNRELATED which usually segment their ENTITYUNRELATED .
The method allows a user to explore a model of syntax-based statistical machine translation (MT) , to understand the model &apos;s strengths and weaknesses, and to compare it to other MT systems .	model	MT systems	compare	{'e1': {'word': 'model', 'word_index': [(15, 15)], 'id': 'P05-3025.6'}, 'e2': {'word': 'MT systems', 'word_index': [(28, 28)], 'id': 'P05-3025.7'}}	The method allows a ENTITYUNRELATED to explore a ENTITYUNRELATED of ENTITYUNRELATED , to understand the ENTITY &apos ;s strengths and weaknesses , and to compare it to other ENTITYOTHER .
Using this visualization method , we can find and address conceptual and practical problems in an MT system .	visualization method	MT system	usage	{'e1': {'word': 'visualization method', 'word_index': [(2, 2)], 'id': 'P05-3025.8'}, 'e2': {'word': 'MT system', 'word_index': [(15, 15)], 'id': 'P05-3025.9'}}	Using this ENTITY , we can find and address conceptual and practical problems in an ENTITYOTHER .
 In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation (SMT) but which have not been addressed satisfactorily by the SMT research community .	SMT research community	Statistical Machine Translation (SMT)	topic	{'e1': {'word': 'SMT research community', 'word_index': [(25, 25)], 'id': 'E06-1004.2'}, 'e2': {'word': 'Statistical Machine Translation (SMT)', 'word_index': [(15, 15)], 'id': 'E06-1004.1'}}	In this paper we study a set of problems that are of considerable importance to ENTITYOTHER but which have not been addressed satisfactorily by the ENTITY .
Over the last decade, a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT .	computational complexity	SMT	model-feature	{'e1': {'word': 'computational complexity', 'word_index': [(21, 21)], 'id': 'E06-1004.4'}, 'e2': {'word': 'SMT', 'word_index': [(29, 29)], 'id': 'E06-1004.5'}}	Over the last decade , a variety of ENTITYUNRELATED have been built and empirically tested whereas little is known about the ENTITY of some of the fundamental problems of ENTITYOTHER .
Since it is unlikely that there exists a polynomial time solution for any of these hard problems (unless P = NP and P#P = P ), our results highlight and justify the need for developing polynomial time approximations for these computations.	polynomial time solution	hard problems	usage	{'e1': {'word': 'polynomial time solution', 'word_index': [(8, 8)], 'id': 'E06-1004.10'}, 'e2': {'word': 'hard problems', 'word_index': [(13, 13)], 'id': 'E06-1004.11'}}	Since it is unlikely that there exists a ENTITY for any of these ENTITYOTHER ( unless ENTITYUNRELATED and ENTITYUNRELATED ) , our results highlight and justify the need for developing ENTITYUNRELATED for these computations .
We investigate that claim by adopting a simple MT-based paraphrasing technique and evaluating QA system performance on paraphrased questions .	MT-based paraphrasing technique	QA system	usage	{'e1': {'word': 'MT-based paraphrasing technique', 'word_index': [(8, 8)], 'id': 'N06-2009.5'}, 'e2': {'word': 'QA system', 'word_index': [(11, 11)], 'id': 'N06-2009.6'}}	We investigate that claim by adopting a simple ENTITY and evaluating ENTITYOTHER performance on ENTITYUNRELATED .
 There are several approaches that model information extraction as a token classification task , using various tagging strategies to combine multiple tokens .	tagging strategies	token classification task	usage	{'e1': {'word': 'tagging strategies', 'word_index': [(13, 13)], 'id': 'N06-2038.3'}, 'e2': {'word': 'token classification task', 'word_index': [(9, 9)], 'id': 'N06-2038.2'}}	There are several approaches that model ENTITYUNRELATED as a ENTITYOTHER , using various ENTITY to combine multiple ENTITYUNRELATED .
InfoMagnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining .	InfoMagnets	exploratory corpus analysis	usage	{'e1': {'word': 'InfoMagnets', 'word_index': [(0, 0)], 'id': 'N06-4001.3'}, 'e2': {'word': 'exploratory corpus analysis', 'word_index': [(4, 4)], 'id': 'N06-4001.4'}}	ENTITY aims at making ENTITYOTHER accessible to researchers who are not experts in ENTITYUNRELATED .
As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains: tutorial dialogue (Kumar et al., submitted) and on-line communities (Arguello et al., 2006).	behavioral patterns	tutorial dialogue 	part_whole	{'e1': {'word': 'behavioral patterns', 'word_index': [(23, 23)], 'id': 'N06-4001.7'}, 'e2': {'word': 'tutorial dialogue ', 'word_index': [(29, 29)], 'id': 'N06-4001.8'}}	As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between ENTITYUNRELATED and ENTITY in two distinct domains : ENTITYOTHER ( Kumar et al. , submitted ) and ENTITYUNRELATED ( Arguello et al. , 2006 ) .
As an educational tool , it has been used as part of a unit on protocol analysis in an Educational Research Methods course .	educational tool	protocol analysis	usage	{'e1': {'word': 'educational tool', 'word_index': [(2, 2)], 'id': 'N06-4001.10'}, 'e2': {'word': 'protocol analysis', 'word_index': [(14, 14)], 'id': 'N06-4001.11'}}	As an ENTITY , it has been used as part of a unit on ENTITYOTHER in an ENTITYUNRELATED .
The polarization of the objects of the elementary structures controls the saturation of the final structure .	polarization	elementary structures	usage	{'e1': {'word': 'polarization', 'word_index': [(1, 1)], 'id': 'P06-1018.7'}, 'e2': {'word': 'elementary structures', 'word_index': [(7, 7)], 'id': 'P06-1018.8'}}	The ENTITY of the objects of the ENTITYOTHER controls the ENTITYUNRELATED of the final ENTITYUNRELATED .
The polarization of the objects of the elementary structures controls the saturation of the final structure .	saturation	structure	model-feature	{'e1': {'word': 'saturation', 'word_index': [(10, 10)], 'id': 'P06-1018.9'}, 'e2': {'word': 'structure', 'word_index': [(14, 14)], 'id': 'P06-1018.10'}}	The ENTITYUNRELATED of the objects of the ENTITYUNRELATED controls the ENTITY of the final ENTITYOTHER .
 This paper examines what kind of similarity between words can be represented by what kind of word vectors in the vector space model .	word vectors	similarity	model-feature	{'e1': {'word': 'word vectors', 'word_index': [(16, 16)], 'id': 'P06-2110.3'}, 'e2': {'word': 'similarity', 'word_index': [(6, 6)], 'id': 'P06-2110.1'}}	This paper examines what kind of ENTITYOTHER between ENTITYUNRELATED can be represented by what kind of ENTITY in the ENTITYUNRELATED .
Through two experiments, three methods for constructing word vectors , i.e., LSA-based, cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of similarity , i.e., taxonomic similarity and associative similarity .	LSA-based, cooccurrence-based and dictionary-based methods	similarity	usage	{'e1': {'word': 'LSA-based, cooccurrence-based and dictionary-based methods', 'word_index': [(9, 9)], 'id': 'P06-2110.6'}, 'e2': {'word': 'similarity', 'word_index': [(23, 23)], 'id': 'P06-2110.7'}}	Through two experiments , three ENTITYUNRELATED , i.e. , ENTITY , were compared in terms of the ability to represent two kinds of ENTITYOTHER , i.e. , ENTITYUNRELATED and ENTITYUNRELATED .
The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the LSA-based and the cooccurrence-based word vectors better reflect associative similarity .	dictionary-based word vectors	taxonomic similarity	usage	{'e1': {'word': 'dictionary-based word vectors', 'word_index': [(8, 8)], 'id': 'P06-2110.10'}, 'e2': {'word': 'taxonomic similarity', 'word_index': [(11, 11)], 'id': 'P06-2110.11'}}	The result of the comparison was that the ENTITY better reflect ENTITYOTHER , while the ENTITYUNRELATED better reflect ENTITYUNRELATED .
The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the LSA-based and the cooccurrence-based word vectors better reflect associative similarity .	LSA-based and the cooccurrence-based word vectors	associative similarity	usage	{'e1': {'word': 'LSA-based and the cooccurrence-based word vectors', 'word_index': [(15, 15)], 'id': 'P06-2110.12'}, 'e2': {'word': 'associative similarity', 'word_index': [(18, 18)], 'id': 'P06-2110.13'}}	The result of the comparison was that the ENTITYUNRELATED better reflect ENTITYUNRELATED , while the ENTITY better reflect ENTITYOTHER .
In this paper, events are defined as event terms and associated event elements .	event terms	events	model-feature	{'e1': {'word': 'event terms', 'word_index': [(8, 8)], 'id': 'P06-3007.3'}, 'e2': {'word': 'events', 'word_index': [(4, 4)], 'id': 'P06-3007.2'}}	In this paper , ENTITYOTHER are defined as ENTITY and ENTITYUNRELATED .
With relevant approach, we identify important contents by PageRank algorithm on the event map constructed from documents .	documents	event map	usage	{'e1': {'word': 'documents', 'word_index': [(15, 15)], 'id': 'P06-3007.9'}, 'e2': {'word': 'event map', 'word_index': [(12, 12)], 'id': 'P06-3007.8'}}	With relevant approach , we identify important contents by ENTITYUNRELATED on the ENTITYOTHER constructed from ENTITY .
 This paper describes FERRET , an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments.	interactive question-answering (Q/A) system	automatic Q/A	usage	{'e1': {'word': 'interactive question-answering (Q/A) system', 'word_index': [(6, 6)], 'id': 'P06-4007.2'}, 'e2': {'word': 'automatic Q/A', 'word_index': [(14, 14)], 'id': 'P06-4007.3'}}	This paper describes ENTITYUNRELATED , an ENTITY designed to address the challenges of integrating ENTITYOTHER applications into real - world environments .
FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions (and answers ) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario.	predictive questioning	Q/A	usage	{'e1': {'word': 'predictive questioning', 'word_index': [(9, 9)], 'id': 'P06-4007.6'}, 'e2': {'word': 'Q/A', 'word_index': [(6, 6)], 'id': 'P06-4007.5'}}	ENTITYUNRELATED utilizes a novel approach to ENTITYOTHER known as ENTITY which attempts to identify the ENTITYUNRELATED ( and ENTITYUNRELATED ) that ENTITYUNRELATED need by analyzing how a ENTITYUNRELATED interacts with a system while gathering information related to a particular scenario .
 This paper introduces a method for computational analysis of move structures in abstracts of research articles .	abstracts	research articles	part_whole	{'e1': {'word': 'abstracts', 'word_index': [(8, 8)], 'id': 'P06-4011.2'}, 'e2': {'word': 'research articles', 'word_index': [(10, 10)], 'id': 'P06-4011.3'}}	This paper introduces a method for ENTITYUNRELATED in ENTITY of ENTITYOTHER .
In our approach, sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions .	sentences	abstract	part_whole	{'e1': {'word': 'sentences', 'word_index': [(4, 4)], 'id': 'P06-4011.4'}, 'e2': {'word': 'abstract', 'word_index': [(8, 8)], 'id': 'P06-4011.5'}}	In our approach , ENTITY in a given ENTITYOTHER are analyzed and labeled with a specific ENTITYUNRELATED in light of various ENTITYUNRELATED .
The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves .	abstracts	Web	part_whole	{'e1': {'word': 'abstracts', 'word_index': [(9, 9)], 'id': 'P06-4011.8'}, 'e2': {'word': 'Web', 'word_index': [(12, 12)], 'id': 'P06-4011.9'}}	The method involves automatically gathering a large number of ENTITY from the ENTITYOTHER and building a ENTITYUNRELATED of ENTITYUNRELATED .
The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves .	language model	abstract moves	model-feature	{'e1': {'word': 'language model', 'word_index': [(16, 16)], 'id': 'P06-4011.10'}, 'e2': {'word': 'abstract moves', 'word_index': [(18, 18)], 'id': 'P06-4011.11'}}	The method involves automatically gathering a large number of ENTITYUNRELATED from the ENTITYUNRELATED and building a ENTITY of ENTITYOTHER .
We also present a prototype concordancer , CARE , which exploits the move-tagged abstracts for digital learning .	move-tagged abstracts	digital learning	usage	{'e1': {'word': 'move-tagged abstracts', 'word_index': [(12, 12)], 'id': 'P06-4011.14'}, 'e2': {'word': 'digital learning', 'word_index': [(14, 14)], 'id': 'P06-4011.15'}}	We also present a prototype ENTITYUNRELATED , ENTITYUNRELATED , which exploits the ENTITY for ENTITYOTHER .
 The LOGON MT demonstrator assembles independently valuable general-purpose NLP components into a machine translation pipeline that capitalizes on output quality .	general-purpose NLP components	machine translation pipeline	part_whole	{'e1': {'word': 'general-purpose NLP components', 'word_index': [(5, 5)], 'id': 'P06-4014.2'}, 'e2': {'word': 'machine translation pipeline', 'word_index': [(8, 8)], 'id': 'P06-4014.3'}}	The ENTITYUNRELATED assembles independently valuable ENTITY into a ENTITYOTHER that capitalizes on ENTITYUNRELATED .
In this format, developed by the LNR research group at The University of California at San Diego, verbs are represented as interconnected sets of subpredicates .	subpredicates	verbs	model-feature	{'e1': {'word': 'subpredicates', 'word_index': [(26, 26)], 'id': 'T78-1001.3'}, 'e2': {'word': 'verbs', 'word_index': [(19, 19)], 'id': 'T78-1001.2'}}	In this format , developed by the LNR research group at The University of California at San Diego , ENTITYOTHER are represented as interconnected sets of ENTITY .
These subpredicates may be thought of as the almost inevitable inferences that a listener makes when a verb is used in a sentence .	verb	sentence	part_whole	{'e1': {'word': 'verb', 'word_index': [(17, 17)], 'id': 'T78-1001.7'}, 'e2': {'word': 'sentence', 'word_index': [(22, 22)], 'id': 'T78-1001.8'}}	These ENTITYUNRELATED may be thought of as the almost inevitable ENTITYUNRELATED that a ENTITYUNRELATED makes when a ENTITY is used in a ENTITYOTHER .
They confer a meaning structure on the sentence in which the verb is used.	verb	sentence	part_whole	{'e1': {'word': 'verb', 'word_index': [(10, 10)], 'id': 'T78-1001.11'}, 'e2': {'word': 'sentence', 'word_index': [(6, 6)], 'id': 'T78-1001.10'}}	They confer a ENTITYUNRELATED on the ENTITYOTHER in which the ENTITY is used .
 The paper outlines a computational theory of human plausible reasoning constructed from analysis of people&apos;s answers to everyday questions.	computational theory	human plausible reasoning	model-feature	{'e1': {'word': 'computational theory', 'word_index': [(4, 4)], 'id': 'T78-1028.1'}, 'e2': {'word': 'human plausible reasoning', 'word_index': [(6, 6)], 'id': 'T78-1028.2'}}	The paper outlines a ENTITY of ENTITYOTHER constructed from analysis of people&apos ;s answers to everyday questions .
Like logic , the theory is expressed in a content-independent formalism .	content-independent formalism	theory	model-feature	{'e1': {'word': 'content-independent formalism', 'word_index': [(9, 9)], 'id': 'T78-1028.5'}, 'e2': {'word': 'theory', 'word_index': [(4, 4)], 'id': 'T78-1028.4'}}	Like ENTITYUNRELATED , the ENTITYOTHER is expressed in a ENTITY .
Unlike logic , the theory specifies how different information in memory affects the certainty of the conclusions drawn.	logic	theory	compare	{'e1': {'word': 'logic', 'word_index': [(1, 1)], 'id': 'T78-1028.6'}, 'e2': {'word': 'theory', 'word_index': [(4, 4)], 'id': 'T78-1028.7'}}	Unlike ENTITY , the ENTITYOTHER specifies how different information in ENTITYUNRELATED affects the certainty of the conclusions drawn .
The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person&apos;s knowledge about his own knowledge.	dimensionalized space	theory	part_whole	{'e1': {'word': 'dimensionalized space', 'word_index': [(5, 5)], 'id': 'T78-1028.10'}, 'e2': {'word': 'theory', 'word_index': [(1, 1)], 'id': 'T78-1028.9'}}	The ENTITYOTHER consists of a ENTITY of different ENTITYUNRELATED and their ENTITYUNRELATED , including a variety of ENTITYUNRELATED where the ENTITYUNRELATED depends on the person&apos ;s knowledge about his own knowledge .
The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person&apos;s knowledge about his own knowledge.	certainty conditions	inference types	model-feature	{'e1': {'word': 'certainty conditions', 'word_index': [(11, 11)], 'id': 'T78-1028.12'}, 'e2': {'word': 'inference types', 'word_index': [(8, 8)], 'id': 'T78-1028.11'}}	The ENTITYUNRELATED consists of a ENTITYUNRELATED of different ENTITYOTHER and their ENTITY , including a variety of ENTITYUNRELATED where the ENTITYUNRELATED depends on the person&apos ;s knowledge about his own knowledge .
The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person&apos;s knowledge about his own knowledge.	meta-inference types	inference	model-feature	{'e1': {'word': 'meta-inference types', 'word_index': [(17, 17)], 'id': 'T78-1028.13'}, 'e2': {'word': 'inference', 'word_index': [(20, 20)], 'id': 'T78-1028.14'}}	The ENTITYUNRELATED consists of a ENTITYUNRELATED of different ENTITYUNRELATED and their ENTITYUNRELATED , including a variety of ENTITY where the ENTITYOTHER depends on the person&apos ;s knowledge about his own knowledge .
Path-based inference rules may be written using a binary relational calculus notation .	binary relational calculus notation	Path-based inference rules	usage	{'e1': {'word': 'binary relational calculus notation', 'word_index': [(6, 6)], 'id': 'T78-1031.10'}, 'e2': {'word': 'Path-based inference rules', 'word_index': [(0, 0)], 'id': 'T78-1031.9'}}	ENTITYOTHER may be written using a ENTITY .
Node-based inference allows a structure of nodes to be inferred from the existence of an instance of a pattern of node structures .	node structures	Node-based inference	usage	{'e1': {'word': 'node structures', 'word_index': [(19, 19)], 'id': 'T78-1031.14'}, 'e2': {'word': 'Node-based inference', 'word_index': [(0, 0)], 'id': 'T78-1031.11'}}	ENTITYOTHER allows a ENTITYUNRELATED of ENTITYUNRELATED to be inferred from the existence of an instance of a pattern of ENTITY .
Node-based inference rules can be constructed in a semantic network using a variant of a predicate calculus notation .	predicate calculus notation	Node-based inference rules	usage	{'e1': {'word': 'predicate calculus notation', 'word_index': [(12, 12)], 'id': 'T78-1031.17'}, 'e2': {'word': 'Node-based inference rules', 'word_index': [(0, 0)], 'id': 'T78-1031.15'}}	ENTITYOTHER can be constructed in a ENTITYUNRELATED using a variant of a ENTITY .
Path-based inference is more efficient, while node-based inference is more general.	Path-based inference	node-based inference	compare	{'e1': {'word': 'Path-based inference', 'word_index': [(0, 0)], 'id': 'T78-1031.18'}, 'e2': {'word': 'node-based inference', 'word_index': [(6, 6)], 'id': 'T78-1031.19'}}	ENTITY is more efficient , while ENTITYOTHER is more general .
Applications of path-based inference rules to the representation of the extensional equivalence of intensional concepts , and to the explication of inheritance in hierarchies are sketched.	inheritance	hierarchies	model-feature	{'e1': {'word': 'inheritance', 'word_index': [(17, 17)], 'id': 'T78-1031.24'}, 'e2': {'word': 'hierarchies', 'word_index': [(19, 19)], 'id': 'T78-1031.25'}}	Applications of ENTITYUNRELATED to the representation of the ENTITYUNRELATED of ENTITYUNRELATED , and to the ENTITYUNRELATED of ENTITY in ENTITYOTHER are sketched .
By using commands or rules which are defined to facilitate the construction of format expected or some mathematical expressions , elaborate and pretty documents can be successfully obtained.	rules	mathematical expressions	usage	{'e1': {'word': 'rules', 'word_index': [(4, 4)], 'id': 'C80-1039.7'}, 'e2': {'word': 'mathematical expressions', 'word_index': [(17, 17)], 'id': 'C80-1039.8'}}	By using commands or ENTITY which are defined to facilitate the construction of format expected or some ENTITYOTHER , elaborate and pretty documents can be successfully obtained .
 An attempt has been made to use an Augmented Transition Network as a procedural dialog model .	Augmented Transition Network	dialog model	usage	{'e1': {'word': 'Augmented Transition Network', 'word_index': [(8, 8)], 'id': 'C80-1073.1'}, 'e2': {'word': 'dialog model', 'word_index': [(12, 12)], 'id': 'C80-1073.2'}}	An attempt has been made to use an ENTITY as a procedural ENTITYOTHER .
The development of such a model appears to be important in several respects: as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs .	conversation analysis	dialog schemata	topic	{'e1': {'word': 'conversation analysis', 'word_index': [(27, 27)], 'id': 'C80-1073.5'}, 'e2': {'word': 'dialog schemata', 'word_index': [(23, 23)], 'id': 'C80-1073.4'}}	The development of such a ENTITYUNRELATED appears to be important in several respects : as a device to represent and to use different ENTITYOTHER proposed in empirical ENTITY ; as a device to represent and to use ENTITYUNRELATED ; as a device combining knowledge about ENTITYUNRELATED and about ENTITYUNRELATED with knowledge about ENTITYUNRELATED .
A standard ATN should be further developed in order to account for the verbal interactions of task-oriented dialogs .	verbal interactions	task-oriented dialogs	part_whole	{'e1': {'word': 'verbal interactions', 'word_index': [(13, 13)], 'id': 'C80-1073.11'}, 'e2': {'word': 'task-oriented dialogs', 'word_index': [(15, 15)], 'id': 'C80-1073.12'}}	A standard ENTITYUNRELATED should be further developed in order to account for the ENTITY of ENTITYOTHER .
 Interpreting metaphors is an integral and inescapable process in human understanding of natural language .	metaphors	human understanding of natural language	part_whole	{'e1': {'word': 'metaphors', 'word_index': [(1, 1)], 'id': 'P80-1004.1'}, 'e2': {'word': 'human understanding of natural language', 'word_index': [(9, 9)], 'id': 'P80-1004.2'}}	Interpreting ENTITY is an integral and inescapable process in ENTITYOTHER .
This paper discusses a method of analyzing metaphors based on the existence of a small number of generalized metaphor mappings .	generalized metaphor mappings	method of analyzing metaphors	usage	{'e1': {'word': 'generalized metaphor mappings', 'word_index': [(14, 14)], 'id': 'P80-1004.4'}, 'e2': {'word': 'method of analyzing metaphors', 'word_index': [(4, 4)], 'id': 'P80-1004.3'}}	This paper discusses a ENTITYOTHER based on the existence of a small number of ENTITY .
Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component .	recognition network	generalized metaphor	part_whole	{'e1': {'word': 'recognition network', 'word_index': [(4, 4)], 'id': 'P80-1004.6'}, 'e2': {'word': 'generalized metaphor', 'word_index': [(1, 1)], 'id': 'P80-1004.5'}}	Each ENTITYOTHER contains a ENTITY , a ENTITYUNRELATED , additional ENTITYUNRELATED , and an ENTITYUNRELATED .
 Current natural language interfaces have concentrated largely on determining the literal meaning of input from their users .	meaning	input	model-feature	{'e1': {'word': 'meaning', 'word_index': [(9, 9)], 'id': 'P80-1019.2'}, 'e2': {'word': 'input', 'word_index': [(11, 11)], 'id': 'P80-1019.3'}}	Current ENTITYUNRELATED have concentrated largely on determining the literal ENTITY of ENTITYOTHER from their ENTITYUNRELATED .
While such decoding is an essential underpinning, much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous non-literal aspects of communication , such as robust communication procedures .	non-literal aspects of communication	decoding	usage	{'e1': {'word': 'non-literal aspects of communication', 'word_index': [(25, 25)], 'id': 'P80-1019.7'}, 'e2': {'word': 'decoding', 'word_index': [(2, 2)], 'id': 'P80-1019.5'}}	While such ENTITYOTHER is an essential underpinning , much recent work suggests that ENTITYUNRELATED will never appear cooperative or graceful unless they also incorporate numerous ENTITY , such as robust ENTITYUNRELATED .
This paper defends that view, but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful personal computers with integral graphics displays offers techniques superior to those of humans for these aspects, while still satisfying human communication needs .	graphics displays	personal computers	part_whole	{'e1': {'word': 'graphics displays', 'word_index': [(35, 35)], 'id': 'P80-1019.11'}, 'e2': {'word': 'personal computers', 'word_index': [(32, 32)], 'id': 'P80-1019.10'}}	This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these ENTITYUNRELATED ; that the new technology of powerful ENTITYOTHER with integral ENTITY offers techniques superior to those of humans for these aspects , while still satisfying ENTITYUNRELATED .
The paper proposes interfaces based on a judicious mixture of these techniques and the still valuable methods of more traditional natural language interfaces.	interfaces	natural language interfaces	compare	{'e1': {'word': 'interfaces', 'word_index': [(3, 3)], 'id': 'P80-1019.13'}, 'e2': {'word': 'natural language interfaces', 'word_index': [(20, 20)], 'id': 'P80-1019.14'}}	The paper proposes ENTITY based on a judicious mixture of these techniques and the still valuable methods of more traditional ENTITYOTHER .
We go, on to describe FlexP , a bottom-up pattern-matching parser that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited-domain computer system.	bottom-up pattern-matching parser	restricted natural language	usage	{'e1': {'word': 'bottom-up pattern-matching parser', 'word_index': [(9, 9)], 'id': 'P80-1026.8'}, 'e2': {'word': 'restricted natural language', 'word_index': [(21, 21)], 'id': 'P80-1026.9'}}	We go , on to describe ENTITYUNRELATED , a ENTITY that we have designed and implemented to provide these flexibilities for ENTITYOTHER input to a limited - domain computer system .
 This paper proposes a series of modifications to the left corner parsing algorithm for context-free grammars .	left corner parsing algorithm	context-free grammars	usage	{'e1': {'word': 'left corner parsing algorithm', 'word_index': [(9, 9)], 'id': 'C82-1054.1'}, 'e2': {'word': 'context-free grammars', 'word_index': [(11, 11)], 'id': 'C82-1054.2'}}	This paper proposes a series of modifications to the ENTITY for ENTITYOTHER .
It is argued that the resulting algorithm is both efficient and flexible and is, therefore, a good choice for the parser used in a natural language interface .	parser	natural language interface	usage	{'e1': {'word': 'parser', 'word_index': [(22, 22)], 'id': 'C82-1054.3'}, 'e2': {'word': 'natural language interface', 'word_index': [(26, 26)], 'id': 'C82-1054.4'}}	It is argued that the resulting algorithm is both efficient and flexible and is , therefore , a good choice for the ENTITY used in a ENTITYOTHER .
The system is implemented entirely in Prolog , a programming language based on logic .	logic	programming language	usage	{'e1': {'word': 'logic', 'word_index': [(12, 12)], 'id': 'J82-3002.6'}, 'e2': {'word': 'programming language', 'word_index': [(9, 9)], 'id': 'J82-3002.5'}}	The system is implemented entirely in ENTITYUNRELATED , a ENTITYOTHER based on ENTITY .
With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic .	Chat-80	English questions	usage	{'e1': {'word': 'Chat-80', 'word_index': [(9, 9)], 'id': 'J82-3002.9'}, 'e2': {'word': 'English questions', 'word_index': [(11, 11)], 'id': 'J82-3002.10'}}	With the aid of a ENTITYUNRELATED called ENTITYUNRELATED , ENTITY translates ENTITYOTHER into the ENTITYUNRELATED ENTITYUNRELATED .
With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic .	subset of logic	Prolog	part_whole	{'e1': {'word': 'subset of logic', 'word_index': [(15, 15)], 'id': 'J82-3002.12'}, 'e2': {'word': 'Prolog', 'word_index': [(14, 14)], 'id': 'J82-3002.11'}}	With the aid of a ENTITYUNRELATED called ENTITYUNRELATED , ENTITYUNRELATED translates ENTITYUNRELATED into the ENTITYOTHER ENTITY .
query optimisation in a relational database .	query optimisation	relational database	usage	{'e1': {'word': 'query optimisation', 'word_index': [(0, 0)], 'id': 'J82-3002.16'}, 'e2': {'word': 'relational database', 'word_index': [(3, 3)], 'id': 'J82-3002.17'}}	ENTITY in a ENTITYOTHER .
However, a great deal of natural language texts e.g., memos , rough drafts , conversation transcripts etc., have features that differ significantly from neat texts , posing special problems for readers, such as misspelled words , missing words , poor syntactic construction , missing periods , etc.	conversation transcripts	neat texts	compare	{'e1': {'word': 'conversation transcripts', 'word_index': [(14, 14)], 'id': 'P82-1035.8'}, 'e2': {'word': 'neat texts', 'word_index': [(23, 23)], 'id': 'P82-1035.9'}}	However , a great deal of ENTITYUNRELATED e.g. , ENTITYUNRELATED , rough ENTITYUNRELATED , ENTITY etc. , have features that differ significantly from ENTITYOTHER , posing special problems for readers , such as ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , etc.
Our solution to these problems is to make use of expectations , based both on knowledge of surface English and on world knowledge of the situation being described.	surface English	expectations	usage	{'e1': {'word': 'surface English', 'word_index': [(17, 17)], 'id': 'P82-1035.15'}, 'e2': {'word': 'expectations', 'word_index': [(10, 10)], 'id': 'P82-1035.14'}}	Our solution to these problems is to make use of ENTITYOTHER , based both on knowledge of ENTITY and on ENTITYUNRELATED of the situation being described .
These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ).	context	unknown words	model-feature	{'e1': {'word': 'context', 'word_index': [(10, 10)], 'id': 'P82-1035.19'}, 'e2': {'word': 'unknown words', 'word_index': [(8, 8)], 'id': 'P82-1035.18'}}	These ENTITYUNRELATED can be used to figure out ENTITYOTHER from ENTITY , constrain the possible ENTITYUNRELATED of ENTITYUNRELATED ( ENTITYUNRELATED ) , fill in ENTITYUNRELATED ( ENTITYUNRELATED ) , and resolve ENTITYUNRELATED ( ENTITYUNRELATED ) .
These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ).	word-senses	words with multiple meanings	model-feature	{'e1': {'word': 'word-senses', 'word_index': [(15, 15)], 'id': 'P82-1035.20'}, 'e2': {'word': 'words with multiple meanings', 'word_index': [(17, 17)], 'id': 'P82-1035.21'}}	These ENTITYUNRELATED can be used to figure out ENTITYUNRELATED from ENTITYUNRELATED , constrain the possible ENTITY of ENTITYOTHER ( ENTITYUNRELATED ) , fill in ENTITYUNRELATED ( ENTITYUNRELATED ) , and resolve ENTITYUNRELATED ( ENTITYUNRELATED ) .
These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ).	missing words	ellipsis	part_whole	{'e1': {'word': 'missing words', 'word_index': [(24, 24)], 'id': 'P82-1035.23'}, 'e2': {'word': 'ellipsis', 'word_index': [(26, 26)], 'id': 'P82-1035.24'}}	These ENTITYUNRELATED can be used to figure out ENTITYUNRELATED from ENTITYUNRELATED , constrain the possible ENTITYUNRELATED of ENTITYUNRELATED ( ENTITYUNRELATED ) , fill in ENTITY ( ENTITYOTHER ) , and resolve ENTITYUNRELATED ( ENTITYUNRELATED ) .
These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ).	referents	anaphora	part_whole	{'e1': {'word': 'referents', 'word_index': [(31, 31)], 'id': 'P82-1035.25'}, 'e2': {'word': 'anaphora', 'word_index': [(33, 33)], 'id': 'P82-1035.26'}}	These ENTITYUNRELATED can be used to figure out ENTITYUNRELATED from ENTITYUNRELATED , constrain the possible ENTITYUNRELATED of ENTITYUNRELATED ( ENTITYUNRELATED ) , fill in ENTITYUNRELATED ( ENTITYUNRELATED ) , and resolve ENTITY ( ENTITYOTHER ) .
This method of using expectations to aid the understanding of scruffy texts has been incorporated into a working computer program called NOMAD , which understands scruffy texts in the domain of Navy messages.	expectations	scruffy texts	usage	{'e1': {'word': 'expectations', 'word_index': [(4, 4)], 'id': 'P82-1035.27'}, 'e2': {'word': 'scruffy texts', 'word_index': [(10, 10)], 'id': 'P82-1035.28'}}	This method of using ENTITY to aid the understanding of ENTITYOTHER has been incorporated into a working ENTITYUNRELATED called ENTITYUNRELATED , which understands ENTITYUNRELATED in the domain of Navy messages .
 This abstract describes a natural language system which deals usefully with ungrammatical input and describes some actual and potential applications of it in computer aided second language learning .	natural language system	ungrammatical input	usage	{'e1': {'word': 'natural language system', 'word_index': [(4, 4)], 'id': 'P84-1020.1'}, 'e2': {'word': 'ungrammatical input', 'word_index': [(9, 9)], 'id': 'P84-1020.2'}}	This abstract describes a ENTITY which deals usefully with ENTITYOTHER and describes some actual and potential applications of it in ENTITYUNRELATED .
For English-Japanese machine translation , the syntax directed approach is effective where the Heuristic Parsing Model (HPM) and the Syntactic Role System play important roles.	syntax directed approach	English-Japanese machine translation	usage	{'e1': {'word': 'syntax directed approach', 'word_index': [(4, 4)], 'id': 'P84-1034.5'}, 'e2': {'word': 'English-Japanese machine translation', 'word_index': [(1, 1)], 'id': 'P84-1034.4'}}	For ENTITYOTHER , the ENTITY is effective where the ENTITYUNRELATED and the ENTITYUNRELATED play important roles .
For Japanese-English translation , the semantics directed approach is powerful where the Conceptual Dependency Diagram (CDD) and the Augmented Case Marker System (which is a kind of Semantic Role System ) play essential roles.	Japanese-English translation	semantics directed approach	usage	{'e1': {'word': 'Japanese-English translation', 'word_index': [(1, 1)], 'id': 'P84-1034.8'}, 'e2': {'word': 'semantics directed approach', 'word_index': [(4, 4)], 'id': 'P84-1034.9'}}	For ENTITY , the ENTITYOTHER is powerful where the ENTITYUNRELATED and the ENTITYUNRELATED ( which is a kind of ENTITYUNRELATED ) play essential roles .
Some examples of the difference between Japanese sentence structure and English sentence structure , which is vital to machine translation are also discussed together with various interesting ambiguities .	Japanese sentence structure	English sentence structure	compare	{'e1': {'word': 'Japanese sentence structure', 'word_index': [(6, 6)], 'id': 'P84-1034.13'}, 'e2': {'word': 'English sentence structure', 'word_index': [(8, 8)], 'id': 'P84-1034.14'}}	Some examples of the difference between ENTITY and ENTITYOTHER , which is vital to ENTITYUNRELATED are also discussed together with various interesting ENTITYUNRELATED .
In this approach, the definitions of the structure and surface representation of domain entities are grouped together.	surface representation	domain entities	model-feature	{'e1': {'word': 'surface representation', 'word_index': [(10, 10)], 'id': 'P84-1047.3'}, 'e2': {'word': 'domain entities', 'word_index': [(12, 12)], 'id': 'P84-1047.4'}}	In this approach , the definitions of the ENTITYUNRELATED and ENTITY of ENTITYOTHER are grouped together .
In addition, it facilitates fragmentary recognition and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input .	multiple parsing strategies	recognition of extra-grammatical input	usage	{'e1': {'word': 'multiple parsing strategies', 'word_index': [(10, 10)], 'id': 'P84-1047.8'}, 'e2': {'word': 'recognition of extra-grammatical input', 'word_index': [(19, 19)], 'id': 'P84-1047.9'}}	In addition , it facilitates ENTITYUNRELATED and the use of ENTITY , and so is particularly useful for robust ENTITYOTHER .
Representative samples from an entity-oriented language definition are presented, along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses .	control structure	entity-oriented parser	usage	{'e1': {'word': 'control structure', 'word_index': [(11, 11)], 'id': 'P84-1047.12'}, 'e2': {'word': 'entity-oriented parser', 'word_index': [(14, 14)], 'id': 'P84-1047.13'}}	Representative samples from an ENTITYUNRELATED are presented , along with a ENTITY for an ENTITYOTHER , some ENTITYUNRELATED that use the ENTITYUNRELATED , and worked examples of ENTITYUNRELATED .
Representative samples from an entity-oriented language definition are presented, along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses .	control structure	parsing strategies	usage	{'e1': {'word': 'control structure', 'word_index': [(21, 21)], 'id': 'P84-1047.15'}, 'e2': {'word': 'parsing strategies', 'word_index': [(17, 17)], 'id': 'P84-1047.14'}}	Representative samples from an ENTITYUNRELATED are presented , along with a ENTITYUNRELATED for an ENTITYUNRELATED , some ENTITYOTHER that use the ENTITY , and worked examples of ENTITYUNRELATED .
A parser incorporating the control structure and the parsing strategies is currently under implementation .	control structure	parser	part_whole	{'e1': {'word': 'control structure', 'word_index': [(4, 4)], 'id': 'P84-1047.18'}, 'e2': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'P84-1047.17'}}	A ENTITYOTHER incorporating the ENTITY and the ENTITYUNRELATED is currently under ENTITYUNRELATED .
An idea which underlies the theory described in this paper is that a disposition may be viewed as a proposition with implicit fuzzy quantifiers which are approximations to all and always, e.g., almost all, almost always, most, frequently, etc.	fuzzy quantifiers	proposition	part_whole	{'e1': {'word': 'fuzzy quantifiers', 'word_index': [(22, 22)], 'id': 'P84-1064.7'}, 'e2': {'word': 'proposition', 'word_index': [(19, 19)], 'id': 'P84-1064.6'}}	An idea which underlies the theory described in this paper is that a ENTITYUNRELATED may be viewed as a ENTITYOTHER with implicit ENTITY which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc.
For example, birds can fly may be interpreted as the result of suppressing the fuzzy quantifier most in the proposition most birds can fly.	fuzzy quantifier	proposition	part_whole	{'e1': {'word': 'fuzzy quantifier', 'word_index': [(15, 15)], 'id': 'P84-1064.8'}, 'e2': {'word': 'proposition', 'word_index': [(19, 19)], 'id': 'P84-1064.9'}}	For example , birds can fly may be interpreted as the result of suppressing the ENTITY most in the ENTITYOTHER most birds can fly .
Explicitation sets the stage for representing the meaning of a proposition through the use of test-score semantics (Zadeh, 1978, 1982).	meaning	proposition	model-feature	{'e1': {'word': 'meaning', 'word_index': [(7, 7)], 'id': 'P84-1064.15'}, 'e2': {'word': 'proposition', 'word_index': [(10, 10)], 'id': 'P84-1064.16'}}	ENTITYUNRELATED sets the stage for representing the ENTITY of a ENTITYOTHER through the use of ENTITYUNRELATED ( Zadeh , 1978 , 1982 ) .
In this approach to semantics , the meaning of a proposition , p, is represented as a procedure which tests, scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to reasoning with dispositions which is based on the concept of a fuzzy syllogism .	meaning	proposition	model-feature	{'e1': {'word': 'meaning', 'word_index': [(7, 7)], 'id': 'P84-1064.19'}, 'e2': {'word': 'proposition', 'word_index': [(10, 10)], 'id': 'P84-1064.20'}}	In this approach to ENTITYUNRELATED , the ENTITY of a ENTITYOTHER , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to ENTITYUNRELATED which is based on the concept of a ENTITYUNRELATED .
In this approach to semantics , the meaning of a proposition , p, is represented as a procedure which tests, scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to reasoning with dispositions which is based on the concept of a fuzzy syllogism .	fuzzy syllogism	reasoning with dispositions	usage	{'e1': {'word': 'fuzzy syllogism', 'word_index': [(52, 52)], 'id': 'P84-1064.22'}, 'e2': {'word': 'reasoning with dispositions', 'word_index': [(43, 43)], 'id': 'P84-1064.21'}}	In this approach to ENTITYUNRELATED , the ENTITYUNRELATED of a ENTITYUNRELATED , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to ENTITYOTHER which is based on the concept of a ENTITY .
Syllogistic reasoning with dispositions has an important bearing on commonsense reasoning as well as on the management of uncertainty in expert systems .	Syllogistic reasoning with dispositions	commonsense reasoning	result	{'e1': {'word': 'Syllogistic reasoning with dispositions', 'word_index': [(0, 0)], 'id': 'P84-1064.23'}, 'e2': {'word': 'commonsense reasoning', 'word_index': [(6, 6)], 'id': 'P84-1064.24'}}	ENTITY has an important bearing on ENTITYOTHER as well as on the ENTITYUNRELATED in ENTITYUNRELATED .
Syllogistic reasoning with dispositions has an important bearing on commonsense reasoning as well as on the management of uncertainty in expert systems .	management of uncertainty	expert systems	part_whole	{'e1': {'word': 'management of uncertainty', 'word_index': [(12, 12)], 'id': 'P84-1064.25'}, 'e2': {'word': 'expert systems', 'word_index': [(14, 14)], 'id': 'P84-1064.26'}}	ENTITYUNRELATED has an important bearing on ENTITYUNRELATED as well as on the ENTITY in ENTITYOTHER .
As a simple application of the techniques described in this paper, we formulate a definition of typicality -- a concept which plays an important role in human cognition and is of relevance to default reasoning .	typicality	human cognition	result	{'e1': {'word': 'typicality', 'word_index': [(17, 17)], 'id': 'P84-1064.27'}, 'e2': {'word': 'human cognition', 'word_index': [(27, 27)], 'id': 'P84-1064.28'}}	As a simple application of the techniques described in this paper , we formulate a definition of ENTITY -- a concept which plays an important role in ENTITYOTHER and is of relevance to ENTITYUNRELATED .
 This report describes Paul , a computer text generation system designed to create cohesive text through the use of lexical substitutions .	lexical substitutions	computer text generation system	usage	{'e1': {'word': 'lexical substitutions', 'word_index': [(15, 15)], 'id': 'P84-1078.4'}, 'e2': {'word': 'computer text generation system', 'word_index': [(6, 6)], 'id': 'P84-1078.2'}}	This report describes ENTITYUNRELATED , a ENTITYOTHER designed to create ENTITYUNRELATED through the use of ENTITY .
Specifically, this system is designed to deterministically choose between pronominalization , superordinate substitution , and definite noun phrase reiteration .	pronominalization	superordinate substitution	compare	{'e1': {'word': 'pronominalization', 'word_index': [(10, 10)], 'id': 'P84-1078.5'}, 'e2': {'word': 'superordinate substitution', 'word_index': [(12, 12)], 'id': 'P84-1078.6'}}	Specifically , this system is designed to deterministically choose between ENTITY , ENTITYOTHER , and definite ENTITYUNRELATED .
The system identifies a strength of antecedence recovery for each of the lexical substitutions , and matches them against the strength of potential antecedence of each element in the text to select the proper substitutions for these elements.	antecedence recovery	lexical substitutions	model-feature	{'e1': {'word': 'antecedence recovery', 'word_index': [(6, 6)], 'id': 'P84-1078.8'}, 'e2': {'word': 'lexical substitutions', 'word_index': [(11, 11)], 'id': 'P84-1078.9'}}	The system identifies a strength of ENTITY for each of the ENTITYOTHER , and matches them against the ENTITYUNRELATED of each element in the ENTITYUNRELATED to select the proper ENTITYUNRELATED for these elements .
The system identifies a strength of antecedence recovery for each of the lexical substitutions , and matches them against the strength of potential antecedence of each element in the text to select the proper substitutions for these elements.	strength of potential antecedence	substitutions	model-feature	{'e1': {'word': 'strength of potential antecedence', 'word_index': [(18, 18)], 'id': 'P84-1078.10'}, 'e2': {'word': 'substitutions', 'word_index': [(29, 29)], 'id': 'P84-1078.12'}}	The system identifies a strength of ENTITYUNRELATED for each of the ENTITYUNRELATED , and matches them against the ENTITY of each element in the ENTITYUNRELATED to select the proper ENTITYOTHER for these elements .
 Determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the global meaning  of a sentence , even if not in a precise way.	meaning	utterance	model-feature	{'e1': {'word': 'meaning', 'word_index': [(8, 8)], 'id': 'C86-1081.2'}, 'e2': {'word': 'utterance', 'word_index': [(11, 11)], 'id': 'C86-1081.3'}}	ENTITYUNRELATED play an important role in conveying the ENTITY of an ENTITYOTHER , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the ENTITYUNRELATED of a ENTITYUNRELATED , even if not in a precise way .
 Determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the global meaning  of a sentence , even if not in a precise way.	global meaning 	sentence	model-feature	{'e1': {'word': 'global meaning ', 'word_index': [(32, 32)], 'id': 'C86-1081.4'}, 'e2': {'word': 'sentence', 'word_index': [(35, 35)], 'id': 'C86-1081.5'}}	ENTITYUNRELATED play an important role in conveying the ENTITYUNRELATED of an ENTITYUNRELATED , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the ENTITY of a ENTITYOTHER , even if not in a precise way .
Another problem with determiners is their inherent ambiguity .	ambiguity	determiners	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(7, 7)], 'id': 'C86-1081.7'}, 'e2': {'word': 'determiners', 'word_index': [(3, 3)], 'id': 'C86-1081.6'}}	Another problem with ENTITYOTHER is their inherent ENTITY .
In this paper we propose a logical formalism , which, among other things, is suitable for representing determiners without forcing a particular interpretation when their meaning is still not clear.	interpretation	meaning	model-feature	{'e1': {'word': 'interpretation', 'word_index': [(23, 23)], 'id': 'C86-1081.10'}, 'e2': {'word': 'meaning', 'word_index': [(26, 26)], 'id': 'C86-1081.11'}}	In this paper we propose a ENTITYUNRELATED , which , among other things , is suitable for representing ENTITYUNRELATED without forcing a particular ENTITY when their ENTITYOTHER is still not clear .
 This paper describes a system ( RAREAS ) which synthesizes marine weather forecasts directly from formatted weather data .	RAREAS	formatted weather data	usage	{'e1': {'word': 'RAREAS', 'word_index': [(6, 6)], 'id': 'C86-1132.1'}, 'e2': {'word': 'formatted weather data', 'word_index': [(15, 15)], 'id': 'C86-1132.2'}}	This paper describes a system ( ENTITY ) which synthesizes marine weather forecasts directly from ENTITYOTHER .
Such synthesis appears feasible in certain natural sublanguages with stereotyped text structure .	stereotyped text structure	natural sublanguages	model-feature	{'e1': {'word': 'stereotyped text structure', 'word_index': [(8, 8)], 'id': 'C86-1132.5'}, 'e2': {'word': 'natural sublanguages', 'word_index': [(6, 6)], 'id': 'C86-1132.4'}}	Such ENTITYUNRELATED appears feasible in certain ENTITYOTHER with ENTITY .
RAREAS draws on several kinds of linguistic and non-linguistic knowledge and mirrors a forecaster&apos;s apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events.	linguistic and non-linguistic knowledge	RAREAS	usage	{'e1': {'word': 'linguistic and non-linguistic knowledge', 'word_index': [(6, 6)], 'id': 'C86-1132.7'}, 'e2': {'word': 'RAREAS', 'word_index': [(0, 0)], 'id': 'C86-1132.6'}}	ENTITYOTHER draws on several kinds of ENTITY and mirrors a forecaster&apos ;s apparent tendency to ascribe less precise ENTITYUNRELATED to more remote meteorological events .
 A method for error correction of ill-formed input is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs.	error correction	ill-formed input	usage	{'e1': {'word': 'error correction', 'word_index': [(3, 3)], 'id': 'J86-1002.1'}, 'e2': {'word': 'ill-formed input', 'word_index': [(5, 5)], 'id': 'J86-1002.2'}}	A method for ENTITY of ENTITYOTHER is described that acquires ENTITYUNRELATED in typical usage and uses these ENTITYUNRELATED to predict new inputs .
A dialogue acquisition and tracking algorithm is presented along with a description of its implementation in a voice interactive system .	dialogue acquisition and tracking algorithm	voice interactive system	part_whole	{'e1': {'word': 'dialogue acquisition and tracking algorithm', 'word_index': [(1, 1)], 'id': 'J86-1002.9'}, 'e2': {'word': 'voice interactive system', 'word_index': [(13, 13)], 'id': 'J86-1002.11'}}	A ENTITY is presented along with a description of its ENTITYUNRELATED in a ENTITYOTHER .
 In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse .	processing	discourse	result	{'e1': {'word': 'processing', 'word_index': [(15, 15)], 'id': 'J86-3001.3'}, 'e2': {'word': 'discourse', 'word_index': [(17, 17)], 'id': 'J86-3001.4'}}	In this paper we explore a new ENTITYUNRELATED that stresses the role of ENTITYUNRELATED and ENTITY in ENTITYOTHER .
In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of purposes (called the intentional structure ), and the state of focus of attention (called the attentional state ).	linguistic structure	utterances	model-feature	{'e1': {'word': 'linguistic structure', 'word_index': [(24, 24)], 'id': 'J86-3001.7'}, 'e2': {'word': 'utterances', 'word_index': [(20, 20)], 'id': 'J86-3001.6'}}	In this theory , ENTITYUNRELATED is composed of three separate but interrelated components : the structure of the sequence of ENTITYOTHER ( called the ENTITY ) , a structure of ENTITYUNRELATED ( called the ENTITYUNRELATED ) , and the state of ENTITYUNRELATED ( called the ENTITYUNRELATED ) .
In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of purposes (called the intentional structure ), and the state of focus of attention (called the attentional state ).	intentional structure	purposes	model-feature	{'e1': {'word': 'intentional structure', 'word_index': [(34, 34)], 'id': 'J86-3001.9'}, 'e2': {'word': 'purposes', 'word_index': [(30, 30)], 'id': 'J86-3001.8'}}	In this theory , ENTITYUNRELATED is composed of three separate but interrelated components : the structure of the sequence of ENTITYUNRELATED ( called the ENTITYUNRELATED ) , a structure of ENTITYOTHER ( called the ENTITY ) , and the state of ENTITYUNRELATED ( called the ENTITYUNRELATED ) .
In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of purposes (called the intentional structure ), and the state of focus of attention (called the attentional state ).	attentional state	focus of attention	model-feature	{'e1': {'word': 'attentional state', 'word_index': [(45, 45)], 'id': 'J86-3001.11'}, 'e2': {'word': 'focus of attention', 'word_index': [(41, 41)], 'id': 'J86-3001.10'}}	In this theory , ENTITYUNRELATED is composed of three separate but interrelated components : the structure of the sequence of ENTITYUNRELATED ( called the ENTITYUNRELATED ) , a structure of ENTITYUNRELATED ( called the ENTITYUNRELATED ) , and the state of ENTITYOTHER ( called the ENTITY ) .
The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate.	utterances	discourse	part_whole	{'e1': {'word': 'utterances', 'word_index': [(11, 11)], 'id': 'J86-3001.14'}, 'e2': {'word': 'discourse', 'word_index': [(7, 7)], 'id': 'J86-3001.13'}}	The ENTITYUNRELATED consists of segments of the ENTITYOTHER into which the ENTITY naturally aggregate .
The intentional structure captures the discourse-relevant purposes , expressed in each of the linguistic segments as well as relationships among them.	intentional structure	discourse-relevant purposes	model-feature	{'e1': {'word': 'intentional structure', 'word_index': [(1, 1)], 'id': 'J86-3001.15'}, 'e2': {'word': 'discourse-relevant purposes', 'word_index': [(4, 4)], 'id': 'J86-3001.16'}}	The ENTITY captures the ENTITYOTHER , expressed in each of the ENTITYUNRELATED as well as relationships among them .
The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds.	attentional state	focus of attention	model-feature	{'e1': {'word': 'attentional state', 'word_index': [(1, 1)], 'id': 'J86-3001.18'}, 'e2': {'word': 'focus of attention', 'word_index': [(7, 7)], 'id': 'J86-3001.19'}}	The ENTITY is an abstraction of the ENTITYOTHER of the ENTITYUNRELATED as the ENTITYUNRELATED unfolds .
The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses .	theory of attention, intention, and aggregation of utterances	discourses	model-feature	{'e1': {'word': 'theory of attention, intention, and aggregation of utterances', 'word_index': [(1, 1)], 'id': 'J86-3001.28'}, 'e2': {'word': 'discourses', 'word_index': [(12, 12)], 'id': 'J86-3001.29'}}	The ENTITY is illustrated in the paper with a number of example ENTITYOTHER .
This theory provides a framework for describing the processing of utterances in a discourse .	utterances	discourse	part_whole	{'e1': {'word': 'utterances', 'word_index': [(10, 10)], 'id': 'J86-3001.35'}, 'e2': {'word': 'discourse', 'word_index': [(13, 13)], 'id': 'J86-3001.36'}}	This ENTITYUNRELATED provides a framework for describing the processing of ENTITY in a ENTITYOTHER .
Discourse processing requires recognizing how the utterances of the discourse aggregate into segments , recognizing the intentions expressed in the discourse and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state .	utterances	discourse	part_whole	{'e1': {'word': 'utterances', 'word_index': [(5, 5)], 'id': 'J86-3001.38'}, 'e2': {'word': 'discourse', 'word_index': [(8, 8)], 'id': 'J86-3001.39'}}	ENTITYUNRELATED requires recognizing how the ENTITY of the ENTITYOTHER aggregate into ENTITYUNRELATED , recognizing the ENTITYUNRELATED expressed in the ENTITYUNRELATED and the relationships among ENTITYUNRELATED , and tracking the ENTITYUNRELATED through the operation of the mechanisms associated with ENTITYUNRELATED .
Discourse processing requires recognizing how the utterances of the discourse aggregate into segments , recognizing the intentions expressed in the discourse and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state .	intentions	discourse	part_whole	{'e1': {'word': 'intentions', 'word_index': [(15, 15)], 'id': 'J86-3001.41'}, 'e2': {'word': 'discourse', 'word_index': [(19, 19)], 'id': 'J86-3001.42'}}	ENTITYUNRELATED requires recognizing how the ENTITYUNRELATED of the ENTITYUNRELATED aggregate into ENTITYUNRELATED , recognizing the ENTITY expressed in the ENTITYOTHER and the relationships among ENTITYUNRELATED , and tracking the ENTITYUNRELATED through the operation of the mechanisms associated with ENTITYUNRELATED .
 The goal of this work is the enrichment of human-machine interactions in a natural language environment .	natural language environment	human-machine interactions	model-feature	{'e1': {'word': 'natural language environment', 'word_index': [(12, 12)], 'id': 'J86-4002.2'}, 'e2': {'word': 'human-machine interactions', 'word_index': [(9, 9)], 'id': 'J86-4002.1'}}	The goal of this work is the enrichment of ENTITYOTHER in a ENTITY .
Because a speaker and listener cannot be assured to have the same beliefs , contexts , perceptions , backgrounds , or goals , at each point in a conversation , difficulties and mistakes arise when a listener interprets a speaker&apos;s utterance .	speaker	listener	compare	{'e1': {'word': 'speaker', 'word_index': [(2, 2)], 'id': 'J86-4002.3'}, 'e2': {'word': 'listener', 'word_index': [(4, 4)], 'id': 'J86-4002.4'}}	Because a ENTITY and ENTITYOTHER cannot be assured to have the same ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , or ENTITYUNRELATED , at each point in a ENTITYUNRELATED , difficulties and mistakes arise when a ENTITYUNRELATED interprets a ENTITYUNRELATED .
 We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .	Tree Adjoining Grammars	Head Grammars	compare	{'e1': {'word': 'Tree Adjoining Grammars', 'word_index': [(9, 9)], 'id': 'P86-1011.2'}, 'e2': {'word': 'Head Grammars', 'word_index': [(11, 11)], 'id': 'P86-1011.3'}}	We examine the relationship between the two ENTITYUNRELATED : ENTITY and ENTITYOTHER .
We briefly investigate the weak equivalence of the two formalisms .	equivalence	formalisms	model-feature	{'e1': {'word': 'equivalence', 'word_index': [(5, 5)], 'id': 'P86-1011.4'}, 'e2': {'word': 'formalisms', 'word_index': [(9, 9)], 'id': 'P86-1011.5'}}	We briefly investigate the weak ENTITY of the two ENTITYOTHER .
We then turn to a discussion comparing the linguistic expressiveness of the two formalisms .	linguistic expressiveness	formalisms	model-feature	{'e1': {'word': 'linguistic expressiveness', 'word_index': [(8, 8)], 'id': 'P86-1011.6'}, 'e2': {'word': 'formalisms', 'word_index': [(12, 12)], 'id': 'P86-1011.7'}}	We then turn to a discussion comparing the ENTITY of the two ENTITYOTHER .
 Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects .	features	linguistic objects	model-feature	{'e1': {'word': 'features', 'word_index': [(6, 6)], 'id': 'P86-1038.2'}, 'e2': {'word': 'linguistic objects', 'word_index': [(9, 9)], 'id': 'P86-1038.3'}}	ENTITYUNRELATED use structures containing sets of ENTITY to describe ENTITYOTHER .
We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them.	feature structures	logical formulas	model-feature	{'e1': {'word': 'feature structures', 'word_index': [(9, 9)], 'id': 'P86-1038.7'}, 'e2': {'word': 'logical formulas', 'word_index': [(14, 14)], 'id': 'P86-1038.8'}}	We have developed a ENTITYUNRELATED in which descriptions of ENTITY can be regarded as ENTITYOTHER , and interpreted by sets of ENTITYUNRELATED which satisfy them .
These graphs are, in fact, transition graphs for a special type of deterministic finite automaton .	transition graphs	deterministic finite automaton	part_whole	{'e1': {'word': 'transition graphs', 'word_index': [(7, 7)], 'id': 'P86-1038.11'}, 'e2': {'word': 'deterministic finite automaton', 'word_index': [(13, 13)], 'id': 'P86-1038.12'}}	These ENTITYUNRELATED are , in fact , ENTITY for a special type of ENTITYOTHER .
This semantics for feature structures extends the ideas of Pereira and Shieber [11], by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	semantics	feature structures	model-feature	{'e1': {'word': 'semantics', 'word_index': [(1, 1)], 'id': 'P86-1038.13'}, 'e2': {'word': 'feature structures', 'word_index': [(3, 3)], 'id': 'P86-1038.14'}}	This ENTITY for ENTITYOTHER extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by ENTITYUNRELATED and ENTITYUNRELATED embedded within ENTITYUNRELATED .
This semantics for feature structures extends the ideas of Pereira and Shieber [11], by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	path values	disjunctions	part_whole	{'e1': {'word': 'path values', 'word_index': [(27, 27)], 'id': 'P86-1038.16'}, 'e2': {'word': 'disjunctions', 'word_index': [(30, 30)], 'id': 'P86-1038.17'}}	This ENTITYUNRELATED for ENTITYUNRELATED extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by ENTITYUNRELATED and ENTITY embedded within ENTITYOTHER .
Our interpretation differs from that of Pereira and Shieber by using a logical model in place of a denotational semantics .	logical model	denotational semantics	compare	{'e1': {'word': 'logical model', 'word_index': [(12, 12)], 'id': 'P86-1038.18'}, 'e2': {'word': 'denotational semantics', 'word_index': [(17, 17)], 'id': 'P86-1038.19'}}	Our interpretation differs from that of Pereira and Shieber by using a ENTITY in place of a ENTITYOTHER .
This logical model yields a calculus of equivalences , which can be used to simplify formulas .	logical model	formulas	usage	{'e1': {'word': 'logical model', 'word_index': [(1, 1)], 'id': 'P86-1038.20'}, 'e2': {'word': 'formulas', 'word_index': [(14, 14)], 'id': 'P86-1038.22'}}	This ENTITY yields a calculus of ENTITYUNRELATED , which can be used to simplify ENTITYOTHER .
Our model allows a careful examination of the computational complexity of unification .	computational complexity	unification	model-feature	{'e1': {'word': 'computational complexity', 'word_index': [(8, 8)], 'id': 'P86-1038.25'}, 'e2': {'word': 'unification', 'word_index': [(10, 10)], 'id': 'P86-1038.26'}}	Our ENTITYUNRELATED allows a careful examination of the ENTITY of ENTITYOTHER .
We have shown that the consistency problem for formulas with disjunctive values is NP-complete .	consistency problem	formulas	model-feature	{'e1': {'word': 'consistency problem', 'word_index': [(5, 5)], 'id': 'P86-1038.27'}, 'e2': {'word': 'formulas', 'word_index': [(7, 7)], 'id': 'P86-1038.28'}}	We have shown that the ENTITY for ENTITYOTHER with ENTITYUNRELATED is ENTITYUNRELATED .
Multimedia answers include videodisc images and heuristically-produced complete sentences in text or text-to-speech form .	text	text-to-speech form	compare	{'e1': {'word': 'text', 'word_index': [(10, 10)], 'id': 'A88-1001.7'}, 'e2': {'word': 'text-to-speech form', 'word_index': [(12, 12)], 'id': 'A88-1001.8'}}	ENTITYUNRELATED include ENTITYUNRELATED and heuristically - produced complete ENTITYUNRELATED in ENTITY or ENTITYOTHER .
Deictic reference and feedback about the discourse are enabled.	feedback	discourse	model-feature	{'e1': {'word': 'feedback', 'word_index': [(2, 2)], 'id': 'A88-1001.10'}, 'e2': {'word': 'discourse', 'word_index': [(5, 5)], 'id': 'A88-1001.11'}}	ENTITYUNRELATED and ENTITY about the ENTITYOTHER are enabled .
 In this paper, we describe the pronominal anaphora resolution module of Lucy , a portable English understanding system .	pronominal anaphora resolution module	Lucy	part_whole	{'e1': {'word': 'pronominal anaphora resolution module', 'word_index': [(7, 7)], 'id': 'A88-1003.1'}, 'e2': {'word': 'Lucy', 'word_index': [(9, 9)], 'id': 'A88-1003.2'}}	In this paper , we describe the ENTITY of ENTITYOTHER , a portable ENTITYUNRELATED .
Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose candidate antecedents and to evaluate each other&apos;s proposals.	partial theories	blackboard-like architecture	part_whole	{'e1': {'word': 'partial theories', 'word_index': [(9, 9)], 'id': 'A88-1003.6'}, 'e2': {'word': 'blackboard-like architecture', 'word_index': [(5, 5)], 'id': 'A88-1003.5'}}	Thus we have implemented a ENTITYOTHER in which individual ENTITY can be encoded as separate modules that can interact to propose candidate ENTITYUNRELATED and to evaluate each other&apos ;s proposals .
 This paper discusses the application of Unification Categorial Grammar (UCG) to the framework of Isomorphic Grammars for Machine Translation pioneered by Landsbergen.	Unification Categorial Grammar (UCG)	Machine Translation	usage	{'e1': {'word': 'Unification Categorial Grammar (UCG)', 'word_index': [(6, 6)], 'id': 'C88-1007.1'}, 'e2': {'word': 'Machine Translation', 'word_index': [(13, 13)], 'id': 'C88-1007.3'}}	This paper discusses the application of ENTITY to the framework of ENTITYUNRELATED for ENTITYOTHER pioneered by Landsbergen .
The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel, in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations .	grammars	Source and Target languages	model-feature	{'e1': {'word': 'grammars', 'word_index': [(5, 5)], 'id': 'C88-1007.5'}, 'e2': {'word': 'Source and Target languages', 'word_index': [(8, 8)], 'id': 'C88-1007.6'}}	The ENTITYUNRELATED involves developing the ENTITY of the ENTITYOTHER in parallel , in order to ensure that ENTITYUNRELATED and ENTITYUNRELATED expressions which stand in the ENTITYUNRELATED have ENTITYUNRELATED .
The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel, in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations .	SL	TL	compare	{'e1': {'word': 'SL', 'word_index': [(17, 17)], 'id': 'C88-1007.7'}, 'e2': {'word': 'TL', 'word_index': [(19, 19)], 'id': 'C88-1007.8'}}	The ENTITYUNRELATED involves developing the ENTITYUNRELATED of the ENTITYUNRELATED in parallel , in order to ensure that ENTITY and ENTITYOTHER expressions which stand in the ENTITYUNRELATED have ENTITYUNRELATED .
Semantic and other information may still be incorporated, but as constraints on the translation relation , not as levels of textual representation .	translation relation	textual representation	compare	{'e1': {'word': 'translation relation', 'word_index': [(14, 14)], 'id': 'C88-1007.13'}, 'e2': {'word': 'textual representation', 'word_index': [(20, 20)], 'id': 'C88-1007.14'}}	ENTITYUNRELATED and other information may still be incorporated , but as constraints on the ENTITY , not as levels of ENTITYOTHER .
 This paper presents necessary and sufficient conditions for the use of demonstrative expressions in English and discusses implications for current discourse processing algorithms .	demonstrative expressions	English	part_whole	{'e1': {'word': 'demonstrative expressions', 'word_index': [(11, 11)], 'id': 'C88-1044.1'}, 'e2': {'word': 'English', 'word_index': [(13, 13)], 'id': 'C88-1044.2'}}	This paper presents necessary and sufficient conditions for the use of ENTITY in ENTITYOTHER and discusses implications for current ENTITYUNRELATED .
We examine a broad range of texts to show how the distribution of demonstrative forms and functions is genre dependent .	demonstrative forms and functions	texts	part_whole	{'e1': {'word': 'demonstrative forms and functions', 'word_index': [(13, 13)], 'id': 'C88-1044.5'}, 'e2': {'word': 'texts', 'word_index': [(6, 6)], 'id': 'C88-1044.4'}}	We examine a broad range of ENTITYOTHER to show how the distribution of ENTITY is ENTITYUNRELATED .
CCRs are Boolean conditions on the cooccurrence of categories in local trees which allow the statement of generalizations which cannot be captured in other current syntax formalisms .	Boolean conditions	categories	model-feature	{'e1': {'word': 'Boolean conditions', 'word_index': [(2, 2)], 'id': 'C88-1066.4'}, 'e2': {'word': 'categories', 'word_index': [(7, 7)], 'id': 'C88-1066.5'}}	ENTITYUNRELATED are ENTITY on the cooccurrence of ENTITYOTHER in ENTITYUNRELATED which allow the ENTITYUNRELATED which cannot be captured in other current ENTITYUNRELATED .
The use of CCRs leads to syntactic descriptions formulated entirely with restrictive statements .	restrictive statements	syntactic descriptions	model-feature	{'e1': {'word': 'restrictive statements', 'word_index': [(10, 10)], 'id': 'C88-1066.11'}, 'e2': {'word': 'syntactic descriptions', 'word_index': [(6, 6)], 'id': 'C88-1066.10'}}	The use of ENTITYUNRELATED leads to ENTITYOTHER formulated entirely with ENTITY .
The paper shows how conventional algorithms for the analysis of context free languages can be adapted to the CCR formalism .	context free languages	CCR formalism	compare	{'e1': {'word': 'context free languages', 'word_index': [(10, 10)], 'id': 'C88-1066.12'}, 'e2': {'word': 'CCR formalism', 'word_index': [(16, 16)], 'id': 'C88-1066.13'}}	The paper shows how conventional algorithms for the analysis of ENTITY can be adapted to the ENTITYOTHER .
Special attention is given to the part of the parser that checks the fulfillment of logical well-formedness conditions on trees .	logical well-formedness conditions	trees	model-feature	{'e1': {'word': 'logical well-formedness conditions', 'word_index': [(15, 15)], 'id': 'C88-1066.15'}, 'e2': {'word': 'trees', 'word_index': [(17, 17)], 'id': 'C88-1066.16'}}	Special attention is given to the part of the ENTITYUNRELATED that checks the fulfillment of ENTITY on ENTITYOTHER .
By reappraising these insightful counterexamples, the inferential theory for natural language presuppositions described in Mercer 1987, 1988 gives a simple and straightforward explanation for the presuppositional nature of these sentences .	presuppositional nature	sentences	model-feature	{'e1': {'word': 'presuppositional nature', 'word_index': [(22, 22)], 'id': 'C88-2086.3'}, 'e2': {'word': 'sentences', 'word_index': [(25, 25)], 'id': 'C88-2086.4'}}	By reappraising these insightful counterexamples , the ENTITYUNRELATED described in Mercer 1987 , 1988 gives a simple and straightforward explanation for the ENTITY of these ENTITYOTHER .
 We have developed a computational model of the process of describing the layout of an apartment or house, a much-studied discourse task first characterized linguistically by Linde (1974).	computational model	discourse task	usage	{'e1': {'word': 'computational model', 'word_index': [(4, 4)], 'id': 'C88-2130.1'}, 'e2': {'word': 'discourse task', 'word_index': [(22, 22)], 'id': 'C88-2130.2'}}	We have developed a ENTITY of the process of describing the layout of an apartment or house , a much - studied ENTITYOTHER first characterized linguistically by Linde ( 1974 ) .
The model is embodied in a program, APT , that can reproduce segments of actual tape-recorded descriptions, using organizational and discourse strategies derived through analysis of our corpus .	model	APT	part_whole	{'e1': {'word': 'model', 'word_index': [(1, 1)], 'id': 'C88-2130.3'}, 'e2': {'word': 'APT', 'word_index': [(8, 8)], 'id': 'C88-2130.4'}}	The ENTITY is embodied in a program , ENTITYOTHER , that can reproduce segments of actual tape - recorded descriptions , using ENTITYUNRELATED derived through analysis of our ENTITYUNRELATED .
So, for any place where the easily identifiable fragments occur in the sentence , the process will extend to both the left and the right of the islands , until possibly completely missing fragments are reached.	fragments	sentence	part_whole	{'e1': {'word': 'fragments', 'word_index': [(9, 9)], 'id': 'C88-2132.7'}, 'e2': {'word': 'sentence', 'word_index': [(13, 13)], 'id': 'C88-2132.8'}}	So , for any place where the easily identifiable ENTITY occur in the ENTITYOTHER , the process will extend to both the left and the right of the ENTITYUNRELATED , until possibly completely missing ENTITYUNRELATED are reached .
This paper presents a new interactive disambiguation scheme based on the paraphrasing of a parser &apos;s multiple output.	parser	paraphrasing	result	{'e1': {'word': 'parser', 'word_index': [(12, 12)], 'id': 'C88-2160.10'}, 'e2': {'word': 'paraphrasing', 'word_index': [(9, 9)], 'id': 'C88-2160.9'}}	This paper presents a new ENTITYUNRELATED based on the ENTITYOTHER of a ENTITY &apos ;s multiple output .
Some examples of paraphrasing ambiguous sentences are presented.	paraphrasing	sentences	model-feature	{'e1': {'word': 'paraphrasing', 'word_index': [(3, 3)], 'id': 'C88-2160.11'}, 'e2': {'word': 'sentences', 'word_index': [(5, 5)], 'id': 'C88-2160.12'}}	Some examples of ENTITY ambiguous ENTITYOTHER are presented .
For one thing, learning methodology applicable in general domains does not readily lend itself in the linguistic domain .	general domains	linguistic domain	compare	{'e1': {'word': 'general domains', 'word_index': [(7, 7)], 'id': 'C88-2162.3'}, 'e2': {'word': 'linguistic domain', 'word_index': [(15, 15)], 'id': 'C88-2162.4'}}	For one thing , ENTITYUNRELATED applicable in ENTITY does not readily lend itself in the ENTITYOTHER .
For another, linguistic representation used by language processing systems is not geared to learning .	linguistic representation	language processing systems	usage	{'e1': {'word': 'linguistic representation', 'word_index': [(3, 3)], 'id': 'C88-2162.5'}, 'e2': {'word': 'language processing systems', 'word_index': [(6, 6)], 'id': 'C88-2162.6'}}	For another , ENTITY used by ENTITYOTHER is not geared to ENTITYUNRELATED .
We introduced a new linguistic representation , the Dynamic Hierarchical Phrasal Lexicon (DHPL) [Zernik88], to facilitate language acquisition .	Dynamic Hierarchical Phrasal Lexicon (DHPL)	language acquisition	usage	{'e1': {'word': 'Dynamic Hierarchical Phrasal Lexicon (DHPL)', 'word_index': [(7, 7)], 'id': 'C88-2162.9'}, 'e2': {'word': 'language acquisition', 'word_index': [(15, 15)], 'id': 'C88-2162.10'}}	We introduced a new ENTITYUNRELATED , the ENTITY [ Zernik 88 ] , to facilitate ENTITYOTHER .
From this, a language learning model was implemented in the program RINA , which enhances its own lexical hierarchy by processing examples in context.	lexical hierarchy	language learning model	part_whole	{'e1': {'word': 'lexical hierarchy', 'word_index': [(16, 16)], 'id': 'C88-2162.13'}, 'e2': {'word': 'language learning model', 'word_index': [(4, 4)], 'id': 'C88-2162.11'}}	From this , a ENTITYOTHER was implemented in the program ENTITYUNRELATED , which enhances its own ENTITY by processing examples in context .
We identified two tasks: First, how linguistic concepts are acquired from training examples and organized in a hierarchy ; this task was discussed in previous papers [Zernik87].	linguistic concepts	training examples	model-feature	{'e1': {'word': 'linguistic concepts', 'word_index': [(8, 8)], 'id': 'C88-2162.14'}, 'e2': {'word': 'training examples', 'word_index': [(12, 12)], 'id': 'C88-2162.15'}}	We identified two tasks : First , how ENTITY are acquired from ENTITYOTHER and organized in a ENTITYUNRELATED ; this task was discussed in previous papers [ Zernik 87 ] .
Second, we show in this paper how a lexical hierarchy is used in predicting new linguistic concepts .	lexical hierarchy	linguistic concepts	usage	{'e1': {'word': 'lexical hierarchy', 'word_index': [(9, 9)], 'id': 'C88-2162.17'}, 'e2': {'word': 'linguistic concepts', 'word_index': [(15, 15)], 'id': 'C88-2162.18'}}	Second , we show in this paper how a ENTITY is used in predicting new ENTITYOTHER .
Thus, a program does not stall even in the presence of a lexical unknown , and a hypothesis can be produced for covering that lexical gap .	hypothesis	lexical unknown	model-feature	{'e1': {'word': 'hypothesis', 'word_index': [(17, 17)], 'id': 'C88-2162.21'}, 'e2': {'word': 'lexical unknown', 'word_index': [(13, 13)], 'id': 'C88-2162.20'}}	Thus , a ENTITYUNRELATED does not stall even in the presence of a ENTITYOTHER , and a ENTITY can be produced for covering that ENTITYUNRELATED .
 Although every natural language system needs a computational lexicon , each system puts different amounts and types of information into its lexicon according to its individual needs.	computational lexicon	natural language system	usage	{'e1': {'word': 'computational lexicon', 'word_index': [(5, 5)], 'id': 'C88-2166.2'}, 'e2': {'word': 'natural language system', 'word_index': [(2, 2)], 'id': 'C88-2166.1'}}	Although every ENTITYOTHER needs a ENTITY , each system puts different amounts and types of information into its ENTITYUNRELATED according to its individual needs .
This paper presents our experience in planning and building COMPLEX , a computational lexicon designed to be a repository of shared lexical information for use by Natural Language Processing (NLP) systems .	shared lexical information	computational lexicon	part_whole	{'e1': {'word': 'shared lexical information', 'word_index': [(19, 19)], 'id': 'C88-2166.6'}, 'e2': {'word': 'computational lexicon', 'word_index': [(12, 12)], 'id': 'C88-2166.5'}}	This paper presents our experience in planning and building ENTITYUNRELATED , a ENTITYOTHER designed to be a repository of ENTITY for use by ENTITYUNRELATED .
We have drawn primarily on explicit and implicit information from machine-readable dictionaries (MRD&apos;s) to create a broad coverage lexicon .	machine-readable dictionaries (MRD&apos;s)	broad coverage lexicon	usage	{'e1': {'word': 'machine-readable dictionaries (MRD&apos;s)', 'word_index': [(10, 10)], 'id': 'C88-2166.8'}, 'e2': {'word': 'broad coverage lexicon', 'word_index': [(14, 14)], 'id': 'C88-2166.9'}}	We have drawn primarily on explicit and implicit information from ENTITY to create a ENTITYOTHER .
This paper explores the role of user modeling in such systems .	user modeling	systems	result	{'e1': {'word': 'user modeling', 'word_index': [(6, 6)], 'id': 'J88-3002.4'}, 'e2': {'word': 'systems', 'word_index': [(9, 9)], 'id': 'J88-3002.5'}}	This paper explores the role of ENTITY in such ENTITYOTHER .
The types of information that a user model may be required to keep about a user are then identified and discussed.	user model	user	model-feature	{'e1': {'word': 'user model', 'word_index': [(6, 6)], 'id': 'J88-3002.7'}, 'e2': {'word': 'user', 'word_index': [(14, 14)], 'id': 'J88-3002.8'}}	The types of information that a ENTITY may be required to keep about a ENTITYOTHER are then identified and discussed .
Since acquiring the knowledge for a user model is a fundamental problem in user modeling , a section is devoted to this topic.	user model	user modeling	part_whole	{'e1': {'word': 'user model', 'word_index': [(6, 6)], 'id': 'J88-3002.10'}, 'e2': {'word': 'user modeling', 'word_index': [(12, 12)], 'id': 'J88-3002.11'}}	Since acquiring the knowledge for a ENTITY is a fundamental problem in ENTITYOTHER , a section is devoted to this topic .
 This article introduces a bidirectional grammar generation system called feature structure-directed generation , developed for a dialogue translation system .	feature structure-directed generation	dialogue translation system	usage	{'e1': {'word': 'feature structure-directed generation', 'word_index': [(6, 6)], 'id': 'C90-1013.2'}, 'e2': {'word': 'dialogue translation system', 'word_index': [(11, 11)], 'id': 'C90-1013.3'}}	This article introduces a ENTITYUNRELATED called ENTITY , developed for a ENTITYOTHER .
The system utilizes typed feature structures to control the top-down derivation in a declarative way.	typed feature structures	top-down derivation	usage	{'e1': {'word': 'typed feature structures', 'word_index': [(3, 3)], 'id': 'C90-1013.4'}, 'e2': {'word': 'top-down derivation', 'word_index': [(7, 7)], 'id': 'C90-1013.5'}}	The system utilizes ENTITY to control the ENTITYOTHER in a declarative way .
This generation system also uses disjunctive feature structures to reduce the number of copies of the derivation tree .	disjunctive feature structures	generation system	usage	{'e1': {'word': 'disjunctive feature structures', 'word_index': [(4, 4)], 'id': 'C90-1013.7'}, 'e2': {'word': 'generation system', 'word_index': [(1, 1)], 'id': 'C90-1013.6'}}	This ENTITYOTHER also uses ENTITY to reduce the number of copies of the ENTITYUNRELATED .
The grammar for this generator is designed to properly generate the speaker&apos;s intention in a telephone dialogue .	grammar	generator	usage	{'e1': {'word': 'grammar', 'word_index': [(1, 1)], 'id': 'C90-1013.9'}, 'e2': {'word': 'generator', 'word_index': [(4, 4)], 'id': 'C90-1013.10'}}	The ENTITY for this ENTITYOTHER is designed to properly generate the ENTITYUNRELATED in a ENTITYUNRELATED .
 This paper proposes document oriented preference sets(DoPS) for the disambiguation of the dependency structure of sentences .	dependency structure	sentences	model-feature	{'e1': {'word': 'dependency structure', 'word_index': [(9, 9)], 'id': 'C90-2032.2'}, 'e2': {'word': 'sentences', 'word_index': [(11, 11)], 'id': 'C90-2032.3'}}	This paper proposes ENTITYUNRELATED for the disambiguation of the ENTITY of ENTITYOTHER .
The DoPS system extracts preference knowledge from a target document or other documents automatically.	DoPS system	target document	usage	{'e1': {'word': 'DoPS system', 'word_index': [(1, 1)], 'id': 'C90-2032.4'}, 'e2': {'word': 'target document', 'word_index': [(7, 7)], 'id': 'C90-2032.5'}}	The ENTITY extracts preference knowledge from a ENTITYOTHER or other ENTITYUNRELATED automatically .
Implementation and empirical results are described for the the analysis of dependency structures of Japanese patent claim sentences .	dependency structures	Japanese patent claim sentences	model-feature	{'e1': {'word': 'dependency structures', 'word_index': [(10, 10)], 'id': 'C90-2032.11'}, 'e2': {'word': 'Japanese patent claim sentences', 'word_index': [(12, 12)], 'id': 'C90-2032.12'}}	ENTITYUNRELATED and ENTITYUNRELATED are described for the the analysis of ENTITY of ENTITYOTHER .
 This paper describes the framework of a Korean phonological knowledge base system using the unification-based grammar formalism : Korean Phonology Structure Grammar (KPSG) .	unification-based grammar formalism	Korean phonological knowledge base system	usage	{'e1': {'word': 'unification-based grammar formalism', 'word_index': [(10, 10)], 'id': 'C90-3014.2'}, 'e2': {'word': 'Korean phonological knowledge base system', 'word_index': [(7, 7)], 'id': 'C90-3014.1'}}	This paper describes the framework of a ENTITYOTHER using the ENTITY : ENTITYUNRELATED .
The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system .	KPSG	phonological system	usage	{'e1': {'word': 'KPSG', 'word_index': [(3, 3)], 'id': 'C90-3014.4'}, 'e2': {'word': 'phonological system', 'word_index': [(13, 13)], 'id': 'C90-3014.5'}}	The approach of ENTITY provides an explicit development model for constructing a computational ENTITYOTHER : ENTITYUNRELATED and ENTITYUNRELATED .
 The unique properties of tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax , for instance, to the task of semantic interpretation or automatic translation of natural language .	syntax	semantic interpretation	compare	{'e1': {'word': 'syntax', 'word_index': [(18, 18)], 'id': 'C90-3045.3'}, 'e2': {'word': 'semantic interpretation', 'word_index': [(27, 27)], 'id': 'C90-3045.4'}}	The unique properties of ENTITYUNRELATED present a challenge for the application of ENTITYUNRELATED beyond the limited confines of ENTITY , for instance , to the task of ENTITYOTHER or ENTITYUNRELATED .
The formalism&apos;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper .	logical form language	semantics	model-feature	{'e1': {'word': 'logical form language', 'word_index': [(16, 16)], 'id': 'C90-3045.11'}, 'e2': {'word': 'semantics', 'word_index': [(12, 12)], 'id': 'C90-3045.10'}}	The formalism&apos ;s intended usage is to relate ENTITYUNRELATED to their associated ENTITYOTHER represented in a ENTITY , or to their ENTITYUNRELATED in another ENTITYUNRELATED ; in summary , we intend it to allow ENTITYUNRELATED to be used beyond their role in ENTITYUNRELATED .
The formalism&apos;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper .	translates	natural language	model-feature	{'e1': {'word': 'translates', 'word_index': [(21, 21)], 'id': 'C90-3045.12'}, 'e2': {'word': 'natural language', 'word_index': [(24, 24)], 'id': 'C90-3045.13'}}	The formalism&apos ;s intended usage is to relate ENTITYUNRELATED to their associated ENTITYUNRELATED represented in a ENTITYUNRELATED , or to their ENTITY in another ENTITYOTHER ; in summary , we intend it to allow ENTITYUNRELATED to be used beyond their role in ENTITYUNRELATED .
The formalism&apos;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper .	TAGs	syntax proper	usage	{'e1': {'word': 'TAGs', 'word_index': [(34, 34)], 'id': 'C90-3045.14'}, 'e2': {'word': 'syntax proper', 'word_index': [(42, 42)], 'id': 'C90-3045.15'}}	The formalism&apos ;s intended usage is to relate ENTITYUNRELATED to their associated ENTITYUNRELATED represented in a ENTITYUNRELATED , or to their ENTITYUNRELATED in another ENTITYUNRELATED ; in summary , we intend it to allow ENTITY to be used beyond their role in ENTITYOTHER .
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	sentence analysis	defeasible reasoning	model-feature	{'e1': {'word': 'sentence analysis', 'word_index': [(4, 4)], 'id': 'C90-3046.1'}, 'e2': {'word': 'defeasible reasoning', 'word_index': [(9, 9)], 'id': 'C90-3046.2'}}	This paper proposes that ENTITY should be treated as ENTITYOTHER , and presents such a treatment for ENTITYUNRELATED using an ENTITYUNRELATED by Konolige , which is a ENTITYUNRELATED of ENTITYUNRELATED , that includes ENTITYUNRELATED and ENTITYUNRELATED that capture ENTITYUNRELATED .
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	argumentation system	Japanese sentence analyses	usage	{'e1': {'word': 'argumentation system', 'word_index': [(20, 20)], 'id': 'C90-3046.4'}, 'e2': {'word': 'Japanese sentence analyses', 'word_index': [(17, 17)], 'id': 'C90-3046.3'}}	This paper proposes that ENTITYUNRELATED should be treated as ENTITYUNRELATED , and presents such a treatment for ENTITYOTHER using an ENTITY by Konolige , which is a ENTITYUNRELATED of ENTITYUNRELATED , that includes ENTITYUNRELATED and ENTITYUNRELATED that capture ENTITYUNRELATED .
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	formalization	defeasible reasoning	model-feature	{'e1': {'word': 'formalization', 'word_index': [(27, 27)], 'id': 'C90-3046.5'}, 'e2': {'word': 'defeasible reasoning', 'word_index': [(29, 29)], 'id': 'C90-3046.6'}}	This paper proposes that ENTITYUNRELATED should be treated as ENTITYUNRELATED , and presents such a treatment for ENTITYUNRELATED using an ENTITYUNRELATED by Konolige , which is a ENTITY of ENTITYOTHER , that includes ENTITYUNRELATED and ENTITYUNRELATED that capture ENTITYUNRELATED .
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .	defeat rules	defeasibility	model-feature	{'e1': {'word': 'defeat rules', 'word_index': [(35, 35)], 'id': 'C90-3046.8'}, 'e2': {'word': 'defeasibility', 'word_index': [(38, 38)], 'id': 'C90-3046.9'}}	This paper proposes that ENTITYUNRELATED should be treated as ENTITYUNRELATED , and presents such a treatment for ENTITYUNRELATED using an ENTITYUNRELATED by Konolige , which is a ENTITYUNRELATED of ENTITYUNRELATED , that includes ENTITYUNRELATED and ENTITY that capture ENTITYOTHER .
 Spelling-checkers have become an integral part of most text processing software .	Spelling-checkers	text processing software	part_whole	{'e1': {'word': 'Spelling-checkers', 'word_index': [(0, 0)], 'id': 'C90-3072.1'}, 'e2': {'word': 'text processing software', 'word_index': [(8, 8)], 'id': 'C90-3072.2'}}	ENTITY have become an integral part of most ENTITYOTHER .
From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words .	dictionaries of word forms	words	compare	{'e1': {'word': 'dictionaries of word forms', 'word_index': [(15, 15)], 'id': 'C90-3072.3'}, 'e2': {'word': 'words', 'word_index': [(18, 18)], 'id': 'C90-3072.4'}}	From different reasons among which the speed of processing prevails they are usually based on ENTITY instead of ENTITYOTHER .
This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .	inflection	English	model-feature	{'e1': {'word': 'inflection', 'word_index': [(8, 8)], 'id': 'C90-3072.5'}, 'e2': {'word': 'English', 'word_index': [(11, 11)], 'id': 'C90-3072.6'}}	This approach is sufficient for languages with little ENTITY such as ENTITYOTHER , but fails for ENTITYUNRELATED such as ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED or other ENTITYUNRELATED .
This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .	highly inflective languages	Czech	model-feature	{'e1': {'word': 'highly inflective languages', 'word_index': [(16, 16)], 'id': 'C90-3072.7'}, 'e2': {'word': 'Czech', 'word_index': [(19, 19)], 'id': 'C90-3072.8'}}	This approach is sufficient for languages with little ENTITYUNRELATED such as ENTITYUNRELATED , but fails for ENTITY such as ENTITYOTHER , ENTITYUNRELATED , ENTITYUNRELATED or other ENTITYUNRELATED .
The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million (for Czech ).	spelling-checkers	English	usage	{'e1': {'word': 'spelling-checkers', 'word_index': [(16, 16)], 'id': 'C90-3072.14'}, 'e2': {'word': 'English', 'word_index': [(18, 18)], 'id': 'C90-3072.15'}}	The speed of the resulting program lies somewhere in the middle of the scale of existing ENTITY for ENTITYOTHER and the main ENTITYUNRELATED fits into the standard ENTITYUNRELATED , whereas the number of recognized ENTITYUNRELATED exceeds 6 million ( for ENTITYUNRELATED ) .
The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million (for Czech ).	word forms	Czech	part_whole	{'e1': {'word': 'word forms', 'word_index': [(34, 34)], 'id': 'C90-3072.18'}, 'e2': {'word': 'Czech', 'word_index': [(40, 40)], 'id': 'C90-3072.19'}}	The speed of the resulting program lies somewhere in the middle of the scale of existing ENTITYUNRELATED for ENTITYUNRELATED and the main ENTITYUNRELATED fits into the standard ENTITYUNRELATED , whereas the number of recognized ENTITY exceeds 6 million ( for ENTITYOTHER ) .
To avoid grammar coverage problems we use a fully-connected first-order statistical class grammar .	fully-connected first-order statistical class grammar	grammar coverage problems	usage	{'e1': {'word': 'fully-connected first-order statistical class grammar', 'word_index': [(6, 6)], 'id': 'H90-1016.4'}, 'e2': {'word': 'grammar coverage problems', 'word_index': [(2, 2)], 'id': 'H90-1016.3'}}	To avoid ENTITYOTHER we use a ENTITY .
The speech-search algorithm is implemented on a board with a single Intel i860 chip , which provides a factor of 5 speed-up over a SUN 4 for straight C code .	Intel i860 chip	board	model-feature	{'e1': {'word': 'Intel i860 chip', 'word_index': [(10, 10)], 'id': 'H90-1016.7'}, 'e2': {'word': 'board', 'word_index': [(6, 6)], 'id': 'H90-1016.6'}}	The ENTITYUNRELATED is implemented on a ENTITYOTHER with a single ENTITY , which provides a factor of 5 speed - up over a ENTITYUNRELATED for ENTITYUNRELATED .
The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and application back end .	VME bus	SUN4	part_whole	{'e1': {'word': 'VME bus', 'word_index': [(6, 6)], 'id': 'H90-1016.11'}, 'e2': {'word': 'SUN4', 'word_index': [(9, 9)], 'id': 'H90-1016.12'}}	The ENTITYUNRELATED plugs directly into the ENTITY of the ENTITYOTHER , which controls the system and contains the ENTITYUNRELATED and ENTITYUNRELATED .
First, we present a new paradigm for speaker-independent (SI) training of hidden Markov models (HMM) , which uses a large amount of speech from a few speakers instead of the traditional practice of using a little speech from many speakers .	speech	speaker-independent (SI) training	usage	{'e1': {'word': 'speech', 'word_index': [(18, 18)], 'id': 'H90-1060.4'}, 'e2': {'word': 'speaker-independent (SI) training', 'word_index': [(8, 8)], 'id': 'H90-1060.2'}}	First , we present a new paradigm for ENTITYOTHER of ENTITYUNRELATED , which uses a large amount of ENTITY from a few ENTITYUNRELATED instead of the traditional practice of using a little ENTITYUNRELATED from many ENTITYUNRELATED .
In addition, combination of the training speakers is done by averaging the statistics of independently trained models rather than the usual pooling of all the speech data from many speakers prior to training .	statistics	speech data	compare	{'e1': {'word': 'statistics', 'word_index': [(12, 12)], 'id': 'H90-1060.9'}, 'e2': {'word': 'speech data', 'word_index': [(23, 23)], 'id': 'H90-1060.11'}}	In addition , combination of the ENTITYUNRELATED is done by averaging the ENTITY of ENTITYUNRELATED rather than the usual pooling of all the ENTITYOTHER from many ENTITYUNRELATED prior to ENTITYUNRELATED .
With only 12 training speakers for SI recognition , we achieved a 7.5% word error rate on a standard grammar and test set from the DARPA Resource Management corpus .	training speakers	word error rate	result	{'e1': {'word': 'training speakers', 'word_index': [(3, 3)], 'id': 'H90-1060.14'}, 'e2': {'word': 'word error rate', 'word_index': [(12, 12)], 'id': 'H90-1060.16'}}	With only 12 ENTITY for ENTITYUNRELATED , we achieved a 7.5 % ENTITYOTHER on a standard ENTITYUNRELATED and ENTITYUNRELATED from the ENTITYUNRELATED .
With only 12 training speakers for SI recognition , we achieved a 7.5% word error rate on a standard grammar and test set from the DARPA Resource Management corpus .	test set	DARPA Resource Management corpus	part_whole	{'e1': {'word': 'test set', 'word_index': [(18, 18)], 'id': 'H90-1060.18'}, 'e2': {'word': 'DARPA Resource Management corpus', 'word_index': [(21, 21)], 'id': 'H90-1060.19'}}	With only 12 ENTITYUNRELATED for ENTITYUNRELATED , we achieved a 7.5 % ENTITYUNRELATED on a standard ENTITYUNRELATED and ENTITY from the ENTITYOTHER .
Second, we show a significant improvement for speaker adaptation (SA) using the new SI corpus and a small amount of speech from the new (target) speaker .	SI corpus	speaker adaptation (SA)	usage	{'e1': {'word': 'SI corpus', 'word_index': [(12, 12)], 'id': 'H90-1060.23'}, 'e2': {'word': 'speaker adaptation (SA)', 'word_index': [(8, 8)], 'id': 'H90-1060.22'}}	Second , we show a significant improvement for ENTITYOTHER using the new ENTITY and a small amount of ENTITYUNRELATED from the new ( target ) ENTITYUNRELATED .
A probabilistic spectral mapping is estimated independently for each training (reference) speaker and the target speaker .	probabilistic spectral mapping	training (reference) speaker	model-feature	{'e1': {'word': 'probabilistic spectral mapping', 'word_index': [(1, 1)], 'id': 'H90-1060.26'}, 'e2': {'word': 'training (reference) speaker', 'word_index': [(7, 7)], 'id': 'H90-1060.27'}}	A ENTITY is estimated independently for each ENTITYOTHER and the ENTITYUNRELATED .
Each reference model is transformed to the space of the target speaker and combined by averaging .	averaging	reference model	usage	{'e1': {'word': 'averaging', 'word_index': [(13, 13)], 'id': 'H90-1060.32'}, 'e2': {'word': 'reference model', 'word_index': [(1, 1)], 'id': 'H90-1060.29'}}	Each ENTITYOTHER is transformed to the ENTITYUNRELATED of the ENTITYUNRELATED and combined by ENTITY .
Using only 40 utterances from the target speaker for adaptation , the error rate dropped to 4.1% --- a 45% reduction in error compared to the SI result.	utterances	adaptation	usage	{'e1': {'word': 'utterances', 'word_index': [(3, 3)], 'id': 'H90-1060.33'}, 'e2': {'word': 'adaptation', 'word_index': [(8, 8)], 'id': 'H90-1060.35'}}	Using only 40 ENTITY from the ENTITYUNRELATED for ENTITYOTHER , the ENTITYUNRELATED dropped to 4.1 % --- a 45 % reduction in error compared to the ENTITYUNRELATED result .
 This paper presents a specialized editor for a highly structured dictionary .	editor	dictionary	usage	{'e1': {'word': 'editor', 'word_index': [(5, 5)], 'id': 'J90-3002.1'}, 'e2': {'word': 'dictionary', 'word_index': [(10, 10)], 'id': 'J90-3002.2'}}	This paper presents a specialized ENTITY for a highly structured ENTITYOTHER .
The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent dictionary on the basis of a linguistic theory .	editor	lexicologists	usage	{'e1': {'word': 'editor', 'word_index': [(6, 6)], 'id': 'J90-3002.3'}, 'e2': {'word': 'lexicologists', 'word_index': [(15, 15)], 'id': 'J90-3002.4'}}	The basic goal in building that ENTITY was to provide an adequate tool to help ENTITYOTHER produce a valid and coherent ENTITYUNRELATED on the basis of a ENTITYUNRELATED .
The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent dictionary on the basis of a linguistic theory .	linguistic theory	dictionary	usage	{'e1': {'word': 'linguistic theory', 'word_index': [(27, 27)], 'id': 'J90-3002.6'}, 'e2': {'word': 'dictionary', 'word_index': [(21, 21)], 'id': 'J90-3002.5'}}	The basic goal in building that ENTITYUNRELATED was to provide an adequate tool to help ENTITYUNRELATED produce a valid and coherent ENTITYOTHER on the basis of a ENTITY .
If we want valuable lexicons and grammars to achieve complex natural language processing , we must provide very powerful tools to help create and ensure the validity of such complex linguistic databases .	grammars	natural language processing	usage	{'e1': {'word': 'grammars', 'word_index': [(6, 6)], 'id': 'J90-3002.8'}, 'e2': {'word': 'natural language processing', 'word_index': [(10, 10)], 'id': 'J90-3002.9'}}	If we want valuable ENTITYUNRELATED and ENTITY to achieve complex ENTITYOTHER , we must provide very powerful tools to help create and ensure the validity of such complex ENTITYUNRELATED .
 The principle known as free indexation plays an important role in the determination of the referential properties of noun phrases in the principle-and-parameters language framework .	free indexation	referential properties of noun phrases	result	{'e1': {'word': 'free indexation', 'word_index': [(4, 4)], 'id': 'P90-1014.1'}, 'e2': {'word': 'referential properties of noun phrases', 'word_index': [(14, 14)], 'id': 'P90-1014.2'}}	The principle known as ENTITY plays an important role in the determination of the ENTITYOTHER in the ENTITYUNRELATED .
We describe three techniques for making syntactic analysis more robust---an agenda-based scheduling parser , a recovery technique for failed parses , and a new technique called terminal substring parsing .	agenda-based scheduling parser	syntactic analysis	usage	{'e1': {'word': 'agenda-based scheduling parser', 'word_index': [(11, 11)], 'id': 'A92-1026.6'}, 'e2': {'word': 'syntactic analysis', 'word_index': [(6, 6)], 'id': 'A92-1026.5'}}	We describe three techniques for making ENTITYOTHER more robust --- an ENTITY , a ENTITYUNRELATED , and a new technique called ENTITYUNRELATED .
For pragmatics processing , we describe how the method of abductive inference is inherently robust, in that an interpretation is always possible, so that in the absence of the required world knowledge , performance degrades gracefully.	abductive inference	pragmatics processing	usage	{'e1': {'word': 'abductive inference', 'word_index': [(9, 9)], 'id': 'A92-1026.10'}, 'e2': {'word': 'pragmatics processing', 'word_index': [(1, 1)], 'id': 'A92-1026.9'}}	For ENTITYOTHER , we describe how the method of ENTITY is inherently robust , in that an interpretation is always possible , so that in the absence of the required ENTITYUNRELATED , performance degrades gracefully .
 We present an efficient algorithm for chart-based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task.	chart-based phrase structure parsing	natural language	usage	{'e1': {'word': 'chart-based phrase structure parsing', 'word_index': [(6, 6)], 'id': 'A92-1027.1'}, 'e2': {'word': 'natural language', 'word_index': [(8, 8)], 'id': 'A92-1027.2'}}	We present an efficient algorithm for ENTITY of ENTITYOTHER that is tailored to the problem of extracting specific information from ENTITYUNRELATED where many of the ENTITYUNRELATED are unknown and much of the ENTITYUNRELATED is irrelevant to the task .
 We present an efficient algorithm for chart-based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task.	words	unrestricted texts	part_whole	{'e1': {'word': 'words', 'word_index': [(25, 25)], 'id': 'A92-1027.4'}, 'e2': {'word': 'unrestricted texts', 'word_index': [(20, 20)], 'id': 'A92-1027.3'}}	We present an efficient algorithm for ENTITYUNRELATED of ENTITYUNRELATED that is tailored to the problem of extracting specific information from ENTITYOTHER where many of the ENTITY are unknown and much of the ENTITYUNRELATED is irrelevant to the task .
As each new edge is added to the chart , the algorithm checks only the topmost of the edges adjacent to it, rather than all such edges as in conventional treatments.	edges	edges	compare	{'e1': {'word': 'edges', 'word_index': [(18, 18)], 'id': 'A92-1027.11'}, 'e2': {'word': 'edges', 'word_index': [(27, 27)], 'id': 'A92-1027.12'}}	As each new ENTITYUNRELATED is added to the ENTITYUNRELATED , the algorithm checks only the topmost of the ENTITY adjacent to it , rather than all such ENTITYOTHER as in conventional treatments .
This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .	function words	phrase boundary heuristics	usage	{'e1': {'word': 'function words', 'word_index': [(13, 13)], 'id': 'A92-1027.18'}, 'e2': {'word': 'phrase boundary heuristics', 'word_index': [(7, 7)], 'id': 'A92-1027.17'}}	This is facilitated through the use of ENTITYOTHER based on the placement of ENTITY , and by ENTITYUNRELATED that permit certain kinds of ENTITYUNRELATED to be deduced despite the presence of ENTITYUNRELATED .
This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .	unknown words	phrases	part_whole	{'e1': {'word': 'unknown words', 'word_index': [(31, 31)], 'id': 'A92-1027.21'}, 'e2': {'word': 'phrases', 'word_index': [(23, 23)], 'id': 'A92-1027.20'}}	This is facilitated through the use of ENTITYUNRELATED based on the placement of ENTITYUNRELATED , and by ENTITYUNRELATED that permit certain kinds of ENTITYOTHER to be deduced despite the presence of ENTITY .
A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced.	semantic	syntactic categories	compare	{'e1': {'word': 'semantic', 'word_index': [(7, 7)], 'id': 'A92-1027.23'}, 'e2': {'word': 'syntactic categories', 'word_index': [(10, 10)], 'id': 'A92-1027.24'}}	A further ENTITYUNRELATED is achieved by using ENTITY rather than ENTITYOTHER on the ENTITYUNRELATED , thereby reducing the amount of ENTITYUNRELATED and thus the number of ENTITYUNRELATED , since only ENTITYUNRELATED with a valid ENTITYUNRELATED interpretation are ever introduced .
A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced.	ambiguity	edges	result	{'e1': {'word': 'ambiguity', 'word_index': [(20, 20)], 'id': 'A92-1027.26'}, 'e2': {'word': 'edges', 'word_index': [(26, 26)], 'id': 'A92-1027.27'}}	A further ENTITYUNRELATED is achieved by using ENTITYUNRELATED rather than ENTITYUNRELATED on the ENTITYUNRELATED , thereby reducing the amount of ENTITY and thus the number of ENTITYOTHER , since only ENTITYUNRELATED with a valid ENTITYUNRELATED interpretation are ever introduced .
A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced.	semantic	edges	model-feature	{'e1': {'word': 'semantic', 'word_index': [(34, 34)], 'id': 'A92-1027.29'}, 'e2': {'word': 'edges', 'word_index': [(30, 30)], 'id': 'A92-1027.28'}}	A further ENTITYUNRELATED is achieved by using ENTITYUNRELATED rather than ENTITYUNRELATED on the ENTITYUNRELATED , thereby reducing the amount of ENTITYUNRELATED and thus the number of ENTITYUNRELATED , since only ENTITYOTHER with a valid ENTITY interpretation are ever introduced .
 In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed.	discourse segments	discourse segmentation	model-feature	{'e1': {'word': 'discourse segments', 'word_index': [(3, 3)], 'id': 'C92-1052.1'}, 'e2': {'word': 'discourse segmentation', 'word_index': [(10, 10)], 'id': 'C92-1052.2'}}	In this paper ENTITY are defined and a method for ENTITYOTHER primarily based on ENTITYUNRELATED of ENTITYUNRELATED between ENTITYUNRELATED is proposed .
 In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed.	abduction	discourse segmentation	usage	{'e1': {'word': 'abduction', 'word_index': [(14, 14)], 'id': 'C92-1052.3'}, 'e2': {'word': 'discourse segmentation', 'word_index': [(10, 10)], 'id': 'C92-1052.2'}}	In this paper ENTITYUNRELATED are defined and a method for ENTITYOTHER primarily based on ENTITY of ENTITYUNRELATED between ENTITYUNRELATED is proposed .
 In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed.	temporal relations	segments	model-feature	{'e1': {'word': 'temporal relations', 'word_index': [(16, 16)], 'id': 'C92-1052.4'}, 'e2': {'word': 'segments', 'word_index': [(18, 18)], 'id': 'C92-1052.5'}}	In this paper ENTITYUNRELATED are defined and a method for ENTITYUNRELATED primarily based on ENTITYUNRELATED of ENTITY between ENTITYOTHER is proposed .
This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora resolution .	computationally feasible	temporal anaphora resolution	usage	{'e1': {'word': 'computationally feasible', 'word_index': [(5, 5)], 'id': 'C92-1052.6'}, 'e2': {'word': 'temporal anaphora resolution', 'word_index': [(16, 16)], 'id': 'C92-1052.7'}}	This method is precise and ENTITY and is supported by previous work in the area of ENTITYOTHER .
 In this paper, a discrimination and robustness oriented adaptive learning procedure is proposed to deal with the task of syntactic ambiguity resolution .	adaptive learning procedure	syntactic ambiguity resolution	usage	{'e1': {'word': 'adaptive learning procedure', 'word_index': [(9, 9)], 'id': 'C92-1055.1'}, 'e2': {'word': 'syntactic ambiguity resolution', 'word_index': [(18, 18)], 'id': 'C92-1055.2'}}	In this paper , a discrimination and robustness oriented ENTITY is proposed to deal with the task of ENTITYOTHER .
Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications.	language model	approximation error	result	{'e1': {'word': 'language model', 'word_index': [(11, 11)], 'id': 'C92-1055.5'}, 'e2': {'word': 'approximation error', 'word_index': [(7, 7)], 'id': 'C92-1055.4'}}	Owing to the problem of ENTITYUNRELATED and ENTITYOTHER introduced by the ENTITY , traditional ENTITYUNRELATED , which resolve ENTITYUNRELATED by indirectly and implicitly using ENTITYUNRELATED , fail to achieve high ENTITYUNRELATED in real applications .
Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications.	maximum likelihood method	statistical approaches	usage	{'e1': {'word': 'maximum likelihood method', 'word_index': [(24, 24)], 'id': 'C92-1055.8'}, 'e2': {'word': 'statistical approaches', 'word_index': [(14, 14)], 'id': 'C92-1055.6'}}	Owing to the problem of ENTITYUNRELATED and ENTITYUNRELATED introduced by the ENTITYUNRELATED , traditional ENTITYOTHER , which resolve ENTITYUNRELATED by indirectly and implicitly using ENTITY , fail to achieve high ENTITYUNRELATED in real applications .
The accuracy rate of syntactic disambiguation is raised from 46.0% to 60.62% by using this novel approach.	syntactic disambiguation	accuracy rate	result	{'e1': {'word': 'syntactic disambiguation', 'word_index': [(3, 3)], 'id': 'C92-1055.14'}, 'e2': {'word': 'accuracy rate', 'word_index': [(1, 1)], 'id': 'C92-1055.13'}}	The ENTITYOTHER of ENTITY is raised from 46.0 % to 60.62 % by using this novel approach .
 Graph unification remains the most expensive part of unification-based grammar parsing .	Graph unification	unification-based grammar parsing	part_whole	{'e1': {'word': 'Graph unification', 'word_index': [(0, 0)], 'id': 'C92-2068.1'}, 'e2': {'word': 'unification-based grammar parsing', 'word_index': [(7, 7)], 'id': 'C92-2068.2'}}	ENTITY remains the most expensive part of ENTITYOTHER .
We propose a method of attaining such a design through a method of structure-sharing which avoids log(d) overheads often associated with structure-sharing of graphs without any use of costly dependency pointers .	log(d) overheads	structure-sharing of graphs	model-feature	{'e1': {'word': 'log(d) overheads', 'word_index': [(16, 16)], 'id': 'C92-2068.7'}, 'e2': {'word': 'structure-sharing of graphs', 'word_index': [(20, 20)], 'id': 'C92-2068.8'}}	We propose a method of attaining such a design through a method of ENTITYUNRELATED which avoids ENTITY often associated with ENTITYOTHER without any use of costly ENTITYUNRELATED .
 The transfer phase in machine translation (MT) systems has been considered to be more complicated than analysis and generation , since it is inherently a conglomeration of individual lexical rules .	transfer phase	analysis	compare	{'e1': {'word': 'transfer phase', 'word_index': [(1, 1)], 'id': 'C92-2115.1'}, 'e2': {'word': 'analysis', 'word_index': [(12, 12)], 'id': 'C92-2115.3'}}	The ENTITY in ENTITYUNRELATED has been considered to be more complicated than ENTITYOTHER and ENTITYUNRELATED , since it is inherently a conglomeration of individual ENTITYUNRELATED .
Currently some attempts are being made to use case-based reasoning in machine translation , that is, to make decisions on the basis of translation examples at appropriate pints in MT .	case-based reasoning	machine translation	usage	{'e1': {'word': 'case-based reasoning', 'word_index': [(8, 8)], 'id': 'C92-2115.6'}, 'e2': {'word': 'machine translation', 'word_index': [(10, 10)], 'id': 'C92-2115.7'}}	Currently some attempts are being made to use ENTITY in ENTITYOTHER , that is , to make decisions on the basis of ENTITYUNRELATED at appropriate pints in ENTITYUNRELATED .
This paper proposes a new type of transfer system , called a Similarity-driven Transfer System (SimTran) , for use in such case-based MT (CBMT) .	Similarity-driven Transfer System (SimTran)	case-based MT (CBMT)	usage	{'e1': {'word': 'Similarity-driven Transfer System (SimTran)', 'word_index': [(11, 11)], 'id': 'C92-2115.11'}, 'e2': {'word': 'case-based MT (CBMT)', 'word_index': [(17, 17)], 'id': 'C92-2115.12'}}	This paper proposes a new type of ENTITYUNRELATED , called a ENTITY , for use in such ENTITYOTHER .
When a very noisy portion is detected, the parser skips that portion using a fake non-terminal symbol .	non-terminal symbol	parser	usage	{'e1': {'word': 'non-terminal symbol', 'word_index': [(16, 16)], 'id': 'C92-3165.7'}, 'e2': {'word': 'parser', 'word_index': [(9, 9)], 'id': 'C92-3165.5'}}	When a very noisy ENTITYUNRELATED is detected , the ENTITYOTHER skips that ENTITYUNRELATED using a fake ENTITY .
The unidentified portion is resolved by re-utterance of that portion which is parsed very efficiently by using the parse record of the first utterance .	parse record	utterance	model-feature	{'e1': {'word': 'parse record', 'word_index': [(18, 18)], 'id': 'C92-3165.11'}, 'e2': {'word': 'utterance', 'word_index': [(22, 22)], 'id': 'C92-3165.12'}}	The unidentified ENTITYUNRELATED is resolved by ENTITYUNRELATED of that ENTITYUNRELATED which is parsed very efficiently by using the ENTITY of the first ENTITYOTHER .
Detected unknown words can be incrementally incorporated into the dictionary after the interaction with the user .	unknown words	dictionary	part_whole	{'e1': {'word': 'unknown words', 'word_index': [(1, 1)], 'id': 'C92-3165.16'}, 'e2': {'word': 'dictionary', 'word_index': [(8, 8)], 'id': 'C92-3165.17'}}	Detected ENTITY can be incrementally incorporated into the ENTITYOTHER after the interaction with the ENTITYUNRELATED .
In this paper, a new mechanism, based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in Chinese newspapers .	sublanguage	unknown words	usage	{'e1': {'word': 'sublanguage', 'word_index': [(13, 13)], 'id': 'C92-4199.3'}, 'e2': {'word': 'unknown words', 'word_index': [(19, 19)], 'id': 'C92-4199.4'}}	In this paper , a new mechanism , based on the concept of ENTITY , is proposed for identifying ENTITYOTHER , especially ENTITYUNRELATED , in ENTITYUNRELATED .
In this paper, a new mechanism, based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in Chinese newspapers .	personal names	Chinese newspapers	part_whole	{'e1': {'word': 'personal names', 'word_index': [(22, 22)], 'id': 'C92-4199.5'}, 'e2': {'word': 'Chinese newspapers', 'word_index': [(25, 25)], 'id': 'C92-4199.6'}}	In this paper , a new mechanism , based on the concept of ENTITYUNRELATED , is proposed for identifying ENTITYUNRELATED , especially ENTITY , in ENTITYOTHER .
 This paper describes the understanding process of the spatial descriptions in Japanese .	spatial descriptions	Japanese	part_whole	{'e1': {'word': 'spatial descriptions', 'word_index': [(8, 8)], 'id': 'C92-4207.1'}, 'e2': {'word': 'Japanese', 'word_index': [(10, 10)], 'id': 'C92-4207.2'}}	This paper describes the understanding process of the ENTITY in ENTITYOTHER .
It is done by an experimental computer program SPRINT , which takes natural language texts and produces a model of the described world .	model	world	model-feature	{'e1': {'word': 'model', 'word_index': [(15, 15)], 'id': 'C92-4207.7'}, 'e2': {'word': 'world', 'word_index': [(19, 19)], 'id': 'C92-4207.8'}}	It is done by an experimental ENTITYUNRELATED ENTITYUNRELATED , which takes ENTITYUNRELATED and produces a ENTITY of the described ENTITYOTHER .
To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the spatial attributes of the entities .	qualitative spatial constraints	text	part_whole	{'e1': {'word': 'qualitative spatial constraints', 'word_index': [(9, 9)], 'id': 'C92-4207.10'}, 'e2': {'word': 'text', 'word_index': [(12, 12)], 'id': 'C92-4207.11'}}	To reconstruct the ENTITYUNRELATED , the authors extract the ENTITY from the ENTITYOTHER , and represent them as the ENTITYUNRELATED on the ENTITYUNRELATED of the ENTITYUNRELATED .
To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the spatial attributes of the entities .	spatial attributes	entities	model-feature	{'e1': {'word': 'spatial attributes', 'word_index': [(22, 22)], 'id': 'C92-4207.13'}, 'e2': {'word': 'entities', 'word_index': [(25, 25)], 'id': 'C92-4207.14'}}	To reconstruct the ENTITYUNRELATED , the authors extract the ENTITYUNRELATED from the ENTITYUNRELATED , and represent them as the ENTITYUNRELATED on the ENTITY of the ENTITYOTHER .
The interpretation reflects the temporary belief about the world .	temporary belief	world	model-feature	{'e1': {'word': 'temporary belief', 'word_index': [(4, 4)], 'id': 'C92-4207.16'}, 'e2': {'word': 'world', 'word_index': [(7, 7)], 'id': 'C92-4207.17'}}	The interpretation reflects the ENTITY about the ENTITYOTHER .
 This paper describes a recently collected spoken language corpus for the ATIS (Air Travel Information System) domain .	ATIS (Air Travel Information System) domain	spoken language corpus	model-feature	{'e1': {'word': 'ATIS (Air Travel Information System) domain', 'word_index': [(9, 9)], 'id': 'H92-1003.2'}, 'e2': {'word': 'spoken language corpus', 'word_index': [(6, 6)], 'id': 'H92-1003.1'}}	This paper describes a recently collected ENTITYOTHER for the ENTITY .
We summarize the motivation for this effort, the goals, the implementation of a multi-site data collection paradigm , and the accomplishments of MADCOW in monitoring the collection and distribution of 12,000 utterances of spontaneous speech from five sites for use in a multi-site common evaluation of speech, natural language and spoken language .</abstract>	spontaneous speech	utterances	model-feature	{'e1': {'word': 'spontaneous speech', 'word_index': [(32, 32)], 'id': 'H92-1003.8'}, 'e2': {'word': 'utterances', 'word_index': [(30, 30)], 'id': 'H92-1003.7'}}	We summarize the motivation for this effort , the goals , the implementation of a ENTITYUNRELATED , and the accomplishments of ENTITYUNRELATED in monitoring the ENTITYUNRELATED and distribution of 12,000 ENTITYOTHER of ENTITY from five sites for use in a ENTITYUNRELATED .< / abstract >
 The paper provides an overview of the research conducted at LIMSI in the field of speech processing , but also in the related areas of Human-Machine Communication , including Natural Language Processing , Non Verbal and Multimodal Communication .	speech processing	Human-Machine Communication	compare	{'e1': {'word': 'speech processing', 'word_index': [(15, 15)], 'id': 'H92-1010.2'}, 'e2': {'word': 'Human-Machine Communication', 'word_index': [(24, 24)], 'id': 'H92-1010.3'}}	The paper provides an overview of the research conducted at ENTITYUNRELATED in the field of ENTITY , but also in the related areas of ENTITYOTHER , including ENTITYUNRELATED , ENTITYUNRELATED .
 This paper describes three relatively domain-independent capabilities recently added to the Paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and database query paraphrase .	domain-independent capabilities	Paramax spoken language understanding system	model-feature	{'e1': {'word': 'domain-independent capabilities', 'word_index': [(5, 5)], 'id': 'H92-1017.1'}, 'e2': {'word': 'Paramax spoken language understanding system', 'word_index': [(10, 10)], 'id': 'H92-1017.2'}}	This paper describes three relatively ENTITY recently added to the ENTITYOTHER : ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED .
Finally, we briefly describe an experiment which we have done in extending the n-best speech/language integration architecture to improving OCR accuracy .	n-best speech/language integration architecture	accuracy	result	{'e1': {'word': 'n-best speech/language integration architecture', 'word_index': [(14, 14)], 'id': 'H92-1017.8'}, 'e2': {'word': 'accuracy', 'word_index': [(18, 18)], 'id': 'H92-1017.10'}}	Finally , we briefly describe an experiment which we have done in extending the ENTITY to improving ENTITYUNRELATED ENTITYOTHER .
 We describe a generative probabilistic model of natural language , which we call HBG , that takes advantage of detailed linguistic information to resolve ambiguity .	linguistic information	ambiguity	usage	{'e1': {'word': 'linguistic information', 'word_index': [(15, 15)], 'id': 'H92-1026.3'}, 'e2': {'word': 'ambiguity', 'word_index': [(18, 18)], 'id': 'H92-1026.4'}}	We describe a ENTITYUNRELATED , which we call ENTITYUNRELATED , that takes advantage of detailed ENTITY to resolve ENTITYOTHER .
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.	lexical, syntactic, semantic, and structural information	HBG	usage	{'e1': {'word': 'lexical, syntactic, semantic, and structural information', 'word_index': [(2, 2)], 'id': 'H92-1026.6'}, 'e2': {'word': 'HBG', 'word_index': [(0, 0)], 'id': 'H92-1026.5'}}	ENTITYOTHER incorporates ENTITY from the ENTITYUNRELATED into the ENTITYUNRELATED in a novel way .
We use a corpus of bracketed sentences , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence .	parse	sentence	model-feature	{'e1': {'word': 'parse', 'word_index': [(27, 27)], 'id': 'H92-1026.13'}, 'e2': {'word': 'sentence', 'word_index': [(30, 30)], 'id': 'H92-1026.14'}}	We use a ENTITYUNRELATED , called a ENTITYUNRELATED , in combination with ENTITYUNRELATED to tease out the relevant aspects of a ENTITYUNRELATED that will determine the correct ENTITY of a ENTITYOTHER .
This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse .	linguistic introspection	grammar	usage	{'e1': {'word': 'linguistic introspection', 'word_index': [(15, 15)], 'id': 'H92-1026.16'}, 'e2': {'word': 'grammar', 'word_index': [(10, 10)], 'id': 'H92-1026.15'}}	This stands in contrast to the usual approach of further ENTITYOTHER tailoring via the usual ENTITY in the hope of generating the correct ENTITYUNRELATED .
In head-to-head tests against one of the best existing robust probabilistic parsing models , which we call P-CFG, the HBG model significantly outperforms P-CFG , increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.	HBG model	P-CFG	compare	{'e1': {'word': 'HBG model', 'word_index': [(17, 17)], 'id': 'H92-1026.21'}, 'e2': {'word': 'P-CFG', 'word_index': [(20, 20)], 'id': 'H92-1026.22'}}	In ENTITYUNRELATED against one of the best existing robust ENTITYUNRELATED , which we call ENTITYUNRELATED , the ENTITY significantly outperforms ENTITYOTHER , increasing the ENTITYUNRELATED rate from 60 % to 75 % , a 37 % reduction in error .
The classical MLE reestimation algorithms , namely the forward-backward algorithm and the segmental k-means algorithm , are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities .	reestimation formulas	HMM with Gaussian mixture observation densities	usage	{'e1': {'word': 'reestimation formulas', 'word_index': [(14, 14)], 'id': 'H92-1036.6'}, 'e2': {'word': 'HMM with Gaussian mixture observation densities', 'word_index': [(18, 18)], 'id': 'H92-1036.7'}}	The classical ENTITYUNRELATED , namely the ENTITYUNRELATED and the ENTITYUNRELATED , are expanded and ENTITY are given for ENTITYOTHER .
Because of its adaptive nature, Bayesian learning serves as a unified approach for the following four speech recognition applications, namely parameter smoothing , speaker adaptation , speaker group modeling and corrective training .	Bayesian learning	speech recognition	usage	{'e1': {'word': 'Bayesian learning', 'word_index': [(6, 6)], 'id': 'H92-1036.8'}, 'e2': {'word': 'speech recognition', 'word_index': [(16, 16)], 'id': 'H92-1036.9'}}	Because of its adaptive nature , ENTITY serves as a unified approach for the following four ENTITYOTHER applications , namely ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED .
 It is well-known that there are polysemous words like sentence whose meaning or sense depends on the context of use.	meaning	sentence	model-feature	{'e1': {'word': 'meaning', 'word_index': [(12, 12)], 'id': 'H92-1045.3'}, 'e2': {'word': 'sentence', 'word_index': [(10, 10)], 'id': 'H92-1045.2'}}	It is well - known that there are ENTITYUNRELATED like ENTITYOTHER whose ENTITY or ENTITYUNRELATED depends on the context of use .
We have recently reported on two new word-sense disambiguation systems , one trained on bilingual material (the Canadian Hansards ) and the other trained on monolingual material ( Roget&apos;s Thesaurus and Grolier&apos;s Encyclopedia ).	bilingual material	word-sense disambiguation systems	usage	{'e1': {'word': 'bilingual material', 'word_index': [(12, 12)], 'id': 'H92-1045.6'}, 'e2': {'word': 'word-sense disambiguation systems', 'word_index': [(7, 7)], 'id': 'H92-1045.5'}}	We have recently reported on two new ENTITYOTHER , one trained on ENTITY ( the ENTITYUNRELATED ) and the other trained on ENTITYUNRELATED ( ENTITYUNRELATED and ENTITYUNRELATED ) .
That is, if a polysemous word such as sentence appears two or more times in a well-written discourse , it is extremely likely that they will all share the same sense .	sentence	well-written discourse	part_whole	{'e1': {'word': 'sentence', 'word_index': [(8, 8)], 'id': 'H92-1045.13'}, 'e2': {'word': 'well-written discourse', 'word_index': [(16, 16)], 'id': 'H92-1045.14'}}	That is , if a ENTITYUNRELATED such as ENTITY appears two or more times in a ENTITYOTHER , it is extremely likely that they will all share the same ENTITYUNRELATED .
This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98%).	sense	discourse	part_whole	{'e1': {'word': 'sense', 'word_index': [(16, 16)], 'id': 'H92-1045.16'}, 'e2': {'word': 'discourse', 'word_index': [(20, 20)], 'id': 'H92-1045.17'}}	This paper describes an experiment which confirmed this hypothesis and found that the tendency to share ENTITY in the same ENTITYOTHER is extremely strong ( 98 % ) .
This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algorithm .	constraint	word-sense disambiguation algorithm	usage	{'e1': {'word': 'constraint', 'word_index': [(10, 10)], 'id': 'H92-1045.18'}, 'e2': {'word': 'word-sense disambiguation algorithm', 'word_index': [(17, 17)], 'id': 'H92-1045.19'}}	This result can be used as an additional source of ENTITY for improving the performance of the ENTITYOTHER .
In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint .	discourse constraint	disambiguation algorithms	usage	{'e1': {'word': 'discourse constraint', 'word_index': [(19, 19)], 'id': 'H92-1045.21'}, 'e2': {'word': 'disambiguation algorithms', 'word_index': [(11, 11)], 'id': 'H92-1045.20'}}	In addition , it could also be used to help evaluate ENTITYOTHER that did not make use of the ENTITY .
In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction.	dependency relation paths	answer extraction	usage	{'e1': {'word': 'dependency relation paths', 'word_index': [(8, 8)], 'id': 'P06-1112.1'}, 'e2': {'word': 'answer extraction', 'word_index': [(14, 14)], 'id': 'P06-1112.2'}}	In this paper , we explore correlation of ENTITY to rank candidate answers in ENTITYOTHER .
Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question.	dependency relations	question phrases	model-feature	{'e1': {'word': 'dependency relations', 'word_index': [(6, 6)], 'id': 'P06-1112.4'}, 'e2': {'word': 'question phrases', 'word_index': [(13, 13)], 'id': 'P06-1112.5'}}	Using the ENTITYUNRELATED , we compare ENTITY of a candidate answer and mapped ENTITYOTHER in ENTITYUNRELATED with the corresponding ENTITYUNRELATED in question .
Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure.	mapping score	correlation measure	part_whole	{'e1': {'word': 'mapping score', 'word_index': [(12, 12)], 'id': 'P06-1112.9'}, 'e2': {'word': 'correlation measure', 'word_index': [(15, 15)], 'id': 'P06-1112.10'}}	Different from previous studies , we propose an ENTITYUNRELATED and incorporate the ENTITY into the ENTITYOTHER .
This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus.	cooccurrence patterns	corpus	part_whole	{'e1': {'word': 'cooccurrence patterns', 'word_index': [(10, 10)], 'id': 'C90-3063.3'}, 'e2': {'word': 'corpus', 'word_index': [(14, 14)], 'id': 'C90-3063.4'}}	This paper presents an automatic scheme for collecting statistics on ENTITY in a large ENTITYOTHER .
To a large extent, these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities.	semantic constraints	anaphora references	usage	{'e1': {'word': 'semantic constraints', 'word_index': [(8, 8)], 'id': 'C90-3063.5'}, 'e2': {'word': 'anaphora references', 'word_index': [(15, 15)], 'id': 'C90-3063.6'}}	To a large extent , these statistics reflect ENTITY and thus are used to disambiguate ENTITYOTHER and ENTITYUNRELATED .
"An experiment was performed to resolve references of the pronoun ""it"" in sentences that were randomly selected from the corpus."	references	"pronoun ""it"""	model-feature	"{'e1': {'word': 'references', 'word_index': [(6, 6)], 'id': 'C90-3063.8'}, 'e2': {'word': 'pronoun ""it""', 'word_index': [(9, 9)], 'id': 'C90-3063.9'}}"	An experiment was performed to resolve ENTITY of the ENTITYOTHER in ENTITYUNRELATED that were randomly selected from the ENTITYUNRELATED .
"An experiment was performed to resolve references of the pronoun ""it"" in sentences that were randomly selected from the corpus."	sentences	corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(11, 11)], 'id': 'C90-3063.10'}, 'e2': {'word': 'corpus', 'word_index': [(18, 18)], 'id': 'C90-3063.11'}}	An experiment was performed to resolve ENTITYUNRELATED of the ENTITYUNRELATED in ENTITY that were randomly selected from the ENTITYOTHER .
The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool.	cooccurrence statistics	disambiguation tool	usage	{'e1': {'word': 'cooccurrence statistics', 'word_index': [(13, 13)], 'id': 'C90-3063.12'}, 'e2': {'word': 'disambiguation tool', 'word_index': [(26, 26)], 'id': 'C90-3063.14'}}	The results of the experiment show that in most of the cases the ENTITY indeed reflect the ENTITYUNRELATED and thus provide a basis for a useful ENTITYOTHER .
We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a probabilistic context-free grammar and a probabilistic finite automaton.	probabilistic context-free grammar	probabilistic finite automaton	compare	{'e1': {'word': 'probabilistic context-free grammar', 'word_index': [(16, 16)], 'id': 'C04-1011.3'}, 'e2': {'word': 'probabilistic finite automaton', 'word_index': [(19, 19)], 'id': 'C04-1011.4'}}	We consider the problem of computing the ENTITYUNRELATED , also called the ENTITYUNRELATED , between a ENTITY and a ENTITYOTHER .
We show that there is a closed-form (analytical) solution for one part of the Kullback-Leibler distance, viz. the cross-entropy.	cross-entropy	Kullback-Leibler distance	part_whole	{'e1': {'word': 'cross-entropy', 'word_index': [(17, 17)], 'id': 'C04-1011.7'}, 'e2': {'word': 'Kullback-Leibler distance', 'word_index': [(12, 12)], 'id': 'C04-1011.6'}}	We show that there is a ENTITYUNRELATED for one part of the ENTITYOTHER , viz . the ENTITY .
We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata.	probabilistic finite automata	distributional approximation	usage	{'e1': {'word': 'probabilistic finite automata', 'word_index': [(17, 17)], 'id': 'C04-1011.10'}, 'e2': {'word': 'distributional approximation', 'word_index': [(11, 11)], 'id': 'C04-1011.8'}}	We discuss several applications of the result to the problem of ENTITYOTHER of ENTITYUNRELATED by means of ENTITY .
Methods developed for spelling correction for languages like English (see the review by Kukich (Kukich, 1992)) are not readily applicable to agglutinative languages.	spelling correction	languages	usage	{'e1': {'word': 'spelling correction', 'word_index': [(3, 3)], 'id': 'A94-1037.1'}, 'e2': {'word': 'languages', 'word_index': [(5, 5)], 'id': 'A94-1037.2'}}	Methods developed for ENTITY for ENTITYOTHER like ENTITYUNRELATED ( see the review by Kukich ( Kukich , 1992 ) ) are not readily applicable to ENTITYUNRELATED .
This poster presents an approach to spelling correction in agglutinative languages that is based on two-level morphology and a dynamic-programming based search algorithm.	spelling correction	agglutinative languages	usage	{'e1': {'word': 'spelling correction', 'word_index': [(6, 6)], 'id': 'A94-1037.5'}, 'e2': {'word': 'agglutinative languages', 'word_index': [(8, 8)], 'id': 'A94-1037.6'}}	This poster presents an approach to ENTITY in ENTITYOTHER that is based on ENTITYUNRELATED and a ENTITYUNRELATED .
After an overview of our approach, we present results from experiments with spelling correction in Turkish.	spelling correction	Turkish	usage	{'e1': {'word': 'spelling correction', 'word_index': [(13, 13)], 'id': 'A94-1037.9'}, 'e2': {'word': 'Turkish', 'word_index': [(15, 15)], 'id': 'A94-1037.10'}}	After an overview of our approach , we present results from experiments with ENTITY in ENTITYOTHER .
The major objective of this program is to develop and demonstrate robust, high performance continuous speech recognition (CSR) techniques focussed on application in Spoken Language Systems (SLS) which will enhance the effectiveness of military and civilian computer-based systems.	continuous speech recognition (CSR) techniques	Spoken Language Systems (SLS)	usage	{'e1': {'word': 'continuous speech recognition (CSR) techniques', 'word_index': [(15, 15)], 'id': 'H94-1102.1'}, 'e2': {'word': 'Spoken Language Systems (SLS)', 'word_index': [(20, 20)], 'id': 'H94-1102.2'}}	The major objective of this program is to develop and demonstrate robust , high performance ENTITY focussed on application in ENTITYOTHER which will enhance the effectiveness of ENTITYUNRELATED .
A key complementary objective is to define and develop applications of robust speech recognition and understanding systems, and to help catalyze the transition of spoken language technology into military and civilian systems, with particular focus on application of robust CSR to mobile military command and control.	spoken language technology	military and civilian systems	usage	{'e1': {'word': 'spoken language technology', 'word_index': [(21, 21)], 'id': 'H94-1102.5'}, 'e2': {'word': 'military and civilian systems', 'word_index': [(23, 23)], 'id': 'H94-1102.6'}}	A key complementary objective is to define and develop applications of robust ENTITYUNRELATED , and to help catalyze the transition of ENTITY into ENTITYOTHER , with particular focus on application of robust ENTITYUNRELATED to ENTITYUNRELATED .
A key complementary objective is to define and develop applications of robust speech recognition and understanding systems, and to help catalyze the transition of spoken language technology into military and civilian systems, with particular focus on application of robust CSR to mobile military command and control.	CSR	mobile military command and control	usage	{'e1': {'word': 'CSR', 'word_index': [(32, 32)], 'id': 'H94-1102.7'}, 'e2': {'word': 'mobile military command and control', 'word_index': [(34, 34)], 'id': 'H94-1102.8'}}	A key complementary objective is to define and develop applications of robust ENTITYUNRELATED , and to help catalyze the transition of ENTITYUNRELATED into ENTITYUNRELATED , with particular focus on application of robust ENTITY to ENTITYOTHER .
The research effort focusses on developing advanced acoustic modelling, rapid search, and recognition-time adaptation techniques for robust large-vocabulary CSR, and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks.	large-vocabulary CSR	ARPA large-vocabulary CSR corpora	usage	{'e1': {'word': 'large-vocabulary CSR', 'word_index': [(16, 16)], 'id': 'H94-1102.11'}, 'e2': {'word': 'ARPA large-vocabulary CSR corpora', 'word_index': [(26, 26)], 'id': 'H94-1102.12'}}	The research effort focusses on developing advanced ENTITYUNRELATED , rapid search , and ENTITYUNRELATED for robust ENTITY , and on applying these techniques to the new ENTITYOTHER and to military application tasks .
Presentor offers intuitive and powerful declarative languages specifying the presentation at different levels: macro-planning, micro-planning, realization, and formatting.	declarative languages	Presentor	part_whole	{'e1': {'word': 'declarative languages', 'word_index': [(5, 5)], 'id': 'P98-1118.4'}, 'e2': {'word': 'Presentor', 'word_index': [(0, 0)], 'id': 'P98-1118.3'}}	ENTITYOTHER offers intuitive and powerful ENTITY specifying the presentation at different levels : macro-planning , micro-planning , realization , and formatting .
Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems (SLS) program to agree on a methodology for comparative evaluation of SLS systems, and that methodology has been put into practice several times in comparative tests of several SLS systems.	DARPA Spoken Language Systems (SLS) program	SLS systems	topic	{'e1': {'word': 'DARPA Spoken Language Systems (SLS) program', 'word_index': [(14, 14)], 'id': 'A92-1023.4'}, 'e2': {'word': 'SLS systems', 'word_index': [(24, 24)], 'id': 'A92-1023.5'}}	Recently considerable progress has been made by a number of groups involved in the ENTITY to agree on a methodology for comparative evaluation of ENTITYOTHER , and that methodology has been put into practice several times in comparative tests of several ENTITYUNRELATED .
These evaluations are probably the only NL evaluations other than the series of Message Understanding Conferences (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991).	Message Understanding Conferences	NL evaluations	topic	{'e1': {'word': 'Message Understanding Conferences', 'word_index': [(12, 12)], 'id': 'A92-1023.8'}, 'e2': {'word': 'NL evaluations', 'word_index': [(6, 6)], 'id': 'A92-1023.7'}}	These evaluations are probably the only ENTITYOTHER other than the series of ENTITY ( Sundheim , 1989 ; Sundheim , 1991 ) to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems ( Palmer et al. , 1989 ; Neal et al. , 1991 ) .
"This paper describes a practical ""black-box"" methodology for automatic evaluation of question-answering NL systems."	"""black-box"" methodology"	question-answering NL systems	usage	"{'e1': {'word': '""black-box"" methodology', 'word_index': [(5, 5)], 'id': 'A92-1023.9'}, 'e2': {'word': 'question-answering NL systems', 'word_index': [(10, 10)], 'id': 'A92-1023.10'}}"	This paper describes a practical ENTITY for automatic evaluation of ENTITYOTHER .
The psycholinguistic literature provides evidence for syntactic priming, i.e., the tendency to repeat structures.	psycholinguistic literature	syntactic priming	topic	{'e1': {'word': 'psycholinguistic literature', 'word_index': [(1, 1)], 'id': 'P06-1053.1'}, 'e2': {'word': 'syntactic priming', 'word_index': [(5, 5)], 'id': 'P06-1053.2'}}	The ENTITY provides evidence for ENTITYOTHER , i.e. , the tendency to repeat structures .
This paper describes a method for incorporating priming into an incremental probabilistic parser.	priming	incremental probabilistic parser	usage	{'e1': {'word': 'priming', 'word_index': [(7, 7)], 'id': 'P06-1053.3'}, 'e2': {'word': 'incremental probabilistic parser', 'word_index': [(10, 10)], 'id': 'P06-1053.4'}}	This paper describes a method for incorporating ENTITY into an ENTITYOTHER .
These models simulate the reading time advantage for parallel structures found in human data, and also yield a small increase in overall parsing accuracy.	parallel structures	human data	part_whole	{'e1': {'word': 'parallel structures', 'word_index': [(8, 8)], 'id': 'P06-1053.10'}, 'e2': {'word': 'human data', 'word_index': [(11, 11)], 'id': 'P06-1053.11'}}	These models simulate the reading time advantage for ENTITY found in ENTITYOTHER , and also yield a small increase in overall ENTITYUNRELATED .
Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word).	sense priors	word	model-feature	{'e1': {'word': 'sense priors', 'word_index': [(11, 11)], 'id': 'P06-1012.3'}, 'e2': {'word': 'word', 'word_index': [(3, 3)], 'id': 'P06-1012.1'}}	Instances of a ENTITYOTHER drawn from different ENTITYUNRELATED may have different ENTITY ( the proportions of the different ENTITYUNRELATED of a ENTITYUNRELATED ) .
Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word).	senses	word	model-feature	{'e1': {'word': 'senses', 'word_index': [(18, 18)], 'id': 'P06-1012.4'}, 'e2': {'word': 'word', 'word_index': [(21, 21)], 'id': 'P06-1012.5'}}	Instances of a ENTITYUNRELATED drawn from different ENTITYUNRELATED may have different ENTITYUNRELATED ( the proportions of the different ENTITY of a ENTITYOTHER ) .
This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains.	word sense disambiguation (WSD) systems	domains	usage	{'e1': {'word': 'word sense disambiguation (WSD) systems', 'word_index': [(7, 7)], 'id': 'P06-1012.6'}, 'e2': {'word': 'domains', 'word_index': [(13, 13)], 'id': 'P06-1012.7'}}	This in turn affects the accuracy of ENTITY trained and applied on different ENTITYOTHER .
This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations.	sense priors	words	model-feature	{'e1': {'word': 'sense priors', 'word_index': [(8, 8)], 'id': 'P06-1012.8'}, 'e2': {'word': 'words', 'word_index': [(10, 10)], 'id': 'P06-1012.9'}}	This paper presents a method to estimate the ENTITY of ENTITYOTHER drawn from a new ENTITYUNRELATED , and highlights the importance of using ENTITYUNRELATED when performing these ENTITYUNRELATED .
This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations.	well calibrated probabilities	estimations	usage	{'e1': {'word': 'well calibrated probabilities', 'word_index': [(23, 23)], 'id': 'P06-1012.11'}, 'e2': {'word': 'estimations', 'word_index': [(27, 27)], 'id': 'P06-1012.12'}}	This paper presents a method to estimate the ENTITYUNRELATED of ENTITYUNRELATED drawn from a new ENTITYUNRELATED , and highlights the importance of using ENTITY when performing these ENTITYOTHER .
By using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy.	well calibrated probabilities	sense priors	usage	{'e1': {'word': 'well calibrated probabilities', 'word_index': [(2, 2)], 'id': 'P06-1012.13'}, 'e2': {'word': 'sense priors', 'word_index': [(10, 10)], 'id': 'P06-1012.14'}}	By using ENTITY , we are able to estimate the ENTITYOTHER effectively to achieve significant improvements in ENTITYUNRELATED .
How to obtain hierarchical relations(e.g. superordinate -hyponym relation, synonym relation) is one of the most important problems for thesaurus construction.	hierarchical relations	thesaurus construction	part_whole	{'e1': {'word': 'hierarchical relations', 'word_index': [(3, 3)], 'id': 'C86-1105.1'}, 'e2': {'word': 'thesaurus construction', 'word_index': [(18, 18)], 'id': 'C86-1105.4'}}	How to obtain ENTITY ( e.g. ENTITYUNRELATED , ENTITYUNRELATED ) is one of the most important problems for ENTITYOTHER .
A pilot system for extracting these relations automatically from an ordinary Japanese language dictionary (Shinmeikai Kokugojiten, published by Sansei-do, in machine readable form) is given.	relations	Japanese language dictionary	part_whole	{'e1': {'word': 'relations', 'word_index': [(6, 6)], 'id': 'C86-1105.5'}, 'e2': {'word': 'Japanese language dictionary', 'word_index': [(11, 11)], 'id': 'C86-1105.6'}}	A pilot system for extracting these ENTITY automatically from an ordinary ENTITYOTHER ( Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form ) is given .
The features of the definition sentences in the dictionary, the mechanical extraction of the hierarchical relations and the estimation of the results are discussed.	definition sentences	dictionary	part_whole	{'e1': {'word': 'definition sentences', 'word_index': [(4, 4)], 'id': 'C86-1105.7'}, 'e2': {'word': 'dictionary', 'word_index': [(7, 7)], 'id': 'C86-1105.8'}}	The features of the ENTITY in the ENTITYOTHER , the mechanical extraction of the ENTITYUNRELATED and the estimation of the results are discussed .
The interlingual approach to MT has been repeatedly advocated by researchers originally interested in natural language understanding who take machine translation to be one possible application.	natural language understanding	machine translation	usage	{'e1': {'word': 'natural language understanding', 'word_index': [(11, 11)], 'id': 'C86-1021.2'}, 'e2': {'word': 'machine translation', 'word_index': [(14, 14)], 'id': 'C86-1021.3'}}	The ENTITYUNRELATED has been repeatedly advocated by researchers originally interested in ENTITY who take ENTITYOTHER to be one possible application .
However, not only the ambiguity but also the vagueness which every natural language inevitably has leads this approach into essential difficulties.	ambiguity	natural language	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(5, 5)], 'id': 'C86-1021.4'}, 'e2': {'word': 'natural language', 'word_index': [(12, 12)], 'id': 'C86-1021.5'}}	However , not only the ENTITY but also the vagueness which every ENTITYOTHER inevitably has leads this approach into essential difficulties .
In contrast, our project, the Mu-project, adopts the transfer approach as the basic framework of MT.	transfer approach	MT	usage	{'e1': {'word': 'transfer approach', 'word_index': [(11, 11)], 'id': 'C86-1021.7'}, 'e2': {'word': 'MT', 'word_index': [(17, 17)], 'id': 'C86-1021.8'}}	In contrast , our project , the ENTITYUNRELATED , adopts the ENTITY as the basic framework of ENTITYOTHER .
This paper describes the detailed construction of the transfer phase of our system from Japanese to English, and gives some examples of problems which seem difficult to treat in the interlingual approach.	transfer phase	interlingual approach	compare	{'e1': {'word': 'transfer phase', 'word_index': [(8, 8)], 'id': 'C86-1021.9'}, 'e2': {'word': 'interlingual approach', 'word_index': [(30, 30)], 'id': 'C86-1021.12'}}	This paper describes the detailed construction of the ENTITY of our system from ENTITYUNRELATED to ENTITYUNRELATED , and gives some examples of problems which seem difficult to treat in the ENTITYOTHER .
Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers, boosting and SVMs are applied to language processing tasks, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point.	SVMs	language processing tasks	usage	{'e1': {'word': 'SVMs', 'word_index': [(19, 19)], 'id': 'C04-1058.4'}, 'e2': {'word': 'language processing tasks', 'word_index': [(23, 23)], 'id': 'C04-1058.5'}}	Empirical experience and observations have shown us when powerful and highly tunable ENTITYUNRELATED such as ENTITYUNRELATED , ENTITYUNRELATED and ENTITY are applied to ENTITYOTHER , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .
To further improve performance, various error correction mechanisms have been developed, but in practice, most of them cannot be relied on to predictably improve performance on unseen data; indeed, depending upon the test set, they are as likely to degrade accuracy as to improve it.	error correction mechanisms	unseen data	usage	{'e1': {'word': 'error correction mechanisms', 'word_index': [(6, 6)], 'id': 'C04-1058.6'}, 'e2': {'word': 'unseen data', 'word_index': [(27, 27)], 'id': 'C04-1058.7'}}	To further improve performance , various ENTITY have been developed , but in practice , most of them cannot be relied on to predictably improve performance on ENTITYOTHER ; indeed , depending upon the ENTITYUNRELATED , they are as likely to degrade accuracy as to improve it .
The study addresses the problem of automatic acquisition of entailment relations between verbs.	entailment relations	verbs	model-feature	{'e1': {'word': 'entailment relations', 'word_index': [(8, 8)], 'id': 'N06-1007.2'}, 'e2': {'word': 'verbs', 'word_index': [(10, 10)], 'id': 'N06-1007.3'}}	The study addresses the problem of ENTITYUNRELATED of ENTITY between ENTITYOTHER .
While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations.	semantic equivalence	verbs	model-feature	{'e1': {'word': 'semantic equivalence', 'word_index': [(13, 13)], 'id': 'N06-1007.5'}, 'e2': {'word': 'verbs', 'word_index': [(15, 15)], 'id': 'N06-1007.6'}}	While this task has much in common with ENTITYUNRELATED which aims to discover ENTITY between ENTITYOTHER , the main challenge of ENTITYUNRELATED is to capture ENTITYUNRELATED .
While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations.	entailment acquisition	asymmetric, or directional, relations	topic	{'e1': {'word': 'entailment acquisition', 'word_index': [(21, 21)], 'id': 'N06-1007.7'}, 'e2': {'word': 'asymmetric, or directional, relations', 'word_index': [(25, 25)], 'id': 'N06-1007.8'}}	While this task has much in common with ENTITYUNRELATED which aims to discover ENTITYUNRELATED between ENTITYUNRELATED , the main challenge of ENTITY is to capture ENTITYOTHER .
Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus.	local structure	coherent text	model-feature	{'e1': {'word': 'local structure', 'word_index': [(9, 9)], 'id': 'N06-1007.9'}, 'e2': {'word': 'coherent text', 'word_index': [(11, 11)], 'id': 'N06-1007.10'}}	Motivated by the intuition that it often underlies the ENTITY of ENTITYOTHER , we develop a method that discovers ENTITYUNRELATED using evidence about ENTITYUNRELATED between ENTITYUNRELATED available in a ENTITYUNRELATED .
Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus.	discourse relations	clauses	model-feature	{'e1': {'word': 'discourse relations', 'word_index': [(23, 23)], 'id': 'N06-1007.12'}, 'e2': {'word': 'clauses', 'word_index': [(25, 25)], 'id': 'N06-1007.13'}}	Motivated by the intuition that it often underlies the ENTITYUNRELATED of ENTITYUNRELATED , we develop a method that discovers ENTITYUNRELATED using evidence about ENTITY between ENTITYOTHER available in a ENTITYUNRELATED .
In comparison with earlier work, the proposed method covers a much wider range of verb entailment types and learns the mapping between verbs with highly varied argument structures.	verbs	argument structures	model-feature	{'e1': {'word': 'verbs', 'word_index': [(21, 21)], 'id': 'N06-1007.17'}, 'e2': {'word': 'argument structures', 'word_index': [(25, 25)], 'id': 'N06-1007.18'}}	In comparison with earlier work , the proposed method covers a much wider range of ENTITYUNRELATED and learns the ENTITYUNRELATED between ENTITY with highly varied ENTITYOTHER .
This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one.	phrases	trees	model-feature	{'e1': {'word': 'phrases', 'word_index': [(11, 11)], 'id': 'A00-2023.2'}, 'e2': {'word': 'trees', 'word_index': [(18, 18)], 'id': 'A00-2023.3'}}	This paper presents a new approach to ENTITYUNRELATED in which alternative ENTITY are represented as packed sets of ENTITYOTHER , or ENTITYUNRELATED , and then ranked statistically to choose the best one .
An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach.	ranking algorithm	lattice-based approach	compare	{'e1': {'word': 'ranking algorithm', 'word_index': [(2, 2)], 'id': 'A00-2023.8'}, 'e2': {'word': 'lattice-based approach', 'word_index': [(18, 18)], 'id': 'A00-2023.9'}}	An efficient ENTITY is described , together with experimental results showing significant improvements over simple enumeration or a ENTITYOTHER .
This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1.	sentences	summary generation	usage	{'e1': {'word': 'sentences', 'word_index': [(13, 13)], 'id': 'X98-1022.6'}, 'e2': {'word': 'summary generation', 'word_index': [(15, 15)], 'id': 'X98-1022.7'}}	This paper focuses on the ENTITYUNRELATED and proposes two different models to extract ENTITY for ENTITYOTHER under two tasks initiated by ENTITYUNRELATED .
For categorization task, positive feature vectors and negative feature vectors are used cooperatively to construct generic, indicative summaries.	positive feature vectors	categorization task	usage	{'e1': {'word': 'positive feature vectors', 'word_index': [(3, 3)], 'id': 'X98-1022.10'}, 'e2': {'word': 'categorization task', 'word_index': [(1, 1)], 'id': 'X98-1022.9'}}	For ENTITYOTHER , ENTITY and ENTITYUNRELATED are used cooperatively to construct generic , indicative ENTITYUNRELATED .
For adhoc task, a text model based on relationship between nouns and verbs is used to filter out irrelevant discourse segment, to rank relevant sentences, and to generate the user-directed summaries.	text model	user-directed summaries	usage	{'e1': {'word': 'text model', 'word_index': [(5, 5)], 'id': 'X98-1022.13'}, 'e2': {'word': 'user-directed summaries', 'word_index': [(30, 30)], 'id': 'X98-1022.18'}}	For adhoc task , a ENTITY based on relationship between ENTITYUNRELATED and ENTITYUNRELATED is used to filter out irrelevant ENTITYUNRELATED , to rank relevant ENTITYUNRELATED , and to generate the ENTITYOTHER .
A set of articles is represented by a set of word vectors, and the similarity between the vectors is then calculated.	similarity	vectors	model-feature	{'e1': {'word': 'similarity', 'word_index': [(14, 14)], 'id': 'P98-2213.3'}, 'e2': {'word': 'vectors', 'word_index': [(17, 17)], 'id': 'P98-2213.4'}}	A set of articles is represented by a set of ENTITYUNRELATED , and the ENTITY between the ENTITYOTHER is then calculated .
By applying some constraints on the chronological ordering of articles, an efficient threading algorithm that runs in 0(n) time (where n is the number of articles) is obtained.	threading algorithm	0(n) time	model-feature	{'e1': {'word': 'threading algorithm', 'word_index': [(13, 13)], 'id': 'P98-2213.8'}, 'e2': {'word': '0(n) time', 'word_index': [(17, 17)], 'id': 'P98-2213.9'}}	By applying some ENTITYUNRELATED on the chronological ordering of articles , an efficient ENTITY that runs in ENTITYOTHER ( where n is the number of articles ) is obtained .
The constructed graph is visualized with words that represent the topics of the threads, and words that represent new information in each article.	topics	threads	model-feature	{'e1': {'word': 'topics', 'word_index': [(10, 10)], 'id': 'P98-2213.12'}, 'e2': {'word': 'threads', 'word_index': [(13, 13)], 'id': 'P98-2213.13'}}	The constructed ENTITYUNRELATED is visualized with ENTITYUNRELATED that represent the ENTITY of the ENTITYOTHER , and ENTITYUNRELATED that represent new ENTITYUNRELATED in each article .
The constructed graph is visualized with words that represent the topics of the threads, and words that represent new information in each article.	words	information	model-feature	{'e1': {'word': 'words', 'word_index': [(16, 16)], 'id': 'P98-2213.14'}, 'e2': {'word': 'information', 'word_index': [(20, 20)], 'id': 'P98-2213.15'}}	The constructed ENTITYUNRELATED is visualized with ENTITYUNRELATED that represent the ENTITYUNRELATED of the ENTITYUNRELATED , and ENTITY that represent new ENTITYOTHER in each article .
In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree.	SSTC	sentence	model-feature	{'e1': {'word': 'SSTC', 'word_index': [(12, 12)], 'id': 'P98-1113.6'}, 'e2': {'word': 'sentence', 'word_index': [(15, 15)], 'id': 'P98-1113.7'}}	In our approach , examples are annotated with the ENTITYUNRELATED where each ENTITY describes a ENTITYOTHER , a ENTITYUNRELATED as well as the correspondence between ENTITYUNRELATED in the ENTITYUNRELATED and ENTITYUNRELATED in the ENTITYUNRELATED .
In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree.	substrings	sentence	part_whole	{'e1': {'word': 'substrings', 'word_index': [(25, 25)], 'id': 'P98-1113.9'}, 'e2': {'word': 'sentence', 'word_index': [(28, 28)], 'id': 'P98-1113.10'}}	In our approach , examples are annotated with the ENTITYUNRELATED where each ENTITYUNRELATED describes a ENTITYUNRELATED , a ENTITYUNRELATED as well as the correspondence between ENTITY in the ENTITYOTHER and ENTITYUNRELATED in the ENTITYUNRELATED .
In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree.	subtrees	representation tree	part_whole	{'e1': {'word': 'subtrees', 'word_index': [(30, 30)], 'id': 'P98-1113.11'}, 'e2': {'word': 'representation tree', 'word_index': [(33, 33)], 'id': 'P98-1113.12'}}	In our approach , examples are annotated with the ENTITYUNRELATED where each ENTITYUNRELATED describes a ENTITYUNRELATED , a ENTITYUNRELATED as well as the correspondence between ENTITYUNRELATED in the ENTITYUNRELATED and ENTITY in the ENTITYOTHER .
In the process of parsing, we first try to build subtrees for phrases in the input sentence which have been successfully found in the example-base - a bottom up approach.	subtrees	phrases	model-feature	{'e1': {'word': 'subtrees', 'word_index': [(11, 11)], 'id': 'P98-1113.14'}, 'e2': {'word': 'phrases', 'word_index': [(13, 13)], 'id': 'P98-1113.15'}}	In the process of ENTITYUNRELATED , we first try to build ENTITY for ENTITYOTHER in the ENTITYUNRELATED which have been successfully found in the ENTITYUNRELATED - a bottom up approach .
This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.	HMM-based chunk tagger	named entity (NE) recognition (NER) system	usage	{'e1': {'word': 'HMM-based chunk tagger', 'word_index': [(7, 7)], 'id': 'P02-1060.2'}, 'e2': {'word': 'named entity (NE) recognition (NER) system', 'word_index': [(12, 12)], 'id': 'P02-1060.3'}}	This paper proposes a ENTITYUNRELATED and an ENTITY , from which a ENTITYOTHER is built to recognize and classify ENTITYUNRELATED , ENTITYUNRELATED .
Through the HMM, our system is able to apply and integrate four types of internal and external evidences : 1) simple deterministic internal feature of the words, such as capitalization and digitalization ; 2) internal semantic feature of important triggers ; 3) internal gazetteer feature; 4) external macro context feature.	capitalization	words	model-feature	{'e1': {'word': 'capitalization', 'word_index': [(32, 32)], 'id': 'P02-1060.8'}, 'e2': {'word': 'words', 'word_index': [(28, 28)], 'id': 'P02-1060.7'}}	Through the ENTITYUNRELATED , our system is able to apply and integrate four types of internal and external evidences : 1 ) simple deterministic internal feature of the ENTITYOTHER , such as ENTITY and digitalization ; 2 ) ENTITYUNRELATED of important triggers ; 3 ) ENTITYUNRELATED ; 4 ) ENTITYUNRELATED .
Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.	system	F-measures	result	{'e1': {'word': 'system', 'word_index': [(3, 3)], 'id': 'P02-1060.13'}, 'e2': {'word': 'F-measures', 'word_index': [(7, 7)], 'id': 'P02-1060.15'}}	Evaluation of our ENTITY on ENTITYUNRELATED achieves ENTITYOTHER of 96.6 % and 94.1 % respectively .
Porting a Natural Language Processing (NLP) system to a new domain remains one of the bottlenecks in syntactic parsing, because of the amount of effort required to fix gaps in the lexicon, and to attune the existing grammar to the idiosyncracies of the new sublanguage.	new domain	Natural Language Processing (NLP) system	usage	{'e1': {'word': 'new domain', 'word_index': [(5, 5)], 'id': 'C96-2213.2'}, 'e2': {'word': 'Natural Language Processing (NLP) system', 'word_index': [(2, 2)], 'id': 'C96-2213.1'}}	Porting a ENTITYOTHER to a ENTITY remains one of the bottlenecks in ENTITYUNRELATED , because of the amount of effort required to fix gaps in the ENTITYUNRELATED , and to attune the ENTITYUNRELATED to the idiosyncracies of the ENTITYUNRELATED .
Porting a Natural Language Processing (NLP) system to a new domain remains one of the bottlenecks in syntactic parsing, because of the amount of effort required to fix gaps in the lexicon, and to attune the existing grammar to the idiosyncracies of the new sublanguage.	existing grammar	new sublanguage	usage	{'e1': {'word': 'existing grammar', 'word_index': [(32, 32)], 'id': 'C96-2213.5'}, 'e2': {'word': 'new sublanguage', 'word_index': [(38, 38)], 'id': 'C96-2213.6'}}	Porting a ENTITYUNRELATED to a ENTITYUNRELATED remains one of the bottlenecks in ENTITYUNRELATED , because of the amount of effort required to fix gaps in the ENTITYUNRELATED , and to attune the ENTITY to the idiosyncracies of the ENTITYOTHER .
This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach.	lexicalized grammar	domain	usage	{'e1': {'word': 'lexicalized grammar', 'word_index': [(9, 9)], 'id': 'C96-2213.7'}, 'e2': {'word': 'domain', 'word_index': [(12, 12)], 'id': 'C96-2213.8'}}	This paper shows how the process of fitting a ENTITY to a ENTITYOTHER can be automated to a great extent by using a ENTITYUNRELATED that combines ENTITYUNRELATED with a ENTITYUNRELATED .
This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach.	traditional knowledge-based techniques	corpus-based approach	compare	{'e1': {'word': 'traditional knowledge-based techniques', 'word_index': [(26, 26)], 'id': 'C96-2213.10'}, 'e2': {'word': 'corpus-based approach', 'word_index': [(29, 29)], 'id': 'C96-2213.11'}}	This paper shows how the process of fitting a ENTITYUNRELATED to a ENTITYUNRELATED can be automated to a great extent by using a ENTITYUNRELATED that combines ENTITY with a ENTITYOTHER .
We propose a detection method for orthographic variants caused by transliteration in a large corpus.	transliteration	corpus	part_whole	{'e1': {'word': 'transliteration', 'word_index': [(9, 9)], 'id': 'C04-1102.2'}, 'e2': {'word': 'corpus', 'word_index': [(13, 13)], 'id': 'C04-1102.3'}}	We propose a ENTITYUNRELATED for orthographic variants caused by ENTITY in a large ENTITYOTHER .
One is string similarity based on edit distance.	edit distance	string similarity	usage	{'e1': {'word': 'edit distance', 'word_index': [(5, 5)], 'id': 'C04-1102.6'}, 'e2': {'word': 'string similarity', 'word_index': [(2, 2)], 'id': 'C04-1102.5'}}	One is ENTITYOTHER based on ENTITY .
The other is contextual similarity by a vector space model.	vector space model	contextual similarity	usage	{'e1': {'word': 'vector space model', 'word_index': [(6, 6)], 'id': 'C04-1102.8'}, 'e2': {'word': 'contextual similarity', 'word_index': [(3, 3)], 'id': 'C04-1102.7'}}	The other is ENTITYOTHER by a ENTITY .
However most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts.	temporal expressions	newswire texts	part_whole	{'e1': {'word': 'temporal expressions', 'word_index': [(15, 15)], 'id': 'N06-1018.2'}, 'e2': {'word': 'newswire texts', 'word_index': [(17, 17)], 'id': 'N06-1018.3'}}	However most of the works found in the literature have focused on identifying and understanding ENTITY in ENTITYOTHER .
In this paper we report our work on anchoring temporal expressions in a novel genre, emails.	genre	temporal expressions	model-feature	{'e1': {'word': 'genre', 'word_index': [(13, 13)], 'id': 'N06-1018.5'}, 'e2': {'word': 'temporal expressions', 'word_index': [(9, 9)], 'id': 'N06-1018.4'}}	In this paper we report our work on anchoring ENTITYOTHER in a novel ENTITY , emails .
We have developed and evaluated a Temporal Expression Anchoror (TEA), and the result shows that it performs significantly better than the baseline, and compares favorably with some of the closely related work.</abstract>	Temporal Expression Anchoror (TEA)	baseline	compare	{'e1': {'word': 'Temporal Expression Anchoror (TEA)', 'word_index': [(6, 6)], 'id': 'N06-1018.9'}, 'e2': {'word': 'baseline', 'word_index': [(19, 19)], 'id': 'N06-1018.10'}}	We have developed and evaluated a ENTITY , and the result shows that it performs significantly better than the ENTITYOTHER , and compares favorably with some of the closely related work . < / abstract >
The goal of this research is to develop a spoken language system that will demonstrate the usefulness of voice input for interactive problem solving.	voice input	interactive problem solving	usage	{'e1': {'word': 'voice input', 'word_index': [(16, 16)], 'id': 'H89-2066.2'}, 'e2': {'word': 'interactive problem solving', 'word_index': [(18, 18)], 'id': 'H89-2066.3'}}	The goal of this research is to develop a ENTITYUNRELATED that will demonstrate the usefulness of ENTITY for ENTITYOTHER .
Combining speech recognition and natural language processing to achieve speech understanding, the system will be demonstrated in an application domain relevant to the DoD.	natural language processing	speech understanding	usage	{'e1': {'word': 'natural language processing', 'word_index': [(3, 3)], 'id': 'H89-2066.8'}, 'e2': {'word': 'speech understanding', 'word_index': [(6, 6)], 'id': 'H89-2066.9'}}	Combining ENTITYUNRELATED and ENTITY to achieve ENTITYOTHER , the system will be demonstrated in an ENTITYUNRELATED relevant to the DoD.
The objective of this project is to develop a robust and high-performance speech recognition system using a segment-based approach to phonetic recognition.	segment-based approach	phonetic recognition	usage	{'e1': {'word': 'segment-based approach', 'word_index': [(12, 12)], 'id': 'H89-2066.12'}, 'e2': {'word': 'phonetic recognition', 'word_index': [(14, 14)], 'id': 'H89-2066.13'}}	The objective of this project is to develop a ENTITYUNRELATED using a ENTITY to ENTITYOTHER .
The recognition system will eventually be integrated with natural language processing to achieve spoken language understanding.	natural language processing	spoken language understanding	usage	{'e1': {'word': 'natural language processing', 'word_index': [(7, 7)], 'id': 'H89-2066.15'}, 'e2': {'word': 'spoken language understanding', 'word_index': [(10, 10)], 'id': 'H89-2066.16'}}	The ENTITYUNRELATED will eventually be integrated with ENTITY to achieve ENTITYOTHER .
The knowledge to be expressed in text is first divided into small propositional units, which are then composed into appropriate combinations and converted into text.KDS (Knowledge Delivery System), which embodies this paradigm, has distinct parts devoted to creation of the propositional units, to organization of the text, to prevention of excess redundancy, to creation of combinations of units, to evaluation of these combinations as potential sentences, to selection of the best among competing combinations, and to creation of the final text.	knowledge	text	part_whole	{'e1': {'word': 'knowledge', 'word_index': [(1, 1)], 'id': 'J81-1002.4'}, 'e2': {'word': 'text', 'word_index': [(6, 6)], 'id': 'J81-1002.5'}}	The ENTITY to be expressed in ENTITYOTHER is first divided into small ENTITYUNRELATED , which are then composed into appropriate combinations and converted into ENTITYUNRELATED . ENTITYUNRELATED , which embodies this paradigm , has distinct parts devoted to creation of the ENTITYUNRELATED , to organization of the ENTITYUNRELATED , to prevention of ENTITYUNRELATED , to creation of combinations of units , to evaluation of these combinations as potential ENTITYUNRELATED , to selection of the best among competing combinations , and to creation of the ENTITYUNRELATED .
The Fragment-and-Compose paradigm and the computational methods of KDS are described.	computational methods	KDS	usage	{'e1': {'word': 'computational methods', 'word_index': [(4, 4)], 'id': 'J81-1002.15'}, 'e2': {'word': 'KDS', 'word_index': [(6, 6)], 'id': 'J81-1002.16'}}	The ENTITYUNRELATED and the ENTITY of ENTITYOTHER are described .
A deterministic parser is under development which represents a departure from traditional deterministic parsers in that it combines both symbolic and connectionist components.	A deterministic parser	traditional deterministic parsers	compare	{'e1': {'word': 'A deterministic parser', 'word_index': [(0, 0)], 'id': 'C90-1002.1'}, 'e2': {'word': 'traditional deterministic parsers', 'word_index': [(9, 9)], 'id': 'C90-1002.2'}}	ENTITY is under development which represents a departure from ENTITYOTHER in that it combines both ENTITYUNRELATED .
The connectionist component is trained either from patterns derived from the rules of a deterministic grammar.	rules	deterministic grammar	part_whole	{'e1': {'word': 'rules', 'word_index': [(11, 11)], 'id': 'C90-1002.5'}, 'e2': {'word': 'deterministic grammar', 'word_index': [(14, 14)], 'id': 'C90-1002.6'}}	The connectionist component is trained either from ENTITYUNRELATED derived from the ENTITY of a ENTITYOTHER .
The development and evolution of such a hybrid architecture has lead to a parser which is superior to any known deterministic parser.	parser	known deterministic parser	compare	{'e1': {'word': 'parser', 'word_index': [(12, 12)], 'id': 'C90-1002.8'}, 'e2': {'word': 'known deterministic parser', 'word_index': [(18, 18)], 'id': 'C90-1002.9'}}	The development and evolution of such a ENTITYUNRELATED has lead to a ENTITY which is superior to any ENTITYOTHER .
Experiments are described and powerful training techniques are demonstrated that permit decision-making by the connectionist component in the parsing process.	training techniques	decision-making	usage	{'e1': {'word': 'training techniques', 'word_index': [(5, 5)], 'id': 'C90-1002.10'}, 'e2': {'word': 'decision-making', 'word_index': [(10, 10)], 'id': 'C90-1002.11'}}	Experiments are described and powerful ENTITY are demonstrated that permit ENTITYOTHER by the ENTITYUNRELATED in the ENTITYUNRELATED .
Experiments are described and powerful training techniques are demonstrated that permit decision-making by the connectionist component in the parsing process.	connectionist component	parsing process	part_whole	{'e1': {'word': 'connectionist component', 'word_index': [(13, 13)], 'id': 'C90-1002.12'}, 'e2': {'word': 'parsing process', 'word_index': [(16, 16)], 'id': 'C90-1002.13'}}	Experiments are described and powerful ENTITYUNRELATED are demonstrated that permit ENTITYUNRELATED by the ENTITY in the ENTITYOTHER .
This approach has permitted some simplifications to the rules of other deterministic parsers, including the elimination of rule packets and priorities.	rules	deterministic parsers	part_whole	{'e1': {'word': 'rules', 'word_index': [(8, 8)], 'id': 'C90-1002.14'}, 'e2': {'word': 'deterministic parsers', 'word_index': [(11, 11)], 'id': 'C90-1002.15'}}	This approach has permitted some simplifications to the ENTITY of other ENTITYOTHER , including the elimination of ENTITYUNRELATED and priorities .
Data are presented which show how a connectionist (neural) network trained with linguistic rules can parse both expected (grammatical) sentences as well as some novel (ungrammatical or lexically ambiguous) sentences.	connectionist (neural) network	expected (grammatical) sentences	usage	{'e1': {'word': 'connectionist (neural) network', 'word_index': [(7, 7)], 'id': 'C90-1002.18'}, 'e2': {'word': 'expected (grammatical) sentences', 'word_index': [(14, 14)], 'id': 'C90-1002.20'}}	Data are presented which show how a ENTITY trained with ENTITYUNRELATED can parse both ENTITYOTHER as well as some novel ( ungrammatical or lexically ambiguous ) sentences .
The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization.	supervised learning	summarization	usage	{'e1': {'word': 'supervised learning', 'word_index': [(9, 9)], 'id': 'P02-1059.1'}, 'e2': {'word': 'summarization', 'word_index': [(18, 18)], 'id': 'P02-1059.3'}}	The paper proposes and empirically motivates an integration of ENTITY with ENTITYUNRELATED to deal with human biases in ENTITYOTHER .
The corpus of human created extracts is created from a newspaper corpus and used as a test set.	newspaper corpus	corpus	part_whole	{'e1': {'word': 'newspaper corpus', 'word_index': [(10, 10)], 'id': 'P02-1059.7'}, 'e2': {'word': 'corpus', 'word_index': [(1, 1)], 'id': 'P02-1059.6'}}	The ENTITYOTHER of human created extracts is created from a ENTITY and used as a test set .
In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).	dynamic programming (DP)	statistical machine translation (MT)	usage	{'e1': {'word': 'dynamic programming (DP)', 'word_index': [(13, 13)], 'id': 'C00-2123.2'}, 'e2': {'word': 'statistical machine translation (MT)', 'word_index': [(10, 10)], 'id': 'C00-2123.1'}}	In this paper , we describe a search procedure for ENTITYOTHER based on ENTITY .
The experimental tests are carried out on the Verbmobil task (German-English, 8000-word vocabulary), which is a limited-domain spoken-language task.	limited-domain spoken-language task	Verbmobil task	model-feature	{'e1': {'word': 'limited-domain spoken-language task', 'word_index': [(23, 23)], 'id': 'C00-2123.6'}, 'e2': {'word': 'Verbmobil task', 'word_index': [(8, 8)], 'id': 'C00-2123.5'}}	The experimental tests are carried out on the ENTITYOTHER ( German - English , 8000 - word vocabulary ) , which is a ENTITY .
This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources.	word-sense ambiguity	machine-readable resources	part_whole	{'e1': {'word': 'word-sense ambiguity', 'word_index': [(6, 6)], 'id': 'C96-1055.1'}, 'e2': {'word': 'machine-readable resources', 'word_index': [(10, 10)], 'id': 'C96-1055.2'}}	This paper addresses the issue of ENTITY in extraction from ENTITYOTHER for the construction of ENTITYUNRELATED .
These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses.	verb semantics	syntactic behavior	compare	{'e1': {'word': 'verb semantics', 'word_index': [(27, 27)], 'id': 'C96-1055.8'}, 'e2': {'word': 'syntactic behavior', 'word_index': [(29, 29)], 'id': 'C96-1055.9'}}	These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that ENTITY and ENTITYOTHER are predictably related ; ( 2 ) to demonstrate that a 15 - fold improvement can be achieved in deriving ENTITYUNRELATED from ENTITYUNRELATED if we first divide the ENTITYUNRELATED into distinct groupings that correlate with different ENTITYUNRELATED .
These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses.	syntactic cues	semantic information	usage	{'e1': {'word': 'syntactic cues', 'word_index': [(52, 52)], 'id': 'C96-1055.11'}, 'e2': {'word': 'semantic information', 'word_index': [(50, 50)], 'id': 'C96-1055.10'}}	These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that ENTITYUNRELATED and ENTITYUNRELATED are predictably related ; ( 2 ) to demonstrate that a 15 - fold improvement can be achieved in deriving ENTITYOTHER from ENTITY if we first divide the ENTITYUNRELATED into distinct groupings that correlate with different ENTITYUNRELATED .
These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses.	syntactic cues	word senses	compare	{'e1': {'word': 'syntactic cues', 'word_index': [(58, 58)], 'id': 'C96-1055.12'}, 'e2': {'word': 'word senses', 'word_index': [(66, 66)], 'id': 'C96-1055.13'}}	These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that ENTITYUNRELATED and ENTITYUNRELATED are predictably related ; ( 2 ) to demonstrate that a 15 - fold improvement can be achieved in deriving ENTITYUNRELATED from ENTITYUNRELATED if we first divide the ENTITY into distinct groupings that correlate with different ENTITYOTHER .
The objective is a generic system of tools, including a core English lexicon, grammar, and concept representations, for building natural language processing (NLP) systems for text understanding.	natural language processing (NLP) systems	text understanding	usage	{'e1': {'word': 'natural language processing (NLP) systems', 'word_index': [(21, 21)], 'id': 'M91-1029.4'}, 'e2': {'word': 'text understanding', 'word_index': [(23, 23)], 'id': 'M91-1029.5'}}	The objective is a generic system of tools , including a ENTITYUNRELATED , ENTITYUNRELATED , and concept representations , for building ENTITY for ENTITYOTHER .
PAKTUS supports the adaptation of the generic core to a variety of domains: JINTACCS messages, RAINFORM messages, news reports about a specific type of event, such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words, conceptual mappings, and discourse patterns.	PAKTUS	JINTACCS messages	usage	{'e1': {'word': 'PAKTUS', 'word_index': [(0, 0)], 'id': 'M91-1029.9'}, 'e2': {'word': 'JINTACCS messages', 'word_index': [(14, 14)], 'id': 'M91-1029.10'}}	ENTITY supports the adaptation of the generic core to a variety of domains : ENTITYOTHER , ENTITYUNRELATED , ENTITYUNRELATED about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED .
"The multiplicative fragment of linear logic has found a number of applications in computational linguistics: in the ""glue language"" approach to LFG semantics, and in the formulation and parsing of various categorial grammars."	linear logic	computational linguistics	usage	{'e1': {'word': 'linear logic', 'word_index': [(4, 4)], 'id': 'P98-1088.1'}, 'e2': {'word': 'computational linguistics', 'word_index': [(12, 12)], 'id': 'P98-1088.2'}}	The multiplicative fragment of ENTITY has found a number of applications in ENTITYOTHER : in the ENTITYUNRELATED approach to ENTITYUNRELATED , and in the formulation and ENTITYUNRELATED of various ENTITYUNRELATED .
"The multiplicative fragment of linear logic has found a number of applications in computational linguistics: in the ""glue language"" approach to LFG semantics, and in the formulation and parsing of various categorial grammars."	"""glue language"""	LFG semantics	usage	"{'e1': {'word': '""glue language""', 'word_index': [(16, 16)], 'id': 'P98-1088.3'}, 'e2': {'word': 'LFG semantics', 'word_index': [(19, 19)], 'id': 'P98-1088.4'}}"	The multiplicative fragment of ENTITYUNRELATED has found a number of applications in ENTITYUNRELATED : in the ENTITY approach to ENTITYOTHER , and in the formulation and ENTITYUNRELATED of various ENTITYUNRELATED .
We describe an implementation of data-driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system.	embodied conversational agent	dialogue system	part_whole	{'e1': {'word': 'embodied conversational agent', 'word_index': [(13, 13)], 'id': 'E06-1045.1'}, 'e2': {'word': 'dialogue system', 'word_index': [(16, 16)], 'id': 'E06-1045.2'}}	We describe an implementation of data-driven selection of emphatic facial displays for an ENTITY in a ENTITYOTHER .
We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for large-vocabulary speech recognition.	parser	large-vocabulary speech recognition	usage	{'e1': {'word': 'parser', 'word_index': [(17, 17)], 'id': 'P04-1030.3'}, 'e2': {'word': 'large-vocabulary speech recognition', 'word_index': [(19, 19)], 'id': 'P04-1030.4'}}	We present the first application of the ENTITYUNRELATED of Collins ( 1999 ) as a ENTITYUNRELATED and ENTITY for ENTITYOTHER .
The parser uses structural and lexical dependencies not considered by n-gram models, conditioning recognition on more linguistically-grounded relationships.	structural and lexical dependencies	parser	usage	{'e1': {'word': 'structural and lexical dependencies', 'word_index': [(3, 3)], 'id': 'P04-1030.8'}, 'e2': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'P04-1030.7'}}	The ENTITYOTHER uses ENTITY not considered by ENTITYUNRELATED , conditioning recognition on more linguistically - grounded relationships .
Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding.	structural information	speech understanding	usage	{'e1': {'word': 'structural information', 'word_index': [(16, 16)], 'id': 'P04-1030.13'}, 'e2': {'word': 'speech understanding', 'word_index': [(19, 19)], 'id': 'P04-1030.14'}}	Experiments on the ENTITYUNRELATED and lattice corpora show ENTITYUNRELATED competitive with the ENTITYUNRELATED while extracting additional ENTITY useful for ENTITYOTHER .
They are probability, rank, and entropy.	rank	entropy	compare	{'e1': {'word': 'rank', 'word_index': [(4, 4)], 'id': 'P02-1023.4'}, 'e2': {'word': 'entropy', 'word_index': [(7, 7)], 'id': 'P02-1023.5'}}	They are probability , ENTITY , and ENTITYOTHER .
We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate (CER).	pruning criteria	Chinese text input	usage	{'e1': {'word': 'pruning criteria', 'word_index': [(7, 7)], 'id': 'P02-1023.6'}, 'e2': {'word': 'Chinese text input', 'word_index': [(13, 13)], 'id': 'P02-1023.7'}}	We evaluated the performance of the three ENTITY in a real application of ENTITYOTHER in terms of ENTITYUNRELATED .
Johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a unification-based grammar that is used by a multidimensional chart parser to compose inputs.	unification-based grammar	multidimensional chart parser	usage	{'e1': {'word': 'unification-based grammar', 'word_index': [(15, 15)], 'id': 'C00-1054.4'}, 'e2': {'word': 'multidimensional chart parser', 'word_index': [(21, 21)], 'id': 'C00-1054.5'}}	Johnston 1998 presents an approach in which strategies for ENTITYUNRELATED are stated declaratively using a ENTITY that is used by a ENTITYOTHER to compose inputs .
In this paper, we present an alternative approach in which multimodal parsing and understanding are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation.	weighted finite-state device	multimodal parsing and understanding	usage	{'e1': {'word': 'weighted finite-state device', 'word_index': [(16, 16)], 'id': 'C00-1054.8'}, 'e2': {'word': 'multimodal parsing and understanding', 'word_index': [(11, 11)], 'id': 'C00-1054.7'}}	In this paper , we present an alternative approach in which ENTITYOTHER are achieved using a ENTITY which takes ENTITYUNRELATED as inputs and outputs their joint interpretation .
This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.	convolution kernel	syntactic structure information	model-feature	{'e1': {'word': 'convolution kernel', 'word_index': [(6, 6)], 'id': 'N06-1037.1'}, 'e2': {'word': 'syntactic structure information', 'word_index': [(11, 11)], 'id': 'N06-1037.3'}}	This paper proposes to use a ENTITY over ENTITYUNRELATED to model ENTITYOTHER for ENTITYUNRELATED .
Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.	syntactic structure features	parse tree	part_whole	{'e1': {'word': 'syntactic structure features', 'word_index': [(5, 5)], 'id': 'N06-1037.5'}, 'e2': {'word': 'parse tree', 'word_index': [(9, 9)], 'id': 'N06-1037.6'}}	Our study reveals that the ENTITY embedded in a ENTITYOTHER are very effective for ENTITYUNRELATED and these features can be well captured by the ENTITYUNRELATED .
This paper describes a particular approach to parsing that utilizes recent advances in unification-based parsing and in classification-based knowledge representation.	unification-based parsing	parsing	usage	{'e1': {'word': 'unification-based parsing', 'word_index': [(13, 13)], 'id': 'H90-1011.2'}, 'e2': {'word': 'parsing', 'word_index': [(7, 7)], 'id': 'H90-1011.1'}}	This paper describes a particular approach to ENTITYOTHER that utilizes recent advances in ENTITY and in ENTITYUNRELATED .
As unification-based grammatical frameworks are extended to handle richer descriptions of linguistic information, they begin to share many of the properties that have been developed in KL-ONE-like knowledge representation systems.	unification-based grammatical frameworks	KL-ONE-like knowledge representation systems	compare	{'e1': {'word': 'unification-based grammatical frameworks', 'word_index': [(1, 1)], 'id': 'H90-1011.4'}, 'e2': {'word': 'KL-ONE-like knowledge representation systems', 'word_index': [(24, 24)], 'id': 'H90-1011.6'}}	As ENTITY are extended to handle richer descriptions of ENTITYUNRELATED , they begin to share many of the properties that have been developed in ENTITYOTHER .
This commonality suggests that some of the classification-based representation techniques can be applied to unification-based linguistic descriptions.	classification-based representation techniques	unification-based linguistic descriptions	usage	{'e1': {'word': 'classification-based representation techniques', 'word_index': [(7, 7)], 'id': 'H90-1011.7'}, 'e2': {'word': 'unification-based linguistic descriptions', 'word_index': [(12, 12)], 'id': 'H90-1011.8'}}	This commonality suggests that some of the ENTITY can be applied to ENTITYOTHER .
The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system [2], in which parsing is characterized as an inference process called incremental description refinement.	KL-ONE style representation	parsing	usage	{'e1': {'word': 'KL-ONE style representation', 'word_index': [(4, 4)], 'id': 'H90-1011.11'}, 'e2': {'word': 'parsing', 'word_index': [(6, 6)], 'id': 'H90-1011.12'}}	The use of a ENTITY for ENTITYOTHER and ENTITYUNRELATED was first explored in the ENTITYUNRELATED [ 2 ] , in which ENTITYUNRELATED is characterized as an inference process called ENTITYUNRELATED .
The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system [2], in which parsing is characterized as an inference process called incremental description refinement.	incremental description refinement	parsing	usage	{'e1': {'word': 'incremental description refinement', 'word_index': [(29, 29)], 'id': 'H90-1011.16'}, 'e2': {'word': 'parsing', 'word_index': [(21, 21)], 'id': 'H90-1011.15'}}	The use of a ENTITYUNRELATED for ENTITYUNRELATED and ENTITYUNRELATED was first explored in the ENTITYUNRELATED [ 2 ] , in which ENTITYOTHER is characterized as an inference process called ENTITY .
"""To explain complex phenomena, an explanation system must be able to select information from a formal representation of domain knowledge, organize the selected information into multisentential discourse plans, and realize the discourse plans in text."	domain knowledge	explanation system	usage	{'e1': {'word': 'domain knowledge', 'word_index': [(19, 19)], 'id': 'J97-1004.2'}, 'e2': {'word': 'explanation system', 'word_index': [(7, 7)], 'id': 'J97-1004.1'}}	""" To explain complex phenomena , an ENTITYOTHER must be able to select information from a formal representation of ENTITY , organize the selected information into ENTITYUNRELATED , and realize the ENTITYUNRELATED in text ."
This paper reports on a seven-year effort to empirically study explanation generation from semantically rich, large-scale knowledge bases.	semantically rich, large-scale knowledge bases	explanation generation	usage	{'e1': {'word': 'semantically rich, large-scale knowledge bases', 'word_index': [(13, 13)], 'id': 'J97-1004.7'}, 'e2': {'word': 'explanation generation', 'word_index': [(11, 11)], 'id': 'J97-1004.6'}}	This paper reports on a seven- year effort to empirically study ENTITYOTHER from ENTITY .
In particular, it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large-scale knowledge base in the domain of botanical anatomy, physiology, and development.	large-scale knowledge base	robust explanation system	usage	{'e1': {'word': 'large-scale knowledge base', 'word_index': [(13, 13)], 'id': 'J97-1004.10'}, 'e2': {'word': 'robust explanation system', 'word_index': [(6, 6)], 'id': 'J97-1004.8'}}	In particular , it describes a ENTITYOTHER that constructs ENTITYUNRELATED from the a ENTITY in the domain of botanical anatomy , physiology , and development .
Given a particular concept, or word sense, a topic signature is a set of words that tend to co-occur with it.	words	topic signature	part_whole	{'e1': {'word': 'words', 'word_index': [(14, 14)], 'id': 'P04-2005.5'}, 'e2': {'word': 'topic signature', 'word_index': [(9, 9)], 'id': 'P04-2005.4'}}	Given a particular ENTITYUNRELATED , or ENTITYUNRELATED , a ENTITYOTHER is a set of ENTITY that tend to co-occur with it .
Topic signatures can be useful in a number of Natural Language Processing (NLP) applications, such as Word Sense Disambiguation (WSD) and Text Summarisation.	Topic signatures	Natural Language Processing (NLP) applications	usage	{'e1': {'word': 'Topic signatures', 'word_index': [(0, 0)], 'id': 'P04-2005.6'}, 'e2': {'word': 'Natural Language Processing (NLP) applications', 'word_index': [(8, 8)], 'id': 'P04-2005.7'}}	ENTITY can be useful in a number of ENTITYOTHER , such as ENTITYUNRELATED and ENTITYUNRELATED .
Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web.	word senses	English	part_whole	{'e1': {'word': 'word senses', 'word_index': [(10, 10)], 'id': 'P04-2005.10'}, 'e2': {'word': 'English', 'word_index': [(14, 14)], 'id': 'P04-2005.11'}}	Our method takes advantage of the different way in which ENTITY are lexicalised in ENTITYOTHER and ENTITYUNRELATED , and also exploits the large amount of ENTITYUNRELATED available in ENTITYUNRELATED and on the Web .
Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web.	Chinese text	corpora	part_whole	{'e1': {'word': 'Chinese text', 'word_index': [(25, 25)], 'id': 'P04-2005.13'}, 'e2': {'word': 'corpora', 'word_index': [(28, 28)], 'id': 'P04-2005.14'}}	Our method takes advantage of the different way in which ENTITYUNRELATED are lexicalised in ENTITYUNRELATED and ENTITYUNRELATED , and also exploits the large amount of ENTITY available in ENTITYOTHER and on the Web .
We evaluated the topic signatures on a WSD task, where we trained a second-order vector cooccurrence algorithm on standard WSD datasets, with promising results.	topic signatures	WSD task	usage	{'e1': {'word': 'topic signatures', 'word_index': [(3, 3)], 'id': 'P04-2005.15'}, 'e2': {'word': 'WSD task', 'word_index': [(6, 6)], 'id': 'P04-2005.16'}}	We evaluated the ENTITY on a ENTITYOTHER , where we trained a ENTITYUNRELATED on ENTITYUNRELATED , with promising results .
This paper presents a novel ensemble learning approach to resolving German pronouns.	ensemble learning approach	German pronouns	usage	{'e1': {'word': 'ensemble learning approach', 'word_index': [(5, 5)], 'id': 'P04-2010.1'}, 'e2': {'word': 'German pronouns', 'word_index': [(8, 8)], 'id': 'P04-2010.2'}}	This paper presents a novel ENTITY to resolving ENTITYOTHER .
Furthermore, we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process.	standalone system	unannotated text	usage	{'e1': {'word': 'standalone system', 'word_index': [(5, 5)], 'id': 'P04-2010.7'}, 'e2': {'word': 'unannotated text', 'word_index': [(10, 10)], 'id': 'P04-2010.9'}}	Furthermore , we present a ENTITY that resolves ENTITYUNRELATED in ENTITYOTHER by using a fully automatic sequence of ENTITYUNRELATED that mimics the ENTITYUNRELATED .
Although the system performs well within a limited textual domain, further research is needed to make it effective for open-domain question answering and text summarisation.	textual domain	open-domain question answering	compare	{'e1': {'word': 'textual domain', 'word_index': [(8, 8)], 'id': 'P04-2010.12'}, 'e2': {'word': 'open-domain question answering', 'word_index': [(19, 19)], 'id': 'P04-2010.13'}}	Although the system performs well within a limited ENTITY , further research is needed to make it effective for ENTITYOTHER and ENTITYUNRELATED .
This paper presents a machine learning approach to bare slice disambiguation in dialogue.	machine learning approach	bare slice disambiguation	usage	{'e1': {'word': 'machine learning approach', 'word_index': [(4, 4)], 'id': 'C04-1035.1'}, 'e2': {'word': 'bare slice disambiguation', 'word_index': [(6, 6)], 'id': 'C04-1035.2'}}	This paper presents a ENTITY to ENTITYOTHER in ENTITYUNRELATED .
We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses.	probabilistic Horn clauses	heuristic principles	model-feature	{'e1': {'word': 'probabilistic Horn clauses', 'word_index': [(13, 13)], 'id': 'C04-1035.6'}, 'e2': {'word': 'heuristic principles', 'word_index': [(5, 5)], 'id': 'C04-1035.4'}}	We extract a set of ENTITYOTHER from a ENTITYUNRELATED and formulate them as ENTITY .
We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two different machine learning algorithms : SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system.	domain independent features	input dataset	model-feature	{'e1': {'word': 'domain independent features', 'word_index': [(13, 13)], 'id': 'C04-1035.8'}, 'e2': {'word': 'input dataset', 'word_index': [(17, 17)], 'id': 'C04-1035.9'}}	We then use the predicates of such ENTITYUNRELATED to create a set of ENTITY to annotate an ENTITYOTHER , and run two different ENTITYUNRELATED : SLIPPER , a ENTITYUNRELATED , and TiMBL , a ENTITYUNRELATED .
We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two different machine learning algorithms : SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system.	rule-based learning algorithm	memory-based system	compare	{'e1': {'word': 'rule-based learning algorithm', 'word_index': [(28, 28)], 'id': 'C04-1035.11'}, 'e2': {'word': 'memory-based system', 'word_index': [(34, 34)], 'id': 'C04-1035.12'}}	We then use the predicates of such ENTITYUNRELATED to create a set of ENTITYUNRELATED to annotate an ENTITYUNRELATED , and run two different ENTITYUNRELATED : SLIPPER , a ENTITY , and TiMBL , a ENTITYOTHER .
The results show that the features in terms of which we formulate our heuristic principles have significant predictive power, and that rules that closely resemble our Horn clauses can be learnt automatically from these features.	features	heuristic principles	model-feature	{'e1': {'word': 'features', 'word_index': [(5, 5)], 'id': 'C04-1035.14'}, 'e2': {'word': 'heuristic principles', 'word_index': [(13, 13)], 'id': 'C04-1035.15'}}	The results show that the ENTITY in terms of which we formulate our ENTITYOTHER have significant predictive power , and that ENTITYUNRELATED that closely resemble our ENTITYUNRELATED can be learnt automatically from these ENTITYUNRELATED .
The results show that the features in terms of which we formulate our heuristic principles have significant predictive power, and that rules that closely resemble our Horn clauses can be learnt automatically from these features.	rules	Horn clauses	compare	{'e1': {'word': 'rules', 'word_index': [(21, 21)], 'id': 'C04-1035.16'}, 'e2': {'word': 'Horn clauses', 'word_index': [(26, 26)], 'id': 'C04-1035.17'}}	The results show that the ENTITYUNRELATED in terms of which we formulate our ENTITYUNRELATED have significant predictive power , and that ENTITY that closely resemble our ENTITYOTHER can be learnt automatically from these ENTITYUNRELATED .
The new criterion – meaning-entailing substitutability – fits the needs of semantic-oriented NLP applications and can be evaluated directly (independent of an application) at a good level of human agreement.	meaning-entailing substitutability	semantic-oriented NLP applications	usage	{'e1': {'word': 'meaning-entailing substitutability', 'word_index': [(4, 4)], 'id': 'C04-1036.3'}, 'e2': {'word': 'semantic-oriented NLP applications', 'word_index': [(10, 10)], 'id': 'C04-1036.4'}}	The new criterion – ENTITY – fits the needs of ENTITYOTHER and can be evaluated directly ( independent of an application ) at a good level of ENTITYUNRELATED .
Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality.	semantic criterion	distributional word feature vectors	model-feature	{'e1': {'word': 'semantic criterion', 'word_index': [(3, 3)], 'id': 'C04-1036.6'}, 'e2': {'word': 'distributional word feature vectors', 'word_index': [(10, 10)], 'id': 'C04-1036.7'}}	Motivated by this ENTITY we analyze the empirical quality of ENTITYOTHER and its impact on ENTITYUNRELATED , proposing an objective measure for evaluating ENTITYUNRELATED .
Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance.</abstract>	feature weighting and selection function	feature vectors	result	{'e1': {'word': 'feature weighting and selection function', 'word_index': [(4, 4)], 'id': 'C04-1036.10'}, 'e2': {'word': 'feature vectors', 'word_index': [(11, 11)], 'id': 'C04-1036.11'}}	Finally , a novel ENTITY is presented , which yields superior ENTITYOTHER and better ENTITYUNRELATED .< / abstract >
In this paper, we identify features of electronic discussions that influence the clustering process, and offer a filtering mechanism that removes undesirable influences.	features	electronic discussions	model-feature	{'e1': {'word': 'features', 'word_index': [(6, 6)], 'id': 'C04-1068.4'}, 'e2': {'word': 'electronic discussions', 'word_index': [(8, 8)], 'id': 'C04-1068.5'}}	In this paper , we identify ENTITY of ENTITYOTHER that influence the ENTITYUNRELATED , and offer a ENTITYUNRELATED that removes undesirable ENTITYUNRELATED .
We tested the clustering and filtering processes on electronic newsgroup discussions, and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval.	clustering and filtering processes	electronic newsgroup discussions	usage	{'e1': {'word': 'clustering and filtering processes', 'word_index': [(3, 3)], 'id': 'C04-1068.9'}, 'e2': {'word': 'electronic newsgroup discussions', 'word_index': [(5, 5)], 'id': 'C04-1068.10'}}	We tested the ENTITY on ENTITYOTHER , and evaluated their ENTITYUNRELATED by means of two experiments : ENTITYUNRELATED simple ENTITYUNRELATED .
We tested the clustering and filtering processes on electronic newsgroup discussions, and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval.	coarse-level clustering	performance	result	{'e1': {'word': 'coarse-level clustering', 'word_index': [(17, 17)], 'id': 'C04-1068.12'}, 'e2': {'word': 'performance', 'word_index': [(10, 10)], 'id': 'C04-1068.11'}}	We tested the ENTITYUNRELATED on ENTITYUNRELATED , and evaluated their ENTITYOTHER by means of two experiments : ENTITY simple ENTITYUNRELATED .
We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case.	context	word	model-feature	{'e1': {'word': 'context', 'word_index': [(8, 8)], 'id': 'C04-1080.1'}, 'e2': {'word': 'word', 'word_index': [(14, 14)], 'id': 'C04-1080.2'}}	We present a new HMM tagger that exploits ENTITY on both sides of a ENTITYOTHER to be tagged , and evaluate it in both the ENTITYUNRELATED .
Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable.	quality	accuracy	result	{'e1': {'word': 'quality', 'word_index': [(3, 3)], 'id': 'C04-1080.7'}, 'e2': {'word': 'accuracy', 'word_index': [(10, 10)], 'id': 'C04-1080.9'}}	Observing that the ENTITY of the ENTITYUNRELATED greatly impacts the ENTITYOTHER that can be achieved by the ENTITYUNRELATED , we present a method of ENTITYUNRELATED that improves ENTITYUNRELATED when ENTITYUNRELATED of ENTITYUNRELATED is unstable .
Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable.	HMM training	accuracy	result	{'e1': {'word': 'HMM training', 'word_index': [(24, 24)], 'id': 'C04-1080.11'}, 'e2': {'word': 'accuracy', 'word_index': [(27, 27)], 'id': 'C04-1080.12'}}	Observing that the ENTITYUNRELATED of the ENTITYUNRELATED greatly impacts the ENTITYUNRELATED that can be achieved by the ENTITYUNRELATED , we present a method of ENTITY that improves ENTITYOTHER when ENTITYUNRELATED of ENTITYUNRELATED is unstable .
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects.	referring expressions	objects	usage	{'e1': {'word': 'referring expressions', 'word_index': [(4, 4)], 'id': 'C04-1096.1'}, 'e2': {'word': 'objects', 'word_index': [(9, 9)], 'id': 'C04-1096.2'}}	Past work of generating ENTITY mainly utilized attributes of ENTITYOTHER and ENTITYUNRELATED between ENTITYUNRELATED .
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects.	binary relations	objects	model-feature	{'e1': {'word': 'binary relations', 'word_index': [(11, 11)], 'id': 'C04-1096.3'}, 'e2': {'word': 'objects', 'word_index': [(13, 13)], 'id': 'C04-1096.4'}}	Past work of generating ENTITYUNRELATED mainly utilized attributes of ENTITYUNRELATED and ENTITY between ENTITYOTHER .
To overcome this limitation, this paper proposes a method utilizing the perceptual groups of objects and n-ary relations among them.	n-ary relations	objects	model-feature	{'e1': {'word': 'n-ary relations', 'word_index': [(17, 17)], 'id': 'C04-1096.7'}, 'e2': {'word': 'objects', 'word_index': [(15, 15)], 'id': 'C04-1096.6'}}	To overcome this limitation , this paper proposes a method utilizing the perceptual groups of ENTITYOTHER and ENTITY among them .
Machine transliteration/back-transliteration plays an important role in many multilingual speech and language applications.	Machine transliteration/back-transliteration	multilingual speech and language applications	part_whole	{'e1': {'word': 'Machine transliteration/back-transliteration', 'word_index': [(0, 0)], 'id': 'C04-1103.1'}, 'e2': {'word': 'multilingual speech and language applications', 'word_index': [(7, 7)], 'id': 'C04-1103.2'}}	ENTITY plays an important role in many ENTITYOTHER .
In this paper, a novel framework for machine transliteration/backtransliteration that allows us to carry out direct orthographical mapping (DOM) between two different languages is presented.	machine transliteration/backtransliteration	direct orthographical mapping (DOM)	usage	{'e1': {'word': 'machine transliteration/backtransliteration', 'word_index': [(8, 8)], 'id': 'C04-1103.3'}, 'e2': {'word': 'direct orthographical mapping (DOM)', 'word_index': [(15, 15)], 'id': 'C04-1103.4'}}	In this paper , a novel framework for ENTITY that allows us to carry out ENTITYOTHER between two different ENTITYUNRELATED is presented .
Under this framework, a joint source-channel transliteration model, also called n-gram transliteration model (n-gram TM), is further proposed to model the transliteration process.	n-gram transliteration model (n-gram TM)	transliteration process	model-feature	{'e1': {'word': 'n-gram transliteration model (n-gram TM)', 'word_index': [(9, 9)], 'id': 'C04-1103.7'}, 'e2': {'word': 'transliteration process', 'word_index': [(17, 17)], 'id': 'C04-1103.8'}}	Under this framework , a ENTITYUNRELATED , also called ENTITY , is further proposed to model the ENTITYOTHER .
We evaluate the proposed methods through several transliteration/backtransliteration experiments for English/Chinese and English/Japanese language pairs.	transliteration/backtransliteration experiments	English/Chinese and English/Japanese language pairs	usage	{'e1': {'word': 'transliteration/backtransliteration experiments', 'word_index': [(7, 7)], 'id': 'C04-1103.9'}, 'e2': {'word': 'English/Chinese and English/Japanese language pairs', 'word_index': [(9, 9)], 'id': 'C04-1103.10'}}	We evaluate the proposed methods through several ENTITY for ENTITYOTHER .
In this paper, we present a corpus-based supervised word sense disambiguation (WSD) system for Dutch which combines statistical classification (maximum entropy) with linguistic information.	corpus-based supervised word sense disambiguation (WSD) system	Dutch	usage	{'e1': {'word': 'corpus-based supervised word sense disambiguation (WSD) system', 'word_index': [(7, 7)], 'id': 'C04-1112.1'}, 'e2': {'word': 'Dutch', 'word_index': [(9, 9)], 'id': 'C04-1112.2'}}	In this paper , we present a ENTITY for ENTITYOTHER which combines ENTITYUNRELATED ( ENTITYUNRELATED ) with ENTITYUNRELATED .
Instead of building individual classifiers per ambiguous wordform, we introduce a lemma-based approach.	classifiers	lemma-based approach	compare	{'e1': {'word': 'classifiers', 'word_index': [(4, 4)], 'id': 'C04-1112.6'}, 'e2': {'word': 'lemma-based approach', 'word_index': [(11, 11)], 'id': 'C04-1112.8'}}	Instead of building individual ENTITY per ENTITYUNRELATED , we introduce a ENTITYOTHER .
The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier, therefore augmenting the training material available to the algorithm.	inflected forms	ambiguous word	model-feature	{'e1': {'word': 'inflected forms', 'word_index': [(11, 11)], 'id': 'C04-1112.9'}, 'e2': {'word': 'ambiguous word', 'word_index': [(14, 14)], 'id': 'C04-1112.10'}}	The advantage of this novel method is that it clusters all ENTITY of an ENTITYOTHER in one ENTITYUNRELATED , therefore augmenting the ENTITYUNRELATED available to the ENTITYUNRELATED .
The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier, therefore augmenting the training material available to the algorithm.	training material	algorithm	usage	{'e1': {'word': 'training material', 'word_index': [(22, 22)], 'id': 'C04-1112.12'}, 'e2': {'word': 'algorithm', 'word_index': [(26, 26)], 'id': 'C04-1112.13'}}	The advantage of this novel method is that it clusters all ENTITYUNRELATED of an ENTITYUNRELATED in one ENTITYUNRELATED , therefore augmenting the ENTITY available to the ENTITYOTHER .
Testing the lemma-based model on the Dutch Senseval-2 test data, we achieve a significant increase in accuracy over the wordform model.	lemma-based model	Dutch Senseval-2 test data	usage	{'e1': {'word': 'lemma-based model', 'word_index': [(2, 2)], 'id': 'C04-1112.14'}, 'e2': {'word': 'Dutch Senseval-2 test data', 'word_index': [(5, 5)], 'id': 'C04-1112.15'}}	Testing the ENTITY on the ENTITYOTHER , we achieve a significant increase in ENTITYUNRELATED over the ENTITYUNRELATED .
We present a text mining method for finding synonymous expressions based on the distributional hypothesis in a set of coherent corpora.	distributional hypothesis	text mining method	usage	{'e1': {'word': 'distributional hypothesis', 'word_index': [(10, 10)], 'id': 'C04-1116.3'}, 'e2': {'word': 'text mining method', 'word_index': [(3, 3)], 'id': 'C04-1116.1'}}	We present a ENTITYOTHER for finding ENTITYUNRELATED based on the ENTITY in a set of coherent ENTITYUNRELATED .
This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author's text as a coherent corpus.	term aggregation system	accuracy	result	{'e1': {'word': 'term aggregation system', 'word_index': [(12, 12)], 'id': 'C04-1116.6'}, 'e2': {'word': 'accuracy', 'word_index': [(9, 9)], 'id': 'C04-1116.5'}}	This paper proposes a new methodology to improve the ENTITYOTHER of a ENTITY using each author 's ENTITYUNRELATED as a coherent ENTITYUNRELATED .
This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author's text as a coherent corpus.	text	corpus	usage	{'e1': {'word': 'text', 'word_index': [(17, 17)], 'id': 'C04-1116.7'}, 'e2': {'word': 'corpus', 'word_index': [(21, 21)], 'id': 'C04-1116.8'}}	This paper proposes a new methodology to improve the ENTITYUNRELATED of a ENTITYUNRELATED using each author 's ENTITY as a coherent ENTITYOTHER .
Our approach is based on the idea that one person tends to use one expression for one meaning.	meaning	expression	model-feature	{'e1': {'word': 'meaning', 'word_index': [(17, 17)], 'id': 'C04-1116.10'}, 'e2': {'word': 'expression', 'word_index': [(14, 14)], 'id': 'C04-1116.9'}}	Our approach is based on the idea that one person tends to use one ENTITYOTHER for one ENTITY .
According to our assumption, most of the words with similar context features in each author's corpus tend not to be synonymous expressions.	similar context features	words	model-feature	{'e1': {'word': 'similar context features', 'word_index': [(10, 10)], 'id': 'C04-1116.12'}, 'e2': {'word': 'words', 'word_index': [(8, 8)], 'id': 'C04-1116.11'}}	According to our assumption , most of the ENTITYOTHER with ENTITY in each author 's ENTITYUNRELATED tend not to be ENTITYUNRELATED .
Our proposed method improves the accuracy of our term aggregation system, showing that our approach is successful.	term aggregation system	accuracy	result	{'e1': {'word': 'term aggregation system', 'word_index': [(8, 8)], 'id': 'C04-1116.16'}, 'e2': {'word': 'accuracy', 'word_index': [(5, 5)], 'id': 'C04-1116.15'}}	Our proposed method improves the ENTITYOTHER of our ENTITY , showing that our approach is successful .
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent.	sentence extraction	summarization	usage	{'e1': {'word': 'sentence extraction', 'word_index': [(1, 1)], 'id': 'C04-1128.1'}, 'e2': {'word': 'summarization', 'word_index': [(6, 6)], 'id': 'C04-1128.2'}}	While ENTITY as an approach to ENTITYOTHER has been shown to work in ENTITYUNRELATED of certain ENTITYUNRELATED , because of the conversational nature of ENTITYUNRELATED where ENTITYUNRELATED are made in relation to one made previously , ENTITYUNRELATED may not capture the necessary ENTITYUNRELATED of ENTITYUNRELATED that would make a ENTITYUNRELATED coherent .
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent.	genres	documents	model-feature	{'e1': {'word': 'genres', 'word_index': [(16, 16)], 'id': 'C04-1128.4'}, 'e2': {'word': 'documents', 'word_index': [(13, 13)], 'id': 'C04-1128.3'}}	While ENTITYUNRELATED as an approach to ENTITYUNRELATED has been shown to work in ENTITYOTHER of certain ENTITY , because of the conversational nature of ENTITYUNRELATED where ENTITYUNRELATED are made in relation to one made previously , ENTITYUNRELATED may not capture the necessary ENTITYUNRELATED of ENTITYUNRELATED that would make a ENTITYUNRELATED coherent .
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent.	email communication	utterances	part_whole	{'e1': {'word': 'email communication', 'word_index': [(24, 24)], 'id': 'C04-1128.5'}, 'e2': {'word': 'utterances', 'word_index': [(26, 26)], 'id': 'C04-1128.6'}}	While ENTITYUNRELATED as an approach to ENTITYUNRELATED has been shown to work in ENTITYUNRELATED of certain ENTITYUNRELATED , because of the conversational nature of ENTITY where ENTITYOTHER are made in relation to one made previously , ENTITYUNRELATED may not capture the necessary ENTITYUNRELATED of ENTITYUNRELATED that would make a ENTITYUNRELATED coherent .
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent.	segments	dialogue	part_whole	{'e1': {'word': 'segments', 'word_index': [(42, 42)], 'id': 'C04-1128.8'}, 'e2': {'word': 'dialogue', 'word_index': [(44, 44)], 'id': 'C04-1128.9'}}	While ENTITYUNRELATED as an approach to ENTITYUNRELATED has been shown to work in ENTITYUNRELATED of certain ENTITYUNRELATED , because of the conversational nature of ENTITYUNRELATED where ENTITYUNRELATED are made in relation to one made previously , ENTITYUNRELATED may not capture the necessary ENTITY of ENTITYOTHER that would make a ENTITYUNRELATED coherent .
In this paper, we present our work on the detection of question-answer pairs in an email conversation for the task of email summarization.	email conversation	question-answer pairs	part_whole	{'e1': {'word': 'email conversation', 'word_index': [(15, 15)], 'id': 'C04-1128.12'}, 'e2': {'word': 'question-answer pairs', 'word_index': [(12, 12)], 'id': 'C04-1128.11'}}	In this paper , we present our work on the detection of ENTITYOTHER in an ENTITY for the task of ENTITYUNRELATED .
We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.	features	lexical similarity	usage	{'e1': {'word': 'features', 'word_index': [(4, 4)], 'id': 'C04-1128.14'}, 'e2': {'word': 'lexical similarity', 'word_index': [(19, 19)], 'id': 'C04-1128.15'}}	We show that various ENTITY based on the structure of email - threads can be used to improve upon ENTITYOTHER of ENTITYUNRELATED for ENTITYUNRELATED .
The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms, an independence model, and a parametric affinity model.	co-occurrence distribution	terms	model-feature	{'e1': {'word': 'co-occurrence distribution', 'word_index': [(12, 12)], 'id': 'C04-1147.3'}, 'e2': {'word': 'terms', 'word_index': [(16, 16)], 'id': 'C04-1147.4'}}	The framework is composed of a novel algorithm to efficiently compute the ENTITY between pairs of ENTITYOTHER , an ENTITYUNRELATED , and a ENTITYUNRELATED .
In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.	similarity	words	model-feature	{'e1': {'word': 'similarity', 'word_index': [(13, 13)], 'id': 'C04-1147.8'}, 'e2': {'word': 'words', 'word_index': [(15, 15)], 'id': 'C04-1147.9'}}	In comparison with previous ENTITYUNRELATED , which either use arbitrary windows to compute ENTITY between ENTITYOTHER or use ENTITYUNRELATED to create ENTITYUNRELATED , in this paper we focus on ENTITYUNRELATED intended to capture the ENTITYUNRELATED of any pair of ENTITYUNRELATED or ENTITYUNRELATED at any distance in the ENTITYUNRELATED .
In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.	lexical affinity	sequential models	usage	{'e1': {'word': 'lexical affinity', 'word_index': [(18, 18)], 'id': 'C04-1147.10'}, 'e2': {'word': 'sequential models', 'word_index': [(21, 21)], 'id': 'C04-1147.11'}}	In comparison with previous ENTITYUNRELATED , which either use arbitrary windows to compute ENTITYUNRELATED between ENTITYUNRELATED or use ENTITY to create ENTITYOTHER , in this paper we focus on ENTITYUNRELATED intended to capture the ENTITYUNRELATED of any pair of ENTITYUNRELATED or ENTITYUNRELATED at any distance in the ENTITYUNRELATED .
In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.	co-occurrence patterns	words	model-feature	{'e1': {'word': 'co-occurrence patterns', 'word_index': [(34, 34)], 'id': 'C04-1147.13'}, 'e2': {'word': 'words', 'word_index': [(39, 39)], 'id': 'C04-1147.14'}}	In comparison with previous ENTITYUNRELATED , which either use arbitrary windows to compute ENTITYUNRELATED between ENTITYUNRELATED or use ENTITYUNRELATED to create ENTITYUNRELATED , in this paper we focus on ENTITYUNRELATED intended to capture the ENTITY of any pair of ENTITYOTHER or ENTITYUNRELATED at any distance in the ENTITYUNRELATED .
The paper presents a method for word sense disambiguation based on parallel corpora.	parallel corpora	word sense disambiguation	usage	{'e1': {'word': 'parallel corpora', 'word_index': [(9, 9)], 'id': 'C04-1192.2'}, 'e2': {'word': 'word sense disambiguation', 'word_index': [(6, 6)], 'id': 'C04-1192.1'}}	The paper presents a method for ENTITYOTHER based on ENTITY .
The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus.	automatic extraction	word clustering	usage	{'e1': {'word': 'automatic extraction', 'word_index': [(11, 11)], 'id': 'C04-1192.5'}, 'e2': {'word': 'word clustering', 'word_index': [(8, 8)], 'id': 'C04-1192.4'}}	The method exploits recent advances in ENTITYUNRELATED and ENTITYOTHER based on ENTITY of ENTITYUNRELATED and being supported by available aligned ENTITYUNRELATED for the ENTITYUNRELATED in the ENTITYUNRELATED .
The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet.	alignment errors	multilingually aligned wordnets	part_whole	{'e1': {'word': 'alignment errors', 'word_index': [(16, 16)], 'id': 'C04-1192.14'}, 'e2': {'word': 'multilingually aligned wordnets', 'word_index': [(18, 18)], 'id': 'C04-1192.15'}}	The same system used in a validation mode , can be used to check and spot ENTITY in ENTITYOTHER as ENTITYUNRELATED and ENTITYUNRELATED .
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.	Minimum Bayes-Risk (MBR) decoding	statistical machine translation	usage	{'e1': {'word': 'Minimum Bayes-Risk (MBR) decoding', 'word_index': [(2, 2)], 'id': 'N04-1022.1'}, 'e2': {'word': 'statistical machine translation', 'word_index': [(4, 4)], 'id': 'N04-1022.2'}}	We present ENTITY for ENTITYOTHER .
This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.	loss functions	translation performance	model-feature	{'e1': {'word': 'loss functions', 'word_index': [(10, 10)], 'id': 'N04-1022.5'}, 'e2': {'word': 'translation performance', 'word_index': [(13, 13)], 'id': 'N04-1022.6'}}	This statistical approach aims to minimize ENTITYUNRELATED of ENTITYUNRELATED under ENTITY that measure ENTITYOTHER .
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.	loss functions	linguistic information	model-feature	{'e1': {'word': 'loss functions', 'word_index': [(5, 5)], 'id': 'N04-1022.7'}, 'e2': {'word': 'linguistic information', 'word_index': [(11, 11)], 'id': 'N04-1022.8'}}	We describe a hierarchy of ENTITY that incorporate different levels of ENTITYOTHER from ENTITYUNRELATED , ENTITYUNRELATED from an ENTITYUNRELATED , and ENTITYUNRELATED from ENTITYUNRELATED of ENTITYUNRELATED .
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.	MT system	word-to-word alignments	part_whole	{'e1': {'word': 'MT system', 'word_index': [(18, 18)], 'id': 'N04-1022.11'}, 'e2': {'word': 'word-to-word alignments', 'word_index': [(15, 15)], 'id': 'N04-1022.10'}}	We describe a hierarchy of ENTITYUNRELATED that incorporate different levels of ENTITYUNRELATED from ENTITYUNRELATED , ENTITYOTHER from an ENTITY , and ENTITYUNRELATED from ENTITYUNRELATED of ENTITYUNRELATED .
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.	parse-trees	source and target language sentences	model-feature	{'e1': {'word': 'parse-trees', 'word_index': [(23, 23)], 'id': 'N04-1022.13'}, 'e2': {'word': 'source and target language sentences', 'word_index': [(25, 25)], 'id': 'N04-1022.14'}}	We describe a hierarchy of ENTITYUNRELATED that incorporate different levels of ENTITYUNRELATED from ENTITYUNRELATED , ENTITYUNRELATED from an ENTITYUNRELATED , and ENTITYUNRELATED from ENTITY of ENTITYOTHER .
We report the performance of the MBR decoders on a Chinese-to-English translation task.	performance	MBR decoders	result	{'e1': {'word': 'performance', 'word_index': [(3, 3)], 'id': 'N04-1022.15'}, 'e2': {'word': 'MBR decoders', 'word_index': [(6, 6)], 'id': 'N04-1022.16'}}	We report the ENTITY of the ENTITYOTHER on a ENTITYUNRELATED .
Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.	MBR decoding	statistical MT performance	usage	{'e1': {'word': 'MBR decoding', 'word_index': [(4, 4)], 'id': 'N04-1022.18'}, 'e2': {'word': 'statistical MT performance', 'word_index': [(10, 10)], 'id': 'N04-1022.19'}}	Our results show that ENTITY can be used to tune ENTITYOTHER for specific ENTITYUNRELATED .
Information extraction techniques automatically create structured databases from unstructured data sources, such as the Web or newswire documents.	Information extraction techniques	structured databases	result	{'e1': {'word': 'Information extraction techniques', 'word_index': [(0, 0)], 'id': 'N04-4028.1'}, 'e2': {'word': 'structured databases', 'word_index': [(3, 3)], 'id': 'N04-4028.2'}}	ENTITY automatically create ENTITYOTHER from ENTITYUNRELATED , such as the Web or ENTITYUNRELATED .
For many reasons, it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field.	confidence	extracted field	model-feature	{'e1': {'word': 'confidence', 'word_index': [(12, 12)], 'id': 'N04-4028.6'}, 'e2': {'word': 'extracted field', 'word_index': [(21, 21)], 'id': 'N04-4028.7'}}	For many reasons , it is highly desirable to accurately estimate the ENTITY the system has in the correctness of each ENTITYOTHER .
The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a  Markov model.	linear-chain conditional random field (CRF)	information extraction system	usage	{'e1': {'word': 'linear-chain conditional random field (CRF)', 'word_index': [(8, 8)], 'id': 'N04-4028.9'}, 'e2': {'word': 'information extraction system', 'word_index': [(1, 1)], 'id': 'N04-4028.8'}}	The ENTITYOTHER we evaluate is based on a ENTITY , a ENTITYUNRELATED which has performed well on ENTITYUNRELATED because of its ability to capture arbitrary , overlapping ENTITYUNRELATED of the ENTITYUNRELATED in a ENTITYUNRELATED .
The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a  Markov model.	probabilistic model	information extraction tasks	usage	{'e1': {'word': 'probabilistic model', 'word_index': [(11, 11)], 'id': 'N04-4028.10'}, 'e2': {'word': 'information extraction tasks', 'word_index': [(17, 17)], 'id': 'N04-4028.11'}}	The ENTITYUNRELATED we evaluate is based on a ENTITYUNRELATED , a ENTITY which has performed well on ENTITYOTHER because of its ability to capture arbitrary , overlapping ENTITYUNRELATED of the ENTITYUNRELATED in a ENTITYUNRELATED .
The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a  Markov model.	 Markov model	features	model-feature	{'e1': {'word': ' Markov model', 'word_index': [(33, 33)], 'id': 'N04-4028.14'}, 'e2': {'word': 'features', 'word_index': [(27, 27)], 'id': 'N04-4028.12'}}	The ENTITYUNRELATED we evaluate is based on a ENTITYUNRELATED , a ENTITYUNRELATED which has performed well on ENTITYUNRELATED because of its ability to capture arbitrary , overlapping ENTITYOTHER of the ENTITYUNRELATED in a ENTITY .
We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records.	confidence	extracted fields	model-feature	{'e1': {'word': 'confidence', 'word_index': [(7, 7)], 'id': 'N04-4028.15'}, 'e2': {'word': 'extracted fields', 'word_index': [(10, 10)], 'id': 'N04-4028.16'}}	We implement several techniques to estimate the ENTITY of both ENTITYOTHER and entire ENTITYUNRELATED , obtaining an ENTITYUNRELATED of 98 % for retrieving correct ENTITYUNRELATED and 87 % for multi-field records .
It then presents an implemented graphic interpretation system that takes into account a variety of communicative signals, and an evaluation study showing that evidence obtained from shallow processing of the graphic's caption has a significant impact on the system's success.	communicative signals	graphic interpretation system	usage	{'e1': {'word': 'communicative signals', 'word_index': [(13, 13)], 'id': 'P05-1028.4'}, 'e2': {'word': 'graphic interpretation system', 'word_index': [(5, 5)], 'id': 'P05-1028.3'}}	It then presents an implemented ENTITYOTHER that takes into account a variety of ENTITY , and an evaluation study showing that evidence obtained from ENTITYUNRELATED of the graphic 's caption has a significant impact on the system 's success .
We present a framework for word alignment based on log-linear models.	log-linear models	word alignment	usage	{'e1': {'word': 'log-linear models', 'word_index': [(8, 8)], 'id': 'P05-1057.2'}, 'e2': {'word': 'word alignment', 'word_index': [(5, 5)], 'id': 'P05-1057.1'}}	We present a framework for ENTITYOTHER based on ENTITY .
All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables.	knowledge sources	feature functions	usage	{'e1': {'word': 'knowledge sources', 'word_index': [(1, 1)], 'id': 'P05-1057.3'}, 'e2': {'word': 'feature functions', 'word_index': [(5, 5)], 'id': 'P05-1057.4'}}	All ENTITY are treated as ENTITYOTHER , which depend on the ENTITYUNRELATED , the ENTITYUNRELATED and possible additional variables .
Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.	Log-linear models	statistical alignment models	usage	{'e1': {'word': 'Log-linear models', 'word_index': [(0, 0)], 'id': 'P05-1057.7'}, 'e2': {'word': 'statistical alignment models', 'word_index': [(2, 2)], 'id': 'P05-1057.8'}}	ENTITY allow ENTITYOTHER to be easily extended by incorporating ENTITYUNRELATED .
In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.	bilingual dictionary coverage	features	usage	{'e1': {'word': 'bilingual dictionary coverage', 'word_index': [(11, 11)], 'id': 'P05-1057.12'}, 'e2': {'word': 'features', 'word_index': [(13, 13)], 'id': 'P05-1057.13'}}	In this paper , we use ENTITYUNRELATED , ENTITYUNRELATED , and ENTITY as ENTITYOTHER .
Our experiments show that log-linear models significantly outperform IBM translation models.	log-linear models	IBM translation models	compare	{'e1': {'word': 'log-linear models', 'word_index': [(4, 4)], 'id': 'P05-1057.14'}, 'e2': {'word': 'IBM translation models', 'word_index': [(7, 7)], 'id': 'P05-1057.15'}}	Our experiments show that ENTITY significantly outperform ENTITYOTHER .
This paper presents the results of automatically inducing a Combinatory Categorial Grammar (CCG) lexicon from a Turkish dependency treebank.	Turkish dependency treebank	Combinatory Categorial Grammar (CCG) lexicon	usage	{'e1': {'word': 'Turkish dependency treebank', 'word_index': [(12, 12)], 'id': 'P05-2013.2'}, 'e2': {'word': 'Combinatory Categorial Grammar (CCG) lexicon', 'word_index': [(9, 9)], 'id': 'P05-2013.1'}}	This paper presents the results of automatically inducing a ENTITYOTHER from a ENTITY .
The fact that Turkish is an agglutinating free word order language presents a challenge for language theories.	agglutinating free word order language	Turkish	model-feature	{'e1': {'word': 'agglutinating free word order language', 'word_index': [(6, 6)], 'id': 'P05-2013.4'}, 'e2': {'word': 'Turkish', 'word_index': [(3, 3)], 'id': 'P05-2013.3'}}	The fact that ENTITYOTHER is an ENTITY presents a challenge for ENTITYUNRELATED .
We explored possible ways to obtain a compact lexicon, consistent with CCG principles, from a treebank which is an order of magnitude smaller than Penn WSJ.	treebank	Penn WSJ	compare	{'e1': {'word': 'treebank', 'word_index': [(15, 15)], 'id': 'P05-2013.8'}, 'e2': {'word': 'Penn WSJ', 'word_index': [(24, 24)], 'id': 'P05-2013.9'}}	We explored possible ways to obtain a ENTITYUNRELATED , consistent with ENTITYUNRELATED , from a ENTITY which is an order of magnitude smaller than ENTITYOTHER .
In the Chinese language, a verb may have its dependents on its left, right or on both sides.	verb	Chinese language	part_whole	{'e1': {'word': 'verb', 'word_index': [(5, 5)], 'id': 'I05-2044.2'}, 'e2': {'word': 'Chinese language', 'word_index': [(2, 2)], 'id': 'I05-2044.1'}}	In the ENTITYOTHER , a ENTITY may have its ENTITYUNRELATED on its left , right or on both sides .
The ambiguity resolution of right-side dependencies is essential for dependency parsing of sentences with two or more verbs.	ambiguity resolution	dependency parsing	part_whole	{'e1': {'word': 'ambiguity resolution', 'word_index': [(1, 1)], 'id': 'I05-2044.4'}, 'e2': {'word': 'dependency parsing', 'word_index': [(7, 7)], 'id': 'I05-2044.6'}}	The ENTITY of ENTITYUNRELATED is essential for ENTITYOTHER of ENTITYUNRELATED with two or more ENTITYUNRELATED .
The ambiguity resolution of right-side dependencies is essential for dependency parsing of sentences with two or more verbs.	verbs	sentences	part_whole	{'e1': {'word': 'verbs', 'word_index': [(14, 14)], 'id': 'I05-2044.8'}, 'e2': {'word': 'sentences', 'word_index': [(9, 9)], 'id': 'I05-2044.7'}}	The ENTITYUNRELATED of ENTITYUNRELATED is essential for ENTITYUNRELATED of ENTITYOTHER with two or more ENTITY .
Previous works on shift-reduce dependency parsers may not guarantee the connectivity of a dependency tree due to their weakness at resolving the right-side dependencies.	connectivity	dependency tree	model-feature	{'e1': {'word': 'connectivity', 'word_index': [(8, 8)], 'id': 'I05-2044.10'}, 'e2': {'word': 'dependency tree', 'word_index': [(11, 11)], 'id': 'I05-2044.11'}}	Previous works on ENTITYUNRELATED may not guarantee the ENTITY of a ENTITYOTHER due to their weakness at resolving the ENTITYUNRELATED .
This paper proposes a two-phase shift-reduce dependency parser based on SVM learning.	SVM learning	two-phase shift-reduce dependency parser	usage	{'e1': {'word': 'SVM learning', 'word_index': [(7, 7)], 'id': 'I05-2044.14'}, 'e2': {'word': 'two-phase shift-reduce dependency parser', 'word_index': [(4, 4)], 'id': 'I05-2044.13'}}	This paper proposes a ENTITYOTHER based on ENTITY .
In experimental evaluation, our proposed method outperforms previous shift-reduce dependency parsers for the Chine language, showing improvement of dependency accuracy by 10.08%.	shift-reduce dependency parsers	Chine language	usage	{'e1': {'word': 'shift-reduce dependency parsers', 'word_index': [(9, 9)], 'id': 'I05-2044.18'}, 'e2': {'word': 'Chine language', 'word_index': [(12, 12)], 'id': 'I05-2044.19'}}	In experimental evaluation , our proposed method outperforms previous ENTITY for the ENTITYOTHER , showing improvement of ENTITYUNRELATED by 10.08 %.
We present an operable definition of focus which is argued to be of a cognito-pragmatic nature and explore how it is determined in discourse in a formalized manner.	focus	discourse	part_whole	{'e1': {'word': 'focus', 'word_index': [(6, 6)], 'id': 'E99-1038.1'}, 'e2': {'word': 'discourse', 'word_index': [(25, 25)], 'id': 'E99-1038.2'}}	We present an operable definition of ENTITY which is argued to be of a cognito - pragmatic nature and explore how it is determined in ENTITYOTHER in a formalized manner .
Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via FDA as a discourse-level construct into speech synthesis systems, in particular, concept-to-speech systems, is also briefly discussed.	focus	speech synthesis systems	usage	{'e1': {'word': 'focus', 'word_index': [(16, 16)], 'id': 'E99-1038.9'}, 'e2': {'word': 'speech synthesis systems', 'word_index': [(23, 23)], 'id': 'E99-1038.12'}}	Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of ENTITY via ENTITYUNRELATED as a ENTITYUNRELATED into ENTITYOTHER , in particular , ENTITYUNRELATED , is also briefly discussed .
Currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone, e.g. LFG and PATR-II.	context-free phrase-structure grammar	grammatical formalisms	usage	{'e1': {'word': 'context-free phrase-structure grammar', 'word_index': [(10, 10)], 'id': 'E87-1037.2'}, 'e2': {'word': 'grammatical formalisms', 'word_index': [(2, 2)], 'id': 'E87-1037.1'}}	Currently several ENTITYOTHER converge towards being declarative and towards utilizing ENTITY as a backbone , e.g. ENTITYUNRELATED and ENTITYUNRELATED .
The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing.	rule-invocation strategies	context-free chart parsing	part_whole	{'e1': {'word': 'rule-invocation strategies', 'word_index': [(16, 16)], 'id': 'E87-1037.11'}, 'e2': {'word': 'context-free chart parsing', 'word_index': [(18, 18)], 'id': 'E87-1037.12'}}	The aim of this paper is to provide a survey and a practical comparison of fundamental ENTITY within ENTITYOTHER .
In this paper I will argue for a model of grammatical processing that is based on uniform processing and knowledge sources.	uniform processing	model of grammatical processing	usage	{'e1': {'word': 'uniform processing', 'word_index': [(13, 13)], 'id': 'E91-1043.2'}, 'e2': {'word': 'model of grammatical processing', 'word_index': [(8, 8)], 'id': 'E91-1043.1'}}	In this paper I will argue for a ENTITYOTHER that is based on ENTITY and ENTITYUNRELATED .
The main feature of this model is to view parsing and generation as two strongly interleaved tasks performed by a single parametrized deduction process.	parsing	generation	compare	{'e1': {'word': 'parsing', 'word_index': [(9, 9)], 'id': 'E91-1043.5'}, 'e2': {'word': 'generation', 'word_index': [(11, 11)], 'id': 'E91-1043.6'}}	The main ENTITYUNRELATED of this model is to view ENTITY and ENTITYOTHER as two strongly interleaved tasks performed by a single ENTITYUNRELATED process .
One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity: the generation of multiple analyses for one input word, many of which are implausible.	constituent parts	words	part_whole	{'e1': {'word': 'constituent parts', 'word_index': [(14, 14)], 'id': 'E93-1023.2'}, 'e2': {'word': 'words', 'word_index': [(11, 11)], 'id': 'E93-1023.1'}}	One of the major problems one is faced with when decomposing ENTITYOTHER into their ENTITY is ENTITYUNRELATED : the ENTITYUNRELATED of multiple ENTITYUNRELATED for one ENTITYUNRELATED , many of which are implausible .
One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity: the generation of multiple analyses for one input word, many of which are implausible.	analyses	input word	model-feature	{'e1': {'word': 'analyses', 'word_index': [(22, 22)], 'id': 'E93-1023.5'}, 'e2': {'word': 'input word', 'word_index': [(25, 25)], 'id': 'E93-1023.6'}}	One of the major problems one is faced with when decomposing ENTITYUNRELATED into their ENTITYUNRELATED is ENTITYUNRELATED : the ENTITYUNRELATED of multiple ENTITY for one ENTITYOTHER , many of which are implausible .
In order to deal with ambiguity, the MORphological PArser MORPA is provided with a probabilistic context-free grammar (PCFG), i.e.	probabilistic context-free grammar (PCFG)	MORphological PArser MORPA	part_whole	{'e1': {'word': 'probabilistic context-free grammar (PCFG)', 'word_index': [(13, 13)], 'id': 'E93-1023.9'}, 'e2': {'word': 'MORphological PArser MORPA', 'word_index': [(8, 8)], 'id': 'E93-1023.8'}}	In order to deal with ENTITYUNRELATED , the ENTITYOTHER is provided with a ENTITY , i.e.
"it combines a ""conventional"" context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse."	probability-based scoring function	"""conventional"" context-free morphological grammar"	usage	"{'e1': {'word': 'probability-based scoring function', 'word_index': [(10, 10)], 'id': 'E93-1023.12'}, 'e2': {'word': '""conventional"" context-free morphological grammar', 'word_index': [(3, 3)], 'id': 'E93-1023.10'}}"	it combines a ENTITYOTHER to filter out ENTITYUNRELATED with a ENTITY which determines the likelihood of each successful ENTITYUNRELATED .
MORPA is a fully implemented parser developed for use in a text-to-speech conversion system.	parser	text-to-speech conversion system	usage	{'e1': {'word': 'parser', 'word_index': [(5, 5)], 'id': 'E93-1023.18'}, 'e2': {'word': 'text-to-speech conversion system', 'word_index': [(11, 11)], 'id': 'E93-1023.19'}}	ENTITYUNRELATED is a fully implemented ENTITY developed for use in a ENTITYOTHER .
The output can be customized to meet different segmentation standards through the application of an ordered list of transformation.	segmentation standards	output	model-feature	{'e1': {'word': 'segmentation standards', 'word_index': [(8, 8)], 'id': 'I05-3022.5'}, 'e2': {'word': 'output', 'word_index': [(1, 1)], 'id': 'I05-3022.4'}}	The ENTITYOTHER can be customized to meet different ENTITY through the application of an ordered list of transformation .
The system participated in all the tracks of the segmentation bakeoff -- PK-open, PK-closed, AS-open, AS-closed, HK-open, HK-closed, MSR-open and MSR- closed -- and achieved the state-of-the-art performance in MSR-open, MSR-close and PK-open tracks.	system	state-of-the-art performance	result	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'I05-3022.6'}, 'e2': {'word': 'state-of-the-art performance', 'word_index': [(30, 30)], 'id': 'I05-3022.16'}}	The ENTITY participated in all the tracks of the ENTITYUNRELATED -- ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED -- and achieved the ENTITYOTHER in ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED tracks .
In this paper a morphological component with a limited capability to automatically interpret (and generate) derived words is presented.	morphological component	derived words	usage	{'e1': {'word': 'morphological component', 'word_index': [(4, 4)], 'id': 'E93-1043.1'}, 'e2': {'word': 'derived words', 'word_index': [(16, 16)], 'id': 'E93-1043.2'}}	In this paper a ENTITY with a limited capability to automatically interpret ( and generate ) ENTITYOTHER is presented .
The system combines an extended two-level morphology [Trost, 1991a; Trost, 1991b] with a feature-based word grammar building on a hierarchical lexicon.	hierarchical lexicon	feature-based word grammar	usage	{'e1': {'word': 'hierarchical lexicon', 'word_index': [(23, 23)], 'id': 'E93-1043.5'}, 'e2': {'word': 'feature-based word grammar', 'word_index': [(19, 19)], 'id': 'E93-1043.4'}}	The system combines an extended ENTITYUNRELATED [ Trost , 1991 a ; Trost , 1991 b ] with a ENTITYOTHER building on a ENTITY .
Polymorphemic stems not explicitly stored in the lexicon are given a compositional interpretation.	compositional interpretation	Polymorphemic stems	model-feature	{'e1': {'word': 'compositional interpretation', 'word_index': [(10, 10)], 'id': 'E93-1043.8'}, 'e2': {'word': 'Polymorphemic stems', 'word_index': [(0, 0)], 'id': 'E93-1043.6'}}	ENTITYOTHER not explicitly stored in the ENTITYUNRELATED are given a ENTITY .
This paper proposes an approach to full parsing suitable for Information Extraction from texts.	full parsing	Information Extraction	usage	{'e1': {'word': 'full parsing', 'word_index': [(6, 6)], 'id': 'E99-1014.1'}, 'e2': {'word': 'Information Extraction', 'word_index': [(9, 9)], 'id': 'E99-1014.2'}}	This paper proposes an approach to ENTITY suitable for ENTITYOTHER from ENTITYUNRELATED .
Sequences of cascades of rules deterministically analyze the text, building unambiguous structures.	unambiguous structures	text	model-feature	{'e1': {'word': 'unambiguous structures', 'word_index': [(11, 11)], 'id': 'E99-1014.6'}, 'e2': {'word': 'text', 'word_index': [(8, 8)], 'id': 'E99-1014.5'}}	Sequences of cascades of ENTITYUNRELATED deterministically analyze the ENTITYOTHER , building ENTITY .
It was implemented in the IE module of FACILE, a EU project for multilingual text classification and IE.	IE module	FACILE, a EU project for multilingual text classification and IE	part_whole	{'e1': {'word': 'IE module', 'word_index': [(5, 5)], 'id': 'E99-1014.13'}, 'e2': {'word': 'FACILE, a EU project for multilingual text classification and IE', 'word_index': [(7, 7)], 'id': 'E99-1014.14'}}	It was implemented in the ENTITY of ENTITYOTHER .
A very simple improved duration model has reduced the error rate by about 10% in both triphone and semiphone systems.	duration model	error rate	result	{'e1': {'word': 'duration model', 'word_index': [(4, 4)], 'id': 'H91-1010.3'}, 'e2': {'word': 'error rate', 'word_index': [(8, 8)], 'id': 'H91-1010.4'}}	A very simple improved ENTITY has reduced the ENTITYOTHER by about 10 % in both ENTITYUNRELATED .
Finally, the recognizer has been modified to use bigram back-off language models.	bigram back-off language models	recognizer	usage	{'e1': {'word': 'bigram back-off language models', 'word_index': [(9, 9)], 'id': 'H91-1010.8'}, 'e2': {'word': 'recognizer', 'word_index': [(3, 3)], 'id': 'H91-1010.7'}}	Finally , the ENTITYOTHER has been modified to use ENTITY .
There are four language pairs currently supported by GLOSSER: English-Bulgarian, English-Estonian, English-Hungarian and French-Dutch.	language pairs	GLOSSER	model-feature	{'e1': {'word': 'language pairs', 'word_index': [(3, 3)], 'id': 'A97-1020.3'}, 'e2': {'word': 'GLOSSER', 'word_index': [(7, 7)], 'id': 'A97-1020.4'}}	There are four ENTITY currently supported by ENTITYOTHER : ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED .
A demonstration (in UNIX) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis (ICALL), including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples.	word examples	aligned bilingual corpus	part_whole	{'e1': {'word': 'word examples', 'word_index': [(26, 26)], 'id': 'A97-1020.14'}, 'e2': {'word': 'aligned bilingual corpus', 'word_index': [(24, 24)], 'id': 'A97-1020.13'}}	A demonstration ( in UNIX ) for ENTITYUNRELATED emphasizes components put to novel technical uses in ENTITYUNRELATED , including ENTITYUNRELATED and ENTITYUNRELATED for an ENTITYOTHER of ENTITY .
This paper addresses the problem of identifying likely topics of texts by their position in the text.	topics	texts	model-feature	{'e1': {'word': 'topics', 'word_index': [(8, 8)], 'id': 'A97-1042.1'}, 'e2': {'word': 'texts', 'word_index': [(10, 10)], 'id': 'A97-1042.2'}}	This paper addresses the problem of identifying likely ENTITY of ENTITYOTHER by their position in the ENTITYUNRELATED .
It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure.	genre-specific regularities	discourse structure	model-feature	{'e1': {'word': 'genre-specific regularities', 'word_index': [(22, 22)], 'id': 'A97-1042.7'}, 'e2': {'word': 'discourse structure', 'word_index': [(24, 24)], 'id': 'A97-1042.8'}}	It describes the automated ENTITYUNRELATED and evaluation of an ENTITYUNRELATED , a method of locating the likely positions of ENTITYUNRELATED based on ENTITY of ENTITYOTHER .
We make use of a conditional log-linear model, with hidden variables representing the assignment of lexical items to word clusters or word senses.	hidden variables	conditional log-linear model	model-feature	{'e1': {'word': 'hidden variables', 'word_index': [(8, 8)], 'id': 'H05-1064.4'}, 'e2': {'word': 'conditional log-linear model', 'word_index': [(5, 5)], 'id': 'H05-1064.3'}}	We make use of a ENTITYOTHER , with ENTITY representing the ENTITYUNRELATED of ENTITYUNRELATED to ENTITYUNRELATED or ENTITYUNRELATED .
We make use of a conditional log-linear model, with hidden variables representing the assignment of lexical items to word clusters or word senses.	word clusters	lexical items	model-feature	{'e1': {'word': 'word clusters', 'word_index': [(15, 15)], 'id': 'H05-1064.7'}, 'e2': {'word': 'lexical items', 'word_index': [(13, 13)], 'id': 'H05-1064.6'}}	We make use of a ENTITYUNRELATED , with ENTITYUNRELATED representing the ENTITYUNRELATED of ENTITYOTHER to ENTITY or ENTITYUNRELATED .
The model learns to automatically make these assignments based on a discriminative training criterion.	discriminative training criterion	assignments	usage	{'e1': {'word': 'discriminative training criterion', 'word_index': [(11, 11)], 'id': 'H05-1064.10'}, 'e2': {'word': 'assignments', 'word_index': [(7, 7)], 'id': 'H05-1064.9'}}	The model learns to automatically make these ENTITYOTHER based on a ENTITY .
Training and decoding with the model requires summing over an exponential number of hidden-variable assignments: the required summations can be computed efficiently and exactly using dynamic programming.	dynamic programming	decoding	usage	{'e1': {'word': 'dynamic programming', 'word_index': [(25, 25)], 'id': 'H05-1064.14'}, 'e2': {'word': 'decoding', 'word_index': [(2, 2)], 'id': 'H05-1064.12'}}	ENTITYUNRELATED and ENTITYOTHER with the model requires summing over an exponential number of ENTITYUNRELATED : the required summations can be computed efficiently and exactly using ENTITY .
The model gives an F-measure improvement of ~1.25% beyond the base parser, and an ~0.25% improvement beyond Collins (2000) reranker.	base parser	Collins (2000) reranker	compare	{'e1': {'word': 'base parser', 'word_index': [(11, 11)], 'id': 'H05-1064.17'}, 'e2': {'word': 'Collins (2000) reranker', 'word_index': [(20, 20)], 'id': 'H05-1064.18'}}	The model gives an ENTITYUNRELATED of ~ 1.25 % beyond the ENTITY , and an ~ 0.25 % improvement beyond ENTITYOTHER .
Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan.	scripts	Taiwan Child Language Corpus	part_whole	{'e1': {'word': 'scripts', 'word_index': [(2, 2)], 'id': 'I05-4008.2'}, 'e2': {'word': 'Taiwan Child Language Corpus', 'word_index': [(0, 0)], 'id': 'I05-4008.1'}}	ENTITYOTHER contains ENTITY transcribed from about 330 hours of ENTITYUNRELATED of fourteen young children from ENTITYUNRELATED speaking families in Taiwan .
Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan.	Southern Min Chinese	recordings	model-feature	{'e1': {'word': 'Southern Min Chinese', 'word_index': [(15, 15)], 'id': 'I05-4008.4'}, 'e2': {'word': 'recordings', 'word_index': [(9, 9)], 'id': 'I05-4008.3'}}	ENTITYUNRELATED contains ENTITYUNRELATED transcribed from about 330 hours of ENTITYOTHER of fourteen young children from ENTITY speaking families in Taiwan .
The format of the corpus adopts the Child Language Data Exchange System (CHILDES).	Child Language Data Exchange System (CHILDES)	corpus	model-feature	{'e1': {'word': 'Child Language Data Exchange System (CHILDES)', 'word_index': [(7, 7)], 'id': 'I05-4008.6'}, 'e2': {'word': 'corpus', 'word_index': [(4, 4)], 'id': 'I05-4008.5'}}	The format of the ENTITYOTHER adopts the ENTITY .
The size of the corpus is about 1.6 million words.	words	corpus	part_whole	{'e1': {'word': 'words', 'word_index': [(9, 9)], 'id': 'I05-4008.8'}, 'e2': {'word': 'corpus', 'word_index': [(4, 4)], 'id': 'I05-4008.7'}}	The size of the ENTITYOTHER is about 1.6 million ENTITY .
In this paper, we describe data collection, transcription, word segmentation, and part-of-speech annotation of this corpus.	part-of-speech annotation	corpus	usage	{'e1': {'word': 'part-of-speech annotation', 'word_index': [(13, 13)], 'id': 'I05-4008.12'}, 'e2': {'word': 'corpus', 'word_index': [(16, 16)], 'id': 'I05-4008.13'}}	In this paper , we describe ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and ENTITY of this ENTITYOTHER .
Robust natural language interpretation requires strong semantic domain models, fail-soft recovery heuristics, and very flexible control structures.	semantic domain models	natural language interpretation	usage	{'e1': {'word': 'semantic domain models', 'word_index': [(4, 4)], 'id': 'P81-1032.2'}, 'e2': {'word': 'natural language interpretation', 'word_index': [(1, 1)], 'id': 'P81-1032.1'}}	Robust ENTITYOTHER requires strong ENTITY , ENTITYUNRELATED , and very flexible ENTITYUNRELATED .
Although single-strategy parsers have met with a measure of success, a multi-strategy approach is shown to provide a much higher degree of flexibility, redundancy, and ability to bring task-specific domain knowledge (in addition to general linguistic knowledge) to bear on both grammatical and ungrammatical input.	single-strategy parsers	multi-strategy approach	compare	{'e1': {'word': 'single-strategy parsers', 'word_index': [(1, 1)], 'id': 'P81-1032.5'}, 'e2': {'word': 'multi-strategy approach', 'word_index': [(11, 11)], 'id': 'P81-1032.6'}}	Although ENTITY have met with a measure of success , a ENTITYOTHER is shown to provide a much higher degree of flexibility , redundancy , and ability to bring ENTITYUNRELATED ( in addition to ENTITYUNRELATED ) to bear on both ENTITYUNRELATED .
Although single-strategy parsers have met with a measure of success, a multi-strategy approach is shown to provide a much higher degree of flexibility, redundancy, and ability to bring task-specific domain knowledge (in addition to general linguistic knowledge) to bear on both grammatical and ungrammatical input.	task-specific domain knowledge	general linguistic knowledge	compare	{'e1': {'word': 'task-specific domain knowledge', 'word_index': [(29, 29)], 'id': 'P81-1032.7'}, 'e2': {'word': 'general linguistic knowledge', 'word_index': [(34, 34)], 'id': 'P81-1032.8'}}	Although ENTITYUNRELATED have met with a measure of success , a ENTITYUNRELATED is shown to provide a much higher degree of flexibility , redundancy , and ability to bring ENTITY ( in addition to ENTITYOTHER ) to bear on both ENTITYUNRELATED .
A parsing algorithm is presented that integrates several different parsing strategies, with case-frame instantiation dominating.	parsing strategies	parsing algorithm	usage	{'e1': {'word': 'parsing strategies', 'word_index': [(8, 8)], 'id': 'P81-1032.11'}, 'e2': {'word': 'parsing algorithm', 'word_index': [(1, 1)], 'id': 'P81-1032.10'}}	A ENTITYOTHER is presented that integrates several different ENTITY , with ENTITYUNRELATED dominating .
Each of these parsing strategies exploits different types of knowledge; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and ungrammatical structures, as well as less exotic, grammatically correct input.	types of knowledge	parsing strategies	usage	{'e1': {'word': 'types of knowledge', 'word_index': [(6, 6)], 'id': 'P81-1032.14'}, 'e2': {'word': 'parsing strategies', 'word_index': [(3, 3)], 'id': 'P81-1032.13'}}	Each of these ENTITYOTHER exploits different ENTITY ; and their combination provides a strong framework in which to process ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED , as well as less exotic , ENTITYUNRELATED .
Each of these parsing strategies exploits different types of knowledge; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and ungrammatical structures, as well as less exotic, grammatically correct input.	ungrammatical structures	grammatically correct input	compare	{'e1': {'word': 'ungrammatical structures', 'word_index': [(24, 24)], 'id': 'P81-1032.17'}, 'e2': {'word': 'grammatically correct input', 'word_index': [(32, 32)], 'id': 'P81-1032.18'}}	Each of these ENTITYUNRELATED exploits different ENTITYUNRELATED ; and their combination provides a strong framework in which to process ENTITYUNRELATED , ENTITYUNRELATED , and ENTITY , as well as less exotic , ENTITYOTHER .
Several specific heuristics for handling ungrammatical input are presented within this multi-strategy framework.	specific heuristics	multi-strategy framework	part_whole	{'e1': {'word': 'specific heuristics', 'word_index': [(1, 1)], 'id': 'P81-1032.19'}, 'e2': {'word': 'multi-strategy framework', 'word_index': [(9, 9)], 'id': 'P81-1032.21'}}	Several ENTITY for handling ENTITYUNRELATED are presented within this ENTITYOTHER .
By generalizing the notion of location of a constituent to allow discontinuous locations, one can describe the discontinuous constituents of non-configurational languages.	discontinuous constituents	non-configurational languages	part_whole	{'e1': {'word': 'discontinuous constituents', 'word_index': [(14, 14)], 'id': 'P85-1015.3'}, 'e2': {'word': 'non-configurational languages', 'word_index': [(16, 16)], 'id': 'P85-1015.4'}}	By generalizing the notion of ENTITYUNRELATED to allow ENTITYUNRELATED , one can describe the ENTITY of ENTITYOTHER .
These discontinuous constituents can be described by a variant of definite clause grammars, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages.	definite clause grammars	discontinuous constituents	model-feature	{'e1': {'word': 'definite clause grammars', 'word_index': [(9, 9)], 'id': 'P85-1015.6'}, 'e2': {'word': 'discontinuous constituents', 'word_index': [(1, 1)], 'id': 'P85-1015.5'}}	These ENTITYOTHER can be described by a variant of ENTITY , and these ENTITYUNRELATED can be used in conjunction with a ENTITYUNRELATED to create a ENTITYUNRELATED .
These discontinuous constituents can be described by a variant of definite clause grammars, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages.	grammars	parser for non-configurational languages	usage	{'e1': {'word': 'grammars', 'word_index': [(13, 13)], 'id': 'P85-1015.7'}, 'e2': {'word': 'parser for non-configurational languages', 'word_index': [(25, 25)], 'id': 'P85-1015.9'}}	These ENTITYUNRELATED can be described by a variant of ENTITYUNRELATED , and these ENTITY can be used in conjunction with a ENTITYUNRELATED to create a ENTITYOTHER .
 A system is described for acquiring a context-sensitive, phrase structure grammar which is applied by a best-path, bottom-up, deterministic parser.	context-sensitive, phrase structure grammar	best-path, bottom-up, deterministic parser	usage	{'e1': {'word': 'context-sensitive, phrase structure grammar', 'word_index': [(7, 7)], 'id': 'P91-1016.1'}, 'e2': {'word': 'best-path, bottom-up, deterministic parser', 'word_index': [(13, 13)], 'id': 'P91-1016.2'}}	A system is described for acquiring a ENTITY which is applied by a ENTITYOTHER .
Overall, this research concludes that CSG is a computationally and conceptually tractable approach to the construction of phrase structure grammar for news story text.	CSG	phrase structure grammar	usage	{'e1': {'word': 'CSG', 'word_index': [(6, 6)], 'id': 'P91-1016.6'}, 'e2': {'word': 'phrase structure grammar', 'word_index': [(18, 18)], 'id': 'P91-1016.7'}}	Overall , this research concludes that ENTITY is a computationally and conceptually tractable approach to the construction of ENTITYOTHER for ENTITYUNRELATED .
We present a corpus-based study of methods that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjectives.	semantically unmarked term	antonymous adjectives	part_whole	{'e1': {'word': 'semantically unmarked term', 'word_index': [(16, 16)], 'id': 'P95-1027.3'}, 'e2': {'word': 'antonymous adjectives', 'word_index': [(22, 22)], 'id': 'P95-1027.4'}}	We present a ENTITYUNRELATED of methods that have been proposed in the ENTITYUNRELATED for selecting the ENTITY out of a pair of ENTITYOTHER .
In the paper we propose a black-box method for comparing the lexical coverage of MT systems.	lexical coverage	MT systems	model-feature	{'e1': {'word': 'lexical coverage', 'word_index': [(10, 10)], 'id': 'P97-1015.4'}, 'e2': {'word': 'MT systems', 'word_index': [(12, 12)], 'id': 'P97-1015.5'}}	In the paper we propose a ENTITYUNRELATED for comparing the ENTITY of ENTITYOTHER .
The method is based on lists of words from different frequency classes.	frequency classes	words	model-feature	{'e1': {'word': 'frequency classes', 'word_index': [(10, 10)], 'id': 'P97-1015.7'}, 'e2': {'word': 'words', 'word_index': [(7, 7)], 'id': 'P97-1015.6'}}	The method is based on lists of ENTITYOTHER from different ENTITY .
We describe a method for interpreting abstract flat syntactic representations, LFG f-structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs).	underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs)	abstract flat syntactic representations, LFG f-structures	model-feature	{'e1': {'word': 'underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs)', 'word_index': [(9, 9)], 'id': 'P97-1052.2'}, 'e2': {'word': 'abstract flat syntactic representations, LFG f-structures', 'word_index': [(6, 6)], 'id': 'P97-1052.1'}}	We describe a method for interpreting ENTITYOTHER , as ENTITY .
It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f-structures through the translation images of f-structures as UDRSs.	model theoretic interpretation	f-structures	model-feature	{'e1': {'word': 'model theoretic interpretation', 'word_index': [(3, 3)], 'id': 'P97-1052.6'}, 'e2': {'word': 'f-structures', 'word_index': [(13, 13)], 'id': 'P97-1052.9'}}	It provides a ENTITY and an ENTITYUNRELATED which operates directly on ENTITYUNRELATED for ENTITYOTHER through the ENTITYUNRELATED of ENTITYUNRELATED as ENTITYUNRELATED .
It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f-structures through the translation images of f-structures as UDRSs.	translation images	f-structures	model-feature	{'e1': {'word': 'translation images', 'word_index': [(16, 16)], 'id': 'P97-1052.10'}, 'e2': {'word': 'f-structures', 'word_index': [(18, 18)], 'id': 'P97-1052.11'}}	It provides a ENTITYUNRELATED and an ENTITYUNRELATED which operates directly on ENTITYUNRELATED for ENTITYUNRELATED through the ENTITY of ENTITYOTHER as ENTITYUNRELATED .
In this paper we describe a systematic approach for creating a dialog management system based on a Construct Algebra, a collection of relations and operations on a task representation.	Construct Algebra	dialog management system	usage	{'e1': {'word': 'Construct Algebra', 'word_index': [(15, 15)], 'id': 'P99-1025.2'}, 'e2': {'word': 'dialog management system', 'word_index': [(11, 11)], 'id': 'P99-1025.1'}}	In this paper we describe a systematic approach for creating a ENTITYOTHER based on a ENTITY , a ENTITYUNRELATED on a ENTITYUNRELATED .
In this paper we describe a systematic approach for creating a dialog management system based on a Construct Algebra, a collection of relations and operations on a task representation.	collection of relations and operations	task representation	model-feature	{'e1': {'word': 'collection of relations and operations', 'word_index': [(18, 18)], 'id': 'P99-1025.3'}, 'e2': {'word': 'task representation', 'word_index': [(21, 21)], 'id': 'P99-1025.4'}}	In this paper we describe a systematic approach for creating a ENTITYUNRELATED based on a ENTITYUNRELATED , a ENTITY on a ENTITYOTHER .
These relations and operations are analytical components for building higher level abstractions called dialog motivators.	analytical components	dialog motivators	part_whole	{'e1': {'word': 'analytical components', 'word_index': [(3, 3)], 'id': 'P99-1025.6'}, 'e2': {'word': 'dialog motivators', 'word_index': [(10, 10)], 'id': 'P99-1025.7'}}	These ENTITYUNRELATED are ENTITY for building higher level abstractions called ENTITYOTHER .
The dialog manager, consisting of a collection of dialog motivators, is entirely built using the Construct Algebra.	collection of dialog motivators	dialog manager	part_whole	{'e1': {'word': 'collection of dialog motivators', 'word_index': [(6, 6)], 'id': 'P99-1025.9'}, 'e2': {'word': 'dialog manager', 'word_index': [(1, 1)], 'id': 'P99-1025.8'}}	The ENTITYOTHER , consisting of a ENTITY , is entirely built using the ENTITYUNRELATED .
STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World Wide Web.	language-independent system	automatic discovery of text	usage	{'e1': {'word': 'language-independent system', 'word_index': [(8, 8)], 'id': 'P99-1068.2'}, 'e2': {'word': 'automatic discovery of text', 'word_index': [(10, 10)], 'id': 'P99-1068.3'}}	ENTITYUNRELATED ( Resnik , 1998 ) is a ENTITY for ENTITYOTHER in ENTITYUNRELATED on the World Wide Web .
This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance.	automatic language identification	STRAND	part_whole	{'e1': {'word': 'automatic language identification', 'word_index': [(9, 9)], 'id': 'P99-1068.6'}, 'e2': {'word': 'STRAND', 'word_index': [(5, 5)], 'id': 'P99-1068.5'}}	This paper extends the preliminary ENTITYOTHER results by adding ENTITY , scaling up by orders of magnitude , and formally evaluating performance .
The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language.	English-French document pairs	automatically acquired parallel corpus	part_whole	{'e1': {'word': 'English-French document pairs', 'word_index': [(11, 11)], 'id': 'P99-1068.8'}, 'e2': {'word': 'automatically acquired parallel corpus', 'word_index': [(8, 8)], 'id': 'P99-1068.7'}}	The most recent end - product is an ENTITYOTHER comprising 2491 ENTITY , approximately 1.5 million ENTITYUNRELATED per ENTITYUNRELATED .
The main of this project is computer-assisted acquisition and morpho-syntactic description of verb-noun collocations in Polish.	computer-assisted acquisition and morpho-syntactic description of verb-noun collocations	Polish	usage	{'e1': {'word': 'computer-assisted acquisition and morpho-syntactic description of verb-noun collocations', 'word_index': [(6, 6)], 'id': 'L08-1260.2'}, 'e2': {'word': 'Polish', 'word_index': [(8, 8)], 'id': 'L08-1260.3'}}	The main of this project is ENTITY in ENTITYOTHER .
The presented here corpus-based approach permitted us to triple the size the verb-noun collocation dictionary for Polish.	corpus-based approach	verb-noun collocation dictionary for Polish	usage	{'e1': {'word': 'corpus-based approach', 'word_index': [(3, 3)], 'id': 'L08-1260.9'}, 'e2': {'word': 'verb-noun collocation dictionary for Polish', 'word_index': [(11, 11)], 'id': 'L08-1260.10'}}	The presented here ENTITY permitted us to triple the size the ENTITYOTHER .
In this paper we deal with a recently developed large Czech MWE database containing at the moment 160 000 MWEs (treated as lexical units).	MWEs	large Czech MWE database	part_whole	{'e1': {'word': 'MWEs', 'word_index': [(16, 16)], 'id': 'L08-1540.2'}, 'e2': {'word': 'large Czech MWE database', 'word_index': [(9, 9)], 'id': 'L08-1540.1'}}	In this paper we deal with a recently developed ENTITYOTHER containing at the moment 160 000 ENTITY ( treated as ENTITYUNRELATED ) .
It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others.	proper names	databases	part_whole	{'e1': {'word': 'proper names', 'word_index': [(15, 15)], 'id': 'L08-1540.7'}, 'e2': {'word': 'databases', 'word_index': [(13, 13)], 'id': 'L08-1540.6'}}	It was compiled from various resources such as ENTITYUNRELATED and ENTITYUNRELATED , public ENTITYOTHER of ENTITY and ENTITYUNRELATED , ENTITYUNRELATED obtained from ENTITYUNRELATED , lists of ENTITYUNRELATED and others .
It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others.	collocations	Czech WordNet	part_whole	{'e1': {'word': 'collocations', 'word_index': [(19, 19)], 'id': 'L08-1540.9'}, 'e2': {'word': 'Czech WordNet', 'word_index': [(22, 22)], 'id': 'L08-1540.10'}}	It was compiled from various resources such as ENTITYUNRELATED and ENTITYUNRELATED , public ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED , ENTITY obtained from ENTITYOTHER , lists of ENTITYUNRELATED and others .
We compare the built MWEs database with the corpus data from Czech National Corpus (approx.	MWEs database	corpus data	compare	{'e1': {'word': 'MWEs database', 'word_index': [(4, 4)], 'id': 'L08-1540.14'}, 'e2': {'word': 'corpus data', 'word_index': [(7, 7)], 'id': 'L08-1540.15'}}	We compare the built ENTITY with the ENTITYOTHER from ENTITYUNRELATED ( approx .
To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs.	statistical parameters	Word Sketch Engine	model-feature	{'e1': {'word': 'statistical parameters', 'word_index': [(24, 24)], 'id': 'L08-1540.21'}, 'e2': {'word': 'Word Sketch Engine', 'word_index': [(16, 16)], 'id': 'L08-1540.20'}}	To obtain a more complete list of ENTITYUNRELATED we propose and use a technique exploiting the ENTITYOTHER , which allows us to work with ENTITY such as frequency of ENTITYUNRELATED and their components as well as with the ENTITYUNRELATED for the whole ENTITYUNRELATED .
To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs.	salience	MWEs	model-feature	{'e1': {'word': 'salience', 'word_index': [(38, 38)], 'id': 'L08-1540.23'}, 'e2': {'word': 'MWEs', 'word_index': [(42, 42)], 'id': 'L08-1540.24'}}	To obtain a more complete list of ENTITYUNRELATED we propose and use a technique exploiting the ENTITYUNRELATED , which allows us to work with ENTITYUNRELATED such as frequency of ENTITYUNRELATED and their components as well as with the ENTITY for the whole ENTITYOTHER .
We also discuss exploitation of the database for working out a more adequate tagging and lemmatization.	database	tagging	usage	{'e1': {'word': 'database', 'word_index': [(6, 6)], 'id': 'L08-1540.25'}, 'e2': {'word': 'tagging', 'word_index': [(13, 13)], 'id': 'L08-1540.26'}}	We also discuss exploitation of the ENTITY for working out a more adequate ENTITYOTHER and ENTITYUNRELATED .
The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units, i. e. to make tagging and lemmatization more adequate.	MWEs	corpus text	part_whole	{'e1': {'word': 'MWEs', 'word_index': [(9, 9)], 'id': 'L08-1540.28'}, 'e2': {'word': 'corpus text', 'word_index': [(11, 11)], 'id': 'L08-1540.29'}}	The final goal is to be able to recognize ENTITY in ENTITYOTHER and lemmatize them as complete ENTITYUNRELATED , i. e. to make ENTITYUNRELATED and ENTITYUNRELATED more adequate .
We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses.	statistical techniques	translations	usage	{'e1': {'word': 'statistical techniques', 'word_index': [(8, 8)], 'id': 'L08-1110.1'}, 'e2': {'word': 'translations', 'word_index': [(15, 15)], 'id': 'L08-1110.2'}}	We describe a set of experiments to explore ENTITY for ranking and selecting the best ENTITYOTHER in a ENTITYUNRELATED of ENTITYUNRELATED .
We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses.	translation hypotheses	graph	part_whole	{'e1': {'word': 'translation hypotheses', 'word_index': [(20, 20)], 'id': 'L08-1110.4'}, 'e2': {'word': 'graph', 'word_index': [(18, 18)], 'id': 'L08-1110.3'}}	We describe a set of experiments to explore ENTITYUNRELATED for ranking and selecting the best ENTITYUNRELATED in a ENTITYOTHER of ENTITY .
In a previous paper (Carl, 2007) we have described how the hypotheses graph is generated through shallow mapping and permutation rules.	shallow mapping	hypotheses graph	usage	{'e1': {'word': 'shallow mapping', 'word_index': [(18, 18)], 'id': 'L08-1110.6'}, 'e2': {'word': 'hypotheses graph', 'word_index': [(14, 14)], 'id': 'L08-1110.5'}}	In a previous paper ( Carl , 2007 ) we have described how the ENTITYOTHER is generated through ENTITY and ENTITYUNRELATED .
We have given examples of its nodes consisting of vectors representing morpho-syntactic properties of words and phrases.	vectors representing morpho-syntactic properties	nodes	part_whole	{'e1': {'word': 'vectors representing morpho-syntactic properties', 'word_index': [(9, 9)], 'id': 'L08-1110.9'}, 'e2': {'word': 'nodes', 'word_index': [(6, 6)], 'id': 'L08-1110.8'}}	We have given examples of its ENTITYOTHER consisting of ENTITY of ENTITYUNRELATED and ENTITYUNRELATED .
The feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best M translation paths in the graph.	log-linear combination	translation paths	usage	{'e1': {'word': 'log-linear combination', 'word_index': [(12, 12)], 'id': 'L08-1110.16'}, 'e2': {'word': 'translation paths', 'word_index': [(21, 21)], 'id': 'L08-1110.17'}}	The ENTITYUNRELATED are trained off-line on different types of ENTITYUNRELATED and their ENTITY is then used to retrieve the best M ENTITYOTHER in the ENTITYUNRELATED .
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	CMU	SRI toolkit	compare	{'e1': {'word': 'CMU', 'word_index': [(6, 6)], 'id': 'L08-1110.20'}, 'e2': {'word': 'SRI toolkit', 'word_index': [(9, 9)], 'id': 'L08-1110.21'}}	We compare two ENTITYUNRELATED , the ENTITY and the ENTITYOTHER and arrive at three results : 1 ) ENTITYUNRELATED produce better results than ENTITYUNRELATED , 2 ) adding a ENTITYUNRELATED to the ENTITYUNRELATED improves the output and 3 ) ENTITYUNRELATED for ENTITYUNRELATED are suitable if the ENTITYUNRELATED is similar to the ENTITYUNRELATED to be translated . < / abstract >
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	word-lemma based feature function models	token-based models	compare	{'e1': {'word': 'word-lemma based feature function models', 'word_index': [(18, 18)], 'id': 'L08-1110.22'}, 'e2': {'word': 'token-based models', 'word_index': [(23, 23)], 'id': 'L08-1110.23'}}	We compare two ENTITYUNRELATED , the ENTITYUNRELATED and the ENTITYUNRELATED and arrive at three results : 1 ) ENTITY produce better results than ENTITYOTHER , 2 ) adding a ENTITYUNRELATED to the ENTITYUNRELATED improves the output and 3 ) ENTITYUNRELATED for ENTITYUNRELATED are suitable if the ENTITYUNRELATED is similar to the ENTITYUNRELATED to be translated . < / abstract >
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	PoS-tag feature function	word-lemma model	part_whole	{'e1': {'word': 'PoS-tag feature function', 'word_index': [(29, 29)], 'id': 'L08-1110.24'}, 'e2': {'word': 'word-lemma model', 'word_index': [(32, 32)], 'id': 'L08-1110.25'}}	We compare two ENTITYUNRELATED , the ENTITYUNRELATED and the ENTITYUNRELATED and arrive at three results : 1 ) ENTITYUNRELATED produce better results than ENTITYUNRELATED , 2 ) adding a ENTITY to the ENTITYOTHER improves the output and 3 ) ENTITYUNRELATED for ENTITYUNRELATED are suitable if the ENTITYUNRELATED is similar to the ENTITYUNRELATED to be translated . < / abstract >
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	weights	lexical translations	model-feature	{'e1': {'word': 'weights', 'word_index': [(39, 39)], 'id': 'L08-1110.26'}, 'e2': {'word': 'lexical translations', 'word_index': [(41, 41)], 'id': 'L08-1110.27'}}	We compare two ENTITYUNRELATED , the ENTITYUNRELATED and the ENTITYUNRELATED and arrive at three results : 1 ) ENTITYUNRELATED produce better results than ENTITYUNRELATED , 2 ) adding a ENTITYUNRELATED to the ENTITYUNRELATED improves the output and 3 ) ENTITY for ENTITYOTHER are suitable if the ENTITYUNRELATED is similar to the ENTITYUNRELATED to be translated . < / abstract >
We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.</abstract>	training material	texts	compare	{'e1': {'word': 'training material', 'word_index': [(46, 46)], 'id': 'L08-1110.28'}, 'e2': {'word': 'texts', 'word_index': [(51, 51)], 'id': 'L08-1110.29'}}	We compare two ENTITYUNRELATED , the ENTITYUNRELATED and the ENTITYUNRELATED and arrive at three results : 1 ) ENTITYUNRELATED produce better results than ENTITYUNRELATED , 2 ) adding a ENTITYUNRELATED to the ENTITYUNRELATED improves the output and 3 ) ENTITYUNRELATED for ENTITYUNRELATED are suitable if the ENTITY is similar to the ENTITYOTHER to be translated . < / abstract >
Existing techniques extract term candidates by looking for internal and contextual information associated with domain specific terms.	internal and contextual information	domain specific terms	model-feature	{'e1': {'word': 'internal and contextual information', 'word_index': [(7, 7)], 'id': 'L08-1154.2'}, 'e2': {'word': 'domain specific terms', 'word_index': [(10, 10)], 'id': 'L08-1154.3'}}	Existing techniques extract ENTITYUNRELATED by looking for ENTITY associated with ENTITYOTHER .
The algorithms always face the dilemma that fewer features are not enough to distinguish terms from non-terms whereas more features lead to more conflicts among selected features.	features	terms	model-feature	{'e1': {'word': 'features', 'word_index': [(8, 8)], 'id': 'L08-1154.4'}, 'e2': {'word': 'terms', 'word_index': [(14, 14)], 'id': 'L08-1154.5'}}	The algorithms always face the dilemma that fewer ENTITY are not enough to distinguish ENTITYOTHER from ENTITYUNRELATED whereas more ENTITYUNRELATED lead to more conflicts among selected ENTITYUNRELATED .
This paper presents a novel approach for term extraction based on delimiters which are much more stable and domain independent.	delimiters	term extraction	usage	{'e1': {'word': 'delimiters', 'word_index': [(10, 10)], 'id': 'L08-1154.10'}, 'e2': {'word': 'term extraction', 'word_index': [(7, 7)], 'id': 'L08-1154.9'}}	This paper presents a novel approach for ENTITYOTHER based on ENTITY which are much more stable and domain independent .
The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the discourse relations in Czech.	discourse relations	Czech	part_whole	{'e1': {'word': 'discourse relations', 'word_index': [(14, 14)], 'id': 'L08-1050.2'}, 'e2': {'word': 'Czech', 'word_index': [(16, 16)], 'id': 'L08-1050.3'}}	The present paper reports on a preparatory research for building a ENTITYUNRELATED capturing the ENTITY in ENTITYOTHER .
We primarily focus on the description of the syntactically motivated relations in discourse, basing our findings on the theoretical background of the Prague Dependency Treebank 2.0 and the Penn Discourse Treebank 2.	syntactically motivated relations	discourse	part_whole	{'e1': {'word': 'syntactically motivated relations', 'word_index': [(8, 8)], 'id': 'L08-1050.4'}, 'e2': {'word': 'discourse', 'word_index': [(10, 10)], 'id': 'L08-1050.5'}}	We primarily focus on the description of the ENTITY in ENTITYOTHER , basing our findings on the theoretical background of the ENTITYUNRELATED and the ENTITYUNRELATED .
Our aim is to revisit the present-day syntactico-semantic (tectogrammatical) annotation in the Prague Dependency Treebank, extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new, discourse level of annotation.	syntactico-semantic (tectogrammatical) annotation	Prague Dependency Treebank	part_whole	{'e1': {'word': 'syntactico-semantic (tectogrammatical) annotation', 'word_index': [(9, 9)], 'id': 'L08-1050.8'}, 'e2': {'word': 'Prague Dependency Treebank', 'word_index': [(12, 12)], 'id': 'L08-1050.9'}}	Our aim is to revisit the present - day ENTITY in the ENTITYOTHER , extend it for the purposes of a ENTITYUNRELATED and eventually to design a new , ENTITYUNRELATED of ENTITYUNRELATED .
Our aim is to revisit the present-day syntactico-semantic (tectogrammatical) annotation in the Prague Dependency Treebank, extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new, discourse level of annotation.	discourse level	annotation	model-feature	{'e1': {'word': 'discourse level', 'word_index': [(29, 29)], 'id': 'L08-1050.11'}, 'e2': {'word': 'annotation', 'word_index': [(31, 31)], 'id': 'L08-1050.12'}}	Our aim is to revisit the present - day ENTITYUNRELATED in the ENTITYUNRELATED , extend it for the purposes of a ENTITYUNRELATED and eventually to design a new , ENTITY of ENTITYOTHER .
In this paper, we propose a feasible process of such a transfer, comparing the possibilities the Praguian dependency-based approach offers with the Penn discourse annotation based primarily on the analysis and classification of discourse connectives.	Praguian dependency-based approach	Penn discourse annotation	compare	{'e1': {'word': 'Praguian dependency-based approach', 'word_index': [(18, 18)], 'id': 'L08-1050.13'}, 'e2': {'word': 'Penn discourse annotation', 'word_index': [(22, 22)], 'id': 'L08-1050.14'}}	In this paper , we propose a feasible process of such a transfer , comparing the possibilities the ENTITY offers with the ENTITYOTHER based primarily on the analysis and classification of ENTITYUNRELATED .
In this paper, we reported experiments of unsupervised automatic acquisition of Italian and English verb subcategorization frames (SCFs) from general and domain corpora.	general and domain corpora	unsupervised automatic acquisition	usage	{'e1': {'word': 'general and domain corpora', 'word_index': [(12, 12)], 'id': 'L08-1097.3'}, 'e2': {'word': 'unsupervised automatic acquisition', 'word_index': [(8, 8)], 'id': 'L08-1097.1'}}	In this paper , we reported experiments of ENTITYOTHER of ENTITYUNRELATED from ENTITY .
The proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about SCFs.	lexico-syntactic knowledge	SCFs	model-feature	{'e1': {'word': 'lexico-syntactic knowledge', 'word_index': [(20, 20)], 'id': 'L08-1097.6'}, 'e2': {'word': 'SCFs', 'word_index': [(22, 22)], 'id': 'L08-1097.7'}}	The proposed technique operates on ENTITYUNRELATED on the basis of a limited number of ENTITYUNRELATED not relying on any previous ENTITY about ENTITYOTHER .
The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle (MDL).	SCFs distributions	verbs	model-feature	{'e1': {'word': 'SCFs distributions', 'word_index': [(7, 7)], 'id': 'L08-1097.10'}, 'e2': {'word': 'verbs', 'word_index': [(4, 4)], 'id': 'L08-1097.9'}}	The issue of whether ENTITYOTHER sharing similar ENTITY happen to share ENTITYUNRELATED as well was also explored by clustering ENTITYUNRELATED that share ENTITYUNRELATED with the same ENTITYUNRELATED using the ENTITYUNRELATED .
The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle (MDL).	frames	verbs	model-feature	{'e1': {'word': 'frames', 'word_index': [(22, 22)], 'id': 'L08-1097.13'}, 'e2': {'word': 'verbs', 'word_index': [(19, 19)], 'id': 'L08-1097.12'}}	The issue of whether ENTITYUNRELATED sharing similar ENTITYUNRELATED happen to share ENTITYUNRELATED as well was also explored by clustering ENTITYOTHER that share ENTITY with the same ENTITYUNRELATED using the ENTITYUNRELATED .
The translation of English text into American Sign Language (ASL) animation tests the limits of traditional MT architectural designs.	traditional MT architectural designs	translation	usage	{'e1': {'word': 'traditional MT architectural designs', 'word_index': [(10, 10)], 'id': 'N04-2005.4'}, 'e2': {'word': 'translation', 'word_index': [(1, 1)], 'id': 'N04-2005.1'}}	The ENTITYOTHER of ENTITYUNRELATED into ENTITYUNRELATED tests the limits of ENTITY .
"A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called ""classifier predicates."""	semantic representation	virtual reality 3D scene modeling software	usage	{'e1': {'word': 'semantic representation', 'word_index': [(2, 2)], 'id': 'N04-2005.5'}, 'e2': {'word': 'virtual reality 3D scene modeling software', 'word_index': [(7, 7)], 'id': 'N04-2005.6'}}	"A new ENTITY is proposed that uses ENTITYOTHER to produce ENTITYUNRELATED called "" ENTITYUNRELATED . """
The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system.	interlingua	multi-pathway MT architecture design	part_whole	{'e1': {'word': 'interlingua', 'word_index': [(5, 5)], 'id': 'N04-2005.9'}, 'e2': {'word': 'multi-pathway MT architecture design', 'word_index': [(9, 9)], 'id': 'N04-2005.10'}}	The model acts as an ENTITY within a new ENTITYOTHER that also incorporates ENTITYUNRELATED and ENTITYUNRELATED into a single system .
In our current research into the design of cognitively well-motivated interfaces relying primarily on the display of graphical information, we have observed that graphical information alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users' expectations.	display of graphical information	cognitively well-motivated interfaces	usage	{'e1': {'word': 'display of graphical information', 'word_index': [(13, 13)], 'id': 'A92-1010.2'}, 'e2': {'word': 'cognitively well-motivated interfaces', 'word_index': [(8, 8)], 'id': 'A92-1010.1'}}	In our current research into the design of ENTITYOTHER relying primarily on the ENTITY , we have observed that ENTITYUNRELATED alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users ' expectations .
To solve this problem, we are working towards the integration of natural language generation to augment the interaction</abstract>	natural language generation	interaction	usage	{'e1': {'word': 'natural language generation', 'word_index': [(12, 12)], 'id': 'A92-1010.6'}, 'e2': {'word': 'interaction', 'word_index': [(16, 16)], 'id': 'A92-1010.7'}}	To solve this problem , we are working towards the integration of ENTITY to augment the ENTITYOTHER < / abstract >
For both corpora word recognition experiments were carried out with vocabularies containing up to 20k words.	words	vocabularies	part_whole	{'e1': {'word': 'words', 'word_index': [(13, 13)], 'id': 'H94-1064.8'}, 'e2': {'word': 'vocabularies', 'word_index': [(8, 8)], 'id': 'H94-1064.7'}}	For both ENTITYUNRELATED ENTITYUNRELATED were carried out with ENTITYOTHER containing up to 20k ENTITY .
The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling.	Gaussian mixture	continuous density HMM	model-feature	{'e1': {'word': 'Gaussian mixture', 'word_index': [(7, 7)], 'id': 'H94-1064.10'}, 'e2': {'word': 'continuous density HMM', 'word_index': [(5, 5)], 'id': 'H94-1064.9'}}	The recognizer makes use of ENTITYOTHER with ENTITY for ENTITYUNRELATED and ENTITYUNRELATED estimated on the ENTITYUNRELATED for ENTITYUNRELATED .
The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling.	n-gram statistics	language modeling	usage	{'e1': {'word': 'n-gram statistics', 'word_index': [(11, 11)], 'id': 'H94-1064.12'}, 'e2': {'word': 'language modeling', 'word_index': [(17, 17)], 'id': 'H94-1064.14'}}	The recognizer makes use of ENTITYUNRELATED with ENTITYUNRELATED for ENTITYUNRELATED and ENTITY estimated on the ENTITYUNRELATED for ENTITYOTHER .
A second forward pass, which makes use of a word graph generated with the bigram, incorporates a trigram language model.	word graph	forward pass	usage	{'e1': {'word': 'word graph', 'word_index': [(9, 9)], 'id': 'H94-1064.18'}, 'e2': {'word': 'forward pass', 'word_index': [(2, 2)], 'id': 'H94-1064.17'}}	A second ENTITYOTHER , which makes use of a ENTITY generated with the ENTITYUNRELATED , incorporates a ENTITYUNRELATED .
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex-dependent models.	cepstrum-based features	Acoustic modeling	usage	{'e1': {'word': 'cepstrum-based features', 'word_index': [(2, 2)], 'id': 'H94-1064.22'}, 'e2': {'word': 'Acoustic modeling', 'word_index': [(0, 0)], 'id': 'H94-1064.21'}}	ENTITYOTHER uses ENTITY , ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED .
The system is based on a multi-component architecture where each component is responsible for identifying one class of unknown words.	multi-component architecture	system	usage	{'e1': {'word': 'multi-component architecture', 'word_index': [(6, 6)], 'id': 'A00-1024.3'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'A00-1024.2'}}	The ENTITYOTHER is based on a ENTITY where each ENTITYUNRELATED is responsible for identifying one class of ENTITYUNRELATED .
The focus of this paper is the components that identify names and spelling errors.	components	names	usage	{'e1': {'word': 'components', 'word_index': [(7, 7)], 'id': 'A00-1024.6'}, 'e2': {'word': 'names', 'word_index': [(10, 10)], 'id': 'A00-1024.7'}}	The focus of this paper is the ENTITY that identify ENTITYOTHER and ENTITYUNRELATED .
Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word.	decision tree architecture	component	usage	{'e1': {'word': 'decision tree architecture', 'word_index': [(4, 4)], 'id': 'A00-1024.10'}, 'e2': {'word': 'component', 'word_index': [(1, 1)], 'id': 'A00-1024.9'}}	Each ENTITYOTHER uses a ENTITY to combine multiple types of ENTITYUNRELATED about the ENTITYUNRELATED .
Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word.	evidence	unknown word	model-feature	{'e1': {'word': 'evidence', 'word_index': [(10, 10)], 'id': 'A00-1024.11'}, 'e2': {'word': 'unknown word', 'word_index': [(13, 13)], 'id': 'A00-1024.12'}}	Each ENTITYUNRELATED uses a ENTITYUNRELATED to combine multiple types of ENTITY about the ENTITYOTHER .
The system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words.	unknown words	live closed captions	part_whole	{'e1': {'word': 'unknown words', 'word_index': [(17, 17)], 'id': 'A00-1024.15'}, 'e2': {'word': 'live closed captions', 'word_index': [(7, 7)], 'id': 'A00-1024.14'}}	The ENTITYUNRELATED is evaluated using data from ENTITYOTHER - a genre replete with a wide variety of ENTITY .
Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2]).	Recognition of proper nouns	Japanese text	usage	{'e1': {'word': 'Recognition of proper nouns', 'word_index': [(0, 0)], 'id': 'X96-1059.1'}, 'e2': {'word': 'Japanese text', 'word_index': [(2, 2)], 'id': 'X96-1059.2'}}	ENTITY in ENTITYOTHER has been studied as a part of the more general problem of ENTITYUNRELATED in ENTITYUNRELATED ( [ 1 ] [ 2 ] ) .
Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2]).	morphological analysis	Japanese text processing	part_whole	{'e1': {'word': 'morphological analysis', 'word_index': [(15, 15)], 'id': 'X96-1059.3'}, 'e2': {'word': 'Japanese text processing', 'word_index': [(17, 17)], 'id': 'X96-1059.4'}}	ENTITYUNRELATED in ENTITYUNRELATED has been studied as a part of the more general problem of ENTITY in ENTITYOTHER ( [ 1 ] [ 2 ] ) .
Our approach to the Multi-lingual Evaluation Task (MET) for Japanese text is to consider the given task as a morphological analysis problem in Japanese.	morphological analysis problem	Japanese	usage	{'e1': {'word': 'morphological analysis problem', 'word_index': [(20, 20)], 'id': 'X96-1059.7'}, 'e2': {'word': 'Japanese', 'word_index': [(22, 22)], 'id': 'X96-1059.8'}}	Our approach to the Multi-lingual Evaluation Task ( MET ) for ENTITYUNRELATED is to consider the given task as a ENTITY in ENTITYOTHER .
Our morphological analyzer has done all the necessary work for the recognition and classification of proper names, numerical and temporal expressions, i.ehological analyzer has done all the necessary work for the recognition and classification of proper names, numerical and temporal expressions, i.e.	morphological analyzer	recognition and classification of proper names, numerical and temporal expressions, i.e	usage	{'e1': {'word': 'morphological analyzer', 'word_index': [(1, 1)], 'id': 'X96-1059.9'}, 'e2': {'word': 'recognition and classification of proper names, numerical and temporal expressions, i.e', 'word_index': [(10, 10)], 'id': 'X96-1059.10'}}	Our ENTITY has done all the necessary work for the ENTITYOTHER hological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i.e.
First, it uses several kinds of dictionaries to segment and tag Japanese character strings.	dictionaries	Japanese character strings	usage	{'e1': {'word': 'dictionaries', 'word_index': [(7, 7)], 'id': 'X96-1059.16'}, 'e2': {'word': 'Japanese character strings', 'word_index': [(12, 12)], 'id': 'X96-1059.17'}}	First , it uses several kinds of ENTITY to segment and tag ENTITYOTHER .
Second, based on the information resulting from the dictionary lookup stage, a set of rules is applied to the segmented strings in order to identify NE items.	rules	segmented strings	usage	{'e1': {'word': 'rules', 'word_index': [(14, 14)], 'id': 'X96-1059.19'}, 'e2': {'word': 'segmented strings', 'word_index': [(19, 19)], 'id': 'X96-1059.20'}}	Second , based on the information resulting from the ENTITYUNRELATED , a set of ENTITY is applied to the ENTITYOTHER in order to identify ENTITYUNRELATED .
We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines.	practically unsupervised learning method	single-snippet answers	usage	{'e1': {'word': 'practically unsupervised learning method', 'word_index': [(3, 3)], 'id': 'H05-1041.1'}, 'e2': {'word': 'single-snippet answers', 'word_index': [(6, 6)], 'id': 'H05-1041.2'}}	We present a ENTITY to produce ENTITYOTHER to ENTITYUNRELATED in ENTITYUNRELATED that supplement ENTITYUNRELATED .
The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples, which are then used to train an svm to separate the two classes.	on-line encyclopedias and dictionaries	positive and negative definition examples	usage	{'e1': {'word': 'on-line encyclopedias and dictionaries', 'word_index': [(3, 3)], 'id': 'H05-1041.6'}, 'e2': {'word': 'positive and negative definition examples', 'word_index': [(12, 12)], 'id': 'H05-1041.7'}}	The method exploits ENTITY to generate automatically an arbitrarily large number of ENTITYOTHER , which are then used to train an ENTITYUNRELATED to separate the two classes .
Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms.	terms	corpora	part_whole	{'e1': {'word': 'terms', 'word_index': [(12, 12)], 'id': 'W02-1403.2'}, 'e2': {'word': 'corpora', 'word_index': [(15, 15)], 'id': 'W02-1403.3'}}	ENTITYUNRELATED has been the subject of much work in the context of ENTITY extracted from ENTITYOTHER : given a set of ENTITYUNRELATED , obtained from an existing resource or extracted from a ENTITYUNRELATED , identifying ENTITYUNRELATED between these ENTITYUNRELATED .
Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms.	terms	corpus	part_whole	{'e1': {'word': 'terms', 'word_index': [(21, 21)], 'id': 'W02-1403.4'}, 'e2': {'word': 'corpus', 'word_index': [(32, 32)], 'id': 'W02-1403.5'}}	ENTITYUNRELATED has been the subject of much work in the context of ENTITYUNRELATED extracted from ENTITYUNRELATED : given a set of ENTITY , obtained from an existing resource or extracted from a ENTITYOTHER , identifying ENTITYUNRELATED between these ENTITYUNRELATED .
Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms.	hierarchical (or other types of) relations	terms	model-feature	{'e1': {'word': 'hierarchical (or other types of) relations', 'word_index': [(35, 35)], 'id': 'W02-1403.6'}, 'e2': {'word': 'terms', 'word_index': [(38, 38)], 'id': 'W02-1403.7'}}	ENTITYUNRELATED has been the subject of much work in the context of ENTITYUNRELATED extracted from ENTITYUNRELATED : given a set of ENTITYUNRELATED , obtained from an existing resource or extracted from a ENTITYUNRELATED , identifying ENTITY between these ENTITYOTHER .
The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account.	lexical methods	terminology structuring	usage	{'e1': {'word': 'lexical methods', 'word_index': [(7, 7)], 'id': 'W02-1403.9'}, 'e2': {'word': 'terminology structuring', 'word_index': [(5, 5)], 'id': 'W02-1403.8'}}	The present paper focusses on ENTITYOTHER by ENTITY , which match ENTITYUNRELATED on the basis on their ENTITYUNRELATED , taking ENTITYUNRELATED into account .
The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account.	content words	terms	part_whole	{'e1': {'word': 'content words', 'word_index': [(17, 17)], 'id': 'W02-1403.11'}, 'e2': {'word': 'terms', 'word_index': [(11, 11)], 'id': 'W02-1403.10'}}	The present paper focusses on ENTITYUNRELATED by ENTITYUNRELATED , which match ENTITYOTHER on the basis on their ENTITY , taking ENTITYUNRELATED into account .
Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus.	terms	hierarchically-structured terminology	part_whole	{'e1': {'word': 'terms', 'word_index': [(10, 10)], 'id': 'W02-1403.13'}, 'e2': {'word': 'hierarchically-structured terminology', 'word_index': [(15, 15)], 'id': 'W02-1403.14'}}	Experiments are done on a ' flat ' list of ENTITY obtained from an originally ENTITYOTHER : the French version of the ENTITYUNRELATED .
We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis ofthe 'new' relations not present in the MeSH.	lexically-induced relations	MeSH relations	compare	{'e1': {'word': 'lexically-induced relations', 'word_index': [(3, 3)], 'id': 'W02-1403.16'}, 'e2': {'word': 'MeSH relations', 'word_index': [(7, 7)], 'id': 'W02-1403.17'}}	We compare the ENTITY with the original ENTITYOTHER : after a quantitative evaluation of their congruence through ENTITYUNRELATED , we perform a qualitative , human analysis of the ' new ' ENTITYUNRELATED not present in the ENTITYUNRELATED .
We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis ofthe 'new' relations not present in the MeSH.	relations	MeSH	part_whole	{'e1': {'word': 'relations', 'word_index': [(30, 30)], 'id': 'W02-1403.19'}, 'e2': {'word': 'MeSH', 'word_index': [(35, 35)], 'id': 'W02-1403.20'}}	We compare the ENTITYUNRELATED with the original ENTITYUNRELATED : after a quantitative evaluation of their congruence through ENTITYUNRELATED , we perform a qualitative , human analysis ofthe ' new ' ENTITY not present in the ENTITYOTHER .
On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.	naming conventions	MeSH	model-feature	{'e1': {'word': 'naming conventions', 'word_index': [(13, 13)], 'id': 'W02-1403.22'}, 'e2': {'word': 'MeSH', 'word_index': [(17, 17)], 'id': 'W02-1403.23'}}	On the other hand , it also reveals some specific structuring choices and ENTITY made by the ENTITYOTHER designers , and emphasizes ontological commitments that cannot be left to ENTITYUNRELATED .
In this study, we propose a knowledge-independent method for aligning terms and thus extracting translations from a small, domain-specific corpus consisting of parallel English and Chinese court judgments from Hong Kong.	translations	small, domain-specific corpus	part_whole	{'e1': {'word': 'translations', 'word_index': [(14, 14)], 'id': 'W02-1404.3'}, 'e2': {'word': 'small, domain-specific corpus', 'word_index': [(17, 17)], 'id': 'W02-1404.4'}}	In this study , we propose a ENTITYUNRELATED for aligning ENTITYUNRELATED and thus extracting ENTITY from a ENTITYOTHER consisting of ENTITYUNRELATED from Hong Kong .
With a sentence-aligned corpus, translation equivalences are suggested by analysing the frequency profiles of parallel concordances.	frequency profiles	parallel concordances	model-feature	{'e1': {'word': 'frequency profiles', 'word_index': [(10, 10)], 'id': 'W02-1404.8'}, 'e2': {'word': 'parallel concordances', 'word_index': [(12, 12)], 'id': 'W02-1404.9'}}	With a ENTITYUNRELATED , ENTITYUNRELATED are suggested by analysing the ENTITY of ENTITYOTHER .
The method overcomes the limitations of conventional statistical methods which require large corpora to be effective, and lexical approaches which depend on existing bilingual dictionaries.	large corpora	conventional statistical methods	usage	{'e1': {'word': 'large corpora', 'word_index': [(9, 9)], 'id': 'W02-1404.11'}, 'e2': {'word': 'conventional statistical methods', 'word_index': [(6, 6)], 'id': 'W02-1404.10'}}	The method overcomes the limitations of ENTITYOTHER which require ENTITY to be effective , and ENTITYUNRELATED which depend on existing ENTITYUNRELATED .
The method overcomes the limitations of conventional statistical methods which require large corpora to be effective, and lexical approaches which depend on existing bilingual dictionaries.	bilingual dictionaries	lexical approaches	usage	{'e1': {'word': 'bilingual dictionaries', 'word_index': [(20, 20)], 'id': 'W02-1404.13'}, 'e2': {'word': 'lexical approaches', 'word_index': [(15, 15)], 'id': 'W02-1404.12'}}	The method overcomes the limitations of ENTITYUNRELATED which require ENTITYUNRELATED to be effective , and ENTITYOTHER which depend on existing ENTITY .
Pilot testing on a parallel corpus of about 113K Chinese words and 120K English words gives an encouraging 85% precision and 45% recall.	Chinese words	parallel corpus	part_whole	{'e1': {'word': 'Chinese words', 'word_index': [(8, 8)], 'id': 'W02-1404.15'}, 'e2': {'word': 'parallel corpus', 'word_index': [(4, 4)], 'id': 'W02-1404.14'}}	Pilot testing on a ENTITYOTHER of about 113K ENTITY and 120K ENTITYUNRELATED gives an encouraging 85 % ENTITYUNRELATED and 45 % ENTITYUNRELATED .
Future work includes fine-tuning the algorithm upon the analysis of the errors, and acquiring a translation lexicon for legal terminology by filtering out general terms.	legal terminology	translation lexicon	part_whole	{'e1': {'word': 'legal terminology', 'word_index': [(20, 20)], 'id': 'W02-1404.21'}, 'e2': {'word': 'translation lexicon', 'word_index': [(18, 18)], 'id': 'W02-1404.20'}}	Future work includes fine - tuning the ENTITYUNRELATED upon the analysis of the errors , and acquiring a ENTITYOTHER for ENTITY by filtering out ENTITYUNRELATED .
Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to share text revision across languages.	Coedition	natural language text	usage	{'e1': {'word': 'Coedition', 'word_index': [(0, 0)], 'id': 'W02-1602.1'}, 'e2': {'word': 'natural language text', 'word_index': [(3, 3)], 'id': 'W02-1602.2'}}	ENTITY of a ENTITYOTHER and its representation in some ENTITYUNRELATED seems the best and simplest way to share ENTITYUNRELATED across ENTITYUNRELATED .
We are developing a prototype where, in the simplest sharing scenario, naive users interact directly with the text in their language (L0), and indirectly with the associated graph.	language (L0)	text	model-feature	{'e1': {'word': 'language (L0)', 'word_index': [(21, 21)], 'id': 'W02-1602.10'}, 'e2': {'word': 'text', 'word_index': [(18, 18)], 'id': 'W02-1602.9'}}	We are developing a ENTITYUNRELATED where , in the simplest ENTITYUNRELATED , naive users interact directly with the ENTITYOTHER in their ENTITY , and indirectly with the associated ENTITYUNRELATED .
"Establishing a ""best"" correspondence between the ""UNL-tree+L0"" and the ""MS-L0 structure"", a lattice, may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible."	crossing liaisons	tree	model-feature	{'e1': {'word': 'crossing liaisons', 'word_index': [(39, 39)], 'id': 'W02-1602.35'}, 'e2': {'word': 'tree', 'word_index': [(31, 31)], 'id': 'W02-1602.33'}}	"Establishing a "" best "" correspondence between the "" ENTITYUNRELATED "" and the "" ENTITYUNRELATED "" , a ENTITYUNRELATED , may be done using the ENTITYUNRELATED and trying to align the ENTITYOTHER and the selected ENTITYUNRELATED with as few ENTITY as possible ."
In this paper, we improve an unsupervised learning method using the Expectation-Maximization (EM) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation (WSD) problems.	Expectation-Maximization (EM) algorithm	text classification problems	usage	{'e1': {'word': 'Expectation-Maximization (EM) algorithm', 'word_index': [(10, 10)], 'id': 'W03-0406.2'}, 'e2': {'word': 'text classification problems', 'word_index': [(17, 17)], 'id': 'W03-0406.3'}}	In this paper , we improve an ENTITYUNRELATED using the ENTITY proposed by Nigam et al. for ENTITYOTHER in order to apply it to ENTITYUNRELATED .
The improved method stops the EM algorithm at the optimum iteration number.	optimum iteration number	EM algorithm	model-feature	{'e1': {'word': 'optimum iteration number', 'word_index': [(8, 8)], 'id': 'W03-0406.6'}, 'e2': {'word': 'EM algorithm', 'word_index': [(5, 5)], 'id': 'W03-0406.5'}}	The improved method stops the ENTITYOTHER at the ENTITY .
In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2.	noun WSD problems	Japanese Dictionary Task in SENSEVAL2	part_whole	{'e1': {'word': 'noun WSD problems', 'word_index': [(6, 6)], 'id': 'W03-0406.7'}, 'e2': {'word': 'Japanese Dictionary Task in SENSEVAL2', 'word_index': [(9, 9)], 'id': 'W03-0406.8'}}	In experiments , we solved 50 ENTITY in the ENTITYOTHER .
In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system, a language tutoring application for deaf learners of written English.	user knowledge modeling architecture	ICICLE system	part_whole	{'e1': {'word': 'user knowledge modeling architecture', 'word_index': [(7, 7)], 'id': 'W99-0408.1'}, 'e2': {'word': 'ICICLE system', 'word_index': [(10, 10)], 'id': 'W99-0408.2'}}	In this paper we discuss a proposed ENTITY for the ENTITYOTHER , a ENTITYUNRELATED for deaf learners of ENTITYUNRELATED .
In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system, a language tutoring application for deaf learners of written English.	language tutoring application	written English	usage	{'e1': {'word': 'language tutoring application', 'word_index': [(13, 13)], 'id': 'W99-0408.3'}, 'e2': {'word': 'written English', 'word_index': [(18, 18)], 'id': 'W99-0408.4'}}	In this paper we discuss a proposed ENTITYUNRELATED for the ENTITYUNRELATED , a ENTITY for deaf learners of ENTITYOTHER .
We motivate our model design by citing relevant research on second language and cognitive skill acquisition, and briefly discuss preliminary empirical evidence supporting the design.	second language and cognitive skill acquisition	model design	usage	{'e1': {'word': 'second language and cognitive skill acquisition', 'word_index': [(9, 9)], 'id': 'W99-0408.9'}, 'e2': {'word': 'model design', 'word_index': [(3, 3)], 'id': 'W99-0408.8'}}	We motivate our ENTITYOTHER by citing relevant research on ENTITY , and briefly discuss preliminary empirical evidence supporting the ENTITYUNRELATED .
This paper describes novel and practical Japanese parsers that uses decision trees.	decision trees	Japanese parsers	usage	{'e1': {'word': 'decision trees', 'word_index': [(9, 9)], 'id': 'P98-1083.2'}, 'e2': {'word': 'Japanese parsers', 'word_index': [(6, 6)], 'id': 'P98-1083.1'}}	This paper describes novel and practical ENTITYOTHER that uses ENTITY .
First, we construct a single decision tree to estimate modification probabilities; how one phrase tends to modify another.	decision tree	modification probabilities	model-feature	{'e1': {'word': 'decision tree', 'word_index': [(6, 6)], 'id': 'P98-1083.3'}, 'e2': {'word': 'modification probabilities', 'word_index': [(9, 9)], 'id': 'P98-1083.4'}}	First , we construct a single ENTITY to estimate ENTITYOTHER ; how one ENTITYUNRELATED tends to modify another .
Next, we introduce a boosting algorithm in which several decision trees are constructed and then combined for probability estimation.	decision trees	probability estimation	usage	{'e1': {'word': 'decision trees', 'word_index': [(9, 9)], 'id': 'P98-1083.7'}, 'e2': {'word': 'probability estimation', 'word_index': [(16, 16)], 'id': 'P98-1083.8'}}	Next , we introduce a ENTITYUNRELATED in which several ENTITY are constructed and then combined for ENTITYOTHER .
Automatic estimation of word significance oriented for speech-based Information Retrieval (IR) is addressed.	word significance	speech-based Information Retrieval (IR)	usage	{'e1': {'word': 'word significance', 'word_index': [(2, 2)], 'id': 'I08-1027.2'}, 'e2': {'word': 'speech-based Information Retrieval (IR)', 'word_index': [(5, 5)], 'id': 'I08-1027.3'}}	ENTITYUNRELATED of ENTITY oriented for ENTITYOTHER is addressed .
Since the significance of words differs in IR, automatic speech recognition (ASR) performance has been evaluated based on weighted word error rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly.	significance	words	model-feature	{'e1': {'word': 'significance', 'word_index': [(2, 2)], 'id': 'I08-1027.4'}, 'e2': {'word': 'words', 'word_index': [(4, 4)], 'id': 'I08-1027.5'}}	Since the ENTITY of ENTITYOTHER differs in ENTITYUNRELATED , ENTITYUNRELATED has been evaluated based on ENTITYUNRELATED , which gives a ENTITYUNRELATED on errors from the viewpoint of ENTITYUNRELATED , instead of ENTITYUNRELATED , which treats all ENTITYUNRELATED uniformly .
Since the significance of words differs in IR, automatic speech recognition (ASR) performance has been evaluated based on weighted word error rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly.	weight	word error rate (WER)	compare	{'e1': {'word': 'weight', 'word_index': [(20, 20)], 'id': 'I08-1027.9'}, 'e2': {'word': 'word error rate (WER)', 'word_index': [(31, 31)], 'id': 'I08-1027.11'}}	Since the ENTITYUNRELATED of ENTITYUNRELATED differs in ENTITYUNRELATED , ENTITYUNRELATED has been evaluated based on ENTITYUNRELATED , which gives a ENTITY on errors from the viewpoint of ENTITYUNRELATED , instead of ENTITYOTHER , which treats all ENTITYUNRELATED uniformly .
A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown, and the reduction of errors on both ASR and IR has been reported.	Minimum Bayes-Risk framework	decoding strategy	usage	{'e1': {'word': 'Minimum Bayes-Risk framework', 'word_index': [(8, 8)], 'id': 'I08-1027.15'}, 'e2': {'word': 'decoding strategy', 'word_index': [(1, 1)], 'id': 'I08-1027.13'}}	A ENTITYOTHER that minimizes ENTITYUNRELATED based on a ENTITY has been shown , and the reduction of errors on both ENTITYUNRELATED and ENTITYUNRELATED has been reported .
A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown, and the reduction of errors on both ASR and IR has been reported.	ASR	IR	compare	{'e1': {'word': 'ASR', 'word_index': [(20, 20)], 'id': 'I08-1027.16'}, 'e2': {'word': 'IR', 'word_index': [(22, 22)], 'id': 'I08-1027.17'}}	A ENTITYUNRELATED that minimizes ENTITYUNRELATED based on a ENTITYUNRELATED has been shown , and the reduction of errors on both ENTITY and ENTITYOTHER has been reported .
In this paper, we propose an automatic estimation method for word significance (weights) based on its influence on IR.	automatic estimation method	word significance (weights)	usage	{'e1': {'word': 'automatic estimation method', 'word_index': [(7, 7)], 'id': 'I08-1027.18'}, 'e2': {'word': 'word significance (weights)', 'word_index': [(9, 9)], 'id': 'I08-1027.19'}}	In this paper , we propose an ENTITY for ENTITYOTHER based on its influence on ENTITYUNRELATED .
This study presents a method to automatically acquire paraphrases using bilingual corpora, which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques.	bilingual corpora	method to automatically acquire paraphrases	usage	{'e1': {'word': 'bilingual corpora', 'word_index': [(6, 6)], 'id': 'I08-1043.2'}, 'e2': {'word': 'method to automatically acquire paraphrases', 'word_index': [(4, 4)], 'id': 'I08-1043.1'}}	This study presents a ENTITYOTHER using ENTITY , which utilizes the ENTITYUNRELATED obtained by projecting a ENTITYUNRELATED onto the other language sentence based on ENTITYUNRELATED .
This study presents a method to automatically acquire paraphrases using bilingual corpora, which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques.	statistical alignment techniques	bilingual dependency relations	usage	{'e1': {'word': 'statistical alignment techniques', 'word_index': [(24, 24)], 'id': 'I08-1043.5'}, 'e2': {'word': 'bilingual dependency relations', 'word_index': [(11, 11)], 'id': 'I08-1043.3'}}	This study presents a ENTITYUNRELATED using ENTITYUNRELATED , which utilizes the ENTITYOTHER obtained by projecting a ENTITYUNRELATED onto the other language sentence based on ENTITY .
Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation, it would be possible to obtain interchangeable paraphrases under a given context.	sense	phrase	model-feature	{'e1': {'word': 'sense', 'word_index': [(9, 9)], 'id': 'I08-1043.7'}, 'e2': {'word': 'phrase', 'word_index': [(13, 13)], 'id': 'I08-1043.8'}}	Since the ENTITYUNRELATED is capable of clearly disambiguating the ENTITY of an original ENTITYOTHER using the ENTITYUNRELATED of ENTITYUNRELATED , it would be possible to obtain interchangeable ENTITYUNRELATED under a given ENTITYUNRELATED .
Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation, it would be possible to obtain interchangeable paraphrases under a given context.	context	paraphrases	model-feature	{'e1': {'word': 'context', 'word_index': [(31, 31)], 'id': 'I08-1043.12'}, 'e2': {'word': 'paraphrases', 'word_index': [(27, 27)], 'id': 'I08-1043.11'}}	Since the ENTITYUNRELATED is capable of clearly disambiguating the ENTITYUNRELATED of an original ENTITYUNRELATED using the ENTITYUNRELATED of ENTITYUNRELATED , it would be possible to obtain interchangeable ENTITYOTHER under a given ENTITY .
Also, we provide an advanced method to acquire generalized translation knowledge using the extracted paraphrases.	paraphrases	generalized translation knowledge	usage	{'e1': {'word': 'paraphrases', 'word_index': [(13, 13)], 'id': 'I08-1043.14'}, 'e2': {'word': 'generalized translation knowledge', 'word_index': [(9, 9)], 'id': 'I08-1043.13'}}	Also , we provide an advanced method to acquire ENTITYOTHER using the extracted ENTITY .
We applied the method to acquire the generalized translation knowledge for Korean-English translation.	generalized translation knowledge	Korean-English translation	model-feature	{'e1': {'word': 'generalized translation knowledge', 'word_index': [(7, 7)], 'id': 'I08-1043.15'}, 'e2': {'word': 'Korean-English translation', 'word_index': [(9, 9)], 'id': 'I08-1043.16'}}	We applied the method to acquire the ENTITY for ENTITYOTHER .
Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.	Korean and English language pairs	parallel corpora	part_whole	{'e1': {'word': 'Korean and English language pairs', 'word_index': [(6, 6)], 'id': 'I08-1043.18'}, 'e2': {'word': 'parallel corpora', 'word_index': [(3, 3)], 'id': 'I08-1043.17'}}	Through experiments with ENTITYOTHER of a ENTITY , we show that our ENTITYUNRELATED effectively extracts ENTITYUNRELATED with high ENTITYUNRELATED , 94.3 % and 84.6 % respectively for ENTITYUNRELATED and ENTITYUNRELATED , and the ENTITYUNRELATED extracted from the ENTITYUNRELATED could be generalized successfully using the ENTITYUNRELATED with the 12.5 % ENTITYUNRELATED .
Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.	paraphrasing method	precision	result	{'e1': {'word': 'paraphrasing method', 'word_index': [(12, 12)], 'id': 'I08-1043.19'}, 'e2': {'word': 'precision', 'word_index': [(18, 18)], 'id': 'I08-1043.21'}}	Through experiments with ENTITYUNRELATED of a ENTITYUNRELATED , we show that our ENTITY effectively extracts ENTITYUNRELATED with high ENTITYOTHER , 94.3 % and 84.6 % respectively for ENTITYUNRELATED and ENTITYUNRELATED , and the ENTITYUNRELATED extracted from the ENTITYUNRELATED could be generalized successfully using the ENTITYUNRELATED with the 12.5 % ENTITYUNRELATED .
Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.	translation knowledge	bilingual corpora	part_whole	{'e1': {'word': 'translation knowledge', 'word_index': [(33, 33)], 'id': 'I08-1043.24'}, 'e2': {'word': 'bilingual corpora', 'word_index': [(37, 37)], 'id': 'I08-1043.25'}}	Through experiments with ENTITYUNRELATED of a ENTITYUNRELATED , we show that our ENTITYUNRELATED effectively extracts ENTITYUNRELATED with high ENTITYUNRELATED , 94.3 % and 84.6 % respectively for ENTITYUNRELATED and ENTITYUNRELATED , and the ENTITY extracted from the ENTITYOTHER could be generalized successfully using the ENTITYUNRELATED with the 12.5 % ENTITYUNRELATED .
Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.	compression ratio	paraphrases	model-feature	{'e1': {'word': 'compression ratio', 'word_index': [(49, 49)], 'id': 'I08-1043.27'}, 'e2': {'word': 'paraphrases', 'word_index': [(44, 44)], 'id': 'I08-1043.26'}}	Through experiments with ENTITYUNRELATED of a ENTITYUNRELATED , we show that our ENTITYUNRELATED effectively extracts ENTITYUNRELATED with high ENTITYUNRELATED , 94.3 % and 84.6 % respectively for ENTITYUNRELATED and ENTITYUNRELATED , and the ENTITYUNRELATED extracted from the ENTITYUNRELATED could be generalized successfully using the ENTITYOTHER with the 12.5 % ENTITY .
This paper describes a computational model of word segmentation and presents simulation results on realistic acquisition.	computational model	word segmentation	model-feature	{'e1': {'word': 'computational model', 'word_index': [(4, 4)], 'id': 'W04-1307.1'}, 'e2': {'word': 'word segmentation', 'word_index': [(6, 6)], 'id': 'W04-1307.2'}}	This paper describes a ENTITY of ENTITYOTHER and presents simulation results on ENTITYUNRELATED .
In particular, we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics.	statistical learning mechanisms	cognitive psychology	usage	{'e1': {'word': 'statistical learning mechanisms', 'word_index': [(10, 10)], 'id': 'W04-1307.4'}, 'e2': {'word': 'cognitive psychology', 'word_index': [(17, 17)], 'id': 'W04-1307.5'}}	In particular , we explore the capacity and limitations of ENTITY that have recently gained prominence in ENTITYOTHER and ENTITYUNRELATED .
Dictionary construction, one of the most difficult tasks in developing a machine translation system, is expensive.	Dictionary construction	machine translation system	part_whole	{'e1': {'word': 'Dictionary construction', 'word_index': [(0, 0)], 'id': 'W04-2204.2'}, 'e2': {'word': 'machine translation system', 'word_index': [(11, 11)], 'id': 'W04-2204.3'}}	ENTITY , one of the most difficult tasks in developing a ENTITYOTHER , is expensive .
To avoid this problem, we investigate how we build a dictionary using existing linguistic resources.	linguistic resources	dictionary	usage	{'e1': {'word': 'linguistic resources', 'word_index': [(14, 14)], 'id': 'W04-2204.5'}, 'e2': {'word': 'dictionary', 'word_index': [(11, 11)], 'id': 'W04-2204.4'}}	To avoid this problem , we investigate how we build a ENTITYOTHER using existing ENTITY .
Our algorithm can be applied to any language pairs, but for the present we focus on building a Korean-to-Japanese dictionary using English as a pivot.	algorithm	language pairs	usage	{'e1': {'word': 'algorithm', 'word_index': [(1, 1)], 'id': 'W04-2204.6'}, 'e2': {'word': 'language pairs', 'word_index': [(7, 7)], 'id': 'W04-2204.7'}}	Our ENTITY can be applied to any ENTITYOTHER , but for the present we focus on building a ENTITYUNRELATED using ENTITYUNRELATED as a ENTITYUNRELATED .
Our algorithm can be applied to any language pairs, but for the present we focus on building a Korean-to-Japanese dictionary using English as a pivot.	pivot	Korean-to-Japanese dictionary	usage	{'e1': {'word': 'pivot', 'word_index': [(23, 23)], 'id': 'W04-2204.10'}, 'e2': {'word': 'Korean-to-Japanese dictionary', 'word_index': [(18, 18)], 'id': 'W04-2204.8'}}	Our ENTITYUNRELATED can be applied to any ENTITYUNRELATED , but for the present we focus on building a ENTITYOTHER using ENTITYUNRELATED as a ENTITY .
We attempt three ways of automatic construction to corroborate the effect of the directionality of dictionaries.	directionality	dictionaries	model-feature	{'e1': {'word': 'directionality', 'word_index': [(12, 12)], 'id': 'W04-2204.12'}, 'e2': {'word': 'dictionaries', 'word_index': [(14, 14)], 'id': 'W04-2204.13'}}	We attempt three ways of ENTITYUNRELATED to corroborate the effect of the ENTITY of ENTITYOTHER .
"First, we introduce ""one-time look up"" method using a Korean-to-English and a Japanese-to-English dictionary."	Korean-to-English and a Japanese-to-English dictionary	"""one-time look up"" method"	usage	"{'e1': {'word': 'Korean-to-English and a Japanese-to-English dictionary', 'word_index': [(7, 7)], 'id': 'W04-2204.15'}, 'e2': {'word': '""one-time look up"" method', 'word_index': [(4, 4)], 'id': 'W04-2204.14'}}"	First , we introduce ENTITYOTHER using a ENTITY .
We present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments.	discourse connectives	discourse structure	usage	{'e1': {'word': 'discourse connectives', 'word_index': [(15, 15)], 'id': 'W04-2703.4'}, 'e2': {'word': 'discourse structure', 'word_index': [(9, 9)], 'id': 'W04-2703.3'}}	We present an approach to annotating a level of ENTITYOTHER that is based on identifying ENTITY and their ENTITYUNRELATED .
In this paper, we present a fully automated extraction system, named IntEx, to identify gene and protein interactions in biomedical text.	gene and protein interactions	biomedical text	part_whole	{'e1': {'word': 'gene and protein interactions', 'word_index': [(14, 14)], 'id': 'W05-1308.3'}, 'e2': {'word': 'biomedical text', 'word_index': [(16, 16)], 'id': 'W05-1308.4'}}	In this paper , we present a ENTITYUNRELATED , named ENTITYUNRELATED , to identify ENTITY in ENTITYOTHER .
Our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles.	simple clausal structures	complex sentences	part_whole	{'e1': {'word': 'simple clausal structures', 'word_index': [(9, 9)], 'id': 'W05-1308.6'}, 'e2': {'word': 'complex sentences', 'word_index': [(7, 7)], 'id': 'W05-1308.5'}}	Our approach is based on first splitting ENTITYOTHER into ENTITY made up of ENTITYUNRELATED .
Then, tagging biological entities with the help of biomedical and linguistic ontologies.	biomedical and linguistic ontologies	biological entities	model-feature	{'e1': {'word': 'biomedical and linguistic ontologies', 'word_index': [(8, 8)], 'id': 'W05-1308.9'}, 'e2': {'word': 'biological entities', 'word_index': [(3, 3)], 'id': 'W05-1308.8'}}	Then , tagging ENTITYOTHER with the help of ENTITY .
Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence.	extraction system	complex sentences	usage	{'e1': {'word': 'extraction system', 'word_index': [(1, 1)], 'id': 'W05-1308.12'}, 'e2': {'word': 'complex sentences', 'word_index': [(3, 3)], 'id': 'W05-1308.13'}}	Our ENTITY handles ENTITYOTHER and extracts ENTITYUNRELATED specified in a ENTITYUNRELATED .
Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence.	multiple and nested interactions	sentence	part_whole	{'e1': {'word': 'multiple and nested interactions', 'word_index': [(6, 6)], 'id': 'W05-1308.14'}, 'e2': {'word': 'sentence', 'word_index': [(10, 10)], 'id': 'W05-1308.15'}}	Our ENTITYUNRELATED handles ENTITYUNRELATED and extracts ENTITY specified in a ENTITYOTHER .
Experimental evaluations with two other state of the art extraction systems indicate that the IntEx system achieves better performance without the labor intensive pattern engineering requirement.	IntEx system	performance	result	{'e1': {'word': 'IntEx system', 'word_index': [(13, 13)], 'id': 'W05-1308.17'}, 'e2': {'word': 'performance', 'word_index': [(16, 16)], 'id': 'W05-1308.18'}}	Experimental evaluations with two other state of the art ENTITYUNRELATED indicate that the ENTITY achieves better ENTITYOTHER without the labor intensive ENTITYUNRELATED .
We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences.	distance	concepts	model-feature	{'e1': {'word': 'distance', 'word_index': [(7, 7)], 'id': 'W06-1605.1'}, 'e2': {'word': 'concepts', 'word_index': [(9, 9)], 'id': 'W06-1605.2'}}	We propose a framework to derive the ENTITY between ENTITYOTHER from ENTITYUNRELATED .
We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept-concept matrix roughly.01% the size of that created by existing measures.	categories	thesaurus	part_whole	{'e1': {'word': 'categories', 'word_index': [(3, 3)], 'id': 'W06-1605.4'}, 'e2': {'word': 'thesaurus', 'word_index': [(7, 7)], 'id': 'W06-1605.5'}}	We use the ENTITY in a published ENTITYOTHER as ENTITYUNRELATED , allowing all possible ENTITYUNRELATED to be stored in a ENTITYUNRELATED roughly . 01 % the size of that created by existing measures .
We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept-concept matrix roughly.01% the size of that created by existing measures.	coarse-grained concepts	concept-concept matrix	part_whole	{'e1': {'word': 'coarse-grained concepts', 'word_index': [(9, 9)], 'id': 'W06-1605.6'}, 'e2': {'word': 'concept-concept matrix', 'word_index': [(20, 20)], 'id': 'W06-1605.8'}}	We use the ENTITYUNRELATED in a published ENTITYUNRELATED as ENTITY , allowing all possible ENTITYUNRELATED to be stored in a ENTITYOTHER roughly . 01 % the size of that created by existing measures .
We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors.	concept-distance measures	traditional distributional word-distance measures	compare	{'e1': {'word': 'concept-distance measures', 'word_index': [(6, 6)], 'id': 'W06-1605.9'}, 'e2': {'word': 'traditional distributional word-distance measures', 'word_index': [(8, 8)], 'id': 'W06-1605.10'}}	We show that the newly proposed ENTITY outperform ENTITYOTHER in the tasks of ( 1 ) ranking ENTITYUNRELATED in order of ENTITYUNRELATED , and ( 2 ) correcting ENTITYUNRELATED .
We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors.	semantic distance	word pairs	model-feature	{'e1': {'word': 'semantic distance', 'word_index': [(21, 21)], 'id': 'W06-1605.12'}, 'e2': {'word': 'word pairs', 'word_index': [(17, 17)], 'id': 'W06-1605.11'}}	We show that the newly proposed ENTITYUNRELATED outperform ENTITYUNRELATED in the tasks of ( 1 ) ranking ENTITYOTHER in order of ENTITY , and ( 2 ) correcting ENTITYUNRELATED .
In the latter task, of all the WordNet-based measures, only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures.	WordNet-based measures	distributional concept-distance measures	compare	{'e1': {'word': 'WordNet-based measures', 'word_index': [(8, 8)], 'id': 'W06-1605.14'}, 'e2': {'word': 'distributional concept-distance measures', 'word_index': [(20, 20)], 'id': 'W06-1605.15'}}	In the latter task , of all the ENTITY , only that proposed by Jiang and Conrath outperforms the best ENTITYOTHER .
We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures, and illustrate how this allows one to view NLP tasks as graph transformations.	labeled directed graph	linguistic structures	model-feature	{'e1': {'word': 'labeled directed graph', 'word_index': [(9, 9)], 'id': 'W07-0208.1'}, 'e2': {'word': 'linguistic structures', 'word_index': [(15, 15)], 'id': 'W07-0208.2'}}	We argue in favor of the the use of ENTITY to represent various types of ENTITYOTHER , and illustrate how this allows one to view ENTITYUNRELATED as ENTITYUNRELATED .
We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures, and illustrate how this allows one to view NLP tasks as graph transformations.	graph transformations	NLP tasks	model-feature	{'e1': {'word': 'graph transformations', 'word_index': [(27, 27)], 'id': 'W07-0208.4'}, 'e2': {'word': 'NLP tasks', 'word_index': [(25, 25)], 'id': 'W07-0208.3'}}	We argue in favor of the the use of ENTITYUNRELATED to represent various types of ENTITYUNRELATED , and illustrate how this allows one to view ENTITYOTHER as ENTITY .
We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using Penn Treebank data) and semantic role labeling (using Proposition Bank data).	identification of non-local depenencies	Penn Treebank data	model-feature	{'e1': {'word': 'identification of non-local depenencies', 'word_index': [(22, 22)], 'id': 'W07-0208.7'}, 'e2': {'word': 'Penn Treebank data', 'word_index': [(25, 25)], 'id': 'W07-0208.8'}}	We present a general method for learning such ENTITYUNRELATED from an ENTITYUNRELATED and describe experiments with two applications of the method : ENTITY ( using ENTITYOTHER ) and ENTITYUNRELATED ( using ENTITYUNRELATED ) .
We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using Penn Treebank data) and semantic role labeling (using Proposition Bank data).	semantic role labeling	Proposition Bank data	model-feature	{'e1': {'word': 'semantic role labeling', 'word_index': [(28, 28)], 'id': 'W07-0208.9'}, 'e2': {'word': 'Proposition Bank data', 'word_index': [(31, 31)], 'id': 'W07-0208.10'}}	We present a general method for learning such ENTITYUNRELATED from an ENTITYUNRELATED and describe experiments with two applications of the method : ENTITYUNRELATED ( using ENTITYUNRELATED ) and ENTITY ( using ENTITYOTHER ) .
Topical blog post retrieval is the task of ranking blog posts with respect to their relevance for a given topic.	relevance	blog posts	model-feature	{'e1': {'word': 'relevance', 'word_index': [(11, 11)], 'id': 'P08-1105.3'}, 'e2': {'word': 'blog posts', 'word_index': [(6, 6)], 'id': 'P08-1105.2'}}	ENTITYUNRELATED is the task of ranking ENTITYOTHER with respect to their ENTITY for a given ENTITYUNRELATED .
To improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process.	textual credibility indicators	topical blog post retrieval	usage	{'e1': {'word': 'textual credibility indicators', 'word_index': [(5, 5)], 'id': 'P08-1105.6'}, 'e2': {'word': 'topical blog post retrieval', 'word_index': [(2, 2)], 'id': 'P08-1105.5'}}	To improve ENTITYOTHER we incorporate ENTITY in the ENTITYUNRELATED .
We describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models.	indicators	retrieval approach	usage	{'e1': {'word': 'indicators', 'word_index': [(6, 6)], 'id': 'P08-1105.11'}, 'e2': {'word': 'retrieval approach', 'word_index': [(14, 14)], 'id': 'P08-1105.12'}}	We describe how to estimate these ENTITY and how to integrate them into a ENTITYOTHER based on ENTITYUNRELATED .
Experiments on the TREC Blog track test set show that both groups of credibility indicators significantly improve retrieval effectiveness; the best performance is achieved when combining them.	credibility indicators	retrieval effectiveness	result	{'e1': {'word': 'credibility indicators', 'word_index': [(9, 9)], 'id': 'P08-1105.15'}, 'e2': {'word': 'retrieval effectiveness', 'word_index': [(12, 12)], 'id': 'P08-1105.16'}}	Experiments on the ENTITYUNRELATED show that both groups of ENTITY significantly improve ENTITYOTHER ; the best performance is achieved when combining them .
Four problems render vector space model (VSM)-based text classification approach ineffective: 1) Many words within song lyrics actually contribute little to sentiment; 2) Nouns and verbs used to express sentiment are ambiguous; 3) Negations and modifiers around the sentiment keywords make particular contributions to sentiment; 4) Song lyric is usually very short.	words	song lyrics	part_whole	{'e1': {'word': 'words', 'word_index': [(9, 9)], 'id': 'P08-2034.4'}, 'e2': {'word': 'song lyrics', 'word_index': [(11, 11)], 'id': 'P08-2034.5'}}	Four problems render ENTITYUNRELATED ineffective : 1 ) Many ENTITY within ENTITYOTHER actually contribute little to ENTITYUNRELATED ; 2 ) ENTITYUNRELATED and ENTITYUNRELATED used to express ENTITYUNRELATED are ambiguous ; 3 ) ENTITYUNRELATED and ENTITYUNRELATED around the ENTITYUNRELATED make particular contributions to ENTITYUNRELATED ; 4 ) ENTITYUNRELATED is usually very short .
To address these problems, the sentiment vector space model (s-VSM) is proposed to represent song lyric document.	sentiment vector space model (s-VSM)	song lyric document	model-feature	{'e1': {'word': 'sentiment vector space model (s-VSM)', 'word_index': [(6, 6)], 'id': 'P08-2034.15'}, 'e2': {'word': 'song lyric document', 'word_index': [(11, 11)], 'id': 'P08-2034.16'}}	To address these problems , the ENTITY is proposed to represent ENTITYOTHER .
The preliminary experiments prove that the s-VSM model outperforms the VSM model in the lyric-based song sentiment classification task.	s-VSM model	VSM model	compare	{'e1': {'word': 's-VSM model', 'word_index': [(6, 6)], 'id': 'P08-2034.17'}, 'e2': {'word': 'VSM model', 'word_index': [(9, 9)], 'id': 'P08-2034.18'}}	The preliminary experiments prove that the ENTITY outperforms the ENTITYOTHER in the ENTITYUNRELATED .
This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%-96.7% accuracy depending on classification method).	source language	EUROPARL corpus	model-feature	{'e1': {'word': 'source language', 'word_index': [(12, 12)], 'id': 'C08-1118.1'}, 'e2': {'word': 'EUROPARL corpus', 'word_index': [(20, 20)], 'id': 'C08-1118.2'}}	This paper shows that it is very often possible to identify the ENTITY of medium - length speeches in the ENTITYOTHER on the basis of ENTITYUNRELATED of ENTITYUNRELATED ( 87.2 %-96.7 % ENTITYUNRELATED depending on ENTITYUNRELATED ) .
This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%-96.7% accuracy depending on classification method).	frequency counts	word n-grams	model-feature	{'e1': {'word': 'frequency counts', 'word_index': [(25, 25)], 'id': 'C08-1118.3'}, 'e2': {'word': 'word n-grams', 'word_index': [(27, 27)], 'id': 'C08-1118.4'}}	This paper shows that it is very often possible to identify the ENTITYUNRELATED of medium - length speeches in the ENTITYUNRELATED on the basis of ENTITY of ENTITYOTHER ( 87.2 %-96.7 % ENTITYUNRELATED depending on ENTITYUNRELATED ) .
This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%-96.7% accuracy depending on classification method).	classification method	accuracy	result	{'e1': {'word': 'classification method', 'word_index': [(35, 35)], 'id': 'C08-1118.6'}, 'e2': {'word': 'accuracy', 'word_index': [(32, 32)], 'id': 'C08-1118.5'}}	This paper shows that it is very often possible to identify the ENTITYUNRELATED of medium - length speeches in the ENTITYUNRELATED on the basis of ENTITYUNRELATED of ENTITYUNRELATED ( 87.2 %-96.7 % ENTITYOTHER depending on ENTITY ) .
Words in Chinese text are not naturally separated by delimiters, which poses a challenge to standard machine translation (MT) systems.	Words	Chinese text	part_whole	{'e1': {'word': 'Words', 'word_index': [(0, 0)], 'id': 'C08-1128.1'}, 'e2': {'word': 'Chinese text', 'word_index': [(2, 2)], 'id': 'C08-1128.2'}}	ENTITY in ENTITYOTHER are not naturally separated by ENTITYUNRELATED , which poses a challenge to ENTITYUNRELATED .
In MT, the widely used approach is to apply a Chinese word segmenter trained from manually annotated data, using a fixed lexicon.	Chinese word segmenter	MT	usage	{'e1': {'word': 'Chinese word segmenter', 'word_index': [(11, 11)], 'id': 'C08-1128.6'}, 'e2': {'word': 'MT', 'word_index': [(1, 1)], 'id': 'C08-1128.5'}}	In ENTITYOTHER , the widely used approach is to apply a ENTITY trained from ENTITYUNRELATED , using a fixed ENTITYUNRELATED .
We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT.	monolingual and bilingual information	Bayesian semi-supervised Chinese word segmentation model	usage	{'e1': {'word': 'monolingual and bilingual information', 'word_index': [(7, 7)], 'id': 'C08-1128.12'}, 'e2': {'word': 'Bayesian semi-supervised Chinese word segmentation model', 'word_index': [(3, 3)], 'id': 'C08-1128.11'}}	We propose a ENTITYOTHER which uses both ENTITY to derive a ENTITYUNRELATED suitable for ENTITYUNRELATED .
We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT.	segmentation	MT	usage	{'e1': {'word': 'segmentation', 'word_index': [(11, 11)], 'id': 'C08-1128.13'}, 'e2': {'word': 'MT', 'word_index': [(14, 14)], 'id': 'C08-1128.14'}}	We propose a ENTITYUNRELATED which uses both ENTITYUNRELATED to derive a ENTITY suitable for ENTITYOTHER .
Indeed, automatic evaluations need high-quality data that allow the comparison of both automatic and human translations.	high-quality data	automatic evaluations	usage	{'e1': {'word': 'high-quality data', 'word_index': [(4, 4)], 'id': 'C08-2010.7'}, 'e2': {'word': 'automatic evaluations', 'word_index': [(2, 2)], 'id': 'C08-2010.6'}}	Indeed , ENTITYOTHER need ENTITY that allow the comparison of both ENTITYUNRELATED .
This paper describes the impact of using different-quality references on evaluation.	different-quality references	evaluation	usage	{'e1': {'word': 'different-quality references', 'word_index': [(7, 7)], 'id': 'C08-2010.9'}, 'e2': {'word': 'evaluation', 'word_index': [(9, 9)], 'id': 'C08-2010.10'}}	This paper describes the impact of using ENTITY on ENTITYOTHER .
Thus, the limitations of the automatic metrics used within MT are also discussed in this regard.	automatic metrics	MT	usage	{'e1': {'word': 'automatic metrics', 'word_index': [(6, 6)], 'id': 'C08-2010.11'}, 'e2': {'word': 'MT', 'word_index': [(9, 9)], 'id': 'C08-2010.12'}}	Thus , the limitations of the ENTITY used within ENTITYOTHER are also discussed in this regard .
The tool supports queries with an arbitrary number of wildcards.	wildcards	queries	model-feature	{'e1': {'word': 'wildcards', 'word_index': [(9, 9)], 'id': 'C08-3010.4'}, 'e2': {'word': 'queries', 'word_index': [(3, 3)], 'id': 'C08-3010.3'}}	The tool supports ENTITYOTHER with an arbitrary number of ENTITY .
This paper presents an approach to the unsupervised learning of parts of speech which uses both morphological and syntactic information.	morphological and syntactic information	unsupervised learning	usage	{'e1': {'word': 'morphological and syntactic information', 'word_index': [(13, 13)], 'id': 'W03-2907.3'}, 'e2': {'word': 'unsupervised learning', 'word_index': [(7, 7)], 'id': 'W03-2907.1'}}	This paper presents an approach to the ENTITYOTHER of ENTITYUNRELATED which uses both ENTITY .
While the model is more complex than those which have been employed for unsupervised learning of POS tags in English, which use only syntactic information, the variety of languages in the world requires that we consider morphology as well.	syntactic information	unsupervised learning	usage	{'e1': {'word': 'syntactic information', 'word_index': [(20, 20)], 'id': 'W03-2907.7'}, 'e2': {'word': 'unsupervised learning', 'word_index': [(13, 13)], 'id': 'W03-2907.5'}}	While the ENTITYUNRELATED is more complex than those which have been employed for ENTITYOTHER of ENTITYUNRELATED , which use only ENTITY , the variety of ENTITYUNRELATED in the world requires that we consider ENTITYUNRELATED as well .
While the model is more complex than those which have been employed for unsupervised learning of POS tags in English, which use only syntactic information, the variety of languages in the world requires that we consider morphology as well.	morphology	languages	part_whole	{'e1': {'word': 'morphology', 'word_index': [(33, 33)], 'id': 'W03-2907.9'}, 'e2': {'word': 'languages', 'word_index': [(25, 25)], 'id': 'W03-2907.8'}}	While the ENTITYUNRELATED is more complex than those which have been employed for ENTITYUNRELATED of ENTITYUNRELATED , which use only ENTITYUNRELATED , the variety of ENTITYOTHER in the world requires that we consider ENTITY as well .
In many languages, morphology provides better clues to a word's category than word order.	morphology	word order	compare	{'e1': {'word': 'morphology', 'word_index': [(4, 4)], 'id': 'W03-2907.11'}, 'e2': {'word': 'word order', 'word_index': [(14, 14)], 'id': 'W03-2907.12'}}	In many ENTITYUNRELATED , ENTITY provides better clues to a word 's category than ENTITYOTHER .
We present the computational model for POS learning, and present results for applying it to Bulgarian, a Slavic language with relatively free word order and rich morphology.	computational model	POS learning	usage	{'e1': {'word': 'computational model', 'word_index': [(3, 3)], 'id': 'W03-2907.13'}, 'e2': {'word': 'POS learning', 'word_index': [(5, 5)], 'id': 'W03-2907.14'}}	We present the ENTITY for ENTITYOTHER , and present results for applying it to ENTITYUNRELATED , a ENTITYUNRELATED with relatively ENTITYUNRELATED and ENTITYUNRELATED .
We present the computational model for POS learning, and present results for applying it to Bulgarian, a Slavic language with relatively free word order and rich morphology.	Bulgarian	free word order	model-feature	{'e1': {'word': 'Bulgarian', 'word_index': [(14, 14)], 'id': 'W03-2907.15'}, 'e2': {'word': 'free word order', 'word_index': [(20, 20)], 'id': 'W03-2907.17'}}	We present the ENTITYUNRELATED for ENTITYUNRELATED , and present results for applying it to ENTITY , a ENTITYUNRELATED with relatively ENTITYOTHER and ENTITYUNRELATED .
We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies.	generative history-based latent variable model	CoNLL 2008 shared task	usage	{'e1': {'word': 'generative history-based latent variable model', 'word_index': [(13, 13)], 'id': 'W08-2122.2'}, 'e2': {'word': 'CoNLL 2008 shared task', 'word_index': [(9, 9)], 'id': 'W08-2122.1'}}	We propose a solution to the challenge of the ENTITYOTHER that uses a ENTITY to predict the most likely ENTITYUNRELATED of a ENTITYUNRELATED for both ENTITYUNRELATED .
We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies.	derivation	syntactic and semantic dependencies	model-feature	{'e1': {'word': 'derivation', 'word_index': [(19, 19)], 'id': 'W08-2122.3'}, 'e2': {'word': 'syntactic and semantic dependencies', 'word_index': [(25, 25)], 'id': 'W08-2122.5'}}	We propose a solution to the challenge of the ENTITYUNRELATED that uses a ENTITYUNRELATED to predict the most likely ENTITY of a ENTITYUNRELATED for both ENTITYOTHER .
The submitted model yields 79.1% macro-average F1 performance, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1.	model	macro-average F1 performance	result	{'e1': {'word': 'model', 'word_index': [(2, 2)], 'id': 'W08-2122.6'}, 'e2': {'word': 'macro-average F1 performance', 'word_index': [(6, 6)], 'id': 'W08-2122.7'}}	The submitted ENTITY yields 79.1 % ENTITYOTHER , for the joint task , 86.9 % ENTITYUNRELATED and 71.0 % ENTITYUNRELATED .
A larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1.	model	macro-average F1	result	{'e1': {'word': 'model', 'word_index': [(2, 2)], 'id': 'W08-2122.10'}, 'e2': {'word': 'macro-average F1', 'word_index': [(10, 10)], 'id': 'W08-2122.11'}}	A larger ENTITY trained after the deadline achieves 80.5 % ENTITYOTHER , 87.6 % ENTITYUNRELATED , and 73.1 % ENTITYUNRELATED .
Pipelined Natural Language Generation (NLG) systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions, lexical choice, and revision.	architectural modules	language functionalities	usage	{'e1': {'word': 'architectural modules', 'word_index': [(6, 6)], 'id': 'P03-1034.2'}, 'e2': {'word': 'language functionalities', 'word_index': [(11, 11)], 'id': 'P03-1034.3'}}	ENTITYUNRELATED have grown increasingly complex as ENTITY were added to support ENTITYOTHER such as ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED .
This has given rise to discussions about the relative placement of these new modules in the overall architecture.	modules	architecture	part_whole	{'e1': {'word': 'modules', 'word_index': [(13, 13)], 'id': 'P03-1034.7'}, 'e2': {'word': 'architecture', 'word_index': [(17, 17)], 'id': 'P03-1034.8'}}	This has given rise to discussions about the relative placement of these new ENTITY in the overall ENTITYOTHER .
We present examples which suggest that in a pipelined NLG architecture, the best approach is to strongly tie it to a revision component.	revision component	pipelined NLG architecture	part_whole	{'e1': {'word': 'revision component', 'word_index': [(20, 20)], 'id': 'P03-1034.13'}, 'e2': {'word': 'pipelined NLG architecture', 'word_index': [(8, 8)], 'id': 'P03-1034.12'}}	We present examples which suggest that in a ENTITYOTHER , the best approach is to strongly tie it to a ENTITY .
With performance above 97% accuracy for newspaper text, part of speech (pos) tagging might be considered a solved problem.	part of speech (pos) tagging	accuracy	result	{'e1': {'word': 'part of speech (pos) tagging', 'word_index': [(9, 9)], 'id': 'P06-1088.3'}, 'e2': {'word': 'accuracy', 'word_index': [(5, 5)], 'id': 'P06-1088.1'}}	With performance above 97 % ENTITYOTHER for ENTITYUNRELATED , ENTITY might be considered a solved problem .
Previous studies have shown that allowing the parser to resolve pos tag ambiguity does not improve performance.	parser	pos tag ambiguity	usage	{'e1': {'word': 'parser', 'word_index': [(7, 7)], 'id': 'P06-1088.4'}, 'e2': {'word': 'pos tag ambiguity', 'word_index': [(10, 10)], 'id': 'P06-1088.5'}}	Previous studies have shown that allowing the ENTITY to resolve ENTITYOTHER does not improve performance .
However, for grammar formalisms which use more fine-grained grammatical categories, for example tag and ccg, tagging accuracy is much lower.	fine-grained grammatical categories	grammar formalisms	usage	{'e1': {'word': 'fine-grained grammatical categories', 'word_index': [(7, 7)], 'id': 'P06-1088.7'}, 'e2': {'word': 'grammar formalisms', 'word_index': [(3, 3)], 'id': 'P06-1088.6'}}	However , for ENTITYOTHER which use more ENTITY , for example ENTITYUNRELATED and ENTITYUNRELATED , ENTITYUNRELATED is much lower .
We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient ccg parsing.	multi-tagging approach	ccg parsing	usage	{'e1': {'word': 'multi-tagging approach', 'word_index': [(3, 3)], 'id': 'P06-1088.14'}, 'e2': {'word': 'ccg parsing', 'word_index': [(15, 15)], 'id': 'P06-1088.16'}}	We describe a ENTITY which maintains a suitable level of ENTITYUNRELATED for accurate and efficient ENTITYOTHER .
Although pos tagging accuracy seems high, maintaining some pos tag ambiguity in the language processing pipeline results in more accurate ccg supertagging.	pos tag ambiguity	ccg supertagging	result	{'e1': {'word': 'pos tag ambiguity', 'word_index': [(7, 7)], 'id': 'P06-1088.21'}, 'e2': {'word': 'ccg supertagging', 'word_index': [(15, 15)], 'id': 'P06-1088.23'}}	Although ENTITYUNRELATED seems high , maintaining some ENTITY in the ENTITYUNRELATED results in more accurate ENTITYOTHER .
Both rhetorical structure and punctuation have been helpful in discourse processing.	punctuation	discourse processing	usage	{'e1': {'word': 'punctuation', 'word_index': [(3, 3)], 'id': 'P06-3008.2'}, 'e2': {'word': 'discourse processing', 'word_index': [(8, 8)], 'id': 'P06-3008.3'}}	Both ENTITYUNRELATED and ENTITY have been helpful in ENTITYOTHER .
Based on a corpus annotation project, this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts: Colon, Dash, Ellipsis, Exclamation Mark, Question Mark, and Semicolon.	discursive usage	Chinese punctuation marks	model-feature	{'e1': {'word': 'discursive usage', 'word_index': [(9, 9)], 'id': 'P06-3008.5'}, 'e2': {'word': 'Chinese punctuation marks', 'word_index': [(12, 12)], 'id': 'P06-3008.6'}}	Based on a ENTITYUNRELATED , this paper reports the ENTITY of 6 ENTITYOTHER in ENTITYUNRELATED : ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED .
The rhetorical patterns of these marks are compared against patterns around cue phrases in general.	rhetorical patterns	patterns	compare	{'e1': {'word': 'rhetorical patterns', 'word_index': [(1, 1)], 'id': 'P06-3008.14'}, 'e2': {'word': 'patterns', 'word_index': [(8, 8)], 'id': 'P06-3008.15'}}	The ENTITY of these marks are compared against ENTITYOTHER around ENTITYUNRELATED in general .
Results show that these Chinese punctuation marks, though fewer in number than cue phrases, are easy to identify, have strong correlation with certain relations, and can be used as distinctive indicators of nuclearity in Chinese texts.	Chinese punctuation marks	cue phrases	compare	{'e1': {'word': 'Chinese punctuation marks', 'word_index': [(4, 4)], 'id': 'P06-3008.17'}, 'e2': {'word': 'cue phrases', 'word_index': [(11, 11)], 'id': 'P06-3008.18'}}	Results show that these ENTITY , though fewer in number than ENTITYOTHER , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in ENTITYUNRELATED .
We show that the crucial operation of consistency checking for such descriptions is NP-complete, and therefore probably intractable, but proceed to develop algorithms which can sometimes alleviate the unpleasant consequences of this intractability.	algorithms	intractability	usage	{'e1': {'word': 'algorithms', 'word_index': [(25, 25)], 'id': 'C90-3007.4'}, 'e2': {'word': 'intractability', 'word_index': [(35, 35)], 'id': 'C90-3007.5'}}	We show that the crucial operation of ENTITYUNRELATED for such descriptions is NP - complete , and therefore probably intractable , but proceed to develop ENTITY which can sometimes alleviate the unpleasant consequences of this ENTITYOTHER .
This paper presents an algorithm for selecting an appropriate classifier word for a noun.	classifier word	noun	model-feature	{'e1': {'word': 'classifier word', 'word_index': [(9, 9)], 'id': 'C94-1091.1'}, 'e2': {'word': 'noun', 'word_index': [(12, 12)], 'id': 'C94-1091.2'}}	This paper presents an algorithm for selecting an appropriate ENTITY for a ENTITYOTHER .
In Thai language, it frequently happens that there is fluctuation in the choice of classifier for a given concrete noun, both from the point of view of the whole speech community and individual speakers.	classifier	concrete noun	model-feature	{'e1': {'word': 'classifier', 'word_index': [(14, 14)], 'id': 'C94-1091.4'}, 'e2': {'word': 'concrete noun', 'word_index': [(18, 18)], 'id': 'C94-1091.5'}}	In ENTITYUNRELATED , it frequently happens that there is fluctuation in the choice of ENTITY for a given ENTITYOTHER , both from the point of view of the whole ENTITYUNRELATED and ENTITYUNRELATED .
As far as we can do in the rule-based approach is to give a default rule to pick up a corresponding classifier of each noun.	classifier	noun	model-feature	{'e1': {'word': 'classifier', 'word_index': [(19, 19)], 'id': 'C94-1091.11'}, 'e2': {'word': 'noun', 'word_index': [(22, 22)], 'id': 'C94-1091.12'}}	As far as we can do in the ENTITYUNRELATED is to give a ENTITYUNRELATED to pick up a corresponding ENTITY of each ENTITYOTHER .
Registration of classifier for each noun is limited to the type of unit classifier because other types are open due to the meaning of representation.	classifier	noun	model-feature	{'e1': {'word': 'classifier', 'word_index': [(2, 2)], 'id': 'C94-1091.13'}, 'e2': {'word': 'noun', 'word_index': [(5, 5)], 'id': 'C94-1091.14'}}	Registration of ENTITY for each ENTITYOTHER is limited to the ENTITYUNRELATED because other types are open due to the meaning of representation .
We propose a corpus-based method (Biber,1993; Nagao,1993; Smadja,1993) which generates Noun Classifier Associations (NCA) to overcome the problems in classifier assignment and semantic construction of noun phrase.	corpus-based method	classifier assignment	usage	{'e1': {'word': 'corpus-based method', 'word_index': [(3, 3)], 'id': 'C94-1091.16'}, 'e2': {'word': 'classifier assignment', 'word_index': [(25, 25)], 'id': 'C94-1091.18'}}	We propose a ENTITY ( Biber , 1993 ; Nagao , 1993 ; Smadja , 1993 ) which generates ENTITYUNRELATED to overcome the problems in ENTITYOTHER and ENTITYUNRELATED .
This paper describes an unsupervised learning method for associative relationships between verb phrases, which is important in developing reliable Q&amp;A systems.	unsupervised learning method	associative relationships between verb phrases	usage	{'e1': {'word': 'unsupervised learning method', 'word_index': [(4, 4)], 'id': 'C02-1120.1'}, 'e2': {'word': 'associative relationships between verb phrases', 'word_index': [(6, 6)], 'id': 'C02-1120.2'}}	This paper describes an ENTITY for ENTITYOTHER , which is important in developing reliable ENTITYUNRELATED .
Our aim is to develop an unsupervised learning method that can obtain such an associative relationship, which we call scenario consistency.	unsupervised learning method	associative relationship	usage	{'e1': {'word': 'unsupervised learning method', 'word_index': [(6, 6)], 'id': 'C02-1120.12'}, 'e2': {'word': 'associative relationship', 'word_index': [(12, 12)], 'id': 'C02-1120.13'}}	Our aim is to develop an ENTITY that can obtain such an ENTITYOTHER , which we call ENTITYUNRELATED .
The method we are currently working on uses an expectation-maximization (EM) based word-clustering algorithm, and we have evaluated the effectiveness of this method using Japanese verb phrases.	expectation-maximization (EM) based word-clustering algorithm	Japanese verb phrases	usage	{'e1': {'word': 'expectation-maximization (EM) based word-clustering algorithm', 'word_index': [(9, 9)], 'id': 'C02-1120.15'}, 'e2': {'word': 'Japanese verb phrases', 'word_index': [(21, 21)], 'id': 'C02-1120.16'}}	The method we are currently working on uses an ENTITY , and we have evaluated the effectiveness of this method using ENTITYOTHER .
These models provide principled ways of including additional conditioning variables other than the preceding words, such as morphological or syntactic features.	models	preceding words	compare	{'e1': {'word': 'models', 'word_index': [(1, 1)], 'id': 'C04-1022.4'}, 'e2': {'word': 'preceding words', 'word_index': [(12, 12)], 'id': 'C04-1022.6'}}	These ENTITY provide principled ways of including additional ENTITYUNRELATED other than the ENTITYOTHER , such as ENTITYUNRELATED .
This paper presents an entirely data-driven model selection procedure based on genetic search, which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks (Arabic and Turkish).	genetic search	entirely data-driven model selection procedure	usage	{'e1': {'word': 'genetic search', 'word_index': [(7, 7)], 'id': 'C04-1022.11'}, 'e2': {'word': 'entirely data-driven model selection procedure', 'word_index': [(4, 4)], 'id': 'C04-1022.10'}}	This paper presents an ENTITYOTHER based on ENTITY , which is shown to outperform both ENTITYUNRELATED on two different ENTITYUNRELATED ( ENTITYUNRELATED and ENTITYUNRELATED ) .
This paper presents an entirely data-driven model selection procedure based on genetic search, which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks (Arabic and Turkish).	knowledge-based and random selection procedures	language modeling tasks	usage	{'e1': {'word': 'knowledge-based and random selection procedures', 'word_index': [(15, 15)], 'id': 'C04-1022.12'}, 'e2': {'word': 'language modeling tasks', 'word_index': [(19, 19)], 'id': 'C04-1022.13'}}	This paper presents an ENTITYUNRELATED based on ENTITYUNRELATED , which is shown to outperform both ENTITY on two different ENTITYOTHER ( ENTITYUNRELATED and ENTITYUNRELATED ) .
Landsbergen's advocacy of analytical inverses for compositional syntax rules encourages the application of Definite Clause Grammar techniques to the construction of a parser returning  Montague analysis trees.	analytical inverses	compositional syntax rules	model-feature	{'e1': {'word': 'analytical inverses', 'word_index': [(4, 4)], 'id': 'E85-1004.1'}, 'e2': {'word': 'compositional syntax rules', 'word_index': [(6, 6)], 'id': 'E85-1004.2'}}	Landsbergen 's advocacy of ENTITY for ENTITYOTHER encourages the application of ENTITYUNRELATED to the construction of a ENTITYUNRELATED returning ENTITYUNRELATED .
Landsbergen's advocacy of analytical inverses for compositional syntax rules encourages the application of Definite Clause Grammar techniques to the construction of a parser returning  Montague analysis trees.	Definite Clause Grammar techniques	parser	usage	{'e1': {'word': 'Definite Clause Grammar techniques', 'word_index': [(11, 11)], 'id': 'E85-1004.3'}, 'e2': {'word': 'parser', 'word_index': [(17, 17)], 'id': 'E85-1004.4'}}	Landsbergen 's advocacy of ENTITYUNRELATED for ENTITYUNRELATED encourages the application of ENTITY to the construction of a ENTITYOTHER returning ENTITYUNRELATED .
A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing* and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae.	augmented Friedman - Warren algorithm	parser MDCC	usage	{'e1': {'word': 'augmented Friedman - Warren algorithm', 'word_index': [(7, 7)], 'id': 'E85-1004.7'}, 'e2': {'word': 'parser MDCC', 'word_index': [(1, 1)], 'id': 'E85-1004.6'}}	A ENTITYOTHER is presented which implements an ENTITY permitting ENTITYUNRELATED * and interfaces with a language of ENTITYUNRELATED so as to display the ENTITYUNRELATED of corresponding ENTITYUNRELATED .
A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing* and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae.	derivational history	reduced IL formulae	model-feature	{'e1': {'word': 'derivational history', 'word_index': [(23, 23)], 'id': 'E85-1004.10'}, 'e2': {'word': 'reduced IL formulae', 'word_index': [(26, 26)], 'id': 'E85-1004.11'}}	A ENTITYUNRELATED is presented which implements an ENTITYUNRELATED permitting ENTITYUNRELATED * and interfaces with a language of ENTITYUNRELATED so as to display the ENTITY of corresponding ENTITYOTHER .
Theoretical research in the area of machine translation usually involves the search for and creation of an appropriate formalism.	formalism	machine translation	usage	{'e1': {'word': 'formalism', 'word_index': [(17, 17)], 'id': 'E89-1040.2'}, 'e2': {'word': 'machine translation', 'word_index': [(6, 6)], 'id': 'E89-1040.1'}}	Theoretical research in the area of ENTITYOTHER usually involves the search for and creation of an appropriate ENTITY .
An important issue in this respect is the way in which the compositionality of translation is to be defined.	compositionality	translation	model-feature	{'e1': {'word': 'compositionality', 'word_index': [(12, 12)], 'id': 'E89-1040.3'}, 'e2': {'word': 'translation', 'word_index': [(14, 14)], 'id': 'E89-1040.4'}}	An important issue in this respect is the way in which the ENTITY of ENTITYOTHER is to be defined .
In this paper, we will introduce the anaphoric component of the Mimo formalism.	anaphoric component	Mimo formalism	part_whole	{'e1': {'word': 'anaphoric component', 'word_index': [(8, 8)], 'id': 'E89-1040.5'}, 'e2': {'word': 'Mimo formalism', 'word_index': [(11, 11)], 'id': 'E89-1040.6'}}	In this paper , we will introduce the ENTITY of the ENTITYOTHER .
In Mimo, the translation of anaphoric relations is compositional.	Mimo	translation	usage	{'e1': {'word': 'Mimo', 'word_index': [(1, 1)], 'id': 'E89-1040.11'}, 'e2': {'word': 'translation', 'word_index': [(4, 4)], 'id': 'E89-1040.12'}}	In ENTITY , the ENTITYOTHER of ENTITYUNRELATED is compositional .
The anaphoric component is used to define linguistic phenomena such as wh-movement, the passive and the binding of reflexives and pronouns mono-lingually.	anaphoric component	linguistic phenomena	usage	{'e1': {'word': 'anaphoric component', 'word_index': [(1, 1)], 'id': 'E89-1040.14'}, 'e2': {'word': 'linguistic phenomena', 'word_index': [(6, 6)], 'id': 'E89-1040.15'}}	The ENTITY is used to define ENTITYOTHER such as ENTITYUNRELATED , the ENTITYUNRELATED and the ENTITYUNRELATED mono-lingually .
A domain independent model is proposed for the automated interpretation of nominal compounds in English.	domain independent model	automated interpretation	usage	{'e1': {'word': 'domain independent model', 'word_index': [(1, 1)], 'id': 'C96-1062.1'}, 'e2': {'word': 'automated interpretation', 'word_index': [(6, 6)], 'id': 'C96-1062.2'}}	A ENTITY is proposed for the ENTITYOTHER of ENTITYUNRELATED in ENTITYUNRELATED .
A domain independent model is proposed for the automated interpretation of nominal compounds in English.	nominal compounds	English	part_whole	{'e1': {'word': 'nominal compounds', 'word_index': [(8, 8)], 'id': 'C96-1062.3'}, 'e2': {'word': 'English', 'word_index': [(10, 10)], 'id': 'C96-1062.4'}}	A ENTITYUNRELATED is proposed for the ENTITYUNRELATED of ENTITY in ENTITYOTHER .
This model is meant to account for productive rules of interpretation which are inferred from the morpho-syntactic and semantic characteristics of the nominal constituents.	morpho-syntactic and semantic characteristics	nominal constituents	model-feature	{'e1': {'word': 'morpho-syntactic and semantic characteristics', 'word_index': [(13, 13)], 'id': 'C96-1062.7'}, 'e2': {'word': 'nominal constituents', 'word_index': [(16, 16)], 'id': 'C96-1062.8'}}	This ENTITYUNRELATED is meant to account for ENTITYUNRELATED which are inferred from the ENTITY of the ENTITYOTHER .
In particular, we make extensive use of Pustejovsky's principles concerning the predicative information associated with nominals.	predicative information	nominals	model-feature	{'e1': {'word': 'predicative information', 'word_index': [(13, 13)], 'id': 'C96-1062.9'}, 'e2': {'word': 'nominals', 'word_index': [(16, 16)], 'id': 'C96-1062.10'}}	In particular , we make extensive use of Pustejovsky 's principles concerning the ENTITY associated with ENTITYOTHER .
We argue that it is necessary to draw a line between generalizable semantic principles and domain-specific semantic information.	generalizable semantic principles	domain-specific semantic information	compare	{'e1': {'word': 'generalizable semantic principles', 'word_index': [(11, 11)], 'id': 'C96-1062.11'}, 'e2': {'word': 'domain-specific semantic information', 'word_index': [(13, 13)], 'id': 'C96-1062.12'}}	We argue that it is necessary to draw a line between ENTITY and ENTITYOTHER .
We explain this distinction and we show how this model may be applied to the interpretation of compounds in real texts, provided that complementary semantic information are retrieved.	interpretation	compounds	model-feature	{'e1': {'word': 'interpretation', 'word_index': [(15, 15)], 'id': 'C96-1062.13'}, 'e2': {'word': 'compounds', 'word_index': [(17, 17)], 'id': 'C96-1062.14'}}	We explain this distinction and we show how this model may be applied to the ENTITY of ENTITYOTHER in ENTITYUNRELATED , provided that complementary ENTITYUNRELATED are retrieved .
We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text.	entity-based representation	discourse	model-feature	{'e1': {'word': 'entity-based representation', 'word_index': [(4, 4)], 'id': 'P05-1018.2'}, 'e2': {'word': 'discourse', 'word_index': [(6, 6)], 'id': 'P05-1018.3'}}	We present a novel ENTITY of ENTITYOTHER which is inspired by ENTITYUNRELATED and can be computed automatically from ENTITYUNRELATED .
We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function.	ranking learning problem	coherence assessment	model-feature	{'e1': {'word': 'ranking learning problem', 'word_index': [(5, 5)], 'id': 'P05-1018.7'}, 'e2': {'word': 'coherence assessment', 'word_index': [(2, 2)], 'id': 'P05-1018.6'}}	We view ENTITYOTHER as a ENTITY and show that the proposed ENTITYUNRELATED supports the effective learning of a ENTITYUNRELATED .
Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model.	induced model	state-of-the-art coherence model	compare	{'e1': {'word': 'induced model', 'word_index': [(5, 5)], 'id': 'P05-1018.10'}, 'e2': {'word': 'state-of-the-art coherence model', 'word_index': [(12, 12)], 'id': 'P05-1018.12'}}	Our experiments demonstrate that the ENTITY achieves significantly higher ENTITYUNRELATED than a ENTITYOTHER .
Sentence boundary detection in speech is important for enriching speech recognition output, making it easier for humans to read and downstream modules to process.	Sentence boundary detection	speech recognition	usage	{'e1': {'word': 'Sentence boundary detection', 'word_index': [(0, 0)], 'id': 'P05-1056.1'}, 'e2': {'word': 'speech recognition', 'word_index': [(7, 7)], 'id': 'P05-1056.3'}}	ENTITY in ENTITYUNRELATED is important for enriching ENTITYOTHER output , making it easier for humans to read and downstream modules to process .
In previous work, we have developed hidden Markov model (HMM) and maximum entropy (Maxent) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries.	knowledge sources	hidden Markov model (HMM) and maximum entropy (Maxent) classifiers	usage	{'e1': {'word': 'knowledge sources', 'word_index': [(13, 13)], 'id': 'P05-1056.5'}, 'e2': {'word': 'hidden Markov model (HMM) and maximum entropy (Maxent) classifiers', 'word_index': [(7, 7)], 'id': 'P05-1056.4'}}	In previous work , we have developed ENTITYOTHER that integrate textual and prosodic ENTITY for detecting ENTITYUNRELATED .
We evaluate across two corpora (conversational telephone speech and broadcast news speech) on both human transcriptions and speech recognition output.	human transcriptions	speech recognition	compare	{'e1': {'word': 'human transcriptions', 'word_index': [(13, 13)], 'id': 'P05-1056.10'}, 'e2': {'word': 'speech recognition', 'word_index': [(15, 15)], 'id': 'P05-1056.11'}}	We evaluate across two corpora ( conversational ENTITYUNRELATED and ENTITYUNRELATED ) on both ENTITY and ENTITYOTHER output .
In general, our CRF model yields a lower error rate than the HMM and Max-ent models on the NIST sentence boundary detection task in speech, although it is interesting to note that the best results are achieved by three-way voting among the classifiers.	CRF	HMM and Max-ent models	compare	{'e1': {'word': 'CRF', 'word_index': [(4, 4)], 'id': 'P05-1056.12'}, 'e2': {'word': 'HMM and Max-ent models', 'word_index': [(13, 13)], 'id': 'P05-1056.13'}}	In general , our ENTITY model yields a lower error rate than the ENTITYOTHER on the ENTITYUNRELATED in ENTITYUNRELATED , although it is interesting to note that the best results are achieved by ENTITYUNRELATED among the ENTITYUNRELATED .
Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic.	topic	training and test data	model-feature	{'e1': {'word': 'topic', 'word_index': [(33, 33)], 'id': 'P05-2008.6'}, 'e2': {'word': 'training and test data', 'word_index': [(29, 29)], 'id': 'P05-2008.5'}}	Traditional ENTITYUNRELATED have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the ENTITYOTHER with respect to ENTITY .
This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.	emoticons	training data	model-feature	{'e1': {'word': 'emoticons', 'word_index': [(23, 23)], 'id': 'P05-2008.8'}, 'e2': {'word': 'training data', 'word_index': [(20, 20)], 'id': 'P05-2008.7'}}	This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with ENTITYOTHER labeled with ENTITY , which has the potential of being independent of ENTITYUNRELATED , ENTITYUNRELATED and time .
Using natural language processing, we carried out a trend survey on Japanese natural language processing studies that have been done over the last ten years.	natural language processing	Japanese natural language processing studies	usage	{'e1': {'word': 'natural language processing', 'word_index': [(1, 1)], 'id': 'I05-2043.2'}, 'e2': {'word': 'Japanese natural language processing studies', 'word_index': [(10, 10)], 'id': 'I05-2043.3'}}	Using ENTITY , we carried out a trend survey on ENTITYOTHER that have been done over the last ten years .
This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful.	co-occurrence similarities	terms	model-feature	{'e1': {'word': 'co-occurrence similarities', 'word_index': [(8, 8)], 'id': 'E99-1034.1'}, 'e2': {'word': 'terms', 'word_index': [(10, 10)], 'id': 'E99-1034.2'}}	This paper explores the issue of using different ENTITY between ENTITYOTHER for separating ENTITYUNRELATED that are useful for ENTITYUNRELATED from those that are harmful .
This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful.	query terms	retrieval	usage	{'e1': {'word': 'query terms', 'word_index': [(13, 13)], 'id': 'E99-1034.3'}, 'e2': {'word': 'retrieval', 'word_index': [(18, 18)], 'id': 'E99-1034.4'}}	This paper explores the issue of using different ENTITYUNRELATED between ENTITYUNRELATED for separating ENTITY that are useful for ENTITYOTHER from those that are harmful .
The hypothesis under examination is that useful terms tend to be more similar to each other than to other query terms.	useful terms	query terms	compare	{'e1': {'word': 'useful terms', 'word_index': [(6, 6)], 'id': 'E99-1034.5'}, 'e2': {'word': 'query terms', 'word_index': [(18, 18)], 'id': 'E99-1034.6'}}	The hypothesis under examination is that ENTITY tend to be more similar to each other than to other ENTITYOTHER .
Term similarities could then be used for determining which query terms are useful and best reflect the user's information need.	Term similarities	query terms	model-feature	{'e1': {'word': 'Term similarities', 'word_index': [(0, 0)], 'id': 'E99-1034.8'}, 'e2': {'word': 'query terms', 'word_index': [(8, 8)], 'id': 'E99-1034.9'}}	ENTITY could then be used for determining which ENTITYOTHER are useful and best reflect the user 's information need .
A possible application would be to use this source of evidence for tuning the weights of the query terms.	weights	query terms	model-feature	{'e1': {'word': 'weights', 'word_index': [(14, 14)], 'id': 'E99-1034.10'}, 'e2': {'word': 'query terms', 'word_index': [(17, 17)], 'id': 'E99-1034.11'}}	A possible application would be to use this source of evidence for tuning the ENTITY of the ENTITYOTHER .
We propose a draft scheme of the model formalizing the structure of communicative context in dialogue interaction.	structure of communicative context	dialogue interaction	model-feature	{'e1': {'word': 'structure of communicative context', 'word_index': [(10, 10)], 'id': 'E85-1041.2'}, 'e2': {'word': 'dialogue interaction', 'word_index': [(12, 12)], 'id': 'E85-1041.3'}}	We propose a draft scheme of the ENTITYUNRELATED formalizing the ENTITY in ENTITYOTHER .
Memo-functions also facilitate a simple way to construct a very compact representation of the parse forest.	Memo-functions	parse forest	usage	{'e1': {'word': 'Memo-functions', 'word_index': [(0, 0)], 'id': 'E91-1012.8'}, 'e2': {'word': 'parse forest', 'word_index': [(14, 14)], 'id': 'E91-1012.9'}}	ENTITY also facilitate a simple way to construct a very compact representation of the ENTITYOTHER .
Extended CF grammars (grammars with regular expressions at the right hand side) can be parsed with a simple modification of the LR-parser for normal CF grammars.	regular expressions	grammars	part_whole	{'e1': {'word': 'regular expressions', 'word_index': [(4, 4)], 'id': 'E91-1012.14'}, 'e2': {'word': 'grammars', 'word_index': [(2, 2)], 'id': 'E91-1012.13'}}	ENTITYUNRELATED ( ENTITYOTHER with ENTITY at the right hand side ) can be parsed with a simple modification of the ENTITYUNRELATED for normal ENTITYUNRELATED .
Extended CF grammars (grammars with regular expressions at the right hand side) can be parsed with a simple modification of the LR-parser for normal CF grammars.	LR-parser	CF grammars	usage	{'e1': {'word': 'LR-parser', 'word_index': [(20, 20)], 'id': 'E91-1012.15'}, 'e2': {'word': 'CF grammars', 'word_index': [(23, 23)], 'id': 'E91-1012.16'}}	ENTITYUNRELATED ( ENTITYUNRELATED with ENTITYUNRELATED at the right hand side ) can be parsed with a simple modification of the ENTITY for normal ENTITYOTHER .
This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA.	generative probabilistic model	parse trees	model-feature	{'e1': {'word': 'generative probabilistic model', 'word_index': [(4, 4)], 'id': 'P05-1010.1'}, 'e2': {'word': 'parse trees', 'word_index': [(6, 6)], 'id': 'P05-1010.2'}}	This paper defines a ENTITY of ENTITYOTHER , which we call ENTITYUNRELATED .
Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.	parsed corpus	CFG rules	usage	{'e1': {'word': 'parsed corpus', 'word_index': [(7, 7)], 'id': 'P05-1010.9'}, 'e2': {'word': 'CFG rules', 'word_index': [(1, 1)], 'id': 'P05-1010.8'}}	Finegrained ENTITYOTHER are automatically induced from a ENTITY by ENTITYUNRELATED a ENTITYUNRELATED using an ENTITYUNRELATED .
Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.	EM-algorithm	training	usage	{'e1': {'word': 'EM-algorithm', 'word_index': [(14, 14)], 'id': 'P05-1010.12'}, 'e2': {'word': 'training', 'word_index': [(9, 9)], 'id': 'P05-1010.10'}}	Finegrained ENTITYUNRELATED are automatically induced from a ENTITYUNRELATED by ENTITYOTHER a ENTITYUNRELATED using an ENTITY .
Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.	PCFG-LA	parsing	usage	{'e1': {'word': 'PCFG-LA', 'word_index': [(5, 5)], 'id': 'P05-1010.14'}, 'e2': {'word': 'parsing', 'word_index': [(2, 2)], 'id': 'P05-1010.13'}}	Because exact ENTITYOTHER with a ENTITY is ENTITYUNRELATED , several ENTITYUNRELATED are described and empirically compared .
In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.	model	performance	result	{'e1': {'word': 'model', 'word_index': [(9, 9)], 'id': 'P05-1010.18'}, 'e2': {'word': 'performance', 'word_index': [(12, 12)], 'id': 'P05-1010.19'}}	In experiments using the ENTITYUNRELATED , our automatically trained ENTITY gave a ENTITYOTHER of 86.6 % ( F1 , ENTITYUNRELATED &lt ; 40 ENTITYUNRELATED ) , which is comparable to that of an ENTITYUNRELATED created using extensive ENTITYUNRELATED .
In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.	words	sentences	part_whole	{'e1': {'word': 'words', 'word_index': [(23, 23)], 'id': 'P05-1010.21'}, 'e2': {'word': 'sentences', 'word_index': [(19, 19)], 'id': 'P05-1010.20'}}	In experiments using the ENTITYUNRELATED , our automatically trained ENTITYUNRELATED gave a ENTITYUNRELATED of 86.6 % ( F1 , ENTITYOTHER &lt ; 40 ENTITY ) , which is comparable to that of an ENTITYUNRELATED created using extensive ENTITYUNRELATED .
In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.	manual feature selection	unlexicalized PCFG parser	usage	{'e1': {'word': 'manual feature selection', 'word_index': [(37, 37)], 'id': 'P05-1010.23'}, 'e2': {'word': 'unlexicalized PCFG parser', 'word_index': [(33, 33)], 'id': 'P05-1010.22'}}	In experiments using the ENTITYUNRELATED , our automatically trained ENTITYUNRELATED gave a ENTITYUNRELATED of 86.6 % ( F1 , ENTITYUNRELATED &lt ; 40 ENTITYUNRELATED ) , which is comparable to that of an ENTITYOTHER created using extensive ENTITY .
This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.	SVM	feature-based relation extraction	usage	{'e1': {'word': 'SVM', 'word_index': [(11, 11)], 'id': 'P05-1053.4'}, 'e2': {'word': 'feature-based relation extraction', 'word_index': [(9, 9)], 'id': 'P05-1053.3'}}	This paper investigates the incorporation of diverse ENTITYUNRELATED in ENTITYOTHER using ENTITY .
Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.	phrase chunking	performance improvement	result	{'e1': {'word': 'phrase chunking', 'word_index': [(6, 6)], 'id': 'P05-1053.5'}, 'e2': {'word': 'performance improvement', 'word_index': [(19, 19)], 'id': 'P05-1053.7'}}	Our study illustrates that the base ENTITY information is very effective for ENTITYUNRELATED and contributes to most of the ENTITYOTHER from ENTITYUNRELATED aspect while additional information from ENTITYUNRELATED gives limited further enhancement .
We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.	semantic information	performance	result	{'e1': {'word': 'semantic information', 'word_index': [(4, 4)], 'id': 'P05-1053.13'}, 'e2': {'word': 'performance', 'word_index': [(20, 20)], 'id': 'P05-1053.17'}}	We also demonstrate how ENTITY such as ENTITYUNRELATED and ENTITYUNRELATED , can be used in ENTITYUNRELATED to further improve the ENTITYOTHER .
Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.	system	systems	compare	{'e1': {'word': 'system', 'word_index': [(13, 13)], 'id': 'P05-1053.21'}, 'e2': {'word': 'systems', 'word_index': [(17, 17)], 'id': 'P05-1053.22'}}	ENTITYUNRELATED on the ENTITYUNRELATED shows that effective incorporation of diverse ENTITYUNRELATED enables our ENTITY outperform previously best-reported ENTITYOTHER on the ENTITYUNRELATED and significantly outperforms ENTITYUNRELATED by over 20 in ENTITYUNRELATED on the ENTITYUNRELATED .
This paper describes a novel system for acquiring adjectival subcategorization frames (scfs) and associated frequency information from English corpus data.	system	acquiring adjectival subcategorization frames	usage	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'P05-1076.1'}, 'e2': {'word': 'acquiring adjectival subcategorization frames', 'word_index': [(7, 7)], 'id': 'P05-1076.2'}}	This paper describes a novel ENTITY for ENTITYOTHER ( ENTITYUNRELATED ) and associated frequency information from ENTITYUNRELATED ENTITYUNRELATED .
This paper describes a novel system for acquiring adjectival subcategorization frames (scfs) and associated frequency information from English corpus data.	English	corpus data	model-feature	{'e1': {'word': 'English', 'word_index': [(16, 16)], 'id': 'P05-1076.4'}, 'e2': {'word': 'corpus data', 'word_index': [(17, 17)], 'id': 'P05-1076.5'}}	This paper describes a novel ENTITYUNRELATED for ENTITYUNRELATED ( ENTITYUNRELATED ) and associated frequency information from ENTITY ENTITYOTHER .
The system incorporates a decision-tree classifier for 30 scf types which tests for the presence of grammatical relations (grs) in the output of a robust statistical parser.	decision-tree classifier	system	part_whole	{'e1': {'word': 'decision-tree classifier', 'word_index': [(4, 4)], 'id': 'P05-1076.7'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'P05-1076.6'}}	The ENTITYOTHER incorporates a ENTITY for 30 ENTITYUNRELATED which tests for the presence of ENTITYUNRELATED ( ENTITYUNRELATED ) in the ENTITYUNRELATED of a robust ENTITYUNRELATED .
The system incorporates a decision-tree classifier for 30 scf types which tests for the presence of grammatical relations (grs) in the output of a robust statistical parser.	grammatical relations	output	part_whole	{'e1': {'word': 'grammatical relations', 'word_index': [(14, 14)], 'id': 'P05-1076.9'}, 'e2': {'word': 'output', 'word_index': [(20, 20)], 'id': 'P05-1076.11'}}	The ENTITYUNRELATED incorporates a ENTITYUNRELATED for 30 ENTITYUNRELATED which tests for the presence of ENTITY ( ENTITYUNRELATED ) in the ENTITYOTHER of a robust ENTITYUNRELATED .
It uses a powerful pattern-matching language to classify grs into frames hierarchically in a way that mirrors inheritance-based lexica.	frames	grs	model-feature	{'e1': {'word': 'frames', 'word_index': [(9, 9)], 'id': 'P05-1076.15'}, 'e2': {'word': 'grs', 'word_index': [(7, 7)], 'id': 'P05-1076.14'}}	It uses a powerful ENTITYUNRELATED to classify ENTITYOTHER into ENTITY hierarchically in a way that mirrors ENTITYUNRELATED .
The experiments show that the system is able to detect scf types with 70% precision and 66% recall rate.	system	70% precision	result	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'P05-1076.18'}, 'e2': {'word': '70% precision', 'word_index': [(12, 12)], 'id': 'P05-1076.20'}}	The ENTITYUNRELATED show that the ENTITY is able to detect ENTITYUNRELATED with ENTITYOTHER and ENTITYUNRELATED .
A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.	tool	linguistic annotation	usage	{'e1': {'word': 'tool', 'word_index': [(2, 2)], 'id': 'P05-1076.22'}, 'e2': {'word': 'linguistic annotation', 'word_index': [(4, 4)], 'id': 'P05-1076.23'}}	A new ENTITY for ENTITYOTHER of ENTITYUNRELATED in ENTITYUNRELATED is also introduced which can considerably alleviate the process of obtaining ENTITYUNRELATED for ENTITYUNRELATED .
A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.	scfs	corpus data	part_whole	{'e1': {'word': 'scfs', 'word_index': [(6, 6)], 'id': 'P05-1076.24'}, 'e2': {'word': 'corpus data', 'word_index': [(8, 8)], 'id': 'P05-1076.25'}}	A new ENTITYUNRELATED for ENTITYUNRELATED of ENTITY in ENTITYOTHER is also introduced which can considerably alleviate the process of obtaining ENTITYUNRELATED for ENTITYUNRELATED .
A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.	training and test data	subcategorization acquisition	usage	{'e1': {'word': 'training and test data', 'word_index': [(20, 20)], 'id': 'P05-1076.26'}, 'e2': {'word': 'subcategorization acquisition', 'word_index': [(22, 22)], 'id': 'P05-1076.27'}}	A new ENTITYUNRELATED for ENTITYUNRELATED of ENTITYUNRELATED in ENTITYUNRELATED is also introduced which can considerably alleviate the process of obtaining ENTITY for ENTITYOTHER .
We present a tool, called ILIMP, which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ANA] for anaphoric or [IMP] for impersonal or expletive.	French	raw text	model-feature	{'e1': {'word': 'French', 'word_index': [(15, 15)], 'id': 'I05-2013.4'}, 'e2': {'word': 'raw text', 'word_index': [(13, 13)], 'id': 'I05-2013.3'}}	We present a ENTITYUNRELATED , called ENTITYUNRELATED , which takes as input a ENTITYOTHER in ENTITY and produces as output the same ENTITYUNRELATED in which every occurrence of the ENTITYUNRELATED is tagged either with tag ENTITYUNRELATED for ENTITYUNRELATED or ENTITYUNRELATED for ENTITYUNRELATED or ENTITYUNRELATED .
We present a tool, called ILIMP, which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ANA] for anaphoric or [IMP] for impersonal or expletive.	[ANA]	pronoun il	model-feature	{'e1': {'word': '[ANA]', 'word_index': [(35, 35)], 'id': 'I05-2013.7'}, 'e2': {'word': 'pronoun il', 'word_index': [(29, 29)], 'id': 'I05-2013.6'}}	We present a ENTITYUNRELATED , called ENTITYUNRELATED , which takes as input a ENTITYUNRELATED in ENTITYUNRELATED and produces as output the same ENTITYUNRELATED in which every occurrence of the ENTITYOTHER is tagged either with tag ENTITY for ENTITYUNRELATED or ENTITYUNRELATED for ENTITYUNRELATED or ENTITYUNRELATED .
This tool is therefore designed to distinguish between the anaphoric occurrences of il, for which an anaphora resolution system has to look for an antecedent, and the expletive occurrences of this pronoun, for which it does not make sense to look for an antecedent.	expletive occurrences	pronoun	model-feature	{'e1': {'word': 'expletive occurrences', 'word_index': [(24, 24)], 'id': 'I05-2013.15'}, 'e2': {'word': 'pronoun', 'word_index': [(27, 27)], 'id': 'I05-2013.16'}}	This ENTITYUNRELATED is therefore designed to distinguish between the ENTITYUNRELATED , for which an ENTITYUNRELATED has to look for an antecedent , and the ENTITY of this ENTITYOTHER , for which it does not make sense to look for an antecedent .
The precision rate for ILIMP is 97,5%.	ILIMP	precision rate	result	{'e1': {'word': 'ILIMP', 'word_index': [(3, 3)], 'id': 'I05-2013.18'}, 'e2': {'word': 'precision rate', 'word_index': [(1, 1)], 'id': 'I05-2013.17'}}	The ENTITYOTHER for ENTITY is 97,5 %.
Other tasks using the method developed for ILIMP are described briefly, as well as the use of ILIMP in a modular syntactic analysis system.	method	tasks	usage	{'e1': {'word': 'method', 'word_index': [(4, 4)], 'id': 'I05-2013.21'}, 'e2': {'word': 'tasks', 'word_index': [(1, 1)], 'id': 'I05-2013.20'}}	Other ENTITYOTHER using the ENTITY developed for ENTITYUNRELATED are described briefly , as well as the use of ENTITYUNRELATED in a modular ENTITYUNRELATED .
Other tasks using the method developed for ILIMP are described briefly, as well as the use of ILIMP in a modular syntactic analysis system.	ILIMP	syntactic analysis system	usage	{'e1': {'word': 'ILIMP', 'word_index': [(18, 18)], 'id': 'I05-2013.23'}, 'e2': {'word': 'syntactic analysis system', 'word_index': [(22, 22)], 'id': 'I05-2013.24'}}	Other ENTITYUNRELATED using the ENTITYUNRELATED developed for ENTITYUNRELATED are described briefly , as well as the use of ENTITY in a modular ENTITYOTHER .
Systemic grammar has been used for AI text generation work in the past, but the implementations have tended be ad hoc or inefficient.	Systemic grammar	AI text generation	usage	{'e1': {'word': 'Systemic grammar', 'word_index': [(0, 0)], 'id': 'E85-1037.1'}, 'e2': {'word': 'AI text generation', 'word_index': [(5, 5)], 'id': 'E85-1037.2'}}	ENTITY has been used for ENTITYOTHER work in the past , but the ENTITYUNRELATED have tended be ad hoc or inefficient .
This paper presents an approach to systemic text generation where AI problem solving techniques are applied directly to an unadulterated systemic grammar.	AI problem solving techniques	systemic grammar	usage	{'e1': {'word': 'AI problem solving techniques', 'word_index': [(9, 9)], 'id': 'E85-1037.5'}, 'e2': {'word': 'systemic grammar', 'word_index': [(16, 16)], 'id': 'E85-1037.6'}}	This paper presents an approach to systemic ENTITYUNRELATED where ENTITY are applied directly to an unadulterated ENTITYOTHER .
The result is simple, efficient text generation firmly based in a linguistic theory.	linguistic theory	text generation	usage	{'e1': {'word': 'linguistic theory', 'word_index': [(11, 11)], 'id': 'E85-1037.11'}, 'e2': {'word': 'text generation', 'word_index': [(6, 6)], 'id': 'E85-1037.10'}}	The result is simple , efficient ENTITYOTHER firmly based in a ENTITY .
This paper presents a critical discussion of the various approaches that have been used in the evaluation of Natural Language systems.	critical discussion	evaluation of Natural Language systems	topic	{'e1': {'word': 'critical discussion', 'word_index': [(4, 4)], 'id': 'E89-1016.1'}, 'e2': {'word': 'evaluation of Natural Language systems', 'word_index': [(15, 15)], 'id': 'E89-1016.3'}}	This paper presents a ENTITY of the various ENTITYUNRELATED that have been used in the ENTITYOTHER .
We conclude that previous approaches have neglected to evaluate systems in the context of their use, e.g. solving a task requiring data retrieval.	data retrieval	task	usage	{'e1': {'word': 'data retrieval', 'word_index': [(22, 22)], 'id': 'E89-1016.7'}, 'e2': {'word': 'task', 'word_index': [(20, 20)], 'id': 'E89-1016.6'}}	We conclude that previous ENTITYUNRELATED have neglected to evaluate ENTITYUNRELATED in the context of their use , e.g. solving a ENTITYOTHER requiring ENTITY .
In the second half of the paper, we report a laboratory study using the Wizard of Oz technique to identify NL requirements for carrying out this task.	Wizard of Oz technique	laboratory study	usage	{'e1': {'word': 'Wizard of Oz technique', 'word_index': [(14, 14)], 'id': 'E89-1016.10'}, 'e2': {'word': 'laboratory study', 'word_index': [(11, 11)], 'id': 'E89-1016.9'}}	In the second half of the paper , we report a ENTITYOTHER using the ENTITY to identify ENTITYUNRELATED for carrying out this ENTITYUNRELATED .
We identify three important requirements which arose from the task that we gave our subjects: operators specific to the task of database access, complex contextual reference and reference to the structure of the information source.	structure	information source	model-feature	{'e1': {'word': 'structure', 'word_index': [(30, 30)], 'id': 'E89-1016.19'}, 'e2': {'word': 'information source', 'word_index': [(33, 33)], 'id': 'E89-1016.20'}}	We identify three important requirements which arose from the ENTITYUNRELATED that we gave our subjects : operators specific to the task of ENTITYUNRELATED , complex ENTITYUNRELATED and reference to the ENTITY of the ENTITYOTHER .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts.	Semantic theories	natural language	topic	{'e1': {'word': 'Semantic theories', 'word_index': [(0, 0)], 'id': 'E93-1013.1'}, 'e2': {'word': 'natural language', 'word_index': [(2, 2)], 'id': 'E93-1013.2'}}	ENTITY of ENTITYOTHER associate ENTITYUNRELATED with ENTITYUNRELATED by providing ENTITYUNRELATED for ENTITYUNRELATED and ENTITYUNRELATED for determining the ENTITYUNRELATED of larger ENTITYUNRELATED given the ENTITYUNRELATED of their parts .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts.	meanings	utterances	model-feature	{'e1': {'word': 'meanings', 'word_index': [(4, 4)], 'id': 'E93-1013.3'}, 'e2': {'word': 'utterances', 'word_index': [(6, 6)], 'id': 'E93-1013.4'}}	ENTITYUNRELATED of ENTITYUNRELATED associate ENTITY with ENTITYOTHER by providing ENTITYUNRELATED for ENTITYUNRELATED and ENTITYUNRELATED for determining the ENTITYUNRELATED of larger ENTITYUNRELATED given the ENTITYUNRELATED of their parts .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts.	meanings	lexical items	model-feature	{'e1': {'word': 'meanings', 'word_index': [(9, 9)], 'id': 'E93-1013.5'}, 'e2': {'word': 'lexical items', 'word_index': [(11, 11)], 'id': 'E93-1013.6'}}	ENTITYUNRELATED of ENTITYUNRELATED associate ENTITYUNRELATED with ENTITYUNRELATED by providing ENTITY for ENTITYOTHER and ENTITYUNRELATED for determining the ENTITYUNRELATED of larger ENTITYUNRELATED given the ENTITYUNRELATED of their parts .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts.	meaning	units	model-feature	{'e1': {'word': 'meaning', 'word_index': [(17, 17)], 'id': 'E93-1013.8'}, 'e2': {'word': 'units', 'word_index': [(20, 20)], 'id': 'E93-1013.9'}}	ENTITYUNRELATED of ENTITYUNRELATED associate ENTITYUNRELATED with ENTITYUNRELATED by providing ENTITYUNRELATED for ENTITYUNRELATED and ENTITYUNRELATED for determining the ENTITY of larger ENTITYOTHER given the ENTITYUNRELATED of their parts .
Traditionally, meanings are combined via function composition, which works well when constituent structure trees are used to guide semantic composition.	constituent structure trees	semantic composition	usage	{'e1': {'word': 'constituent structure trees', 'word_index': [(12, 12)], 'id': 'E93-1013.13'}, 'e2': {'word': 'semantic composition', 'word_index': [(17, 17)], 'id': 'E93-1013.14'}}	Traditionally , ENTITYUNRELATED are combined via ENTITYUNRELATED , which works well when ENTITY are used to guide ENTITYOTHER .
More recently, the functional structure of LFG has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format.	functional structure	LFG	part_whole	{'e1': {'word': 'functional structure', 'word_index': [(4, 4)], 'id': 'E93-1013.15'}, 'e2': {'word': 'LFG', 'word_index': [(6, 6)], 'id': 'E93-1013.16'}}	More recently , the ENTITY of ENTITYOTHER has been used to provide the ENTITYUNRELATED necessary for constraining ENTITYUNRELATED of ENTITYUNRELATED in a ENTITYUNRELATED .
More recently, the functional structure of LFG has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format.	syntactic information	derivations	usage	{'e1': {'word': 'syntactic information', 'word_index': [(13, 13)], 'id': 'E93-1013.17'}, 'e2': {'word': 'derivations', 'word_index': [(17, 17)], 'id': 'E93-1013.18'}}	More recently , the ENTITYUNRELATED of ENTITYUNRELATED has been used to provide the ENTITY necessary for constraining ENTITYOTHER of ENTITYUNRELATED in a ENTITYUNRELATED .
In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the functional structure.	compositional approaches	deductive approach	compare	{'e1': {'word': 'compositional approaches', 'word_index': [(3, 3)], 'id': 'E93-1013.24'}, 'e2': {'word': 'deductive approach', 'word_index': [(8, 8)], 'id': 'E93-1013.25'}}	In contrast to ENTITY , we present a ENTITYOTHER to assembling ENTITYUNRELATED , based on ENTITYUNRELATED , which meshes well with the unordered nature of ENTITYUNRELATED in the ENTITYUNRELATED .
In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the functional structure.	information	functional structure	part_whole	{'e1': {'word': 'information', 'word_index': [(25, 25)], 'id': 'E93-1013.28'}, 'e2': {'word': 'functional structure', 'word_index': [(28, 28)], 'id': 'E93-1013.29'}}	In contrast to ENTITYUNRELATED , we present a ENTITYUNRELATED to assembling ENTITYUNRELATED , based on ENTITYUNRELATED , which meshes well with the unordered nature of ENTITY in the ENTITYOTHER .
Our use of linear logic as a 'glue' for assembling meanings also allows for a coherent treatment of modification as well as of the LFG requirements of completeness and coherence.	completeness	LFG	model-feature	{'e1': {'word': 'completeness', 'word_index': [(28, 28)], 'id': 'E93-1013.34'}, 'e2': {'word': 'LFG', 'word_index': [(25, 25)], 'id': 'E93-1013.33'}}	Our use of ENTITYUNRELATED as a ' glue ' for assembling ENTITYUNRELATED also allows for a coherent treatment of ENTITYUNRELATED as well as of the ENTITYOTHER requirements of ENTITY and ENTITYUNRELATED .
This paper presents an analysis of temporal anaphora in sentences which contain quantification over events, within the framework of Discourse Representation Theory.	quantification over events	sentences	part_whole	{'e1': {'word': 'quantification over events', 'word_index': [(11, 11)], 'id': 'E95-1036.3'}, 'e2': {'word': 'sentences', 'word_index': [(8, 8)], 'id': 'E95-1036.2'}}	This paper presents an analysis of ENTITYUNRELATED in ENTITYOTHER which contain ENTITY , within the framework of ENTITYUNRELATED .
The analysis in (Partee, 1984) of quantified sentences, introduced by a temporal connective, gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after.	temporal connective	quantified sentences	part_whole	{'e1': {'word': 'temporal connective', 'word_index': [(14, 14)], 'id': 'E95-1036.6'}, 'e2': {'word': 'quantified sentences', 'word_index': [(9, 9)], 'id': 'E95-1036.5'}}	The analysis in ( Partee , 1984 ) of ENTITYOTHER , introduced by a ENTITY , gives the wrong ENTITYUNRELATED when the ENTITYUNRELATED in the ENTITYUNRELATED is before or after .
The analysis in (Partee, 1984) of quantified sentences, introduced by a temporal connective, gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after.	temporal connective	subordinate clause	part_whole	{'e1': {'word': 'temporal connective', 'word_index': [(22, 22)], 'id': 'E95-1036.8'}, 'e2': {'word': 'subordinate clause', 'word_index': [(25, 25)], 'id': 'E95-1036.9'}}	The analysis in ( Partee , 1984 ) of ENTITYUNRELATED , introduced by a ENTITYUNRELATED , gives the wrong ENTITYUNRELATED when the ENTITY in the ENTITYOTHER is before or after .
This problem has been previously analyzed in (de Swart, 1991) as an instance of the proportion problem and given a solution from a Generalized Quantifier approach.	Generalized Quantifier approach	proportion problem	topic	{'e1': {'word': 'Generalized Quantifier approach', 'word_index': [(25, 25)], 'id': 'E95-1036.12'}, 'e2': {'word': 'proportion problem', 'word_index': [(18, 18)], 'id': 'E95-1036.11'}}	This ENTITYUNRELATED has been previously analyzed in ( de Swart , 1991 ) as an instance of the ENTITYOTHER and given a solution from a ENTITY .
By using a careful distinction between the different notions of reference time based on (Kamp and Reyle, 1993), we propose a solution to this problem, within the framework of DRT.	DRT	problem	usage	{'e1': {'word': 'DRT', 'word_index': [(33, 33)], 'id': 'E95-1036.15'}, 'e2': {'word': 'problem', 'word_index': [(27, 27)], 'id': 'E95-1036.14'}}	By using a careful distinction between the different notions of ENTITYUNRELATED based on ( Kamp and Reyle , 1993 ) , we propose a solution to this ENTITYOTHER , within the framework of ENTITY .
We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences.	solution	temporal anaphora phenomena	usage	{'e1': {'word': 'solution', 'word_index': [(6, 6)], 'id': 'E95-1036.16'}, 'e2': {'word': 'temporal anaphora phenomena', 'word_index': [(9, 9)], 'id': 'E95-1036.17'}}	We show some applications of this ENTITY to additional ENTITYOTHER in ENTITYUNRELATED .
"This paper proposes an automatic, essentially domain-independent means of evaluating Spoken Language Systems (SLS) which combines software we have developed for that purpose (the ""Comparator"") and a set of specifications for answer expressions (the ""Common Answer Specification"", or CAS)."	software	domain-independent means of evaluating Spoken Language Systems (SLS)	usage	{'e1': {'word': 'software', 'word_index': [(10, 10)], 'id': 'H89-2019.2'}, 'e2': {'word': 'domain-independent means of evaluating Spoken Language Systems (SLS)', 'word_index': [(7, 7)], 'id': 'H89-2019.1'}}	"This paper proposes an automatic , essentially ENTITYOTHER which combines ENTITY we have developed for that purpose ( the "" ENTITYUNRELATED "" ) and a set of ENTITYUNRELATED for ENTITYUNRELATED ( the "" ENTITYUNRELATED "" , or ENTITYUNRELATED ) ."
"This paper proposes an automatic, essentially domain-independent means of evaluating Spoken Language Systems (SLS) which combines software we have developed for that purpose (the ""Comparator"") and a set of specifications for answer expressions (the ""Common Answer Specification"", or CAS)."	specifications	answer expressions	model-feature	{'e1': {'word': 'specifications', 'word_index': [(27, 27)], 'id': 'H89-2019.4'}, 'e2': {'word': 'answer expressions', 'word_index': [(29, 29)], 'id': 'H89-2019.5'}}	"This paper proposes an automatic , essentially ENTITYUNRELATED which combines ENTITYUNRELATED we have developed for that purpose ( the "" ENTITYUNRELATED "" ) and a set of ENTITY for ENTITYOTHER ( the "" ENTITYUNRELATED "" , or ENTITYUNRELATED ) ."
The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the procedures used by the Comparator.	syntax	answer expressions	model-feature	{'e1': {'word': 'syntax', 'word_index': [(4, 4)], 'id': 'H89-2019.12'}, 'e2': {'word': 'answer expressions', 'word_index': [(6, 6)], 'id': 'H89-2019.13'}}	The ENTITYUNRELATED determines the ENTITY of ENTITYOTHER , the minimal ENTITYUNRELATED that must be included in them , the ENTITYUNRELATED to be included in and excluded from ENTITYUNRELATED , and the ENTITYUNRELATED used by the ENTITYUNRELATED .
The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the procedures used by the Comparator.	data	test corpora	part_whole	{'e1': {'word': 'data', 'word_index': [(19, 19)], 'id': 'H89-2019.15'}, 'e2': {'word': 'test corpora', 'word_index': [(27, 27)], 'id': 'H89-2019.16'}}	The ENTITYUNRELATED determines the ENTITYUNRELATED of ENTITYUNRELATED , the minimal ENTITYUNRELATED that must be included in them , the ENTITY to be included in and excluded from ENTITYOTHER , and the ENTITYUNRELATED used by the ENTITYUNRELATED .
The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the procedures used by the Comparator.	procedures	Comparator	usage	{'e1': {'word': 'procedures', 'word_index': [(31, 31)], 'id': 'H89-2019.17'}, 'e2': {'word': 'Comparator', 'word_index': [(35, 35)], 'id': 'H89-2019.18'}}	The ENTITYUNRELATED determines the ENTITYUNRELATED of ENTITYUNRELATED , the minimal ENTITYUNRELATED that must be included in them , the ENTITYUNRELATED to be included in and excluded from ENTITYUNRELATED , and the ENTITY used by the ENTITYOTHER .
Though some details of the CAS are particular to individual domains, the Comparator software is domain-independent, as is the CAS approach.	domains	CAS	model-feature	{'e1': {'word': 'domains', 'word_index': [(10, 10)], 'id': 'H89-2019.20'}, 'e2': {'word': 'CAS', 'word_index': [(5, 5)], 'id': 'H89-2019.19'}}	Though some details of the ENTITYOTHER are particular to individual ENTITY , the ENTITYUNRELATED is ENTITYUNRELATED , as is the ENTITYUNRELATED .
Though some details of the CAS are particular to individual domains, the Comparator software is domain-independent, as is the CAS approach.	domain-independent	Comparator software	model-feature	{'e1': {'word': 'domain-independent', 'word_index': [(15, 15)], 'id': 'H89-2019.22'}, 'e2': {'word': 'Comparator software', 'word_index': [(13, 13)], 'id': 'H89-2019.21'}}	Though some details of the ENTITYUNRELATED are particular to individual ENTITYUNRELATED , the ENTITYOTHER is ENTITY , as is the ENTITYUNRELATED .
Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications.	Xerox PARC	speech and text image processing	topic	{'e1': {'word': 'Xerox PARC', 'word_index': [(8, 8)], 'id': 'H93-1076.4'}, 'e2': {'word': 'speech and text image processing', 'word_index': [(5, 5)], 'id': 'H93-1076.3'}}	Two themes have evolved in ENTITYOTHER work at ENTITY that expand and redefine the role of ENTITYUNRELATED in ENTITYUNRELATED .
Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications.	recognition technology	document-oriented applications	part_whole	{'e1': {'word': 'recognition technology', 'word_index': [(16, 16)], 'id': 'H93-1076.5'}, 'e2': {'word': 'document-oriented applications', 'word_index': [(18, 18)], 'id': 'H93-1076.6'}}	Two themes have evolved in ENTITYUNRELATED work at ENTITYUNRELATED that expand and redefine the role of ENTITY in ENTITYOTHER .
One is the development of systems that provide functionality similar to that of text processors but operate directly on audio and scanned image data.	text processors	audio and scanned image data	compare	{'e1': {'word': 'text processors', 'word_index': [(13, 13)], 'id': 'H93-1076.7'}, 'e2': {'word': 'audio and scanned image data', 'word_index': [(18, 18)], 'id': 'H93-1076.8'}}	One is the development of systems that provide functionality similar to that of ENTITY but operate directly on ENTITYOTHER .
A second, related theme is the use of speech and text-image recognition to retrieve arbitrary, user-specified information from documents with signal content.	speech and text-image recognition	documents with signal content	usage	{'e1': {'word': 'speech and text-image recognition', 'word_index': [(9, 9)], 'id': 'H93-1076.9'}, 'e2': {'word': 'documents with signal content', 'word_index': [(17, 17)], 'id': 'H93-1076.10'}}	A second , related theme is the use of ENTITY to retrieve arbitrary , user-specified information from ENTITYOTHER .
In this paper we present a statistical profile of the Named Entity task, a specific information extraction task for which corpora in several languages are available.	statistical profile	Named Entity task	model-feature	{'e1': {'word': 'statistical profile', 'word_index': [(6, 6)], 'id': 'A97-1028.1'}, 'e2': {'word': 'Named Entity task', 'word_index': [(9, 9)], 'id': 'A97-1028.2'}}	In this paper we present a ENTITY of the ENTITYOTHER , a specific ENTITYUNRELATED for which ENTITYUNRELATED in several ENTITYUNRELATED are available .
In this paper we present a statistical profile of the Named Entity task, a specific information extraction task for which corpora in several languages are available.	languages	corpora	model-feature	{'e1': {'word': 'languages', 'word_index': [(19, 19)], 'id': 'A97-1028.5'}, 'e2': {'word': 'corpora', 'word_index': [(16, 16)], 'id': 'A97-1028.4'}}	In this paper we present a ENTITYUNRELATED of the ENTITYUNRELATED , a specific ENTITYUNRELATED for which ENTITYOTHER in several ENTITY are available .
Using the results of the statistical analysis, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.	statistical analysis	results	result	{'e1': {'word': 'statistical analysis', 'word_index': [(5, 5)], 'id': 'A97-1028.7'}, 'e2': {'word': 'results', 'word_index': [(2, 2)], 'id': 'A97-1028.6'}}	Using the ENTITYOTHER of the ENTITY , we propose an ENTITYUNRELATED for ENTITYUNRELATED for ENTITYUNRELATED and discuss the significance of the ENTITYUNRELATED provided by the ENTITYUNRELATED .
Using the results of the statistical analysis, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.	algorithm	lower bound estimation	usage	{'e1': {'word': 'algorithm', 'word_index': [(10, 10)], 'id': 'A97-1028.8'}, 'e2': {'word': 'lower bound estimation', 'word_index': [(12, 12)], 'id': 'A97-1028.9'}}	Using the ENTITYUNRELATED of the ENTITYUNRELATED , we propose an ENTITY for ENTITYOTHER for ENTITYUNRELATED and discuss the significance of the ENTITYUNRELATED provided by the ENTITYUNRELATED .
Using the results of the statistical analysis, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.	analysis	cross-lingual comparisons	topic	{'e1': {'word': 'analysis', 'word_index': [(25, 25)], 'id': 'A97-1028.12'}, 'e2': {'word': 'cross-lingual comparisons', 'word_index': [(21, 21)], 'id': 'A97-1028.11'}}	Using the ENTITYUNRELATED of the ENTITYUNRELATED , we propose an ENTITYUNRELATED for ENTITYUNRELATED for ENTITYUNRELATED and discuss the significance of the ENTITYOTHER provided by the ENTITY .
We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time.	question-focused sentence retrieval	news articles	usage	{'e1': {'word': 'question-focused sentence retrieval', 'word_index': [(5, 5)], 'id': 'H05-1115.1'}, 'e2': {'word': 'news articles', 'word_index': [(8, 8)], 'id': 'H05-1115.2'}}	We consider the problem of ENTITY from complex ENTITYOTHER describing ENTITYUNRELATED .
Annotators generated a list of questions central to understanding each story in our corpus.	story	corpus	part_whole	{'e1': {'word': 'story', 'word_index': [(10, 10)], 'id': 'H05-1115.6'}, 'e2': {'word': 'corpus', 'word_index': [(13, 13)], 'id': 'H05-1115.7'}}	ENTITYUNRELATED generated a list of ENTITYUNRELATED central to understanding each ENTITY in our ENTITYOTHER .
Judges found sentences providing an answer to each question.	answer	sentences	part_whole	{'e1': {'word': 'answer', 'word_index': [(5, 5)], 'id': 'H05-1115.12'}, 'e2': {'word': 'sentences', 'word_index': [(2, 2)], 'id': 'H05-1115.11'}}	ENTITYUNRELATED found ENTITYOTHER providing an ENTITY to each ENTITYUNRELATED .
To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization.	stochastic, graph-based method	sentence retrieval problem	usage	{'e1': {'word': 'stochastic, graph-based method', 'word_index': [(8, 8)], 'id': 'H05-1115.15'}, 'e2': {'word': 'sentence retrieval problem', 'word_index': [(3, 3)], 'id': 'H05-1115.14'}}	To address the ENTITYOTHER , we apply a ENTITY for comparing the relative importance of the ENTITYUNRELATED , which was previously used successfully for ENTITYUNRELATED .
Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap.	method	baseline	compare	{'e1': {'word': 'method', 'word_index': [(9, 9)], 'id': 'H05-1115.18'}, 'e2': {'word': 'baseline', 'word_index': [(18, 18)], 'id': 'H05-1115.19'}}	Currently , we present a topic-sensitive version of our ENTITY and hypothesize that it can outperform a competitive ENTITYOTHER , which compares the ENTITYUNRELATED of each ENTITYUNRELATED to the input ENTITYUNRELATED via ENTITYUNRELATED .
Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap.	similarity	sentence	model-feature	{'e1': {'word': 'similarity', 'word_index': [(23, 23)], 'id': 'H05-1115.20'}, 'e2': {'word': 'sentence', 'word_index': [(26, 26)], 'id': 'H05-1115.21'}}	Currently , we present a topic-sensitive version of our ENTITYUNRELATED and hypothesize that it can outperform a competitive ENTITYUNRELATED , which compares the ENTITY of each ENTITYOTHER to the input ENTITYUNRELATED via ENTITYUNRELATED .
In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline.	method	TRDR score	result	{'e1': {'word': 'method', 'word_index': [(5, 5)], 'id': 'H05-1115.24'}, 'e2': {'word': 'TRDR score', 'word_index': [(8, 8)], 'id': 'H05-1115.25'}}	In our experiments , the ENTITY achieves a ENTITYOTHER that is significantly higher than that of the ENTITYUNRELATED .
A model is presented to characterize the class of languages obtained by adding reduplication to context-free languages.	model	class of languages	model-feature	{'e1': {'word': 'model', 'word_index': [(1, 1)], 'id': 'J89-4003.1'}, 'e2': {'word': 'class of languages', 'word_index': [(7, 7)], 'id': 'J89-4003.2'}}	A ENTITY is presented to characterize the ENTITYOTHER obtained by adding ENTITYUNRELATED to ENTITYUNRELATED .
A model is presented to characterize the class of languages obtained by adding reduplication to context-free languages.	reduplication	context-free languages	part_whole	{'e1': {'word': 'reduplication', 'word_index': [(11, 11)], 'id': 'J89-4003.3'}, 'e2': {'word': 'context-free languages', 'word_index': [(13, 13)], 'id': 'J89-4003.4'}}	A ENTITYUNRELATED is presented to characterize the ENTITYUNRELATED obtained by adding ENTITY to ENTITYOTHER .
The model is a pushdown automaton augmented with the ability to check reduplication by using the stack in a new way.	stack	pushdown automaton	usage	{'e1': {'word': 'stack', 'word_index': [(15, 15)], 'id': 'J89-4003.8'}, 'e2': {'word': 'pushdown automaton', 'word_index': [(4, 4)], 'id': 'J89-4003.6'}}	The ENTITYUNRELATED is a ENTITYOTHER augmented with the ability to check ENTITYUNRELATED by using the ENTITY in a new way .
The model appears capable of accommodating the sort of reduplications that have been observed to occur in natural languages, but it excludes many of the unnatural constructions that other formal models have permitted.	reduplications	natural languages	part_whole	{'e1': {'word': 'reduplications', 'word_index': [(9, 9)], 'id': 'J89-4003.13'}, 'e2': {'word': 'natural languages', 'word_index': [(17, 17)], 'id': 'J89-4003.14'}}	The ENTITYUNRELATED appears capable of accommodating the sort of ENTITY that have been observed to occur in ENTITYOTHER , but it excludes many of the unnatural ENTITYUNRELATED that other ENTITYUNRELATED have permitted .
This article is devoted to the problem of quantifying noun groups in German.	quantifying noun groups	German	part_whole	{'e1': {'word': 'quantifying noun groups', 'word_index': [(8, 8)], 'id': 'I05-6010.1'}, 'e2': {'word': 'German', 'word_index': [(10, 10)], 'id': 'I05-6010.2'}}	This article is devoted to the problem of ENTITY in ENTITYOTHER .
Moreover, some examples are given that underline the necessity of integrating some kind of information other than grammar sensu stricto into the treebank.	treebank	grammar sensu stricto	model-feature	{'e1': {'word': 'treebank', 'word_index': [(21, 21)], 'id': 'I05-6010.5'}, 'e2': {'word': 'grammar sensu stricto', 'word_index': [(18, 18)], 'id': 'I05-6010.4'}}	Moreover , some examples are given that underline the necessity of integrating some kind of information other than ENTITYOTHER into the ENTITY .
We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source of data for theoretical linguistic investigations.	annotation	tree-bank	part_whole	{'e1': {'word': 'annotation', 'word_index': [(10, 10)], 'id': 'I05-6010.6'}, 'e2': {'word': 'tree-bank', 'word_index': [(13, 13)], 'id': 'I05-6010.7'}}	We argue that a more sophisticated and fine - grained ENTITY in the ENTITYOTHER would have very positve effects on ENTITYUNRELATED trained on the ENTITYUNRELATED and on ENTITYUNRELATED induced from the ENTITYUNRELATED , and it would make the ENTITYUNRELATED more valuable as a ENTITYUNRELATED for ENTITYUNRELATED .
We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source of data for theoretical linguistic investigations.	treebank	theoretical linguistic investigations	usage	{'e1': {'word': 'treebank', 'word_index': [(38, 38)], 'id': 'I05-6010.12'}, 'e2': {'word': 'theoretical linguistic investigations', 'word_index': [(45, 45)], 'id': 'I05-6010.14'}}	We argue that a more sophisticated and fine - grained ENTITYUNRELATED in the ENTITYUNRELATED would have very positve effects on ENTITYUNRELATED trained on the ENTITYUNRELATED and on ENTITYUNRELATED induced from the ENTITYUNRELATED , and it would make the ENTITY more valuable as a ENTITYUNRELATED for ENTITYOTHER .
The information gained from corpus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and extraction tool for German text corpora.	extraction tool	German text corpora	usage	{'e1': {'word': 'extraction tool', 'word_index': [(22, 22)], 'id': 'I05-6010.18'}, 'e2': {'word': 'German text corpora', 'word_index': [(24, 24)], 'id': 'I05-6010.19'}}	The information gained from ENTITYUNRELATED and the analyses that are proposed are realized in the framework of ENTITYUNRELATED , a ENTITYUNRELATED and ENTITY for ENTITYOTHER .
Metagrammatical formalisms that combine context-free phrase structure rules and metarules (MPS grammars) allow concise statement of generalizations about the syntax of natural languages.	context-free phrase structure rules	Metagrammatical formalisms	part_whole	{'e1': {'word': 'context-free phrase structure rules', 'word_index': [(3, 3)], 'id': 'P83-1004.2'}, 'e2': {'word': 'Metagrammatical formalisms', 'word_index': [(0, 0)], 'id': 'P83-1004.1'}}	ENTITYOTHER that combine ENTITY and ENTITYUNRELATED allow concise statement of generalizations about the ENTITYUNRELATED of ENTITYUNRELATED .
Metagrammatical formalisms that combine context-free phrase structure rules and metarules (MPS grammars) allow concise statement of generalizations about the syntax of natural languages.	syntax	natural languages	part_whole	{'e1': {'word': 'syntax', 'word_index': [(13, 13)], 'id': 'P83-1004.4'}, 'e2': {'word': 'natural languages', 'word_index': [(15, 15)], 'id': 'P83-1004.5'}}	ENTITYUNRELATED that combine ENTITYUNRELATED and ENTITYUNRELATED allow concise statement of generalizations about the ENTITY of ENTITYOTHER .
In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.	formalization	attentional structure in discourse	model-feature	{'e1': {'word': 'formalization', 'word_index': [(6, 6)], 'id': 'P87-1022.1'}, 'e2': {'word': 'attentional structure in discourse', 'word_index': [(12, 12)], 'id': 'P87-1022.3'}}	In this paper we present a ENTITY of the ENTITYUNRELATED to modeling ENTITYOTHER and use it as the basis for an ENTITYUNRELATED to track ENTITYUNRELATED and bind ENTITYUNRELATED .
In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.	algorithm	discourse context	usage	{'e1': {'word': 'algorithm', 'word_index': [(21, 21)], 'id': 'P87-1022.4'}, 'e2': {'word': 'discourse context', 'word_index': [(24, 24)], 'id': 'P87-1022.5'}}	In this paper we present a ENTITYUNRELATED of the ENTITYUNRELATED to modeling ENTITYUNRELATED and use it as the basis for an ENTITY to track ENTITYOTHER and bind ENTITYUNRELATED .
The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.	HPSG natural language system	database query application	usage	{'e1': {'word': 'HPSG natural language system', 'word_index': [(7, 7)], 'id': 'P87-1022.12'}, 'e2': {'word': 'database query application', 'word_index': [(15, 15)], 'id': 'P87-1022.13'}}	The ENTITYUNRELATED has been implemented in an ENTITY which serves as the interface to a ENTITYOTHER .
While HPSG has a more elaborated principle-based theory of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly.	principle-based theory	HPSG	part_whole	{'e1': {'word': 'principle-based theory', 'word_index': [(6, 6)], 'id': 'P95-1013.6'}, 'e2': {'word': 'HPSG', 'word_index': [(1, 1)], 'id': 'P95-1013.5'}}	While ENTITYOTHER has a more elaborated ENTITY of possible ENTITYUNRELATED , ENTITYUNRELATED provides the means to represent ENTITYUNRELATED more explicitly .
While HPSG has a more elaborated principle-based theory of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly.	TAG	lexicalized structures	usage	{'e1': {'word': 'TAG', 'word_index': [(11, 11)], 'id': 'P95-1013.8'}, 'e2': {'word': 'lexicalized structures', 'word_index': [(17, 17)], 'id': 'P95-1013.9'}}	While ENTITYUNRELATED has a more elaborated ENTITYUNRELATED of possible ENTITYUNRELATED , ENTITY provides the means to represent ENTITYOTHER more explicitly .
Our objectives are met by giving clear definitions that determine the projection of structures from the lexicon, and identify maximal projections, auxiliary trees and foot nodes.	projection of structures	lexicon	part_whole	{'e1': {'word': 'projection of structures', 'word_index': [(11, 11)], 'id': 'P95-1013.10'}, 'e2': {'word': 'lexicon', 'word_index': [(14, 14)], 'id': 'P95-1013.11'}}	Our objectives are met by giving clear definitions that determine the ENTITY from the ENTITYOTHER , and identify ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED .
Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing.	Boolean matrix multiplication (BMM)	CFG parsing	usage	{'e1': {'word': 'Boolean matrix multiplication (BMM)', 'word_index': [(3, 3)], 'id': 'P97-1002.1'}, 'e2': {'word': 'CFG parsing', 'word_index': [(8, 8)], 'id': 'P97-1002.2'}}	Valiant showed that ENTITY can be used for ENTITYOTHER .
We prove a dual result: CFG parsers running in time O(|G||w|3-e) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time O(m3-e/3).	time O(|G||w|3-e)	CFG parsers	model-feature	{'e1': {'word': 'time O(|G||w|3-e)', 'word_index': [(9, 9)], 'id': 'P97-1002.4'}, 'e2': {'word': 'CFG parsers', 'word_index': [(6, 6)], 'id': 'P97-1002.3'}}	We prove a dual result : ENTITYOTHER running in ENTITY on a ENTITYUNRELATED and a ENTITYUNRELATED can be used to multiply ENTITYUNRELATED in ENTITYUNRELATED .
In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang.	formal definition	parsing	model-feature	{'e1': {'word': 'formal definition', 'word_index': [(7, 7)], 'id': 'P97-1002.9'}, 'e2': {'word': 'parsing', 'word_index': [(9, 9)], 'id': 'P97-1002.10'}}	In the process we also provide a ENTITY of ENTITYOTHER motivated by an informal notion due to Lang .
Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist.	CFG parser	BMM algorithm	result	{'e1': {'word': 'CFG parser', 'word_index': [(16, 16)], 'id': 'P97-1002.12'}, 'e2': {'word': 'BMM algorithm', 'word_index': [(23, 23)], 'id': 'P97-1002.13'}}	Our result establishes one of the first limitations on general ENTITYUNRELATED : a fast , practical ENTITY would yield a fast , practical ENTITYOTHER , which is not believed to exist .
This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT.	primitive Optimality Theory (OTP)	OT	model-feature	{'e1': {'word': 'primitive Optimality Theory (OTP)', 'word_index': [(3, 3)], 'id': 'P97-1040.1'}, 'e2': {'word': 'OT', 'word_index': [(10, 10)], 'id': 'P97-1040.2'}}	This paper introduces ENTITY , a linguistically motivated formalization of ENTITYOTHER .
In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994).	Generalized Alignment	theories	usage	{'e1': {'word': 'Generalized Alignment', 'word_index': [(7, 7)], 'id': 'P97-1040.8'}, 'e2': {'word': 'theories', 'word_index': [(5, 5)], 'id': 'P97-1040.7'}}	In contrast to less restricted ENTITYOTHER using ENTITY , ENTITYUNRELATED 's optimal ENTITYUNRELATED can be generated with ENTITYUNRELATED adapted from ( Ellison , 1994 ) .
In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994).	finite-state methods	surface forms	usage	{'e1': {'word': 'finite-state methods', 'word_index': [(17, 17)], 'id': 'P97-1040.11'}, 'e2': {'word': 'surface forms', 'word_index': [(12, 12)], 'id': 'P97-1040.10'}}	In contrast to less restricted ENTITYUNRELATED using ENTITYUNRELATED , ENTITYUNRELATED 's optimal ENTITYOTHER can be generated with ENTITY adapted from ( Ellison , 1994 ) .
Unfortunately these methods take time exponential on the size of the grammar.	time exponential on the size of the grammar	methods	model-feature	{'e1': {'word': 'time exponential on the size of the grammar', 'word_index': [(4, 4)], 'id': 'P97-1040.13'}, 'e2': {'word': 'methods', 'word_index': [(2, 2)], 'id': 'P97-1040.12'}}	Unfortunately these ENTITYOTHER take ENTITY .
Indeed the generation problem is shown NP-complete in this sense.	NP-complete	generation problem	model-feature	{'e1': {'word': 'NP-complete', 'word_index': [(5, 5)], 'id': 'P97-1040.15'}, 'e2': {'word': 'generation problem', 'word_index': [(2, 2)], 'id': 'P97-1040.14'}}	Indeed the ENTITYOTHER is shown ENTITY in this sense .
One avenue for future improvements is a new finite-state notion, factored automata, where regular languages are represented compactly via formal intersections of FSAs.	formal intersections of FSAs	regular languages	model-feature	{'e1': {'word': 'formal intersections of FSAs', 'word_index': [(18, 18)], 'id': 'P97-1040.21'}, 'e2': {'word': 'regular languages', 'word_index': [(13, 13)], 'id': 'P97-1040.20'}}	One avenue for future improvements is a new ENTITYUNRELATED , ENTITYUNRELATED , where ENTITYOTHER are represented compactly via ENTITY .
We report our analysis of a collection of 20 Wall Street Journal articles from the Penn Treebank Corpus and our experiments with WordNet to identify relations between bridging descriptions and their antecedents.	Wall Street Journal articles	Penn Treebank Corpus	part_whole	{'e1': {'word': 'Wall Street Journal articles', 'word_index': [(9, 9)], 'id': 'P97-1072.2'}, 'e2': {'word': 'Penn Treebank Corpus', 'word_index': [(12, 12)], 'id': 'P97-1072.3'}}	We report our analysis of a collection of 20 ENTITY from the ENTITYOTHER and our experiments with ENTITYUNRELATED to identify relations between ENTITYUNRELATED and their ENTITYUNRELATED .
To verify hardware designs by model checking, circuit specifications are commonly expressed in the temporal logic CTL.	model checking	hardware designs	usage	{'e1': {'word': 'model checking', 'word_index': [(4, 4)], 'id': 'P99-1058.2'}, 'e2': {'word': 'hardware designs', 'word_index': [(2, 2)], 'id': 'P99-1058.1'}}	To verify ENTITYOTHER by ENTITY , ENTITYUNRELATED are commonly expressed in the ENTITYUNRELATED .
To verify hardware designs by model checking, circuit specifications are commonly expressed in the temporal logic CTL.	temporal logic CTL	circuit specifications	model-feature	{'e1': {'word': 'temporal logic CTL', 'word_index': [(12, 12)], 'id': 'P99-1058.4'}, 'e2': {'word': 'circuit specifications', 'word_index': [(6, 6)], 'id': 'P99-1058.3'}}	To verify ENTITYUNRELATED by ENTITYUNRELATED , ENTITYOTHER are commonly expressed in the ENTITY .
Automatic conversion of English to CTL requires the definition of an appropriately restricted subset of English.	restricted subset	English	part_whole	{'e1': {'word': 'restricted subset', 'word_index': [(7, 7)], 'id': 'P99-1058.6'}, 'e2': {'word': 'English', 'word_index': [(9, 9)], 'id': 'P99-1058.7'}}	ENTITYUNRELATED requires the definition of an appropriately ENTITY of ENTITYOTHER .
We show how the limited semantic expressibility of CTL can be exploited to derive a hierarchy of subsets.	semantic expressibility	CTL	model-feature	{'e1': {'word': 'semantic expressibility', 'word_index': [(5, 5)], 'id': 'P99-1058.8'}, 'e2': {'word': 'CTL', 'word_index': [(7, 7)], 'id': 'P99-1058.9'}}	We show how the limited ENTITY of ENTITYOTHER can be exploited to derive a hierarchy of ENTITYUNRELATED .
Our strategy avoids potential difficulties with approaches that take existing computational semantic analyses of English as their starting point--such as the need to ensure that all sentences in the subset possess a CTL translation.	computational semantic analyses	English	topic	{'e1': {'word': 'computational semantic analyses', 'word_index': [(10, 10)], 'id': 'P99-1058.11'}, 'e2': {'word': 'English', 'word_index': [(12, 12)], 'id': 'P99-1058.12'}}	Our strategy avoids potential difficulties with approaches that take existing ENTITY of ENTITYOTHER as their starting point -- such as the need to ensure that all ENTITYUNRELATED in the ENTITYUNRELATED possess a ENTITYUNRELATED .
Our strategy avoids potential difficulties with approaches that take existing computational semantic analyses of English as their starting point--such as the need to ensure that all sentences in the subset possess a CTL translation.	sentences	subset	part_whole	{'e1': {'word': 'sentences', 'word_index': [(26, 26)], 'id': 'P99-1058.13'}, 'e2': {'word': 'subset', 'word_index': [(29, 29)], 'id': 'P99-1058.14'}}	Our strategy avoids potential difficulties with approaches that take existing ENTITYUNRELATED of ENTITYUNRELATED as their starting point -- such as the need to ensure that all ENTITY in the ENTITYOTHER possess a ENTITYUNRELATED .
In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus.	unlexicalized parser	German	usage	{'e1': {'word': 'unlexicalized parser', 'word_index': [(7, 7)], 'id': 'P05-1039.1'}, 'e2': {'word': 'German', 'word_index': [(9, 9)], 'id': 'P05-1039.2'}}	In this paper , we present an ENTITY for ENTITYOTHER which employs ENTITYUNRELATED and ENTITYUNRELATED to achieve a ENTITYUNRELATED of 76.2 , higher than previously reported results on the ENTITYUNRELATED .
In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus.	suffix analysis	labelled bracket F-score	result	{'e1': {'word': 'suffix analysis', 'word_index': [(14, 14)], 'id': 'P05-1039.4'}, 'e2': {'word': 'labelled bracket F-score', 'word_index': [(18, 18)], 'id': 'P05-1039.5'}}	In this paper , we present an ENTITYUNRELATED for ENTITYUNRELATED which employs ENTITYUNRELATED and ENTITY to achieve a ENTITYOTHER of 76.2 , higher than previously reported results on the ENTITYUNRELATED .
In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results.	smoothing	unlexicalized parser	usage	{'e1': {'word': 'smoothing', 'word_index': [(13, 13)], 'id': 'P05-1039.8'}, 'e2': {'word': 'unlexicalized parser', 'word_index': [(16, 16)], 'id': 'P05-1039.9'}}	In addition to the high ENTITYUNRELATED of the model , the use of ENTITY in an ENTITYOTHER allows us to better examine the interplay between ENTITYUNRELATED and ENTITYUNRELATED results .
This paper proposes an alignment adaptation approach to improve domain-specific (in-domain) word alignment.	alignment adaptation approach	domain-specific (in-domain) word alignment	usage	{'e1': {'word': 'alignment adaptation approach', 'word_index': [(4, 4)], 'id': 'P05-1058.1'}, 'e2': {'word': 'domain-specific (in-domain) word alignment', 'word_index': [(7, 7)], 'id': 'P05-1058.2'}}	This paper proposes an ENTITY to improve ENTITYOTHER .
The basic idea of alignment adaptation is to use out-of-domain corpus to improve in-domain word alignment results.	out-of-domain corpus	in-domain word alignment	usage	{'e1': {'word': 'out-of-domain corpus', 'word_index': [(8, 8)], 'id': 'P05-1058.4'}, 'e2': {'word': 'in-domain word alignment', 'word_index': [(11, 11)], 'id': 'P05-1058.5'}}	The basic idea of ENTITYUNRELATED is to use ENTITY to improve ENTITYOTHER results .
In this paper, we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively, and then interpolate these two models to improve the domain-specific word alignment.	out-of-domain corpus	in-domain corpus	compare	{'e1': {'word': 'out-of-domain corpus', 'word_index': [(14, 14)], 'id': 'P05-1058.7'}, 'e2': {'word': 'in-domain corpus', 'word_index': [(20, 20)], 'id': 'P05-1058.8'}}	In this paper , we first train two ENTITYUNRELATED with the large - scale ENTITY and the small - scale ENTITYOTHER respectively , and then interpolate these two models to improve the ENTITYUNRELATED .
Experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall, achieving a relative error rate reduction of 6.56% as compared with the state-of-the-art technologies.	domain-specific word alignment	relative error rate reduction	result	{'e1': {'word': 'domain-specific word alignment', 'word_index': [(7, 7)], 'id': 'P05-1058.10'}, 'e2': {'word': 'relative error rate reduction', 'word_index': [(18, 18)], 'id': 'P05-1058.13'}}	Experimental results show that our approach improves ENTITY in terms of both ENTITYUNRELATED and ENTITYUNRELATED , achieving a ENTITYOTHER of 6.56 % as compared with the state - of - the - art technologies .
Our contributions include a concise, modular architecture with reversible processes of understanding and generation, an information-state model of reference, and flexible links between semantics and collaborative problem solving.	understanding	modular architecture	part_whole	{'e1': {'word': 'understanding', 'word_index': [(11, 11)], 'id': 'P05-3001.3'}, 'e2': {'word': 'modular architecture', 'word_index': [(6, 6)], 'id': 'P05-3001.2'}}	Our contributions include a concise , ENTITYOTHER with reversible processes of ENTITY and ENTITYUNRELATED , an ENTITYUNRELATED , and flexible links between ENTITYUNRELATED and ENTITYUNRELATED .
This article deals with the interpretation of conceptual operations underlying the communicative use of natural language (NL) within the Structured Inheritance Network (SI-Nets) paradigm.	interpretation	conceptual operations	model-feature	{'e1': {'word': 'interpretation', 'word_index': [(5, 5)], 'id': 'E83-1021.1'}, 'e2': {'word': 'conceptual operations', 'word_index': [(7, 7)], 'id': 'E83-1021.2'}}	This article deals with the ENTITY of ENTITYOTHER underlying the communicative use of ENTITYUNRELATED within the ENTITYUNRELATED .
The operations are reduced to functions of a formal language, thus changing the level of abstraction of the operations to be performed on SI-Nets.	functions	formal language	part_whole	{'e1': {'word': 'functions', 'word_index': [(5, 5)], 'id': 'E83-1021.5'}, 'e2': {'word': 'formal language', 'word_index': [(8, 8)], 'id': 'E83-1021.6'}}	The operations are reduced to ENTITY of a ENTITYOTHER , thus changing the level of abstraction of the operations to be performed on ENTITYUNRELATED .
In this sense, operations on SI-Nets are not merely isomorphic to single epistemological objects, but can be viewed as a simulation of processes on a different level, that pertaining to the conceptual system of NL.	conceptual system	NL	model-feature	{'e1': {'word': 'conceptual system', 'word_index': [(34, 34)], 'id': 'E83-1021.9'}, 'e2': {'word': 'NL', 'word_index': [(36, 36)], 'id': 'E83-1021.10'}}	In this sense , operations on ENTITYUNRELATED are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the ENTITY of ENTITYOTHER .
For this purpose, we have designed a version of KL-ONE which represents the epistemological level, while the new experimental language, KL-Conc, represents the conceptual level.	KL-ONE	epistemological level	model-feature	{'e1': {'word': 'KL-ONE', 'word_index': [(10, 10)], 'id': 'E83-1021.11'}, 'e2': {'word': 'epistemological level', 'word_index': [(14, 14)], 'id': 'E83-1021.12'}}	For this purpose , we have designed a version of ENTITY which represents the ENTITYOTHER , while the new experimental language , ENTITYUNRELATED , represents the ENTITYUNRELATED .
For this purpose, we have designed a version of KL-ONE which represents the epistemological level, while the new experimental language, KL-Conc, represents the conceptual level.	KL-Conc	conceptual level	model-feature	{'e1': {'word': 'KL-Conc', 'word_index': [(22, 22)], 'id': 'E83-1021.13'}, 'e2': {'word': 'conceptual level', 'word_index': [(26, 26)], 'id': 'E83-1021.14'}}	For this purpose , we have designed a version of ENTITYUNRELATED which represents the ENTITYUNRELATED , while the new experimental language , ENTITY , represents the ENTITYOTHER .
The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present, past or future (= deictic information) 2. whether the event described in a sentence is presented as completed, going on, just starting or being finished (= aspectual information).	information	verb forms	model-feature	{'e1': {'word': 'information', 'word_index': [(10, 10)], 'id': 'E87-1043.2'}, 'e2': {'word': 'verb forms', 'word_index': [(1, 1)], 'id': 'E87-1043.1'}}	The ENTITYOTHER are often claimed to convey two kinds of ENTITY : 1. whether the ENTITYUNRELATED described in a ENTITYUNRELATED is ENTITYUNRELATED , ENTITYUNRELATED or ENTITYUNRELATED (= ENTITYUNRELATED ) 2. whether the ENTITYUNRELATED described in a ENTITYUNRELATED is presented as completed , going on , just starting or being finished (= ENTITYUNRELATED ) .
The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present, past or future (= deictic information) 2. whether the event described in a sentence is presented as completed, going on, just starting or being finished (= aspectual information).	event	sentence	part_whole	{'e1': {'word': 'event', 'word_index': [(15, 15)], 'id': 'E87-1043.3'}, 'e2': {'word': 'sentence', 'word_index': [(19, 19)], 'id': 'E87-1043.4'}}	The ENTITYUNRELATED are often claimed to convey two kinds of ENTITYUNRELATED : 1. whether the ENTITY described in a ENTITYOTHER is ENTITYUNRELATED , ENTITYUNRELATED or ENTITYUNRELATED (= ENTITYUNRELATED ) 2. whether the ENTITYUNRELATED described in a ENTITYUNRELATED is presented as completed , going on , just starting or being finished (= ENTITYUNRELATED ) .
The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present, past or future (= deictic information) 2. whether the event described in a sentence is presented as completed, going on, just starting or being finished (= aspectual information).	event	sentence	part_whole	{'e1': {'word': 'event', 'word_index': [(32, 32)], 'id': 'E87-1043.9'}, 'e2': {'word': 'sentence', 'word_index': [(36, 36)], 'id': 'E87-1043.10'}}	The ENTITYUNRELATED are often claimed to convey two kinds of ENTITYUNRELATED : 1. whether the ENTITYUNRELATED described in a ENTITYUNRELATED is ENTITYUNRELATED , ENTITYUNRELATED or ENTITYUNRELATED (= ENTITYUNRELATED ) 2. whether the ENTITY described in a ENTITYOTHER is presented as completed , going on , just starting or being finished (= ENTITYUNRELATED ) .
It will be demonstrated in this paper that one has to add a third component to the analysis of verb form meanings, namely whether or not they express habituality.	habituality	verb form meanings	model-feature	{'e1': {'word': 'habituality', 'word_index': [(27, 27)], 'id': 'E87-1043.13'}, 'e2': {'word': 'verb form meanings', 'word_index': [(19, 19)], 'id': 'E87-1043.12'}}	It will be demonstrated in this paper that one has to add a third component to the analysis of ENTITYOTHER , namely whether or not they express ENTITY .
Unification is often the appropriate method for expressing relations between representations in the form of feature structures; however, there are circumstances in which a different approach is desirable.	relations	representations	model-feature	{'e1': {'word': 'relations', 'word_index': [(8, 8)], 'id': 'E91-1050.2'}, 'e2': {'word': 'representations', 'word_index': [(10, 10)], 'id': 'E91-1050.3'}}	ENTITYUNRELATED is often the appropriate method for expressing ENTITY between ENTITYOTHER in the form of ENTITYUNRELATED ; however , there are circumstances in which a different approach is desirable .
A declarative formalism is presented which permits direct mappings of one feature structure into another, and illustrative examples are given of its application to areas of current interest.	declarative formalism	mappings	usage	{'e1': {'word': 'declarative formalism', 'word_index': [(1, 1)], 'id': 'E91-1050.5'}, 'e2': {'word': 'mappings', 'word_index': [(7, 7)], 'id': 'E91-1050.6'}}	A ENTITY is presented which permits direct ENTITYOTHER of one ENTITYUNRELATED into another , and illustrative examples are given of its application to areas of current interest .
 We give an analysis of ellipsis resolution in terms of a straightforward discourse copying algorithm that correctly predicts a wide range of phenomena.	discourse copying algorithm	ellipsis resolution	model-feature	{'e1': {'word': 'discourse copying algorithm', 'word_index': [(11, 11)], 'id': 'E93-1025.2'}, 'e2': {'word': 'ellipsis resolution', 'word_index': [(5, 5)], 'id': 'E93-1025.1'}}	We give an analysis of ENTITYOTHER in terms of a straightforward ENTITY that correctly predicts a wide range of phenomena .
Furthermore, in contrast to the approach of Dalrymple et al. [1991], the treatment directly encodes the intuitive distinction between full NPs and the referential elements that corefer with them through what we term role linking.	full NPs	referential elements	compare	{'e1': {'word': 'full NPs', 'word_index': [(23, 23)], 'id': 'E93-1025.4'}, 'e2': {'word': 'referential elements', 'word_index': [(26, 26)], 'id': 'E93-1025.5'}}	Furthermore , in contrast to the approach of Dalrymple et al. [ 1991 ] , the treatment directly encodes the intuitive distinction between ENTITY and the ENTITYOTHER that corefer with them through what we term ENTITYUNRELATED .
The correct predictions for several problematic examples of ellipsis naturally result.	predictions	ellipsis	model-feature	{'e1': {'word': 'predictions', 'word_index': [(2, 2)], 'id': 'E93-1025.7'}, 'e2': {'word': 'ellipsis', 'word_index': [(8, 8)], 'id': 'E93-1025.8'}}	The correct ENTITY for several problematic examples of ENTITYOTHER naturally result .
Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval.	chunks of words	sentences	part_whole	{'e1': {'word': 'chunks of words', 'word_index': [(3, 3)], 'id': 'E99-1023.2'}, 'e2': {'word': 'sentences', 'word_index': [(1, 1)], 'id': 'E99-1023.1'}}	Dividing ENTITYOTHER in ENTITY is a useful preprocessing step for ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED .
Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval.	information extraction	chunks of words	part_whole	{'e1': {'word': 'information extraction', 'word_index': [(12, 12)], 'id': 'E99-1023.4'}, 'e2': {'word': 'chunks of words', 'word_index': [(3, 3)], 'id': 'E99-1023.2'}}	Dividing ENTITYUNRELATED in ENTITYOTHER is a useful preprocessing step for ENTITYUNRELATED , ENTITY and ENTITYUNRELATED .
"(Ramshaw and Marcus, 1995) have introduced a ""convenient"" data representation for chunking by converting it to a tagging task."	data representation	chunking	usage	{'e1': {'word': 'data representation', 'word_index': [(13, 13)], 'id': 'E99-1023.6'}, 'e2': {'word': 'chunking', 'word_index': [(15, 15)], 'id': 'E99-1023.7'}}	"( Ramshaw and Marcus , 1995 ) have introduced a "" convenient "" ENTITY for ENTITYOTHER by converting it to a ENTITYUNRELATED ."
In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks.	data representations	noun phrase chunks	model-feature	{'e1': {'word': 'data representations', 'word_index': [(8, 8)], 'id': 'E99-1023.9'}, 'e2': {'word': 'noun phrase chunks', 'word_index': [(14, 14)], 'id': 'E99-1023.10'}}	In this paper we will examine seven different ENTITY for the problem of recognizing ENTITYOTHER .
We will show that the data representation choice has a minor influence on chunking performance.	data representation choice	chunking performance	result	{'e1': {'word': 'data representation choice', 'word_index': [(5, 5)], 'id': 'E99-1023.11'}, 'e2': {'word': 'chunking performance', 'word_index': [(11, 11)], 'id': 'E99-1023.12'}}	We will show that the ENTITY has a minor influence on ENTITYOTHER .
However, equipped with the most suitabledata representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.	memory-based learning chunker	chunking results	result	{'e1': {'word': 'memory-based learning chunker', 'word_index': [(10, 10)], 'id': 'E99-1023.14'}, 'e2': {'word': 'chunking results', 'word_index': [(18, 18)], 'id': 'E99-1023.15'}}	However , equipped with the most suitable ENTITYUNRELATED , our ENTITY was able to improve the best published ENTITYOTHER for a ENTITYUNRELATED .
In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language.	statistical and constraint-based disambiguation	part-of-speech tagging	usage	{'e1': {'word': 'statistical and constraint-based disambiguation', 'word_index': [(11, 11)], 'id': 'E95-1021.2'}, 'e2': {'word': 'part-of-speech tagging', 'word_index': [(9, 9)], 'id': 'E95-1021.1'}}	In this paper we compare two competing approaches to ENTITYOTHER , ENTITY , using ENTITYUNRELATED as our ENTITYUNRELATED .
In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language.	French	test language	usage	{'e1': {'word': 'French', 'word_index': [(14, 14)], 'id': 'E95-1021.3'}, 'e2': {'word': 'test language', 'word_index': [(17, 17)], 'id': 'E95-1021.4'}}	In this paper we compare two competing approaches to ENTITYUNRELATED , ENTITYUNRELATED , using ENTITY as our ENTITYOTHER .
We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model.	constraint system	statistical model	compare	{'e1': {'word': 'constraint system', 'word_index': [(19, 19)], 'id': 'E95-1021.5'}, 'e2': {'word': 'statistical model', 'word_index': [(39, 39)], 'id': 'E95-1021.6'}}	We imposed a time limit on our experiment : the amount of time spent on the design of our ENTITY was about the same as the time we used to train and test the easy - to - implement ENTITYOTHER .
The accuracy of the statistical method is reasonably good, comparable to taggers for English.	statistical method	taggers	compare	{'e1': {'word': 'statistical method', 'word_index': [(4, 4)], 'id': 'E95-1021.8'}, 'e2': {'word': 'taggers', 'word_index': [(11, 11)], 'id': 'E95-1021.9'}}	The ENTITYUNRELATED of the ENTITY is reasonably good , comparable to ENTITYOTHER for ENTITYUNRELATED .
In order to build robust automatic abstracting systems, there is a need for better training resources than are currently available.	training resources	automatic abstracting systems	usage	{'e1': {'word': 'training resources', 'word_index': [(13, 13)], 'id': 'E99-1015.2'}, 'e2': {'word': 'automatic abstracting systems', 'word_index': [(5, 5)], 'id': 'E99-1015.1'}}	In order to build robust ENTITYOTHER , there is a need for better ENTITY than are currently available .
In this paper, we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way.	annotation scheme	resource	usage	{'e1': {'word': 'annotation scheme', 'word_index': [(7, 7)], 'id': 'E99-1015.3'}, 'e2': {'word': 'resource', 'word_index': [(19, 19)], 'id': 'E99-1015.4'}}	In this paper , we introduce an ENTITY for scientific articles which can be used to build such a ENTITYOTHER in a consistent way .
This paper describes an implemented program that takes a tagged text corpus and generates a partial list of the subcategorization frames in which each verb occurs.	subcategorization frames	verb	model-feature	{'e1': {'word': 'subcategorization frames', 'word_index': [(17, 17)], 'id': 'H91-1067.2'}, 'e2': {'word': 'verb', 'word_index': [(21, 21)], 'id': 'H91-1067.3'}}	This paper describes an implemented program that takes a ENTITYUNRELATED and generates a partial list of the ENTITY in which each ENTITYOTHER occurs .
The completeness of the output list increases monotonically with the total occurrences of each verb in the training corpus.	verb	training corpus	part_whole	{'e1': {'word': 'verb', 'word_index': [(14, 14)], 'id': 'H91-1067.5'}, 'e2': {'word': 'training corpus', 'word_index': [(17, 17)], 'id': 'H91-1067.6'}}	The completeness of the output list increases monotonically with the total ENTITYUNRELATED of each ENTITY in the ENTITYOTHER .
We focus on the problem of building large repositories of lexical conceptual structure (LCS) representations for verbs in multiple languages.	lexical conceptual structure (LCS) representations	verbs	model-feature	{'e1': {'word': 'lexical conceptual structure (LCS) representations', 'word_index': [(10, 10)], 'id': 'A97-1021.2'}, 'e2': {'word': 'verbs', 'word_index': [(12, 12)], 'id': 'A97-1021.3'}}	We focus on the problem of building large ENTITYUNRELATED of ENTITY for ENTITYOTHER in multiple ENTITYUNRELATED .
Our acquisition program - LEXICALL - takes, as input, the result of previous work on verb classification and thematic grid tagging, and outputs LCS representations for different languages.	thematic grid tagging	LCS representations	usage	{'e1': {'word': 'thematic grid tagging', 'word_index': [(15, 15)], 'id': 'A97-1021.9'}, 'e2': {'word': 'LCS representations', 'word_index': [(19, 19)], 'id': 'A97-1021.10'}}	Our ENTITYUNRELATED takes , as input , the result of previous work on ENTITYUNRELATED and ENTITY , and outputs ENTITYOTHER for different ENTITYUNRELATED .
These representations have been ported into English, Arabic and Spanish lexicons, each containing approximately 9000 verbs.	representations	English, Arabic and Spanish lexicons	usage	{'e1': {'word': 'representations', 'word_index': [(1, 1)], 'id': 'A97-1021.12'}, 'e2': {'word': 'English, Arabic and Spanish lexicons', 'word_index': [(6, 6)], 'id': 'A97-1021.13'}}	These ENTITY have been ported into ENTITYOTHER , each containing approximately 9000 ENTITYUNRELATED .
We are currently using these lexicons in an operational foreign language tutoring and machine translation.	lexicons	operational foreign language tutoring	usage	{'e1': {'word': 'lexicons', 'word_index': [(5, 5)], 'id': 'A97-1021.15'}, 'e2': {'word': 'operational foreign language tutoring', 'word_index': [(8, 8)], 'id': 'A97-1021.16'}}	We are currently using these ENTITY in an ENTITYOTHER and ENTITYUNRELATED .
We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.	algorithm for translation lexicon acquisition (SABLE)	translation lexicons	usage	{'e1': {'word': 'algorithm for translation lexicon acquisition (SABLE)', 'word_index': [(6, 6)], 'id': 'A97-1050.1'}, 'e2': {'word': 'translation lexicons', 'word_index': [(18, 18)], 'id': 'A97-1050.3'}}	We investigate the utility of an ENTITY , used previously on a very large ENTITYUNRELATED to acquire general ENTITYOTHER , when that ENTITYUNRELATED is applied to a much smaller ENTITYUNRELATED to produce candidates for ENTITYUNRELATED .
We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.	algorithm	domain-specific translation lexicons	usage	{'e1': {'word': 'algorithm', 'word_index': [(22, 22)], 'id': 'A97-1050.4'}, 'e2': {'word': 'domain-specific translation lexicons', 'word_index': [(34, 34)], 'id': 'A97-1050.6'}}	We investigate the utility of an ENTITYUNRELATED , used previously on a very large ENTITYUNRELATED to acquire general ENTITYUNRELATED , when that ENTITY is applied to a much smaller ENTITYUNRELATED to produce candidates for ENTITYOTHER .
English is shown to be trans-context-free on the basis of coordinations of the respectively type that involve strictly syntactic cross-serial agreement.	coordinations	strictly syntactic cross-serial agreement	model-feature	{'e1': {'word': 'coordinations', 'word_index': [(14, 14)], 'id': 'J87-1003.2'}, 'e2': {'word': 'strictly syntactic cross-serial agreement', 'word_index': [(21, 21)], 'id': 'J87-1003.3'}}	ENTITYUNRELATED is shown to be trans - context - free on the basis of ENTITY of the respectively type that involve ENTITYOTHER .
The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like grammatical gender in languages such as French, is partly arbitrary.	number	nouns	model-feature	{'e1': {'word': 'number', 'word_index': [(5, 5)], 'id': 'J87-1003.5'}, 'e2': {'word': 'nouns', 'word_index': [(7, 7)], 'id': 'J87-1003.6'}}	The ENTITYUNRELATED in question involves ENTITY in ENTITYOTHER and ENTITYUNRELATED and is syntactic rather than semantic in nature because ENTITYUNRELATED in ENTITYUNRELATED , like ENTITYUNRELATED in ENTITYUNRELATED such as ENTITYUNRELATED , is partly arbitrary .
The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like grammatical gender in languages such as French, is partly arbitrary.	grammatical number	English	part_whole	{'e1': {'word': 'grammatical number', 'word_index': [(19, 19)], 'id': 'J87-1003.8'}, 'e2': {'word': 'English', 'word_index': [(21, 21)], 'id': 'J87-1003.9'}}	The ENTITYUNRELATED in question involves ENTITYUNRELATED in ENTITYUNRELATED and ENTITYUNRELATED and is syntactic rather than semantic in nature because ENTITY in ENTITYOTHER , like ENTITYUNRELATED in ENTITYUNRELATED such as ENTITYUNRELATED , is partly arbitrary .
The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like grammatical gender in languages such as French, is partly arbitrary.	grammatical gender	French	part_whole	{'e1': {'word': 'grammatical gender', 'word_index': [(24, 24)], 'id': 'J87-1003.10'}, 'e2': {'word': 'French', 'word_index': [(29, 29)], 'id': 'J87-1003.12'}}	The ENTITYUNRELATED in question involves ENTITYUNRELATED in ENTITYUNRELATED and ENTITYUNRELATED and is syntactic rather than semantic in nature because ENTITYUNRELATED in ENTITYUNRELATED , like ENTITY in ENTITYUNRELATED such as ENTITYOTHER , is partly arbitrary .
The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem.	grammatical sentences	English	part_whole	{'e1': {'word': 'grammatical sentences', 'word_index': [(30, 30)], 'id': 'J87-1003.15'}, 'e2': {'word': 'English', 'word_index': [(25, 25)], 'id': 'J87-1003.14'}}	The formal proof , which makes crucial use of the ENTITYUNRELATED of Ogden et al. , is so constructed as to be valid even if ENTITYOTHER is presumed to contain ENTITY in which respectively operates across a pair of ENTITYUNRELATED one of whose members has fewer ENTITYUNRELATED than the other ; it thus goes through whatever the facts may be regarding ENTITYUNRELATED with unequal numbers of ENTITYUNRELATED in the ENTITYUNRELATED of respectively , whereas other ENTITYUNRELATED have foundered on this problem .
The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem.	conjuncts	coordinate phrases	part_whole	{'e1': {'word': 'conjuncts', 'word_index': [(46, 46)], 'id': 'J87-1003.17'}, 'e2': {'word': 'coordinate phrases', 'word_index': [(39, 39)], 'id': 'J87-1003.16'}}	The formal proof , which makes crucial use of the ENTITYUNRELATED of Ogden et al. , is so constructed as to be valid even if ENTITYUNRELATED is presumed to contain ENTITYUNRELATED in which respectively operates across a pair of ENTITYOTHER one of whose members has fewer ENTITY than the other ; it thus goes through whatever the facts may be regarding ENTITYUNRELATED with unequal numbers of ENTITYUNRELATED in the ENTITYUNRELATED of respectively , whereas other ENTITYUNRELATED have foundered on this problem .
The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem.	conjuncts	constructions	part_whole	{'e1': {'word': 'conjuncts', 'word_index': [(66, 66)], 'id': 'J87-1003.19'}, 'e2': {'word': 'constructions', 'word_index': [(61, 61)], 'id': 'J87-1003.18'}}	The formal proof , which makes crucial use of the ENTITYUNRELATED of Ogden et al. , is so constructed as to be valid even if ENTITYUNRELATED is presumed to contain ENTITYUNRELATED in which respectively operates across a pair of ENTITYUNRELATED one of whose members has fewer ENTITYUNRELATED than the other ; it thus goes through whatever the facts may be regarding ENTITYOTHER with unequal numbers of ENTITY in the ENTITYUNRELATED of respectively , whereas other ENTITYUNRELATED have foundered on this problem .
Towards deep analysis of compositional classes of paraphrases, we have examined a class-oriented framework for collecting paraphrase examples, in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement.	class-oriented framework	paraphrase examples	usage	{'e1': {'word': 'class-oriented framework', 'word_index': [(10, 10)], 'id': 'I05-5004.2'}, 'e2': {'word': 'paraphrase examples', 'word_index': [(13, 13)], 'id': 'I05-5004.3'}}	Towards deep analysis of ENTITYUNRELATED , we have examined a ENTITY for collecting ENTITYOTHER , in which ENTITYUNRELATED are collected for each ENTITYUNRELATED separately by means of ENTITYUNRELATED and ENTITYUNRELATED .
Towards deep analysis of compositional classes of paraphrases, we have examined a class-oriented framework for collecting paraphrase examples, in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement.	sentential paraphrases	paraphrase class	part_whole	{'e1': {'word': 'sentential paraphrases', 'word_index': [(17, 17)], 'id': 'I05-5004.4'}, 'e2': {'word': 'paraphrase class', 'word_index': [(22, 22)], 'id': 'I05-5004.5'}}	Towards deep analysis of ENTITYUNRELATED , we have examined a ENTITYUNRELATED for collecting ENTITYUNRELATED , in which ENTITY are collected for each ENTITYOTHER separately by means of ENTITYUNRELATED and ENTITYUNRELATED .
A flexible parser can deal with input that deviates from its grammar, in addition to input that conforms to it.	grammar	flexible parser	part_whole	{'e1': {'word': 'grammar', 'word_index': [(10, 10)], 'id': 'P81-1033.2'}, 'e2': {'word': 'flexible parser', 'word_index': [(1, 1)], 'id': 'P81-1033.1'}}	A ENTITYOTHER can deal with input that deviates from its ENTITY , in addition to input that conforms to it .
Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to.	construction-specific approach	flexible parsing	usage	{'e1': {'word': 'construction-specific approach', 'word_index': [(8, 8)], 'id': 'P81-1033.8'}, 'e2': {'word': 'flexible parsing', 'word_index': [(10, 10)], 'id': 'P81-1033.9'}}	ENTITYUNRELATED of this kind is facilitated by a ENTITY to ENTITYOTHER , with ENTITYUNRELATED for each type of ENTITYUNRELATED , and specialized ENTITYUNRELATED for each type of ENTITYUNRELATED that a particular ENTITYUNRELATED can give rise to .
Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to.	specialized parsing techniques	construction	usage	{'e1': {'word': 'specialized parsing techniques', 'word_index': [(13, 13)], 'id': 'P81-1033.10'}, 'e2': {'word': 'construction', 'word_index': [(18, 18)], 'id': 'P81-1033.11'}}	ENTITYUNRELATED of this kind is facilitated by a ENTITYUNRELATED to ENTITYUNRELATED , with ENTITY for each type of ENTITYOTHER , and specialized ENTITYUNRELATED for each type of ENTITYUNRELATED that a particular ENTITYUNRELATED can give rise to .
Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to.	ambiguity representations	ambiguity	model-feature	{'e1': {'word': 'ambiguity representations', 'word_index': [(22, 22)], 'id': 'P81-1033.12'}, 'e2': {'word': 'ambiguity', 'word_index': [(27, 27)], 'id': 'P81-1033.13'}}	ENTITYUNRELATED of this kind is facilitated by a ENTITYUNRELATED to ENTITYUNRELATED , with ENTITYUNRELATED for each type of ENTITYUNRELATED , and specialized ENTITY for each type of ENTITYOTHER that a particular ENTITYUNRELATED can give rise to .
A construction-specific approach also aids in task-specific language development by allowing a language definition that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism, thus greatly speeding the testing of changes to the language definition.	construction-specific approach	task-specific language development	usage	{'e1': {'word': 'construction-specific approach', 'word_index': [(1, 1)], 'id': 'P81-1033.15'}, 'e2': {'word': 'task-specific language development', 'word_index': [(5, 5)], 'id': 'P81-1033.16'}}	A ENTITY also aids in ENTITYOTHER by allowing a ENTITYUNRELATED that is natural in terms of the ENTITYUNRELATED to be interpreted directly without compilation into a ENTITYUNRELATED , thus greatly speeding the ENTITYUNRELATED of changes to the ENTITYUNRELATED .
Building on previous work at Carnegie-Mellon University e.g. [4, 5, 8], Plume's approach to parsing is based on semantic caseframe instantiation.	semantic caseframe instantiation	Plume's approach to parsing	usage	"{'e1': {'word': 'semantic caseframe instantiation', 'word_index': [(21, 21)], 'id': 'P85-1019.4'}, 'e2': {'word': ""Plume's approach to parsing"", 'word_index': [(17, 17)], 'id': 'P85-1019.3'}}"	Building on previous work at Carnegie- Mellon University e.g. [ 4 , 5 , 8 ] , ENTITYOTHER is based on ENTITY .
While Plume is well adapted to simple declarative and imperative utterances, it handles passives, relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage.	Plume	syntactic coverage	result	{'e1': {'word': 'Plume', 'word_index': [(1, 1)], 'id': 'P85-1019.9'}, 'e2': {'word': 'syntactic coverage', 'word_index': [(24, 24)], 'id': 'P85-1019.14'}}	While ENTITY is well adapted to simple ENTITYUNRELATED , it handles ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED in an ad hoc manner leading to patchy ENTITYOTHER .
Languages differ in the concepts and real-world entities for which they have words and grammatical constructs.	words	Languages	part_whole	{'e1': {'word': 'words', 'word_index': [(11, 11)], 'id': 'P91-1025.4'}, 'e2': {'word': 'Languages', 'word_index': [(0, 0)], 'id': 'P91-1025.1'}}	ENTITYOTHER differ in the ENTITYUNRELATED and ENTITYUNRELATED for which they have ENTITY and ENTITYUNRELATED .
Therefore translation must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language.	meaning	source language text	model-feature	{'e1': {'word': 'meaning', 'word_index': [(10, 10)], 'id': 'P91-1025.7'}, 'e2': {'word': 'source language text', 'word_index': [(13, 13)], 'id': 'P91-1025.8'}}	Therefore ENTITYUNRELATED must sometimes be a matter of approximating the ENTITY of a ENTITYOTHER rather than finding an exact counterpart in the ENTITYUNRELATED .
We propose a translation framework based on Situation Theory.	Situation Theory	translation framework	usage	{'e1': {'word': 'Situation Theory', 'word_index': [(6, 6)], 'id': 'P91-1025.11'}, 'e2': {'word': 'translation framework', 'word_index': [(3, 3)], 'id': 'P91-1025.10'}}	We propose a ENTITYOTHER based on ENTITY .
The basic ingredients are an information lattice, a representation scheme for utterances embedded in contexts, and a mismatch resolution scheme defined in terms of information flow.	representation scheme	utterances	model-feature	{'e1': {'word': 'representation scheme', 'word_index': [(8, 8)], 'id': 'P91-1025.13'}, 'e2': {'word': 'utterances', 'word_index': [(10, 10)], 'id': 'P91-1025.14'}}	The basic ingredients are an ENTITYUNRELATED , a ENTITY for ENTITYOTHER embedded in ENTITYUNRELATED , and a ENTITYUNRELATED defined in terms of ENTITYUNRELATED .
The basic ingredients are an information lattice, a representation scheme for utterances embedded in contexts, and a mismatch resolution scheme defined in terms of information flow.	information flow	mismatch resolution scheme	model-feature	{'e1': {'word': 'information flow', 'word_index': [(22, 22)], 'id': 'P91-1025.17'}, 'e2': {'word': 'mismatch resolution scheme', 'word_index': [(17, 17)], 'id': 'P91-1025.16'}}	The basic ingredients are an ENTITYUNRELATED , a ENTITYUNRELATED for ENTITYUNRELATED embedded in ENTITYUNRELATED , and a ENTITYOTHER defined in terms of ENTITY .
We motivate our approach with examples of translation between English and Japanese.	translation	English	usage	{'e1': {'word': 'translation', 'word_index': [(7, 7)], 'id': 'P91-1025.18'}, 'e2': {'word': 'English', 'word_index': [(9, 9)], 'id': 'P91-1025.19'}}	We motivate our approach with examples of ENTITY between ENTITYOTHER and ENTITYUNRELATED .
Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual.	knowledge	Large-scale natural language generation	usage	{'e1': {'word': 'knowledge', 'word_index': [(8, 8)], 'id': 'P95-1034.2'}, 'e2': {'word': 'Large-scale natural language generation', 'word_index': [(0, 0)], 'id': 'P95-1034.1'}}	ENTITYOTHER requires the integration of vast amounts of ENTITY : lexical , grammatical , and conceptual .
A robust generator must be able to operate well even when pieces of knowledge are missing.	knowledge	robust generator	usage	{'e1': {'word': 'knowledge', 'word_index': [(12, 12)], 'id': 'P95-1034.4'}, 'e2': {'word': 'robust generator', 'word_index': [(1, 1)], 'id': 'P95-1034.3'}}	A ENTITYOTHER must be able to operate well even when pieces of ENTITY are missing .
To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods.	statistical methods	hybrid generator	usage	{'e1': {'word': 'statistical methods', 'word_index': [(19, 19)], 'id': 'P95-1034.8'}, 'e2': {'word': 'hybrid generator', 'word_index': [(9, 9)], 'id': 'P95-1034.6'}}	To attack these problems , we have built a ENTITYOTHER , in which gaps in ENTITYUNRELATED are filled by ENTITY .
We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.</abstract>	portability	generators	model-feature	{'e1': {'word': 'portability', 'word_index': [(16, 16)], 'id': 'P95-1034.11'}, 'e2': {'word': 'generators', 'word_index': [(12, 12)], 'id': 'P95-1034.10'}}	We also discuss how the ENTITYUNRELATED can be used to simplify current ENTITYOTHER and enhance their ENTITY , even when perfect ENTITYUNRELATED is in principle obtainable . < / abstract >
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.	languages	alphabets	model-feature	{'e1': {'word': 'languages', 'word_index': [(9, 9)], 'id': 'P97-1017.3'}, 'e2': {'word': 'alphabets', 'word_index': [(12, 12)], 'id': 'P97-1017.4'}}	It is challenging to translate ENTITYUNRELATED and ENTITYUNRELATED across ENTITY with different ENTITYOTHER and ENTITYUNRELATED .
For example, computer in English comes out as ~ i/l:::'=--~-- (konpyuutaa) in Japanese.	English	Japanese	compare	{'e1': {'word': 'English', 'word_index': [(5, 5)], 'id': 'P97-1017.7'}, 'e2': {'word': 'Japanese', 'word_index': [(16, 16)], 'id': 'P97-1017.8'}}	For example , computer in ENTITY comes out as ~ i/ l:::'=--~-- ( konpyuutaa ) in ENTITYOTHER .
We describe and evaluate a method for performing backwards transliterations by machine.	machine	backwards transliterations	usage	{'e1': {'word': 'machine', 'word_index': [(10, 10)], 'id': 'P97-1017.14'}, 'e2': {'word': 'backwards transliterations', 'word_index': [(8, 8)], 'id': 'P97-1017.13'}}	We describe and evaluate a method for performing ENTITYOTHER by ENTITY .
This method uses a generative model, incorporating several distinct stages in the transliteration process.	generative model	transliteration process	usage	{'e1': {'word': 'generative model', 'word_index': [(4, 4)], 'id': 'P97-1017.15'}, 'e2': {'word': 'transliteration process', 'word_index': [(12, 12)], 'id': 'P97-1017.16'}}	This method uses a ENTITY , incorporating several distinct stages in the ENTITYOTHER .
Although adequate models of human language for syntactic analysis and semantic interpretation are of at least context-free complexity, for applications such as speech processing in which speed is important finite-state models are often preferred.	finite-state models	speech processing	usage	{'e1': {'word': 'finite-state models', 'word_index': [(25, 25)], 'id': 'P97-1058.6'}, 'e2': {'word': 'speech processing', 'word_index': [(19, 19)], 'id': 'P97-1058.5'}}	Although adequate models of ENTITYUNRELATED for ENTITYUNRELATED and ENTITYUNRELATED are of at least ENTITYUNRELATED , for applications such as ENTITYOTHER in which speed is important ENTITY are often preferred .
These requirements may be reconciled by using the more complex grammar to automatically derive a finite-state approximation which can then be used as a filter to guide speech recognition or to reject many hypotheses at an early stage of processing.	finite-state approximation	speech recognition	usage	{'e1': {'word': 'finite-state approximation', 'word_index': [(15, 15)], 'id': 'P97-1058.8'}, 'e2': {'word': 'speech recognition', 'word_index': [(26, 26)], 'id': 'P97-1058.9'}}	These requirements may be reconciled by using the more complex ENTITYUNRELATED to automatically derive a ENTITY which can then be used as a filter to guide ENTITYOTHER or to reject many hypotheses at an early stage of processing .
We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word.	statistical model	Japanese unknown words	model-feature	{'e1': {'word': 'statistical model', 'word_index': [(3, 3)], 'id': 'P99-1036.1'}, 'e2': {'word': 'Japanese unknown words', 'word_index': [(5, 5)], 'id': 'P99-1036.2'}}	We present a ENTITY of ENTITYOTHER consisting of a set of ENTITYUNRELATED classified by the ENTITYUNRELATED that constitute a ENTITYUNRELATED .
We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word.	character types	word	part_whole	{'e1': {'word': 'character types', 'word_index': [(15, 15)], 'id': 'P99-1036.4'}, 'e2': {'word': 'word', 'word_index': [(19, 19)], 'id': 'P99-1036.5'}}	We present a ENTITYUNRELATED of ENTITYUNRELATED consisting of a set of ENTITYUNRELATED classified by the ENTITY that constitute a ENTITYOTHER .
The point is quite simple: different character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana).	ideograms	Japanese script	part_whole	{'e1': {'word': 'ideograms', 'word_index': [(24, 24)], 'id': 'P99-1036.9'}, 'e2': {'word': 'Japanese script', 'word_index': [(21, 21)], 'id': 'P99-1036.8'}}	The point is quite simple : different ENTITYUNRELATED should be treated differently and the changes between ENTITYUNRELATED are very important because ENTITYOTHER has both ENTITY like ENTITYUNRELATED ( ENTITYUNRELATED ) and ENTITYUNRELATED like ENTITYUNRELATED ( ENTITYUNRELATED ) .
This paper discusses a decision-tree approach to the problem of assigning probabilities to words following a given text.	decision-tree approach	probabilities	usage	{'e1': {'word': 'decision-tree approach', 'word_index': [(4, 4)], 'id': 'P99-1080.1'}, 'e2': {'word': 'probabilities', 'word_index': [(10, 10)], 'id': 'P99-1080.2'}}	This paper discusses a ENTITY to the problem of assigning ENTITYOTHER to ENTITYUNRELATED following a given ENTITYUNRELATED .
This paper discusses a decision-tree approach to the problem of assigning probabilities to words following a given text.	words	text	part_whole	{'e1': {'word': 'words', 'word_index': [(12, 12)], 'id': 'P99-1080.3'}, 'e2': {'word': 'text', 'word_index': [(16, 16)], 'id': 'P99-1080.4'}}	This paper discusses a ENTITYUNRELATED to the problem of assigning ENTITYUNRELATED to ENTITY following a given ENTITYOTHER .
This poster paper describes a full scale two-level morphological description (Karttunen, 1983; Koskenniemi, 1983) of Turkish word structures.	full scale two-level morphological description	Turkish word structures	model-feature	{'e1': {'word': 'full scale two-level morphological description', 'word_index': [(5, 5)], 'id': 'E93-1066.1'}, 'e2': {'word': 'Turkish word structures', 'word_index': [(16, 16)], 'id': 'E93-1066.2'}}	This poster paper describes a ENTITY ( Karttunen , 1983 ; Koskenniemi , 1983 ) of ENTITYOTHER .
The description has been implemented using the PC-KIMMO environment (Antworth, 1990) and is based on a root word lexicon of about 23,000 roots words.	roots words	root word lexicon	part_whole	{'e1': {'word': 'roots words', 'word_index': [(22, 22)], 'id': 'E93-1066.5'}, 'e2': {'word': 'root word lexicon', 'word_index': [(18, 18)], 'id': 'E93-1066.4'}}	The description has been implemented using the ENTITYUNRELATED ( Antworth , 1990 ) and is based on a ENTITYOTHER of about 23,000 ENTITY .
Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words.	Turkish	agglutinative language	model-feature	{'e1': {'word': 'Turkish', 'word_index': [(0, 0)], 'id': 'E93-1066.7'}, 'e2': {'word': 'agglutinative language', 'word_index': [(3, 3)], 'id': 'E93-1066.8'}}	ENTITY is an ENTITYOTHER with ENTITYUNRELATED formed by ENTITYUNRELATED to ENTITYUNRELATED .
The surface realizations of morphological constructions are constrained and modified by a number of phonetic rules such as vowel harmony.	surface realizations	morphological constructions	model-feature	{'e1': {'word': 'surface realizations', 'word_index': [(1, 1)], 'id': 'E93-1066.21'}, 'e2': {'word': 'morphological constructions', 'word_index': [(3, 3)], 'id': 'E93-1066.22'}}	The ENTITY of ENTITYOTHER are constrained and modified by a number of ENTITYUNRELATED such as ENTITYUNRELATED .
The TIPSTER Architecture has been designed to enable a variety of different text applications to use a set of common text processing modules.	common text processing modules	text applications	usage	{'e1': {'word': 'common text processing modules', 'word_index': [(17, 17)], 'id': 'X96-1041.3'}, 'e2': {'word': 'text applications', 'word_index': [(11, 11)], 'id': 'X96-1041.2'}}	The ENTITYUNRELATED has been designed to enable a variety of different ENTITYOTHER to use a set of ENTITY .
Since user interfaces work best when customized for particular applications , it is appropriator that no particular user interface styles or conventions are described in the TIPSTER Architecture specification.	TIPSTER Architecture specification	user interface styles or conventions	topic	{'e1': {'word': 'TIPSTER Architecture specification', 'word_index': [(20, 20)], 'id': 'X96-1041.7'}, 'e2': {'word': 'user interface styles or conventions', 'word_index': [(15, 15)], 'id': 'X96-1041.6'}}	Since ENTITYUNRELATED work best when customized for ENTITYUNRELATED , it is appropriator that no particular ENTITYOTHER are described in the ENTITY .
However, the Computing Research Laboratory (CRL) has constructed several TIPSTER applications that use a common set of configurable Graphical User Interface (GUI) functions.	Graphical User Interface (GUI) functions	TIPSTER applications	usage	{'e1': {'word': 'Graphical User Interface (GUI) functions', 'word_index': [(15, 15)], 'id': 'X96-1041.10'}, 'e2': {'word': 'TIPSTER applications', 'word_index': [(7, 7)], 'id': 'X96-1041.9'}}	However , the ENTITYUNRELATED has constructed several ENTITYOTHER that use a common set of configurable ENTITY .
These GUIs were constructed using CRL's TIPSTER User Interface Toolkit (TUIT).	CRL's TIPSTER User Interface Toolkit (TUIT)	GUIs	usage	"{'e1': {'word': ""CRL's TIPSTER User Interface Toolkit (TUIT)"", 'word_index': [(5, 5)], 'id': 'X96-1041.12'}, 'e2': {'word': 'GUIs', 'word_index': [(1, 1)], 'id': 'X96-1041.11'}}"	These ENTITYOTHER were constructed using ENTITY .
TUIT is a software library that can be used to construct multilingual TIPSTER user interfaces for a set of common user tasks.	software library	multilingual TIPSTER user interfaces	usage	{'e1': {'word': 'software library', 'word_index': [(3, 3)], 'id': 'X96-1041.14'}, 'e2': {'word': 'multilingual TIPSTER user interfaces', 'word_index': [(10, 10)], 'id': 'X96-1041.15'}}	ENTITYUNRELATED is a ENTITY that can be used to construct ENTITYOTHER for a set of common user tasks .
This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish.	shallow techniques	deep processing	usage	{'e1': {'word': 'shallow techniques', 'word_index': [(10, 10)], 'id': 'C02-1071.2'}, 'e2': {'word': 'deep processing', 'word_index': [(6, 6)], 'id': 'C02-1071.1'}}	This paper describes to what extent ENTITYOTHER may benefit from ENTITY and it presents a ENTITYUNRELATED which integrates a ENTITYUNRELATED as a preprocessing module of a ENTITYUNRELATED .
This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish.	linguistic PoS tagger and chunker	NLP system	part_whole	{'e1': {'word': 'linguistic PoS tagger and chunker', 'word_index': [(19, 19)], 'id': 'C02-1071.4'}, 'e2': {'word': 'NLP system', 'word_index': [(15, 15)], 'id': 'C02-1071.3'}}	This paper describes to what extent ENTITYUNRELATED may benefit from ENTITYUNRELATED and it presents a ENTITYOTHER which integrates a ENTITY as a preprocessing module of a ENTITYUNRELATED .
Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the precision of the grammar.	robustness	linguistic processing	model-feature	{'e1': {'word': 'robustness', 'word_index': [(17, 17)], 'id': 'C02-1071.7'}, 'e2': {'word': 'linguistic processing', 'word_index': [(20, 20)], 'id': 'C02-1071.8'}}	Experiments show that the ENTITYUNRELATED of the overall analysis improves significantly and that our system also provides ENTITY to the ENTITYOTHER while maintaining both the ENTITYUNRELATED and the ENTITYUNRELATED of the ENTITYUNRELATED .
Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the precision of the grammar.	grammar	precision	result	{'e1': {'word': 'grammar', 'word_index': [(31, 31)], 'id': 'C02-1071.11'}, 'e2': {'word': 'precision', 'word_index': [(28, 28)], 'id': 'C02-1071.10'}}	Experiments show that the ENTITYUNRELATED of the overall analysis improves significantly and that our system also provides ENTITYUNRELATED to the ENTITYUNRELATED while maintaining both the ENTITYUNRELATED and the ENTITYOTHER of the ENTITY .
In this paper, we compare the performance of a state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken language and in generating sub-categorization cues from written and spoken language.	statistical parser	written and spoken language	usage	{'e1': {'word': 'statistical parser', 'word_index': [(17, 17)], 'id': 'P06-2067.1'}, 'e2': {'word': 'written and spoken language', 'word_index': [(25, 25)], 'id': 'P06-2067.2'}}	In this paper , we compare the performance of a state - of - the - art ENTITY ( Bikel , 2004 ) in parsing ENTITYOTHER and in generating ENTITYUNRELATED from ENTITYUNRELATED .
Although Bikel's parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language.	Bikel's parser	accuracy	result	"{'e1': {'word': ""Bikel's parser"", 'word_index': [(1, 1)], 'id': 'P06-2067.5'}, 'e2': {'word': 'accuracy', 'word_index': [(5, 5)], 'id': 'P06-2067.6'}}"	Although ENTITY achieves a higher ENTITYOTHER for parsing ENTITYUNRELATED , it achieves a higher ENTITYUNRELATED when extracting ENTITYUNRELATED from ENTITYUNRELATED .
Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language.	extracting subcategorization frames	written texts	usage	{'e1': {'word': 'extracting subcategorization frames', 'word_index': [(8, 8)], 'id': 'P06-2067.11'}, 'e2': {'word': 'written texts', 'word_index': [(12, 12)], 'id': 'P06-2067.12'}}	Our experiments also show that current technology for ENTITY initially designed for ENTITYOTHER works equally well for ENTITYUNRELATED .
Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues.	punctuation	parsing	usage	{'e1': {'word': 'punctuation', 'word_index': [(7, 7)], 'id': 'P06-2067.14'}, 'e2': {'word': 'parsing', 'word_index': [(10, 10)], 'id': 'P06-2067.15'}}	Additionally , we explore the utility of ENTITY in helping ENTITYOTHER and ENTITYUNRELATED of ENTITYUNRELATED .
Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language.	subcategorization cues	spoken language	part_whole	{'e1': {'word': 'subcategorization cues', 'word_index': [(14, 14)], 'id': 'P06-2067.20'}, 'e2': {'word': 'spoken language', 'word_index': [(16, 16)], 'id': 'P06-2067.21'}}	Our experiments show that ENTITYUNRELATED is of little help in parsing ENTITYUNRELATED and extracting ENTITY from ENTITYOTHER .
This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers.	punctuation	spoken corpora	part_whole	{'e1': {'word': 'punctuation', 'word_index': [(9, 9)], 'id': 'P06-2067.22'}, 'e2': {'word': 'spoken corpora', 'word_index': [(12, 12)], 'id': 'P06-2067.23'}}	This indicates that there is no need to add ENTITY in transcribing ENTITYOTHER simply in order to help ENTITYUNRELATED .
This paper describes a characters-based Chinese collocation system and discusses the advantages of it over a traditional word-based system.	characters-based Chinese collocation system	word-based system	compare	{'e1': {'word': 'characters-based Chinese collocation system', 'word_index': [(4, 4)], 'id': 'C94-1088.1'}, 'e2': {'word': 'word-based system', 'word_index': [(14, 14)], 'id': 'C94-1088.2'}}	This paper describes a ENTITY and discusses the advantages of it over a traditional ENTITYOTHER .
Since wordbreaks are not conventionally marked in Chinese text corpora, a character-based collocation system has the dual advantages of avoiding pre-processing distortion and directly accessing sub-lexical information.	wordbreaks	Chinese text corpora	part_whole	{'e1': {'word': 'wordbreaks', 'word_index': [(1, 1)], 'id': 'C94-1088.3'}, 'e2': {'word': 'Chinese text corpora', 'word_index': [(7, 7)], 'id': 'C94-1088.4'}}	Since ENTITY are not conventionally marked in ENTITYOTHER , a ENTITYUNRELATED has the dual advantages of avoiding ENTITYUNRELATED and directly accessing ENTITYUNRELATED .
Furthermore, word-based collocational properties can be obtained through an auxiliary module of automatic segmentation.	automatic segmentation	word-based collocational properties	usage	{'e1': {'word': 'automatic segmentation', 'word_index': [(11, 11)], 'id': 'C94-1088.9'}, 'e2': {'word': 'word-based collocational properties', 'word_index': [(2, 2)], 'id': 'C94-1088.8'}}	Furthermore , ENTITYOTHER can be obtained through an auxiliary module of ENTITY .
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.	bit-vector-based CKY-style parser	context-free parsing	usage	{'e1': {'word': 'bit-vector-based CKY-style parser', 'word_index': [(2, 2)], 'id': 'C04-1024.1'}, 'e2': {'word': 'context-free parsing', 'word_index': [(4, 4)], 'id': 'C04-1024.2'}}	An efficient ENTITY for ENTITYOTHER is presented .
The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences.	parse forest representation	analyses for large treebank grammars	model-feature	{'e1': {'word': 'parse forest representation', 'word_index': [(5, 5)], 'id': 'C04-1024.4'}, 'e2': {'word': 'analyses for large treebank grammars', 'word_index': [(12, 12)], 'id': 'C04-1024.5'}}	The ENTITYUNRELATED computes a compact ENTITY of the complete set of possible ENTITYOTHER and long ENTITYUNRELATED .
The parser uses bit-vector operations to parallelise the basic parsing operations.	bit-vector operations	parser	usage	{'e1': {'word': 'bit-vector operations', 'word_index': [(3, 3)], 'id': 'C04-1024.8'}, 'e2': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'C04-1024.7'}}	The ENTITYOTHER uses ENTITY to parallelise the ENTITYUNRELATED .
We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.	language model	answers	usage	{'e1': {'word': 'language model', 'word_index': [(16, 16)], 'id': 'N04-1008.4'}, 'e2': {'word': 'answers', 'word_index': [(18, 18)], 'id': 'N04-1008.5'}}	We focus on ENTITYUNRELATED , and build our system around a ENTITYUNRELATED which exploits both a ENTITY for ENTITYOTHER and a ENTITYUNRELATED for ENTITYUNRELATED , trained on a ENTITYUNRELATED of 1 million ENTITYUNRELATED collected from the Web .
We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.	transformation model	answer/question terms	usage	{'e1': {'word': 'transformation model', 'word_index': [(21, 21)], 'id': 'N04-1008.6'}, 'e2': {'word': 'answer/question terms', 'word_index': [(23, 23)], 'id': 'N04-1008.7'}}	We focus on ENTITYUNRELATED , and build our system around a ENTITYUNRELATED which exploits both a ENTITYUNRELATED for ENTITYUNRELATED and a ENTITY for ENTITYOTHER , trained on a ENTITYUNRELATED of 1 million ENTITYUNRELATED collected from the Web .
We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.	question/answer pairs	corpus	part_whole	{'e1': {'word': 'question/answer pairs', 'word_index': [(32, 32)], 'id': 'N04-1008.9'}, 'e2': {'word': 'corpus', 'word_index': [(28, 28)], 'id': 'N04-1008.8'}}	We focus on ENTITYUNRELATED , and build our system around a ENTITYUNRELATED which exploits both a ENTITYUNRELATED for ENTITYUNRELATED and a ENTITYUNRELATED for ENTITYUNRELATED , trained on a ENTITYOTHER of 1 million ENTITY collected from the Web .
The applicability of many current information extraction techniques is severely limited by the need for supervised training data.	supervised training data	information extraction techniques	usage	{'e1': {'word': 'supervised training data', 'word_index': [(13, 13)], 'id': 'P05-1046.2'}, 'e2': {'word': 'information extraction techniques', 'word_index': [(5, 5)], 'id': 'P05-1046.1'}}	The applicability of many current ENTITYOTHER is severely limited by the need for ENTITY .
We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion.	prior knowledge	field structured extraction tasks	usage	{'e1': {'word': 'prior knowledge', 'word_index': [(18, 18)], 'id': 'P05-1046.4'}, 'e2': {'word': 'field structured extraction tasks', 'word_index': [(5, 5)], 'id': 'P05-1046.3'}}	We demonstrate that for certain ENTITYOTHER , such as classified advertisements and bibliographic citations , small amounts of ENTITY can be used to learn effective models in a primarily unsupervised fashion .
Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains.	generative model	field structured text	model-feature	{'e1': {'word': 'generative model', 'word_index': [(5, 5)], 'id': 'P05-1046.6'}, 'e2': {'word': 'field structured text', 'word_index': [(7, 7)], 'id': 'P05-1046.7'}}	Although ENTITYUNRELATED provide a suitable ENTITY for ENTITYOTHER , general ENTITYUNRELATED fails to learn useful structure in either of our domains .
In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.	unsupervised methods	accuracies	result	{'e1': {'word': 'unsupervised methods', 'word_index': [(7, 7)], 'id': 'P05-1046.10'}, 'e2': {'word': 'accuracies', 'word_index': [(10, 10)], 'id': 'P05-1046.11'}}	In both domains , we found that ENTITY can attain ENTITYOTHER with 400 ENTITYUNRELATED comparable to those attained by ENTITYUNRELATED on 50 ENTITYUNRELATED , and that ENTITYUNRELATED can make good use of small amounts of ENTITYUNRELATED .
In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.	labeled data	semi-supervised methods	usage	{'e1': {'word': 'labeled data', 'word_index': [(35, 35)], 'id': 'P05-1046.16'}, 'e2': {'word': 'semi-supervised methods', 'word_index': [(26, 26)], 'id': 'P05-1046.15'}}	In both domains , we found that ENTITYUNRELATED can attain ENTITYUNRELATED with 400 ENTITYUNRELATED comparable to those attained by ENTITYUNRELATED on 50 ENTITYUNRELATED , and that ENTITYOTHER can make good use of small amounts of ENTITY .
Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.	independent classifiers	semantic role labeling	usage	{'e1': {'word': 'independent classifiers', 'word_index': [(13, 13)], 'id': 'P05-1073.2'}, 'e2': {'word': 'semantic role labeling', 'word_index': [(6, 6)], 'id': 'P05-1073.1'}}	Despite much recent progress on accurate ENTITYOTHER , previous work has largely used ENTITY , possibly combined with separate ENTITYUNRELATED via ENTITYUNRELATED .
This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.	dependencies	arguments	model-feature	{'e1': {'word': 'dependencies', 'word_index': [(19, 19)], 'id': 'P05-1073.6'}, 'e2': {'word': 'arguments', 'word_index': [(21, 21)], 'id': 'P05-1073.7'}}	This stands in stark contrast to the linguistic observation that a ENTITYUNRELATED is a joint structure , with strong ENTITY between ENTITYOTHER .
We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.	joint model	argument frames	model-feature	{'e1': {'word': 'joint model', 'word_index': [(6, 6)], 'id': 'P05-1073.8'}, 'e2': {'word': 'argument frames', 'word_index': [(8, 8)], 'id': 'P05-1073.9'}}	We show how to build a ENTITY of ENTITYOTHER , incorporating novel ENTITYUNRELATED that model these interactions into ENTITYUNRELATED .
We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.	features	discriminative log-linear models	part_whole	{'e1': {'word': 'features', 'word_index': [(12, 12)], 'id': 'P05-1073.10'}, 'e2': {'word': 'discriminative log-linear models', 'word_index': [(18, 18)], 'id': 'P05-1073.11'}}	We show how to build a ENTITYUNRELATED of ENTITYUNRELATED , incorporating novel ENTITY that model these interactions into ENTITYOTHER .
This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.	gold-standard parse trees	PropBank	part_whole	{'e1': {'word': 'gold-standard parse trees', 'word_index': [(27, 27)], 'id': 'P05-1073.16'}, 'e2': {'word': 'PropBank', 'word_index': [(29, 29)], 'id': 'P05-1073.17'}}	This system achieves an ENTITYUNRELATED of 22 % on all ENTITYUNRELATED and 32 % on ENTITYUNRELATED over a state - of - the art independent ENTITYUNRELATED for ENTITY on ENTITYOTHER .
It enables us to select a concise set of reading texts (from a target corpus) that contains all the target vocabulary to be learned.	texts	target corpus	part_whole	{'e1': {'word': 'texts', 'word_index': [(10, 10)], 'id': 'P05-3030.2'}, 'e2': {'word': 'target corpus', 'word_index': [(14, 14)], 'id': 'P05-3030.3'}}	It enables us to select a concise set of reading ENTITY ( from a ENTITYOTHER ) that contains all the ENTITYUNRELATED to be learned .
We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia, a free-content encyclopedia, as the target corpus.	vocabulary	target vocabulary	usage	{'e1': {'word': 'vocabulary', 'word_index': [(4, 4)], 'id': 'P05-3030.5'}, 'e2': {'word': 'target vocabulary', 'word_index': [(12, 12)], 'id': 'P05-3030.6'}}	We used a specialized ENTITY for an English certification test as the ENTITYOTHER and used ENTITYUNRELATED , a free - content encyclopedia , as the ENTITYUNRELATED .
We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia, a free-content encyclopedia, as the target corpus.	English Wikipedia	target corpus	usage	{'e1': {'word': 'English Wikipedia', 'word_index': [(15, 15)], 'id': 'P05-3030.7'}, 'e2': {'word': 'target corpus', 'word_index': [(25, 25)], 'id': 'P05-3030.8'}}	We used a specialized ENTITYUNRELATED for an English certification test as the ENTITYUNRELATED and used ENTITY , a free - content encyclopedia , as the ENTITYOTHER .
Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary.	Procedural Systemic Grammar	syntactic analyzer	usage	{'e1': {'word': 'Procedural Systemic Grammar', 'word_index': [(17, 17)], 'id': 'E83-1029.3'}, 'e2': {'word': 'syntactic analyzer', 'word_index': [(12, 12)], 'id': 'E83-1029.2'}}	Specifically , the following components of the system are described : the ENTITYOTHER , based on a ENTITY , the ENTITYUNRELATED relying on the ENTITYUNRELATED , and the ENTITYUNRELATED .
Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary.	Conceptual Dependency Theory	semantic analyzer	usage	{'e1': {'word': 'Conceptual Dependency Theory', 'word_index': [(24, 24)], 'id': 'E83-1029.5'}, 'e2': {'word': 'semantic analyzer', 'word_index': [(20, 20)], 'id': 'E83-1029.4'}}	Specifically , the following components of the system are described : the ENTITYUNRELATED , based on a ENTITYUNRELATED , the ENTITYOTHER relying on the ENTITY , and the ENTITYUNRELATED .
A proposal to deal with French tenses in the framework of Discourse Representation Theory is presented, as it has been implemented for a fragment at the IMS.	Discourse Representation Theory	French tenses	model-feature	{'e1': {'word': 'Discourse Representation Theory', 'word_index': [(10, 10)], 'id': 'E89-1006.2'}, 'e2': {'word': 'French tenses', 'word_index': [(5, 5)], 'id': 'E89-1006.1'}}	A proposal to deal with ENTITYOTHER in the framework of ENTITY is presented , as it has been implemented for a fragment at the ENTITYUNRELATED .
Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text.	meaning	tenses	model-feature	{'e1': {'word': 'meaning', 'word_index': [(7, 7)], 'id': 'E89-1006.6'}, 'e2': {'word': 'tenses', 'word_index': [(10, 10)], 'id': 'E89-1006.7'}}	Instead of using ENTITYUNRELATED to express the ENTITY of the ENTITYOTHER the Reichenbachian point of view is adopted and refined such that the impact of the ENTITYUNRELATED with respect to the ENTITYUNRELATED of the ENTITYUNRELATED is understood as contribution to the integration of the ENTITYUNRELATED of a ENTITYUNRELATED in the ENTITYUNRELATED of the preceeding ENTITYUNRELATED .
Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text.	meaning	text	model-feature	{'e1': {'word': 'meaning', 'word_index': [(31, 31)], 'id': 'E89-1006.9'}, 'e2': {'word': 'text', 'word_index': [(34, 34)], 'id': 'E89-1006.10'}}	Instead of using ENTITYUNRELATED to express the ENTITYUNRELATED of the ENTITYUNRELATED the Reichenbachian point of view is adopted and refined such that the impact of the ENTITYUNRELATED with respect to the ENTITY of the ENTITYOTHER is understood as contribution to the integration of the ENTITYUNRELATED of a ENTITYUNRELATED in the ENTITYUNRELATED of the preceeding ENTITYUNRELATED .
Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text.	events	sentence	part_whole	{'e1': {'word': 'events', 'word_index': [(44, 44)], 'id': 'E89-1006.11'}, 'e2': {'word': 'sentence', 'word_index': [(47, 47)], 'id': 'E89-1006.12'}}	Instead of using ENTITYUNRELATED to express the ENTITYUNRELATED of the ENTITYUNRELATED the Reichenbachian point of view is adopted and refined such that the impact of the ENTITYUNRELATED with respect to the ENTITYUNRELATED of the ENTITYUNRELATED is understood as contribution to the integration of the ENTITY of a ENTITYOTHER in the ENTITYUNRELATED of the preceeding ENTITYUNRELATED .
Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text.	event structure	text	model-feature	{'e1': {'word': 'event structure', 'word_index': [(50, 50)], 'id': 'E89-1006.13'}, 'e2': {'word': 'text', 'word_index': [(54, 54)], 'id': 'E89-1006.14'}}	Instead of using ENTITYUNRELATED to express the ENTITYUNRELATED of the ENTITYUNRELATED the Reichenbachian point of view is adopted and refined such that the impact of the ENTITYUNRELATED with respect to the ENTITYUNRELATED of the ENTITYUNRELATED is understood as contribution to the integration of the ENTITYUNRELATED of a ENTITYUNRELATED in the ENTITY of the preceeding ENTITYOTHER .
Thereby a system of relevant times provided by the preceeding text and by the temporal adverbials of the sentence being processed is used.	temporal adverbials	sentence	part_whole	{'e1': {'word': 'temporal adverbials', 'word_index': [(11, 11)], 'id': 'E89-1006.17'}, 'e2': {'word': 'sentence', 'word_index': [(14, 14)], 'id': 'E89-1006.18'}}	Thereby a ENTITYUNRELATED provided by the preceeding ENTITYUNRELATED and by the ENTITY of the ENTITYOTHER being processed is used .
In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the resolution component and not in the process of syntactic analysis.	meaning	tenses	model-feature	{'e1': {'word': 'meaning', 'word_index': [(11, 11)], 'id': 'E89-1006.26'}, 'e2': {'word': 'tenses', 'word_index': [(14, 14)], 'id': 'E89-1006.27'}}	In opposition to the approach of Kamp and Rohrer the exact ENTITY of the ENTITYOTHER is fixed by the ENTITYUNRELATED and not in the process of ENTITYUNRELATED .
In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the resolution component and not in the process of syntactic analysis.	resolution component	syntactic analysis	compare	{'e1': {'word': 'resolution component', 'word_index': [(19, 19)], 'id': 'E89-1006.28'}, 'e2': {'word': 'syntactic analysis', 'word_index': [(26, 26)], 'id': 'E89-1006.29'}}	In opposition to the approach of Kamp and Rohrer the exact ENTITYUNRELATED of the ENTITYUNRELATED is fixed by the ENTITY and not in the process of ENTITYOTHER .
The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously, and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT (LF).	languages	grammatical frameworks	model-feature	{'e1': {'word': 'languages', 'word_index': [(5, 5)], 'id': 'E93-1004.7'}, 'e2': {'word': 'grammatical frameworks', 'word_index': [(12, 12)], 'id': 'E93-1004.8'}}	The motivation for introducing these ENTITY is to provide tools for formalising ENTITYOTHER perspicuously , and the paper illustrates this by showing how the leading ideas of ENTITYUNRELATED can be captured in ENTITYUNRELATED .
The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously, and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT (LF).	GPSG	LT (LF)	compare	{'e1': {'word': 'GPSG', 'word_index': [(27, 27)], 'id': 'E93-1004.9'}, 'e2': {'word': 'LT (LF)', 'word_index': [(32, 32)], 'id': 'E93-1004.10'}}	The motivation for introducing these ENTITYUNRELATED is to provide tools for formalising ENTITYUNRELATED perspicuously , and the paper illustrates this by showing how the leading ideas of ENTITY can be captured in ENTITYOTHER .
One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality (EDOL).	Tree Adjoining Grammars	extended domain of locality (EDOL)	model-feature	{'e1': {'word': 'Tree Adjoining Grammars', 'word_index': [(6, 6)], 'id': 'E99-1029.1'}, 'e2': {'word': 'extended domain of locality (EDOL)', 'word_index': [(12, 12)], 'id': 'E99-1029.2'}}	One of the claimed benefits of ENTITY is that they have an ENTITYOTHER .
We consider how this can be exploited to limit the need for feature structure unification during parsing.	feature structure unification	parsing	usage	{'e1': {'word': 'feature structure unification', 'word_index': [(12, 12)], 'id': 'E99-1029.3'}, 'e2': {'word': 'parsing', 'word_index': [(14, 14)], 'id': 'E99-1029.4'}}	We consider how this can be exploited to limit the need for ENTITY during ENTITYOTHER .
We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways.	LEXSYS	XTAG	compare	{'e1': {'word': 'LEXSYS', 'word_index': [(8, 8)], 'id': 'E99-1029.6'}, 'e2': {'word': 'XTAG', 'word_index': [(10, 10)], 'id': 'E99-1029.7'}}	We compare two wide - coverage ENTITYUNRELATED , ENTITY and ENTITYOTHER , finding that the two ENTITYUNRELATED exploit ENTITYUNRELATED in different ways .
We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways.	EDOL	grammars	usage	{'e1': {'word': 'EDOL', 'word_index': [(18, 18)], 'id': 'E99-1029.9'}, 'e2': {'word': 'grammars', 'word_index': [(16, 16)], 'id': 'E99-1029.8'}}	We compare two wide - coverage ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED , finding that the two ENTITYOTHER exploit ENTITY in different ways .
We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model.	dependency-based grammar model	sentence-level and text-level anaphora	model-feature	{'e1': {'word': 'dependency-based grammar model', 'word_index': [(12, 12)], 'id': 'E95-1033.2'}, 'e2': {'word': 'sentence-level and text-level anaphora', 'word_index': [(6, 6)], 'id': 'E95-1033.1'}}	We provide a unified account of ENTITYOTHER within the framework of a ENTITY .
Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model.	Grosz-Sidner-style focus model	text-level anaphora	model-feature	{'e1': {'word': 'Grosz-Sidner-style focus model', 'word_index': [(21, 21)], 'id': 'E95-1033.7'}, 'e2': {'word': 'text-level anaphora', 'word_index': [(14, 14)], 'id': 'E95-1033.6'}}	Criteria for ENTITYUNRELATED within ENTITYUNRELATED rephrase major concepts from ENTITYUNRELATED , while those for ENTITYOTHER incorporate an adapted version of a ENTITY .
In contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering, our approach attempts to express the speech knowledge within a formal framework using well-defined mathematical tools.	knowledge engineering	heuristic rules	usage	{'e1': {'word': 'knowledge engineering', 'word_index': [(17, 17)], 'id': 'H89-1027.4'}, 'e2': {'word': 'heuristic rules', 'word_index': [(12, 12)], 'id': 'H89-1027.3'}}	In contrast to many of the past efforts that make use of ENTITYOTHER whose development requires intense ENTITY , our approach attempts to express the ENTITYUNRELATED within a formal framework using well - defined mathematical tools .
In our system, features and decision strategies are discovered and trained automatically, using a large body of speech data.	speech data	decision strategies	usage	{'e1': {'word': 'speech data', 'word_index': [(18, 18)], 'id': 'H89-1027.8'}, 'e2': {'word': 'decision strategies', 'word_index': [(6, 6)], 'id': 'H89-1027.7'}}	In our system , ENTITYUNRELATED and ENTITYOTHER are discovered and trained automatically , using a large body of ENTITY .
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses.	WordNet	sense resolution	usage	{'e1': {'word': 'WordNet', 'word_index': [(10, 10)], 'id': 'H91-1077.2'}, 'e2': {'word': 'sense resolution', 'word_index': [(3, 3)], 'id': 'H91-1077.1'}}	A method of ENTITYOTHER is proposed that is based on ENTITY , an on - line ENTITYUNRELATED that incorporates ENTITYUNRELATED ( ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED ) as ENTITYUNRELATED between ENTITYUNRELATED .
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses.	semantic relations	lexical database	part_whole	{'e1': {'word': 'semantic relations', 'word_index': [(19, 19)], 'id': 'H91-1077.4'}, 'e2': {'word': 'lexical database', 'word_index': [(16, 16)], 'id': 'H91-1077.3'}}	A method of ENTITYUNRELATED is proposed that is based on ENTITYUNRELATED , an on - line ENTITYOTHER that incorporates ENTITY ( ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED ) as ENTITYUNRELATED between ENTITYUNRELATED .
With WordNet, it is easy to retrieve sets of semantically related words, a facility that will be used for sense resolution during text processing, as follows.	semantically related words	WordNet	part_whole	{'e1': {'word': 'semantically related words', 'word_index': [(10, 10)], 'id': 'H91-1077.13'}, 'e2': {'word': 'WordNet', 'word_index': [(1, 1)], 'id': 'H91-1077.12'}}	With ENTITYOTHER , it is easy to retrieve sets of ENTITY , a facility that will be used for ENTITYUNRELATED during ENTITYUNRELATED , as follows .
With WordNet, it is easy to retrieve sets of semantically related words, a facility that will be used for sense resolution during text processing, as follows.	sense resolution	text processing	part_whole	{'e1': {'word': 'sense resolution', 'word_index': [(19, 19)], 'id': 'H91-1077.14'}, 'e2': {'word': 'text processing', 'word_index': [(21, 21)], 'id': 'H91-1077.15'}}	With ENTITYUNRELATED , it is easy to retrieve sets of ENTITYUNRELATED , a facility that will be used for ENTITY during ENTITYOTHER , as follows .
When a word with multiple senses is encountered, one of two procedures will be followed.	senses	word	model-feature	{'e1': {'word': 'senses', 'word_index': [(5, 5)], 'id': 'H91-1077.17'}, 'e2': {'word': 'word', 'word_index': [(2, 2)], 'id': 'H91-1077.16'}}	When a ENTITYOTHER with multiple ENTITY is encountered , one of two procedures will be followed .
Either, (1) words related in meaning to the alternative senses of the polysemous word will be retrieved; new strings will be derived by substituting these related words into the context of the polysemous word; a large textual corpus will then be searched for these derived strings; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus.	derived strings	textual corpus	part_whole	{'e1': {'word': 'derived strings', 'word_index': [(45, 45)], 'id': 'H91-1077.27'}, 'e2': {'word': 'textual corpus', 'word_index': [(38, 38)], 'id': 'H91-1077.26'}}	Either , ( 1 ) ENTITYUNRELATED related in ENTITYUNRELATED to the ENTITYUNRELATED of the ENTITYUNRELATED will be retrieved ; new ENTITYUNRELATED will be derived by substituting these related ENTITYUNRELATED into the ENTITYUNRELATED of the ENTITYUNRELATED ; a large ENTITYOTHER will then be searched for these ENTITY ; and that ENTITYUNRELATED will be chosen that corresponds to the ENTITYUNRELATED that is found most often in the ENTITYUNRELATED .
Either, (1) words related in meaning to the alternative senses of the polysemous word will be retrieved; new strings will be derived by substituting these related words into the context of the polysemous word; a large textual corpus will then be searched for these derived strings; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus.	derived string	corpus	part_whole	{'e1': {'word': 'derived string', 'word_index': [(57, 57)], 'id': 'H91-1077.29'}, 'e2': {'word': 'corpus', 'word_index': [(65, 65)], 'id': 'H91-1077.30'}}	Either , ( 1 ) ENTITYUNRELATED related in ENTITYUNRELATED to the ENTITYUNRELATED of the ENTITYUNRELATED will be retrieved ; new ENTITYUNRELATED will be derived by substituting these related ENTITYUNRELATED into the ENTITYUNRELATED of the ENTITYUNRELATED ; a large ENTITYUNRELATED will then be searched for these ENTITYUNRELATED ; and that ENTITYUNRELATED will be chosen that corresponds to the ENTITY that is found most often in the ENTITYOTHER .
Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.	context	polysemous word	model-feature	{'e1': {'word': 'context', 'word_index': [(6, 6)], 'id': 'H91-1077.31'}, 'e2': {'word': 'polysemous word', 'word_index': [(9, 9)], 'id': 'H91-1077.32'}}	Or , ( 2 ) the ENTITY of the ENTITYOTHER will be used as a key to search a large ENTITYUNRELATED ; all ENTITYUNRELATED found to occur in that ENTITYUNRELATED will be noted ; ENTITYUNRELATED will then be used to estimate the ENTITYUNRELATED from those ENTITYUNRELATED to the ENTITYUNRELATED of the ENTITYUNRELATED ; and that ENTITYUNRELATED will be chosen that is closest in ENTITYUNRELATED to other ENTITYUNRELATED occurring in the same ENTITYUNRELATED If successful , this procedure could have practical applications to problems of ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and elsewhere .
Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.	context	words	model-feature	{'e1': {'word': 'context', 'word_index': [(29, 29)], 'id': 'H91-1077.35'}, 'e2': {'word': 'words', 'word_index': [(23, 23)], 'id': 'H91-1077.34'}}	Or , ( 2 ) the ENTITYUNRELATED of the ENTITYUNRELATED will be used as a key to search a large ENTITYUNRELATED ; all ENTITYOTHER found to occur in that ENTITY will be noted ; ENTITYUNRELATED will then be used to estimate the ENTITYUNRELATED from those ENTITYUNRELATED to the ENTITYUNRELATED of the ENTITYUNRELATED ; and that ENTITYUNRELATED will be chosen that is closest in ENTITYUNRELATED to other ENTITYUNRELATED occurring in the same ENTITYUNRELATED If successful , this procedure could have practical applications to problems of ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and elsewhere .
Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.	WordNet	semantic distance	usage	{'e1': {'word': 'WordNet', 'word_index': [(34, 34)], 'id': 'H91-1077.36'}, 'e2': {'word': 'semantic distance', 'word_index': [(42, 42)], 'id': 'H91-1077.37'}}	Or , ( 2 ) the ENTITYUNRELATED of the ENTITYUNRELATED will be used as a key to search a large ENTITYUNRELATED ; all ENTITYUNRELATED found to occur in that ENTITYUNRELATED will be noted ; ENTITY will then be used to estimate the ENTITYOTHER from those ENTITYUNRELATED to the ENTITYUNRELATED of the ENTITYUNRELATED ; and that ENTITYUNRELATED will be chosen that is closest in ENTITYUNRELATED to other ENTITYUNRELATED occurring in the same ENTITYUNRELATED If successful , this procedure could have practical applications to problems of ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and elsewhere .
Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.	context	words	model-feature	{'e1': {'word': 'context', 'word_index': [(71, 71)], 'id': 'H91-1077.44'}, 'e2': {'word': 'words', 'word_index': [(66, 66)], 'id': 'H91-1077.43'}}	Or , ( 2 ) the ENTITYUNRELATED of the ENTITYUNRELATED will be used as a key to search a large ENTITYUNRELATED ; all ENTITYUNRELATED found to occur in that ENTITYUNRELATED will be noted ; ENTITYUNRELATED will then be used to estimate the ENTITYUNRELATED from those ENTITYUNRELATED to the ENTITYUNRELATED of the ENTITYUNRELATED ; and that ENTITYUNRELATED will be chosen that is closest in ENTITYUNRELATED to other ENTITYOTHER occurring in the same ENTITY If successful , this procedure could have practical applications to problems of ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED , and elsewhere .
In this paper, we want to show how the morphological component of an existing NLP-system for Dutch (Dutch Medical Language Processor - DMLP) has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system (Linguistic String Project - Medical Language Processor) of the New York University.	morphological component	NLP-system for Dutch (Dutch Medical Language Processor - DMLP)	part_whole	{'e1': {'word': 'morphological component', 'word_index': [(10, 10)], 'id': 'A97-1027.1'}, 'e2': {'word': 'NLP-system for Dutch (Dutch Medical Language Processor - DMLP)', 'word_index': [(14, 14)], 'id': 'A97-1027.2'}}	In this paper , we want to show how the ENTITY of an existing ENTITYOTHER has been extended in order to produce output that is compatible with the ENTITYUNRELATED of the ENTITYUNRELATED of the New York University .
In this paper, we want to show how the morphological component of an existing NLP-system for Dutch (Dutch Medical Language Processor - DMLP) has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system (Linguistic String Project - Medical Language Processor) of the New York University.	language independent modules	LSP-MLP system (Linguistic String Project - Medical Language Processor)	part_whole	{'e1': {'word': 'language independent modules', 'word_index': [(28, 28)], 'id': 'A97-1027.3'}, 'e2': {'word': 'LSP-MLP system (Linguistic String Project - Medical Language Processor)', 'word_index': [(31, 31)], 'id': 'A97-1027.4'}}	In this paper , we want to show how the ENTITYUNRELATED of an existing ENTITYUNRELATED has been extended in order to produce output that is compatible with the ENTITY of the ENTITYOTHER of the New York University .
The former can take advantage of the language independent developments of the latter, while focusing on idiosyncrasies for Dutch.	idiosyncrasies	Dutch	part_whole	{'e1': {'word': 'idiosyncrasies', 'word_index': [(15, 15)], 'id': 'A97-1027.6'}, 'e2': {'word': 'Dutch', 'word_index': [(17, 17)], 'id': 'A97-1027.7'}}	The former can take advantage of the ENTITYUNRELATED of the latter , while focusing on ENTITY for ENTITYOTHER .
We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora.	textual corpora	subcategorization dictionary	usage	{'e1': {'word': 'textual corpora', 'word_index': [(13, 13)], 'id': 'A97-1052.2'}, 'e2': {'word': 'subcategorization dictionary', 'word_index': [(11, 11)], 'id': 'A97-1052.1'}}	We describe a novel technique and implemented system for constructing a ENTITYOTHER from ENTITY .
Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English.	relative frequency of occurrence	subcategorization classes	model-feature	{'e1': {'word': 'relative frequency of occurrence', 'word_index': [(4, 4)], 'id': 'A97-1052.4'}, 'e2': {'word': 'subcategorization classes', 'word_index': [(10, 10)], 'id': 'A97-1052.5'}}	Each ENTITYUNRELATED encodes the ENTITY of a comprehensive set of ENTITYOTHER for ENTITYUNRELATED .
An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes.	multiple complementation patterns	verbs	model-feature	{'e1': {'word': 'multiple complementation patterns', 'word_index': [(12, 12)], 'id': 'A97-1052.8'}, 'e2': {'word': 'verbs', 'word_index': [(9, 9)], 'id': 'A97-1052.7'}}	An initial experiment , on a sample of 14 ENTITYOTHER which exhibit ENTITY , demonstrates that the technique achieves ENTITYUNRELATED comparable to previous approaches , which are all limited to a highly restricted set of ENTITYUNRELATED .
We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount</abstract>	subcategorization dictionary	accuracy	result	{'e1': {'word': 'subcategorization dictionary', 'word_index': [(5, 5)], 'id': 'A97-1052.11'}, 'e2': {'word': 'accuracy', 'word_index': [(12, 12)], 'id': 'A97-1052.12'}}	We also demonstrate that a ENTITY built with the system improves the ENTITYOTHER of a ENTITYUNRELATED by an appreciable amount < / abstract >
This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns.	phrasal patterns	dictionary word sense definitions	usage	{'e1': {'word': 'phrasal patterns', 'word_index': [(13, 13)], 'id': 'J87-3001.2'}, 'e2': {'word': 'dictionary word sense definitions', 'word_index': [(4, 4)], 'id': 'J87-3001.1'}}	This paper shows how ENTITYOTHER can be analysed by applying a hierarchy of ENTITY .
An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English.	definitions	Longman Dictionary of Contemporary English	part_whole	{'e1': {'word': 'definitions', 'word_index': [(11, 11)], 'id': 'J87-3001.3'}, 'e2': {'word': 'Longman Dictionary of Contemporary English', 'word_index': [(14, 14)], 'id': 'J87-3001.4'}}	An experimental system embodying this mechanism has been implemented for processing ENTITY from the ENTITYOTHER .
A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions.	restricted vocabulary	word sense definitions	usage	{'e1': {'word': 'restricted vocabulary', 'word_index': [(16, 16)], 'id': 'J87-3001.6'}, 'e2': {'word': 'word sense definitions', 'word_index': [(19, 19)], 'id': 'J87-3001.7'}}	A property of this ENTITYUNRELATED , exploited by the system , is that it uses a ENTITY in its ENTITYOTHER .
The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary.	senses	word senses	model-feature	{'e1': {'word': 'senses', 'word_index': [(22, 22)], 'id': 'J87-3001.10'}, 'e2': {'word': 'word senses', 'word_index': [(17, 17)], 'id': 'J87-3001.9'}}	The structures generated by the experimental system are intended to be used for the ENTITYUNRELATED of new ENTITYOTHER in terms of the ENTITY of ENTITYUNRELATED in the ENTITYUNRELATED .
The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary.	words	restricted vocabulary	part_whole	{'e1': {'word': 'words', 'word_index': [(24, 24)], 'id': 'J87-3001.11'}, 'e2': {'word': 'restricted vocabulary', 'word_index': [(27, 27)], 'id': 'J87-3001.12'}}	The structures generated by the experimental system are intended to be used for the ENTITYUNRELATED of new ENTITYUNRELATED in terms of the ENTITYUNRELATED of ENTITY in the ENTITYOTHER .
This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism.	analysis mechanism	definitions	topic	{'e1': {'word': 'analysis mechanism', 'word_index': [(24, 24)], 'id': 'J87-3001.17'}, 'e2': {'word': 'definitions', 'word_index': [(8, 8)], 'id': 'J87-3001.16'}}	This ensures that reasonable incomplete analyses of the ENTITYOTHER are produced when more complete analyses are not possible , resulting in a relatively robust ENTITY .
Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions.	robustness problems	natural language processing systems	model-feature	{'e1': {'word': 'robustness problems', 'word_index': [(6, 6)], 'id': 'J87-3001.18'}, 'e2': {'word': 'natural language processing systems', 'word_index': [(11, 11)], 'id': 'J87-3001.19'}}	Thus the work reported addresses two ENTITY faced by current experimental ENTITYOTHER : coping with an incomplete ENTITYUNRELATED and with incomplete ENTITYUNRELATED of ENTITYUNRELATED .
This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts.	latent variable model	evaluation method	usage	{'e1': {'word': 'latent variable model', 'word_index': [(7, 7)], 'id': 'I05-5009.2'}, 'e2': {'word': 'evaluation method', 'word_index': [(4, 4)], 'id': 'I05-5009.1'}}	This paper presents an ENTITYOTHER employing a ENTITY for ENTITYUNRELATED with their ENTITYUNRELATED .
This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts.	contexts	paraphrases	model-feature	{'e1': {'word': 'contexts', 'word_index': [(12, 12)], 'id': 'I05-5009.4'}, 'e2': {'word': 'paraphrases', 'word_index': [(9, 9)], 'id': 'I05-5009.3'}}	This paper presents an ENTITYUNRELATED employing a ENTITYUNRELATED for ENTITYOTHER with their ENTITY .
We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred.	context	sentence	model-feature	{'e1': {'word': 'context', 'word_index': [(4, 4)], 'id': 'I05-5009.5'}, 'e2': {'word': 'sentence', 'word_index': [(7, 7)], 'id': 'I05-5009.6'}}	We assume that the ENTITY of a ENTITYOTHER is indicated by a ENTITYUNRELATED of the ENTITYUNRELATED as a ENTITYUNRELATED and that the ENTITYUNRELATED of each ENTITYUNRELATED can be inferred .
We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred.	latent variable	model	part_whole	{'e1': {'word': 'latent variable', 'word_index': [(12, 12)], 'id': 'I05-5009.7'}, 'e2': {'word': 'model', 'word_index': [(15, 15)], 'id': 'I05-5009.8'}}	We assume that the ENTITYUNRELATED of a ENTITYUNRELATED is indicated by a ENTITY of the ENTITYOTHER as a ENTITYUNRELATED and that the ENTITYUNRELATED of each ENTITYUNRELATED can be inferred .
We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred.	likelihood	variable	model-feature	{'e1': {'word': 'likelihood', 'word_index': [(22, 22)], 'id': 'I05-5009.10'}, 'e2': {'word': 'variable', 'word_index': [(25, 25)], 'id': 'I05-5009.11'}}	We assume that the ENTITYUNRELATED of a ENTITYUNRELATED is indicated by a ENTITYUNRELATED of the ENTITYUNRELATED as a ENTITYUNRELATED and that the ENTITY of each ENTITYOTHER can be inferred .
A paraphrase is evaluated for whether its sentences are used in the same context.	sentences	context	model-feature	{'e1': {'word': 'sentences', 'word_index': [(7, 7)], 'id': 'I05-5009.13'}, 'e2': {'word': 'context', 'word_index': [(13, 13)], 'id': 'I05-5009.14'}}	A ENTITYUNRELATED is evaluated for whether its ENTITY are used in the same ENTITYOTHER .
The results also revealed an upper bound of accuracy of 77% with the method when using only topic information.	topic information	accuracy	result	{'e1': {'word': 'topic information', 'word_index': [(18, 18)], 'id': 'I05-5009.19'}, 'e2': {'word': 'accuracy', 'word_index': [(8, 8)], 'id': 'I05-5009.17'}}	The results also revealed an upper bound of ENTITYOTHER of 77 % with the ENTITYUNRELATED when using only ENTITY .
An extension to the GPSG grammatical formalism is proposed, allowing non-terminals to consist of finite sequences of category labels, and allowing schematic variables to range over such sequences.	category labels	non-terminals	part_whole	{'e1': {'word': 'category labels', 'word_index': [(16, 16)], 'id': 'P83-1003.3'}, 'e2': {'word': 'non-terminals', 'word_index': [(9, 9)], 'id': 'P83-1003.2'}}	An extension to the ENTITYUNRELATED is proposed , allowing ENTITYOTHER to consist of finite sequences of ENTITY , and allowing ENTITYUNRELATED to range over such sequences .
The extension is shown to be sufficient to provide a strongly adequate grammar for crossed serial dependencies, as found in e.g. Dutch subordinate clauses.	grammar	crossed serial dependencies	usage	{'e1': {'word': 'grammar', 'word_index': [(12, 12)], 'id': 'P83-1003.5'}, 'e2': {'word': 'crossed serial dependencies', 'word_index': [(14, 14)], 'id': 'P83-1003.6'}}	The extension is shown to be sufficient to provide a strongly adequate ENTITY for ENTITYOTHER , as found in e.g. ENTITYUNRELATED .
The extension is shown to be parseable by a simple extension to an existing parsing method for GPSG.	parsing method	GPSG	usage	{'e1': {'word': 'parsing method', 'word_index': [(14, 14)], 'id': 'P83-1003.10'}, 'e2': {'word': 'GPSG', 'word_index': [(16, 16)], 'id': 'P83-1003.11'}}	The extension is shown to be parseable by a simple extension to an existing ENTITY for ENTITYOTHER .
Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity; underspecified representations are particularly well suited for the representation of ambiguousdata because they allow for high informational efficiency	representations	representation	usage	{'e1': {'word': 'representations', 'word_index': [(18, 18)], 'id': 'L08-1450.12'}, 'e2': {'word': 'representation', 'word_index': [(25, 25)], 'id': 'L08-1450.13'}}	ENTITYUNRELATED ENTITYUNRELATED and encoding ENTITYUNRELATED for syntactically annotated ENTITYUNRELATED ENTITYUNRELATED need to ENTITYUNRELATED with ENTITYUNRELATED ENTITYUNRELATED ; underspecified ENTITY are particularly well suited for the ENTITYOTHER of ambiguous ENTITYUNRELATED because they allow for high informational ENTITYUNRELATED
We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses.	documentation	linguistic analyses	topic	{'e1': {'word': 'documentation', 'word_index': [(22, 22)], 'id': 'L08-1450.17'}, 'e2': {'word': 'linguistic analyses', 'word_index': [(24, 24)], 'id': 'L08-1450.18'}}	We discuss the ENTITYUNRELATED of being informationally efficient , and the trade - off between efficient encoding of linguistic annotations and complete ENTITY of ENTITYOTHER .
The main topic of this article is adata model and an encoding scheme based on LAF/GrAF ( Ide and  Romary, 2006 ; Ide and  Suderman, 2007 ) which provides a flexible framework for encoding underspecified representations.	framework	representations	usage	{'e1': {'word': 'framework', 'word_index': [(36, 36)], 'id': 'L08-1450.28'}, 'e2': {'word': 'representations', 'word_index': [(40, 40)], 'id': 'L08-1450.29'}}	The ENTITYUNRELATED ENTITYUNRELATED of this article is a ENTITYUNRELATED ENTITYUNRELATED and an encoding ENTITYUNRELATED ENTITYUNRELATED on LAF / GrAF ( ENTITYUNRELATED and Romary , 2006 ; ENTITYUNRELATED and Suderman , 2007 ) which ENTITYUNRELATED a flexible ENTITY for encoding underspecified ENTITYOTHER .
We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.	dependency structures	sentence	model-feature	{'e1': {'word': 'dependency structures', 'word_index': [(6, 6)], 'id': 'L08-1450.30'}, 'e2': {'word': 'sentence', 'word_index': [(26, 26)], 'id': 'L08-1450.32'}}	We show how a set of ENTITY and a set of ENTITYUNRELATED graphs ( Brants et al. , 2002 ) representing the readings of an ambiguous ENTITYOTHER can be encoded , and we discuss ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED which are encoded using the ENTITYUNRELATED presented here .
We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.	issues	querying	model-feature	{'e1': {'word': 'issues', 'word_index': [(35, 35)], 'id': 'L08-1450.34'}, 'e2': {'word': 'querying', 'word_index': [(37, 37)], 'id': 'L08-1450.35'}}	We show how a set of ENTITYUNRELATED and a set of ENTITYUNRELATED graphs ( Brants et al. , 2002 ) representing the readings of an ambiguous ENTITYUNRELATED can be encoded , and we discuss ENTITYUNRELATED ENTITY in ENTITYOTHER ENTITYUNRELATED which are encoded using the ENTITYUNRELATED presented here .
We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.	framework	corpora	model-feature	{'e1': {'word': 'framework', 'word_index': [(44, 44)], 'id': 'L08-1450.37'}, 'e2': {'word': 'corpora', 'word_index': [(38, 38)], 'id': 'L08-1450.36'}}	We show how a set of ENTITYUNRELATED and a set of ENTITYUNRELATED graphs ( Brants et al. , 2002 ) representing the readings of an ambiguous ENTITYUNRELATED can be encoded , and we discuss ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED ENTITYOTHER which are encoded using the ENTITY presented here .
This paper presents a corpus study of parenthetical constructions in two different corpora: the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ).	paper	study	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'L08-1459.5'}, 'e2': {'word': 'study', 'word_index': [(5, 5)], 'id': 'L08-1459.7'}}	This ENTITY presents a ENTITYUNRELATED ENTITYOTHER of parenthetical ENTITYUNRELATED in two different ENTITYUNRELATED : the Penn ENTITYUNRELATED Treebank ( PDTB , ( PDTB - Group , 2008 ) ) and the RST ENTITYUNRELATED Treebank ( Carlson et al. , 2001 ) .
This paper presents a corpus study of parenthetical constructions in two different corpora: the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ).	constructions	corpora	part_whole	{'e1': {'word': 'constructions', 'word_index': [(8, 8)], 'id': 'L08-1459.8'}, 'e2': {'word': 'corpora', 'word_index': [(12, 12)], 'id': 'L08-1459.9'}}	This ENTITYUNRELATED presents a ENTITYUNRELATED ENTITYUNRELATED of parenthetical ENTITY in two different ENTITYOTHER : the Penn ENTITYUNRELATED Treebank ( PDTB , ( PDTB - Group , 2008 ) ) and the RST ENTITYUNRELATED Treebank ( Carlson et al. , 2001 ) .
The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output.	understanding	properties	topic	{'e1': {'word': 'understanding', 'word_index': [(10, 10)], 'id': 'L08-1459.15'}, 'e2': {'word': 'properties', 'word_index': [(14, 14)], 'id': 'L08-1459.16'}}	The ENTITYUNRELATED for the ENTITYUNRELATED is to ENTITYUNRELATED a better ENTITY of the rhetorical ENTITYOTHER of parentheticals in ENTITYUNRELATED to enable a ENTITYUNRELATED to produce parentheticals as ENTITYUNRELATED of a rhetorically well - formed ENTITYUNRELATED .
The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output.	natural language generation system	output	result	{'e1': {'word': 'natural language generation system', 'word_index': [(22, 22)], 'id': 'L08-1459.18'}, 'e2': {'word': 'output', 'word_index': [(34, 34)], 'id': 'L08-1459.20'}}	The ENTITYUNRELATED for the ENTITYUNRELATED is to ENTITYUNRELATED a better ENTITYUNRELATED of the rhetorical ENTITYUNRELATED of parentheticals in ENTITYUNRELATED to enable a ENTITY to produce parentheticals as ENTITYUNRELATED of a rhetorically well - formed ENTITYOTHER .
We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories: elaboration/expansion-type NP-modifier parentheticals and non-elaboration/expansion-type VP- or S-modifier parentheticals.	correlation	types	model-feature	{'e1': {'word': 'correlation', 'word_index': [(6, 6)], 'id': 'L08-1459.21'}, 'e2': {'word': 'types', 'word_index': [(11, 11)], 'id': 'L08-1459.23'}}	We argue that there is a ENTITY between ENTITYUNRELATED and rhetorical ENTITYOTHER of parentheticals and establish two ENTITYUNRELATED ENTITYUNRELATED : elaboration / ENTITYUNRELATED ENTITYUNRELATED parentheticals and non-elaboration / ENTITYUNRELATED VP - or S- ENTITYUNRELATED parentheticals .
We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines.	strategies	extracting	usage	{'e1': {'word': 'strategies', 'word_index': [(3, 3)], 'id': 'L08-1459.30'}, 'e2': {'word': 'extracting', 'word_index': [(5, 5)], 'id': 'L08-1459.31'}}	We show several ENTITY for ENTITYOTHER these from the two ENTITYUNRELATED and discuss how the seemingly contradictory ENTITYUNRELATED obtained can be reconciled in light of the rhetorical and ENTITYUNRELATED ENTITYUNRELATED of parentheticals as well as the ENTITYUNRELATED taken in the annotation ENTITYUNRELATED .
We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines.	properties	results	result	{'e1': {'word': 'properties', 'word_index': [(29, 29)], 'id': 'L08-1459.35'}, 'e2': {'word': 'results', 'word_index': [(17, 17)], 'id': 'L08-1459.33'}}	We show several ENTITYUNRELATED for ENTITYUNRELATED these from the two ENTITYUNRELATED and discuss how the seemingly contradictory ENTITYOTHER obtained can be reconciled in light of the rhetorical and ENTITYUNRELATED ENTITY of parentheticals as well as the ENTITYUNRELATED taken in the annotation ENTITYUNRELATED .
Machine Learning Approach to Augmenting News Headline Generation	Machine Learning Approach	Generation	usage	{'e1': {'word': 'Machine Learning Approach', 'word_index': [(0, 0)], 'id': 'I05-2027.1'}, 'e2': {'word': 'Generation', 'word_index': [(5, 5)], 'id': 'I05-2027.4'}}	ENTITY to Augmenting ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER
In this paper, we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'I05-2027.5'}, 'e2': {'word': 'system', 'word_index': [(9, 9)], 'id': 'I05-2027.6'}}	In this ENTITY , we present the Hybrid Trim ENTITYOTHER which uses a ENTITYUNRELATED learning ENTITYUNRELATED to combine linguistic , ENTITYUNRELATED and positional ENTITYUNRELATED to identify ENTITYUNRELATED labels for ENTITYUNRELATED in a ENTITYUNRELATED
In this paper, we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text	information	technique	usage	{'e1': {'word': 'information', 'word_index': [(23, 23)], 'id': 'I05-2027.10'}, 'e2': {'word': 'technique', 'word_index': [(15, 15)], 'id': 'I05-2027.8'}}	In this ENTITYUNRELATED , we present the Hybrid Trim ENTITYUNRELATED which uses a ENTITYUNRELATED learning ENTITYOTHER to combine linguistic , ENTITYUNRELATED and positional ENTITY to identify ENTITYUNRELATED labels for ENTITYUNRELATED in a ENTITYUNRELATED
In this paper, we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text	headlines	text	part_whole	{'e1': {'word': 'headlines', 'word_index': [(29, 29)], 'id': 'I05-2027.12'}, 'e2': {'word': 'text', 'word_index': [(32, 32)], 'id': 'I05-2027.13'}}	In this ENTITYUNRELATED , we present the Hybrid Trim ENTITYUNRELATED which uses a ENTITYUNRELATED learning ENTITYUNRELATED to combine linguistic , ENTITYUNRELATED and positional ENTITYUNRELATED to identify ENTITYUNRELATED labels for ENTITY in a ENTITYOTHER
We compare our system with the Topiary system which, in contrast, uses a statistical learning approach to finding topic descriptors for headlines.	system	system	compare	{'e1': {'word': 'system', 'word_index': [(3, 3)], 'id': 'I05-2027.14'}, 'e2': {'word': 'system', 'word_index': [(7, 7)], 'id': 'I05-2027.15'}}	We compare our ENTITY with the Topiary ENTITYOTHER which , in ENTITYUNRELATED , uses a ENTITYUNRELATED ENTITYUNRELATED to finding ENTITYUNRELATED descriptors for ENTITYUNRELATED .
We compare our system with the Topiary system which, in contrast, uses a statistical learning approach to finding topic descriptors for headlines.	topic	headlines	model-feature	{'e1': {'word': 'topic', 'word_index': [(19, 19)], 'id': 'I05-2027.19'}, 'e2': {'word': 'headlines', 'word_index': [(22, 22)], 'id': 'I05-2027.20'}}	We compare our ENTITYUNRELATED with the Topiary ENTITYUNRELATED which , in ENTITYUNRELATED , uses a ENTITYUNRELATED ENTITYUNRELATED to finding ENTITY descriptors for ENTITYOTHER .
The Topiary system uses a statistical learning approach to finding topic labels.	learning approach	system	usage	{'e1': {'word': 'learning approach', 'word_index': [(6, 6)], 'id': 'I05-2027.34'}, 'e2': {'word': 'system', 'word_index': [(2, 2)], 'id': 'I05-2027.32'}}	The Topiary ENTITYOTHER uses a ENTITYUNRELATED ENTITY to finding ENTITYUNRELATED labels .
The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection.	systems	performance	result	{'e1': {'word': 'systems', 'word_index': [(4, 4)], 'id': 'I05-2027.37'}, 'e2': {'word': 'performance', 'word_index': [(1, 1)], 'id': 'I05-2027.36'}}	The ENTITYOTHER of these ENTITY is ENTITYUNRELATED using the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED on the DUC 2004 ENTITYUNRELATED stories ENTITYUNRELATED .
The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection.	collection	ROUGE	usage	{'e1': {'word': 'collection', 'word_index': [(18, 18)], 'id': 'I05-2027.43'}, 'e2': {'word': 'ROUGE', 'word_index': [(9, 9)], 'id': 'I05-2027.39'}}	The ENTITYUNRELATED of these ENTITYUNRELATED is ENTITYUNRELATED using the ENTITYOTHER ENTITYUNRELATED ENTITYUNRELATED on the DUC 2004 ENTITYUNRELATED stories ENTITY .
We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.	method	representations	usage	{'e1': {'word': 'method', 'word_index': [(4, 4)], 'id': 'N03-1014.5'}, 'e2': {'word': 'representations', 'word_index': [(7, 7)], 'id': 'N03-1014.6'}}	We present a ENTITYUNRELATED ENTITY for inducing ENTITYOTHER of ENTITYUNRELATED histories and using these history ENTITYUNRELATED to estimate the ENTITYUNRELATED needed by a ENTITYUNRELATED left - corner ENTITYUNRELATED .
We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.	representations	probabilities	usage	{'e1': {'word': 'representations', 'word_index': [(15, 15)], 'id': 'N03-1014.8'}, 'e2': {'word': 'probabilities', 'word_index': [(19, 19)], 'id': 'N03-1014.9'}}	We present a ENTITYUNRELATED ENTITYUNRELATED for inducing ENTITYUNRELATED of ENTITYUNRELATED histories and using these history ENTITY to estimate the ENTITYOTHER needed by a ENTITYUNRELATED left - corner ENTITYUNRELATED .
The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.	performance	parser	compare	{'e1': {'word': 'performance', 'word_index': [(5, 5)], 'id': 'N03-1014.15'}, 'e2': {'word': 'parser', 'word_index': [(23, 23)], 'id': 'N03-1014.18'}}	The ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED achieves ENTITY ( 89.1 % F-measure ) on the ENTITYUNRELATED which is only 0.6 % below the best ENTITYUNRELATED ENTITYOTHER for this ENTITYUNRELATED , despite using a smaller ENTITYUNRELATED ENTITYUNRELATED and less prior ENTITYUNRELATED .
Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.	biases	success	result	{'e1': {'word': 'biases', 'word_index': [(11, 11)], 'id': 'N03-1014.24'}, 'e2': {'word': 'success', 'word_index': [(3, 3)], 'id': 'N03-1014.23'}}	Crucial to this ENTITYOTHER is the use of structurally determined soft ENTITY in inducing the ENTITYUNRELATED of the ENTITYUNRELATED history , and no use of hard ENTITYUNRELATED ENTITYUNRELATED .
Precision And Recall Of Machine Translation	Machine Translation	Recall	result	{'e1': {'word': 'Machine Translation', 'word_index': [(4, 4)], 'id': 'N03-2021.3'}, 'e2': {'word': 'Recall', 'word_index': [(2, 2)], 'id': 'N03-2021.2'}}	ENTITYUNRELATED And ENTITYOTHER Of ENTITY
These standard measures have significantly higher correlation with human judgments than recently proposed alternatives.	judgments	alternatives	compare	{'e1': {'word': 'judgments', 'word_index': [(9, 9)], 'id': 'N03-2021.10'}, 'e2': {'word': 'alternatives', 'word_index': [(13, 13)], 'id': 'N03-2021.12'}}	These ENTITYUNRELATED measures have significantly higher ENTITYUNRELATED with human ENTITY than recently ENTITYUNRELATED ENTITYOTHER .
More importantly, the standard measures have an intuitive interpretation, which can facilitate insights into how MT systems might be improved.	insights	MT systems	topic	{'e1': {'word': 'insights', 'word_index': [(14, 14)], 'id': 'N03-2021.15'}, 'e2': {'word': 'MT systems', 'word_index': [(17, 17)], 'id': 'N03-2021.16'}}	More importantly , the ENTITYUNRELATED measures have an intuitive ENTITYUNRELATED , which can facilitate ENTITY into how ENTITYOTHER might be ENTITYUNRELATED .
In this paper, we present a rule based model for morphological disambiguation of Turkish.	model	disambiguation	usage	{'e1': {'word': 'model', 'word_index': [(9, 9)], 'id': 'N06-1042.6'}, 'e2': {'word': 'disambiguation', 'word_index': [(12, 12)], 'id': 'N06-1042.7'}}	In this ENTITYUNRELATED , we present a ENTITYUNRELATED ENTITYUNRELATED ENTITY for morphological ENTITYOTHER of Turkish .
The rules are generated by a novel decision list learning algorithm using supervised training.	algorithm	training	usage	{'e1': {'word': 'algorithm', 'word_index': [(10, 10)], 'id': 'N06-1042.12'}, 'e2': {'word': 'training', 'word_index': [(13, 13)], 'id': 'N06-1042.13'}}	The ENTITYUNRELATED are ENTITYUNRELATED by a novel ENTITYUNRELATED ENTITYUNRELATED learning ENTITY using supervised ENTITYOTHER .
Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous.	ambiguity	languages	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(1, 1)], 'id': 'N06-1042.14'}, 'e2': {'word': 'languages', 'word_index': [(18, 18)], 'id': 'N06-1042.17'}}	Morphological ENTITY ( e.g. lives = live +s or life +s ) is a ENTITYUNRELATED ENTITYUNRELATED for agglutinative ENTITYOTHER like Turkish where close to half of the ENTITYUNRELATED in running ENTITYUNRELATED are morphologically ambiguous .
Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous.	words	text	part_whole	{'e1': {'word': 'words', 'word_index': [(27, 27)], 'id': 'N06-1042.18'}, 'e2': {'word': 'text', 'word_index': [(30, 30)], 'id': 'N06-1042.19'}}	Morphological ENTITYUNRELATED ( e.g. lives = live +s or life +s ) is a ENTITYUNRELATED ENTITYUNRELATED for agglutinative ENTITYUNRELATED like Turkish where close to half of the ENTITY in running ENTITYOTHER are morphologically ambiguous .
Furthermore, it is possible for a word to take an unlimited number of suffixes, therefore the number of possible morphological tags is unlimited.	suffixes	word	model-feature	{'e1': {'word': 'suffixes', 'word_index': [(14, 14)], 'id': 'N06-1042.22'}, 'e2': {'word': 'word', 'word_index': [(7, 7)], 'id': 'N06-1042.20'}}	Furthermore , it is possible for a ENTITYOTHER to take an unlimited ENTITYUNRELATED of ENTITY , therefore the ENTITYUNRELATED of possible morphological ENTITYUNRELATED is unlimited .
Furthermore, it is possible for a word to take an unlimited number of suffixes, therefore the number of possible morphological tags is unlimited.	number	tags	model-feature	{'e1': {'word': 'number', 'word_index': [(18, 18)], 'id': 'N06-1042.23'}, 'e2': {'word': 'tags', 'word_index': [(22, 22)], 'id': 'N06-1042.24'}}	Furthermore , it is possible for a ENTITYUNRELATED to take an unlimited ENTITYUNRELATED of ENTITYUNRELATED , therefore the ENTITY of possible morphological ENTITYOTHER is unlimited .
We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer.	model	morphological features	model-feature	{'e1': {'word': 'model', 'word_index': [(11, 11)], 'id': 'N06-1042.27'}, 'e2': {'word': 'morphological features', 'word_index': [(17, 17)], 'id': 'N06-1042.28'}}	We attempted to cope with these ENTITYUNRELATED by ENTITYUNRELATED a separate ENTITY for each of the 126 ENTITYOTHER recognized by the morphological ENTITYUNRELATED .
The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes.	parses	word	model-feature	{'e1': {'word': 'parses', 'word_index': [(11, 11)], 'id': 'N06-1042.33'}, 'e2': {'word': 'word', 'word_index': [(14, 14)], 'id': 'N06-1042.34'}}	The ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED independently vote on each of the potential ENTITY of a ENTITYOTHER and the final ENTITYUNRELATED is selected ENTITYUNRELATED on our ENTITYUNRELATED on these votes .
The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes.	confidence	parse	model-feature	{'e1': {'word': 'confidence', 'word_index': [(24, 24)], 'id': 'N06-1042.37'}, 'e2': {'word': 'parse', 'word_index': [(18, 18)], 'id': 'N06-1042.35'}}	The ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED independently vote on each of the potential ENTITYUNRELATED of a ENTITYUNRELATED and the final ENTITYOTHER is selected ENTITYUNRELATED on our ENTITY on these votes .
The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models.	accuracy	results	compare	{'e1': {'word': 'accuracy', 'word_index': [(1, 1)], 'id': 'N06-1042.38'}, 'e2': {'word': 'results', 'word_index': [(16, 16)], 'id': 'N06-1042.41'}}	The ENTITY of our ENTITYUNRELATED ( 96 % ) is slightly above the best previously ENTITYUNRELATED ENTITYOTHER which use ENTITYUNRELATED .
For comparison, when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy.	tags	list	part_whole	{'e1': {'word': 'tags', 'word_index': [(12, 12)], 'id': 'N06-1042.47'}, 'e2': {'word': 'list', 'word_index': [(9, 9)], 'id': 'N06-1042.46'}}	For ENTITYUNRELATED , when we ENTITYUNRELATED a single ENTITYUNRELATED ENTITYOTHER on full ENTITY instead of using separate ENTITYUNRELATED on each ENTITYUNRELATED we get 91 % ENTITYUNRELATED .
In this paper, we report SRA's results on the MUC-4 task and describe how we trained our natural language processing system for MUC-4.	paper	results	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'M92-1018.4'}, 'e2': {'word': 'results', 'word_index': [(8, 8)], 'id': 'M92-1018.6'}}	In this ENTITY , we ENTITYUNRELATED SRA 's ENTITYOTHER on the MUC -4 ENTITYUNRELATED and describe how we ENTITYUNRELATED our ENTITYUNRELATED for MUC - 4.
Corpus-Based Method For Automatic Identification Of Support Verbs For Nominalization	Method	Identification	usage	{'e1': {'word': 'Method', 'word_index': [(2, 2)], 'id': 'E95-1014.3'}, 'e2': {'word': 'Identification', 'word_index': [(5, 5)], 'id': 'E95-1014.5'}}	ENTITYUNRELATED ENTITYUNRELATED ENTITY For ENTITYUNRELATED ENTITYOTHER Of ENTITYUNRELATED Verbs For Nominalization
Nominalization is a highly productive phenomena in most languages.	phenomena	languages	part_whole	{'e1': {'word': 'phenomena', 'word_index': [(5, 5)], 'id': 'E95-1014.7'}, 'e2': {'word': 'languages', 'word_index': [(8, 8)], 'id': 'E95-1014.8'}}	Nominalization is a highly productive ENTITY in most ENTITYOTHER .
The process of nominalization ejects a verb from its syntactic role into a nominal position.	role	verb	model-feature	{'e1': {'word': 'role', 'word_index': [(10, 10)], 'id': 'E95-1014.12'}, 'e2': {'word': 'verb', 'word_index': [(6, 6)], 'id': 'E95-1014.10'}}	The ENTITYUNRELATED of nominalization ejects a ENTITYOTHER from its ENTITYUNRELATED ENTITY into a nominal position .
The original verb is often replaced by a semantically emptied support verb (e.g., make a proposal).	verb	verb	compare	{'e1': {'word': 'verb', 'word_index': [(2, 2)], 'id': 'E95-1014.13'}, 'e2': {'word': 'verb', 'word_index': [(11, 11)], 'id': 'E95-1014.15'}}	The original ENTITY is often replaced by a semantically emptied ENTITYUNRELATED ENTITYOTHER ( e.g. , make a ENTITYUNRELATED ) .
This paper describes an approach to providing lexical information for natural language processing in unrestricted domains.	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'A00-1030.5'}, 'e2': {'word': 'approach', 'word_index': [(4, 4)], 'id': 'A00-1030.6'}}	This ENTITY describes an ENTITYOTHER to ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED in unrestricted ENTITYUNRELATED .
This paper describes an approach to providing lexical information for natural language processing in unrestricted domains.	lexical information	natural language processing	usage	{'e1': {'word': 'lexical information', 'word_index': [(7, 7)], 'id': 'A00-1030.8'}, 'e2': {'word': 'natural language processing', 'word_index': [(9, 9)], 'id': 'A00-1030.9'}}	This ENTITYUNRELATED describes an ENTITYUNRELATED to ENTITYUNRELATED ENTITY for ENTITYOTHER in unrestricted ENTITYUNRELATED .
A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms.	rules	system	usage	{'e1': {'word': 'rules', 'word_index': [(6, 6)], 'id': 'A00-1030.12'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'A00-1030.11'}}	A ENTITYOTHER of approximately 1200 morphological ENTITY is used to extend a ENTITYUNRELATED ENTITYUNRELATED of 39,000 ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED that exceeds that of a ENTITYUNRELATED of 80,000 ENTITYUNRELATED or 150,000 ENTITYUNRELATED ENTITYUNRELATED .
A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms.	words	lexicon	part_whole	{'e1': {'word': 'words', 'word_index': [(16, 16)], 'id': 'A00-1030.15'}, 'e2': {'word': 'lexicon', 'word_index': [(13, 13)], 'id': 'A00-1030.14'}}	A ENTITYUNRELATED of approximately 1200 morphological ENTITYUNRELATED is used to extend a ENTITYUNRELATED ENTITYOTHER of 39,000 ENTITY to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED that exceeds that of a ENTITYUNRELATED of 80,000 ENTITYUNRELATED or 150,000 ENTITYUNRELATED ENTITYUNRELATED .
The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus.	words	corpus	part_whole	{'e1': {'word': 'words', 'word_index': [(13, 13)], 'id': 'A00-1030.27'}, 'e2': {'word': 'corpus', 'word_index': [(19, 19)], 'id': 'A00-1030.28'}}	The morphological ENTITYUNRELATED is described , and ENTITYUNRELATED ENTITYUNRELATED is ENTITYUNRELATED for random ENTITY chosen from a previously unanalyzed ENTITYOTHER .
In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant.	hypothesis	dialogue systems	usage	{'e1': {'word': 'hypothesis', 'word_index': [(16, 16)], 'id': 'A00-2029.6'}, 'e2': {'word': 'dialogue systems', 'word_index': [(2, 2)], 'id': 'A00-2029.3'}}	In spoken ENTITYOTHER , it is important for a ENTITYUNRELATED to know how likely a ENTITYUNRELATED ENTITY is to be correct , so it can reprompt for fresh ENTITYUNRELATED , or , in ENTITYUNRELATED where many ENTITYUNRELATED have occurred , change its ENTITYUNRELATED ENTITYUNRELATED or switch the caller to a human attendant .
In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant.	errors	strategy	result	{'e1': {'word': 'errors', 'word_index': [(36, 36)], 'id': 'A00-2029.9'}, 'e2': {'word': 'strategy', 'word_index': [(43, 43)], 'id': 'A00-2029.11'}}	In spoken ENTITYUNRELATED , it is important for a ENTITYUNRELATED to know how likely a ENTITYUNRELATED ENTITYUNRELATED is to be correct , so it can reprompt for fresh ENTITYUNRELATED , or , in ENTITYUNRELATED where many ENTITY have occurred , change its ENTITYUNRELATED ENTITYOTHER or switch the caller to a human attendant .
We have discovered prosodie features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition.	error	hypothesis	part_whole	{'e1': {'word': 'error', 'word_index': [(16, 16)], 'id': 'A00-2029.16'}, 'e2': {'word': 'hypothesis', 'word_index': [(12, 12)], 'id': 'A00-2029.14'}}	We have discovered prosodie ENTITYUNRELATED which more accurately predict when a ENTITYUNRELATED ENTITYOTHER contains a ENTITYUNRELATED ENTITY than the acoustic ENTITYUNRELATED score ENTITYUNRELATED traditionally used in ENTITYUNRELATED .
We have discovered prosodie features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition.	thresholds	automatic speech recognition	usage	{'e1': {'word': 'thresholds', 'word_index': [(22, 22)], 'id': 'A00-2029.18'}, 'e2': {'word': 'automatic speech recognition', 'word_index': [(26, 26)], 'id': 'A00-2029.19'}}	We have discovered prosodie ENTITYUNRELATED which more accurately predict when a ENTITYUNRELATED ENTITYUNRELATED contains a ENTITYUNRELATED ENTITYUNRELATED than the acoustic ENTITYUNRELATED score ENTITY traditionally used in ENTITYOTHER .
We present analytic results indicating that there are significant prosodie differences between correctly and incorrectly recognized turns.	differences	results	result	{'e1': {'word': 'differences', 'word_index': [(10, 10)], 'id': 'A00-2029.21'}, 'e2': {'word': 'results', 'word_index': [(3, 3)], 'id': 'A00-2029.20'}}	We present analytic ENTITYOTHER indicating that there are significant prosodie ENTITY between correctly and incorrectly recognized turns .
The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding.	project	information	topic	{'e1': {'word': 'project', 'word_index': [(4, 4)], 'id': 'H92-1099.6'}, 'e2': {'word': 'information', 'word_index': [(15, 15)], 'id': 'H92-1099.8'}}	The ENTITYUNRELATED of this ENTITY is to investigate the use of different ENTITYUNRELATED of prosodie ENTITYOTHER in ENTITYUNRELATED and ENTITYUNRELATED .
In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing.	information	parsing	usage	{'e1': {'word': 'information', 'word_index': [(16, 16)], 'id': 'H92-1099.15'}, 'e2': {'word': 'parsing', 'word_index': [(18, 18)], 'id': 'H92-1099.16'}}	In particular , the ENTITYUNRELATED ENTITYUNRELATED of the work is the use of prosodie ENTITYUNRELATED ENTITYUNRELATED ENTITY in ENTITYOTHER .
The research involves determining a representation of prosodie information suitable for use in a speech understanding system, developing reliable algorithms for detection of the prosodie cues in speech, investigating architectures for integrating prosodie cues in a parser, and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System.	research	representation	topic	{'e1': {'word': 'research', 'word_index': [(1, 1)], 'id': 'H92-1099.17'}, 'e2': {'word': 'representation', 'word_index': [(5, 5)], 'id': 'H92-1099.18'}}	The ENTITY involves determining a ENTITYOTHER of prosodie ENTITYUNRELATED suitable for use in a ENTITYUNRELATED , ENTITYUNRELATED reliable ENTITYUNRELATED for ENTITYUNRELATED of the prosodie ENTITYUNRELATED in ENTITYUNRELATED , investigating ENTITYUNRELATED for integrating prosodie ENTITYUNRELATED in a ENTITYUNRELATED , and ENTITYUNRELATED the potential ENTITYUNRELATED of ENTITYUNRELATED in the ENTITYUNRELATED of the SRI ENTITYUNRELATED .
The research involves determining a representation of prosodie information suitable for use in a speech understanding system, developing reliable algorithms for detection of the prosodie cues in speech, investigating architectures for integrating prosodie cues in a parser, and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System.	information	speech understanding system	usage	{'e1': {'word': 'information', 'word_index': [(8, 8)], 'id': 'H92-1099.19'}, 'e2': {'word': 'speech understanding system', 'word_index': [(14, 14)], 'id': 'H92-1099.20'}}	The ENTITYUNRELATED involves determining a ENTITYUNRELATED of prosodie ENTITY suitable for use in a ENTITYOTHER , ENTITYUNRELATED reliable ENTITYUNRELATED for ENTITYUNRELATED of the prosodie ENTITYUNRELATED in ENTITYUNRELATED , investigating ENTITYUNRELATED for integrating prosodie ENTITYUNRELATED in a ENTITYUNRELATED , and ENTITYUNRELATED the potential ENTITYUNRELATED of ENTITYUNRELATED in the ENTITYUNRELATED of the SRI ENTITYUNRELATED .
The research involves determining a representation of prosodie information suitable for use in a speech understanding system, developing reliable algorithms for detection of the prosodie cues in speech, investigating architectures for integrating prosodie cues in a parser, and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System.	algorithms	detection	usage	{'e1': {'word': 'algorithms', 'word_index': [(18, 18)], 'id': 'H92-1099.22'}, 'e2': {'word': 'detection', 'word_index': [(20, 20)], 'id': 'H92-1099.23'}}	The ENTITYUNRELATED involves determining a ENTITYUNRELATED of prosodie ENTITYUNRELATED suitable for use in a ENTITYUNRELATED , ENTITYUNRELATED reliable ENTITY for ENTITYOTHER of the prosodie ENTITYUNRELATED in ENTITYUNRELATED , investigating ENTITYUNRELATED for integrating prosodie ENTITYUNRELATED in a ENTITYUNRELATED , and ENTITYUNRELATED the potential ENTITYUNRELATED of ENTITYUNRELATED in the ENTITYUNRELATED of the SRI ENTITYUNRELATED .
The research involves determining a representation of prosodie information suitable for use in a speech understanding system, developing reliable algorithms for detection of the prosodie cues in speech, investigating architectures for integrating prosodie cues in a parser, and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System.	cues	speech	part_whole	{'e1': {'word': 'cues', 'word_index': [(24, 24)], 'id': 'H92-1099.24'}, 'e2': {'word': 'speech', 'word_index': [(26, 26)], 'id': 'H92-1099.25'}}	The ENTITYUNRELATED involves determining a ENTITYUNRELATED of prosodie ENTITYUNRELATED suitable for use in a ENTITYUNRELATED , ENTITYUNRELATED reliable ENTITYUNRELATED for ENTITYUNRELATED of the prosodie ENTITY in ENTITYOTHER , investigating ENTITYUNRELATED for integrating prosodie ENTITYUNRELATED in a ENTITYUNRELATED , and ENTITYUNRELATED the potential ENTITYUNRELATED of ENTITYUNRELATED in the ENTITYUNRELATED of the SRI ENTITYUNRELATED .
The research involves determining a representation of prosodie information suitable for use in a speech understanding system, developing reliable algorithms for detection of the prosodie cues in speech, investigating architectures for integrating prosodie cues in a parser, and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System.	cues	parser	usage	{'e1': {'word': 'cues', 'word_index': [(33, 33)], 'id': 'H92-1099.27'}, 'e2': {'word': 'parser', 'word_index': [(36, 36)], 'id': 'H92-1099.28'}}	The ENTITYUNRELATED involves determining a ENTITYUNRELATED of prosodie ENTITYUNRELATED suitable for use in a ENTITYUNRELATED , ENTITYUNRELATED reliable ENTITYUNRELATED for ENTITYUNRELATED of the prosodie ENTITYUNRELATED in ENTITYUNRELATED , investigating ENTITYUNRELATED for integrating prosodie ENTITY in a ENTITYOTHER , and ENTITYUNRELATED the potential ENTITYUNRELATED of ENTITYUNRELATED in the ENTITYUNRELATED of the SRI ENTITYUNRELATED .
The research involves determining a representation of prosodie information suitable for use in a speech understanding system, developing reliable algorithms for detection of the prosodie cues in speech, investigating architectures for integrating prosodie cues in a parser, and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System.	Spoken Language System	improvements	result	{'e1': {'word': 'Spoken Language System', 'word_index': [(51, 51)], 'id': 'H92-1099.33'}, 'e2': {'word': 'improvements', 'word_index': [(42, 42)], 'id': 'H92-1099.30'}}	The ENTITYUNRELATED involves determining a ENTITYUNRELATED of prosodie ENTITYUNRELATED suitable for use in a ENTITYUNRELATED , ENTITYUNRELATED reliable ENTITYUNRELATED for ENTITYUNRELATED of the prosodie ENTITYUNRELATED in ENTITYUNRELATED , investigating ENTITYUNRELATED for integrating prosodie ENTITYUNRELATED in a ENTITYUNRELATED , and ENTITYUNRELATED the potential ENTITYOTHER of ENTITYUNRELATED in the ENTITYUNRELATED of the SRI ENTITY .
There is a mismatch between the distribution of information in text, and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars.	information	text	part_whole	{'e1': {'word': 'information', 'word_index': [(8, 8)], 'id': 'H93-1048.6'}, 'e2': {'word': 'text', 'word_index': [(10, 10)], 'id': 'H93-1048.7'}}	There is a ENTITYUNRELATED between the ENTITYUNRELATED of ENTITY in ENTITYOTHER , and a ENTITYUNRELATED of grammatical ENTITYUNRELATED for describing it , ENTITYUNRELATED ngrams , ENTITYUNRELATED grammars , and ENTITYUNRELATED .
There is a mismatch between the distribution of information in text, and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars.	variety	formalisms	model-feature	{'e1': {'word': 'variety', 'word_index': [(14, 14)], 'id': 'H93-1048.8'}, 'e2': {'word': 'formalisms', 'word_index': [(17, 17)], 'id': 'H93-1048.9'}}	There is a ENTITYUNRELATED between the ENTITYUNRELATED of ENTITYUNRELATED in ENTITYUNRELATED , and a ENTITY of grammatical ENTITYOTHER for describing it , ENTITYUNRELATED ngrams , ENTITYUNRELATED grammars , and ENTITYUNRELATED .
Rather than adding probabilities to existing grammars, it is proposed to collect the distributions of flexibly sized partial trees.	distributions	trees	model-feature	{'e1': {'word': 'distributions', 'word_index': [(14, 14)], 'id': 'H93-1048.15'}, 'e2': {'word': 'trees', 'word_index': [(19, 19)], 'id': 'H93-1048.18'}}	Rather than adding ENTITYUNRELATED to existing grammars , it is ENTITYUNRELATED to collect the ENTITY of flexibly ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
"""Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics."	resources	analysis	usage	{'e1': {'word': 'resources', 'word_index': [(4, 4)], 'id': 'H93-1060.4'}, 'e2': {'word': 'analysis', 'word_index': [(8, 8)], 'id': 'H93-1060.7'}}	""" ENTITYUNRELATED more shareable ENTITY to ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER will make it easier and cheaper to create new ENTITYUNRELATED ENTITYUNRELATED and to ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED ."
"""Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics."	research	computational linguistics	topic	{'e1': {'word': 'research', 'word_index': [(23, 23)], 'id': 'H93-1060.11'}, 'e2': {'word': 'computational linguistics', 'word_index': [(25, 25)], 'id': 'H93-1060.12'}}	""" ENTITYUNRELATED more shareable ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED will make it easier and cheaper to create new ENTITYUNRELATED ENTITYUNRELATED and to ENTITYUNRELATED ENTITY in ENTITYOTHER ."
One natural candidate for such a resource is a broad-coverage dictionary, since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word.	information	word	model-feature	{'e1': {'word': 'information', 'word_index': [(34, 34)], 'id': 'H93-1060.21'}, 'e2': {'word': 'word', 'word_index': [(40, 40)], 'id': 'H93-1060.23'}}	One ENTITYUNRELATED ENTITYUNRELATED for such a ENTITYUNRELATED is a ENTITYUNRELATED ENTITYUNRELATED , since the work ENTITYUNRELATED to create such a ENTITYUNRELATED is large but there is general ENTITYUNRELATED on at least some of the ENTITY to be ENTITYUNRELATED for each ENTITYOTHER .
The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words, intended for automatic language analysis.	Project	dictionary	topic	{'e1': {'word': 'Project', 'word_index': [(6, 6)], 'id': 'H93-1060.34'}, 'e2': {'word': 'dictionary', 'word_index': [(13, 13)], 'id': 'H93-1060.36'}}	The ENTITYUNRELATED of the COMLEX ENTITYUNRELATED ENTITY is to create a ENTITYUNRELATED shareable ENTITYOTHER containing the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED , intended for ENTITYUNRELATED ENTITYUNRELATED .
The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words, intended for automatic language analysis.	syntactic features	words	model-feature	{'e1': {'word': 'syntactic features', 'word_index': [(16, 16)], 'id': 'H93-1060.37'}, 'e2': {'word': 'words', 'word_index': [(19, 19)], 'id': 'H93-1060.39'}}	The ENTITYUNRELATED of the COMLEX ENTITYUNRELATED ENTITYUNRELATED is to create a ENTITYUNRELATED shareable ENTITYUNRELATED containing the ENTITY of ENTITYUNRELATED ENTITYOTHER , intended for ENTITYUNRELATED ENTITYUNRELATED .
We are initially aiming for a dictionary of 35,000 to 40,000 base forms, although this of course may be enlarged if the initial effort is positively received.	forms	dictionary	part_whole	{'e1': {'word': 'forms', 'word_index': [(12, 12)], 'id': 'H93-1060.44'}, 'e2': {'word': 'dictionary', 'word_index': [(6, 6)], 'id': 'H93-1060.42'}}	We are initially aiming for a ENTITYOTHER of 35,000 to 40,000 ENTITYUNRELATED ENTITY , although this of course may be enlarged if the initial ENTITYUNRELATED is positively received .
The dictionary should include detailed syntactic specifications, particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide.	specifications	dictionary	part_whole	{'e1': {'word': 'specifications', 'word_index': [(6, 6)], 'id': 'H93-1060.49'}, 'e2': {'word': 'dictionary', 'word_index': [(1, 1)], 'id': 'H93-1060.46'}}	The ENTITYOTHER should ENTITYUNRELATED detailed ENTITYUNRELATED ENTITY , particularly for subcategorization ; our intent is to ENTITYUNRELATED sufficient ENTITYUNRELATED so that the ENTITYUNRELATED ENTITYUNRELATED by a ENTITYUNRELATED of major ENTITYUNRELATED ENTITYUNRELATED can be automatically derived from the ENTITYUNRELATED we ENTITYUNRELATED .
The dictionary should include detailed syntactic specifications, particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide.	information	analyzers	usage	{'e1': {'word': 'information', 'word_index': [(22, 22)], 'id': 'H93-1060.52'}, 'e2': {'word': 'analyzers', 'word_index': [(30, 30)], 'id': 'H93-1060.56'}}	The ENTITYUNRELATED should ENTITYUNRELATED detailed ENTITYUNRELATED ENTITYUNRELATED , particularly for subcategorization ; our intent is to ENTITYUNRELATED sufficient ENTITYUNRELATED so that the ENTITY ENTITYUNRELATED by a ENTITYUNRELATED of major ENTITYUNRELATED ENTITYOTHER can be automatically derived from the ENTITYUNRELATED we ENTITYUNRELATED .
As with human-human interaction, spoken human-computer dialog will contain situations where there is miscommunication.	situations	dialog	part_whole	{'e1': {'word': 'situations', 'word_index': [(12, 12)], 'id': 'A97-1008.10'}, 'e2': {'word': 'dialog', 'word_index': [(9, 9)], 'id': 'A97-1008.9'}}	As with human - human ENTITYUNRELATED , spoken ENTITYUNRELATED ENTITYOTHER will contain ENTITY where there is miscommunication .
In experimental trials consisting of eight different users, 141 problem-solving dialogs, and 2840 user utterances, the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances.	dialog system	utterances	usage	{'e1': {'word': 'dialog system', 'word_index': [(25, 25)], 'id': 'A97-1008.18'}, 'e2': {'word': 'utterances', 'word_index': [(31, 31)], 'id': 'A97-1008.20'}}	In ENTITYUNRELATED trials consisting of eight different ENTITYUNRELATED , 141 ENTITYUNRELATED ENTITYUNRELATED , and 2840 ENTITYUNRELATED ENTITYUNRELATED , the Circuit Fix - It Shop ENTITYUNRELATED ENTITY misinterpreted 18.5 % of ENTITYUNRELATED ENTITYOTHER .
One natural strategy for reducing the impact of miscommunication is selective verification of the user's utterances.	verification	utterances	usage	{'e1': {'word': 'verification', 'word_index': [(11, 11)], 'id': 'A97-1008.29'}, 'e2': {'word': 'utterances', 'word_index': [(16, 16)], 'id': 'A97-1008.31'}}	One ENTITYUNRELATED ENTITYUNRELATED for reducing the ENTITYUNRELATED of miscommunication is selective ENTITY of the ENTITYUNRELATED 's ENTITYOTHER .
This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.	paper	strategies	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'A97-1008.32'}, 'e2': {'word': 'strategies', 'word_index': [(8, 8)], 'id': 'A97-1008.36'}}	This ENTITY ENTITYUNRELATED on both ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER for ENTITYUNRELATED ENTITYUNRELATED that show that the use of ENTITYUNRELATED ENTITYUNRELATED is crucial for intelligent ENTITYUNRELATED of which ENTITYUNRELATED to verify .
This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.	context	selection	usage	{'e1': {'word': 'context', 'word_index': [(19, 19)], 'id': 'A97-1008.40'}, 'e2': {'word': 'selection', 'word_index': [(24, 24)], 'id': 'A97-1008.41'}}	This ENTITYUNRELATED ENTITYUNRELATED on both ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED that show that the use of ENTITYUNRELATED ENTITY is crucial for intelligent ENTITYOTHER of which ENTITYUNRELATED to verify .
We propose a set of rules for the computation of prosody which are implemented in an existing generic Data-to-Speech system.	rules	computation	usage	{'e1': {'word': 'rules', 'word_index': [(5, 5)], 'id': 'W97-1206.6'}, 'e2': {'word': 'computation', 'word_index': [(8, 8)], 'id': 'W97-1206.7'}}	We ENTITYUNRELATED a set of ENTITY for the ENTITYOTHER of ENTITYUNRELATED which are ENTITYUNRELATED in an existing generic ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
The rules make crucial use of both sentence-internal and sentence-external semantic and syntactic information provided by the system.	syntactic information	rules	usage	{'e1': {'word': 'syntactic information', 'word_index': [(12, 12)], 'id': 'W97-1206.17'}, 'e2': {'word': 'rules', 'word_index': [(1, 1)], 'id': 'W97-1206.13'}}	The ENTITYOTHER make crucial use of both ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED and ENTITY ENTITYUNRELATED by the ENTITYUNRELATED .
In a Text-to-Speech system, this information would have to be obtained through text analysis, but in Data-to-Speech it is readily available, and its reliable and detailed character makes it possible to compute the prosodie properties of generated sentences in a sophisticated way.	text analysis	information	result	{'e1': {'word': 'text analysis', 'word_index': [(14, 14)], 'id': 'W97-1206.24'}, 'e2': {'word': 'information', 'word_index': [(7, 7)], 'id': 'W97-1206.23'}}	In a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , this ENTITYOTHER would have to be obtained through ENTITY , but in ENTITYUNRELATED ENTITYUNRELATED it is readily available , and its reliable and detailed character makes it possible to ENTITYUNRELATED the prosodie ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED in a sophisticated way .
In a Text-to-Speech system, this information would have to be obtained through text analysis, but in Data-to-Speech it is readily available, and its reliable and detailed character makes it possible to compute the prosodie properties of generated sentences in a sophisticated way.	properties	sentences	model-feature	{'e1': {'word': 'properties', 'word_index': [(38, 38)], 'id': 'W97-1206.28'}, 'e2': {'word': 'sentences', 'word_index': [(41, 41)], 'id': 'W97-1206.30'}}	In a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , this ENTITYUNRELATED would have to be obtained through ENTITYUNRELATED , but in ENTITYUNRELATED ENTITYUNRELATED it is readily available , and its reliable and detailed character makes it possible to ENTITYUNRELATED the prosodie ENTITY of ENTITYUNRELATED ENTITYOTHER in a sophisticated way .
This paper describes the state of the art of coding schemes for dialogue acts and the efforts to establish a standard in this field.	schemes	dialogue acts	usage	{'e1': {'word': 'schemes', 'word_index': [(10, 10)], 'id': 'W99-0305.8'}, 'e2': {'word': 'dialogue acts', 'word_index': [(12, 12)], 'id': 'W99-0305.9'}}	This ENTITYUNRELATED describes the state of the art of ENTITYUNRELATED ENTITY for ENTITYOTHER and the ENTITYUNRELATED to establish a ENTITYUNRELATED in this ENTITYUNRELATED .
We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain, task, and language dependencies of schemes.	comparison	schemes	topic	{'e1': {'word': 'comparison', 'word_index': [(5, 5)], 'id': 'W99-0305.14'}, 'e2': {'word': 'schemes', 'word_index': [(9, 9)], 'id': 'W99-0305.15'}}	We present a ENTITYUNRELATED and ENTITY of currently available ENTITYOTHER and ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED we had ENTITYUNRELATED to ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED .
We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain, task, and language dependencies of schemes.	dependencies	schemes	model-feature	{'e1': {'word': 'dependencies', 'word_index': [(25, 25)], 'id': 'W99-0305.23'}, 'e2': {'word': 'schemes', 'word_index': [(27, 27)], 'id': 'W99-0305.24'}}	We present a ENTITYUNRELATED and ENTITYUNRELATED of currently available ENTITYUNRELATED and ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED we had ENTITYUNRELATED to ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED ENTITY of ENTITYOTHER .
Reusability is a crucial point because production and annotation of corpora is very time and cost consuming but the current broad variety of schemes makes reusability of annotated corpora very hard.	variety	schemes	model-feature	{'e1': {'word': 'variety', 'word_index': [(21, 21)], 'id': 'W99-0305.32'}, 'e2': {'word': 'schemes', 'word_index': [(23, 23)], 'id': 'W99-0305.33'}}	Reusability is a crucial point because production and annotation of ENTITYUNRELATED is very ENTITYUNRELATED and ENTITYUNRELATED consuming but the ENTITYUNRELATED broad ENTITY of ENTITYOTHER makes reusability of annotated ENTITYUNRELATED very hard .
MATE aims to develop general methodological guidelines for the creation, annotation, retrieval and analysis of annotated corpora.	guidelines	creation	topic	{'e1': {'word': 'guidelines', 'word_index': [(6, 6)], 'id': 'W99-0305.41'}, 'e2': {'word': 'creation', 'word_index': [(9, 9)], 'id': 'W99-0305.42'}}	ENTITYUNRELATED aims to ENTITYUNRELATED general methodological ENTITY for the ENTITYOTHER , annotation , ENTITYUNRELATED and ENTITYUNRELATED of annotated ENTITYUNRELATED .
We discuss these concepts and the way they are implemented in the architectural framework of the ADAM corpus, which is a corpus of 450 Italian spontaneous dialogues.	dialogues	corpus	part_whole	{'e1': {'word': 'dialogues', 'word_index': [(27, 27)], 'id': 'W00-1002.12'}, 'e2': {'word': 'corpus', 'word_index': [(22, 22)], 'id': 'W00-1002.11'}}	We discuss these ENTITYUNRELATED and the way they are ENTITYUNRELATED in the architectural ENTITYUNRELATED of the ADAM ENTITYUNRELATED , which is a ENTITYOTHER of 450 Italian spontaneous ENTITY .
Annotating Information Structures In Chinese Texts Using HowNet	Structures	Texts	part_whole	{'e1': {'word': 'Structures', 'word_index': [(2, 2)], 'id': 'W00-1213.2'}, 'e2': {'word': 'Texts', 'word_index': [(5, 5)], 'id': 'W00-1213.4'}}	Annotating ENTITYUNRELATED ENTITY In ENTITYUNRELATED ENTITYOTHER Using HowNet
This paper reported our work on annotating Chinese texts with information structures derived from HowNet.	information structures	texts	model-feature	{'e1': {'word': 'information structures', 'word_index': [(10, 10)], 'id': 'W00-1213.9'}, 'e2': {'word': 'texts', 'word_index': [(8, 8)], 'id': 'W00-1213.8'}}	This ENTITYUNRELATED ENTITYUNRELATED our work on annotating ENTITYUNRELATED ENTITYOTHER with ENTITY derived from HowNet .
An information structure consists of two components: HowNet definitions and dependency relations.	components	information structure	part_whole	{'e1': {'word': 'components', 'word_index': [(5, 5)], 'id': 'W00-1213.11'}, 'e2': {'word': 'information structure', 'word_index': [(1, 1)], 'id': 'W00-1213.10'}}	An ENTITYOTHER consists of two ENTITY : How Net ENTITYUNRELATED and ENTITYUNRELATED .
It is the unit of representation of the meaning of texts.	unit	texts	model-feature	{'e1': {'word': 'unit', 'word_index': [(3, 3)], 'id': 'W00-1213.14'}, 'e2': {'word': 'texts', 'word_index': [(10, 10)], 'id': 'W00-1213.16'}}	It is the ENTITY of ENTITYUNRELATED of the meaning of ENTITYOTHER .
This work is part of a multi-sentential approach to Chinese text understanding.	approach	understanding	usage	{'e1': {'word': 'approach', 'word_index': [(7, 7)], 'id': 'W00-1213.18'}, 'e2': {'word': 'understanding', 'word_index': [(11, 11)], 'id': 'W00-1213.21'}}	This work is ENTITYUNRELATED of a multi-sentential ENTITY to ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
An overview of HowNet and information structure are described in this paper.	paper	overview	topic	{'e1': {'word': 'paper', 'word_index': [(10, 10)], 'id': 'W00-1213.24'}, 'e2': {'word': 'overview', 'word_index': [(1, 1)], 'id': 'W00-1213.22'}}	An ENTITYOTHER of HowNet and ENTITYUNRELATED are described in this ENTITY .
While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme.	dialogue acts	schema	usage	{'e1': {'word': 'dialogue acts', 'word_index': [(1, 1)], 'id': 'W02-0221.4'}, 'e2': {'word': 'schema', 'word_index': [(5, 5)], 'id': 'W02-0221.6'}}	While ENTITY ENTITYUNRELATED a useful ENTITYOTHER for characterizing ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED and human-human ENTITYUNRELATED , their ENTITYUNRELATED is ENTITYUNRELATED by the huge ENTITYUNRELATED involved in ENTITYUNRELATED ENTITYUNRELATED with a ENTITYUNRELATED labelling ENTITYUNRELATED .
While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme.	behaviors	dialogues	part_whole	{'e1': {'word': 'behaviors', 'word_index': [(9, 9)], 'id': 'W02-0221.8'}, 'e2': {'word': 'dialogues', 'word_index': [(14, 14)], 'id': 'W02-0221.10'}}	While ENTITYUNRELATED ENTITYUNRELATED a useful ENTITYUNRELATED for characterizing ENTITYUNRELATED ENTITY in ENTITYUNRELATED and human-human ENTITYOTHER , their ENTITYUNRELATED is ENTITYUNRELATED by the huge ENTITYUNRELATED involved in ENTITYUNRELATED ENTITYUNRELATED with a ENTITYUNRELATED labelling ENTITYUNRELATED .
While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme.	effort	utility	result	{'e1': {'word': 'effort', 'word_index': [(23, 23)], 'id': 'W02-0221.13'}, 'e2': {'word': 'utility', 'word_index': [(17, 17)], 'id': 'W02-0221.11'}}	While ENTITYUNRELATED ENTITYUNRELATED a useful ENTITYUNRELATED for characterizing ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED and human-human ENTITYUNRELATED , their ENTITYOTHER is ENTITYUNRELATED by the huge ENTITY involved in ENTITYUNRELATED ENTITYUNRELATED with a ENTITYUNRELATED labelling ENTITYUNRELATED .
While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme.	scheme	dialogues	model-feature	{'e1': {'word': 'scheme', 'word_index': [(32, 32)], 'id': 'W02-0221.17'}, 'e2': {'word': 'dialogues', 'word_index': [(27, 27)], 'id': 'W02-0221.15'}}	While ENTITYUNRELATED ENTITYUNRELATED a useful ENTITYUNRELATED for characterizing ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED and human-human ENTITYUNRELATED , their ENTITYUNRELATED is ENTITYUNRELATED by the huge ENTITYUNRELATED involved in ENTITYUNRELATED ENTITYOTHER with a ENTITYUNRELATED labelling ENTITY .
In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues.	creation	evaluating	usage	{'e1': {'word': 'creation', 'word_index': [(22, 22)], 'id': 'W02-0221.21'}, 'e2': {'word': 'evaluating', 'word_index': [(26, 26)], 'id': 'W02-0221.23'}}	In this work , we examine whether it is possible to fully automate the ENTITYUNRELATED ENTITYUNRELATED with the ENTITYUNRELATED of enabling rapid ENTITY of ENTITYUNRELATED for ENTITYOTHER spoken ENTITYUNRELATED and comparing them to human - human ENTITYUNRELATED .
In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues.	dialogue systems	dialogues	compare	{'e1': {'word': 'dialogue systems', 'word_index': [(28, 28)], 'id': 'W02-0221.24'}, 'e2': {'word': 'dialogues', 'word_index': [(36, 36)], 'id': 'W02-0221.25'}}	In this work , we examine whether it is possible to fully automate the ENTITYUNRELATED ENTITYUNRELATED with the ENTITYUNRELATED of enabling rapid ENTITYUNRELATED of ENTITYUNRELATED for ENTITYUNRELATED spoken ENTITY and comparing them to human - human ENTITYOTHER .
We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags.	classifier	results	result	{'e1': {'word': 'classifier', 'word_index': [(9, 9)], 'id': 'W02-0221.31'}, 'e2': {'word': 'results', 'word_index': [(2, 2)], 'id': 'W02-0221.27'}}	We ENTITYUNRELATED ENTITYOTHER for ENTITYUNRELATED and ENTITYUNRELATED an ENTITYUNRELATED ENTITY to label the ENTITYUNRELATED provider 's ENTITYUNRELATED in spoken ENTITYUNRELATED and human-human ENTITYUNRELATED with ENTITYUNRELATED ( ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ) ENTITYUNRELATED ENTITYUNRELATED .
We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags.	utterances	dialogues	part_whole	{'e1': {'word': 'utterances', 'word_index': [(16, 16)], 'id': 'W02-0221.33'}, 'e2': {'word': 'dialogues', 'word_index': [(22, 22)], 'id': 'W02-0221.35'}}	We ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED and ENTITYUNRELATED an ENTITYUNRELATED ENTITYUNRELATED to label the ENTITYUNRELATED provider 's ENTITY in spoken ENTITYUNRELATED and human-human ENTITYOTHER with ENTITYUNRELATED ( ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ) ENTITYUNRELATED ENTITYUNRELATED .
We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags.	tags	dialogue act	model-feature	{'e1': {'word': 'tags', 'word_index': [(32, 32)], 'id': 'W02-0221.41'}, 'e2': {'word': 'dialogue act', 'word_index': [(31, 31)], 'id': 'W02-0221.40'}}	We ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED and ENTITYUNRELATED an ENTITYUNRELATED ENTITYUNRELATED to label the ENTITYUNRELATED provider 's ENTITYUNRELATED in spoken ENTITYUNRELATED and human-human ENTITYUNRELATED with ENTITYUNRELATED ( ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ) ENTITYOTHER ENTITY .
We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain.	DATE	corpora	usage	{'e1': {'word': 'DATE', 'word_index': [(5, 5)], 'id': 'W02-0221.44'}, 'e2': {'word': 'corpora', 'word_index': [(18, 18)], 'id': 'W02-0221.47'}}	We ENTITYUNRELATED and ENTITYUNRELATED the ENTITY tagger on various ENTITYUNRELATED of the DARPA Communicator June-2000 and October-2001 ENTITYUNRELATED ENTITYOTHER , and the CMU human - human ENTITYUNRELATED in the travel planning ENTITYUNRELATED .
Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computerdata improves accuracy on the human-human data, when only small amounts of human-human trainingdata are available.	data	accuracy	result	{'e1': {'word': 'data', 'word_index': [(20, 20)], 'id': 'W02-0221.53'}, 'e2': {'word': 'accuracy', 'word_index': [(22, 22)], 'id': 'W02-0221.55'}}	Our ENTITYUNRELATED show that we can achieve high ENTITYUNRELATED on the humancomputer data , and surprisingly , that the ENTITYUNRELATED ENTITY ENTITYUNRELATED ENTITYOTHER on the human-human data , when only small ENTITYUNRELATED of human - human ENTITYUNRELATED ENTITYUNRELATED are available .
We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components, especially in a wide-coverage analysis system.	components	machine translation	usage	{'e1': {'word': 'components', 'word_index': [(19, 19)], 'id': 'W02-1504.8'}, 'e2': {'word': 'machine translation', 'word_index': [(3, 3)], 'id': 'W02-1504.4'}}	We ENTITYUNRELATED that ENTITYOTHER ( MT ) is a useful ENTITYUNRELATED for ENTITYUNRELATED and deriving the ENTITYUNRELATED of NL ENTITY , especially in a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
Given the architecture of our MT system, which is a transfer system based on linguistic modules, correct analysis is expected to be a prerequisite for correct translation, suggesting a correlation between the two, given relatively mature transfer and generation components.	modules	system	usage	{'e1': {'word': 'modules', 'word_index': [(15, 15)], 'id': 'W02-1504.17'}, 'e2': {'word': 'system', 'word_index': [(11, 11)], 'id': 'W02-1504.15'}}	Given the ENTITYUNRELATED of our ENTITYUNRELATED , which is a ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED on linguistic ENTITY , correct ENTITYUNRELATED is expected to be a prerequisite for correct ENTITYUNRELATED , suggesting a ENTITYUNRELATED between the two , given relatively mature ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED .
Given the architecture of our MT system, which is a transfer system based on linguistic modules, correct analysis is expected to be a prerequisite for correct translation, suggesting a correlation between the two, given relatively mature transfer and generation components.	analysis	translation	usage	{'e1': {'word': 'analysis', 'word_index': [(18, 18)], 'id': 'W02-1504.18'}, 'e2': {'word': 'translation', 'word_index': [(27, 27)], 'id': 'W02-1504.19'}}	Given the ENTITYUNRELATED of our ENTITYUNRELATED , which is a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED on linguistic ENTITYUNRELATED , correct ENTITY is expected to be a prerequisite for correct ENTITYOTHER , suggesting a ENTITYUNRELATED between the two , given relatively mature ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED .
We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis.	output	analysis	compare	{'e1': {'word': 'output', 'word_index': [(17, 17)], 'id': 'W02-1504.28'}, 'e2': {'word': 'analysis', 'word_index': [(25, 25)], 'id': 'W02-1504.29'}}	We show through ENTITYUNRELATED that there is indeed a strong ENTITYUNRELATED between the ENTITYUNRELATED of the ENTITYUNRELATED ENTITY and the subjectively determined goodness of the ENTITYOTHER .
We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages.	correlation	analysis	usage	{'e1': {'word': 'correlation', 'word_index': [(3, 3)], 'id': 'W02-1504.30'}, 'e2': {'word': 'analysis', 'word_index': [(13, 13)], 'id': 'W02-1504.32'}}	We use this ENTITY as a guide for ENTITYUNRELATED of a coordinated parallel ENTITYOTHER ENTITYUNRELATED in 7 ENTITYUNRELATED .
This paper presents a Named Entity Extraction (NEE) system for the CoNLL-2003 shared task competition.	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W03-0421.5'}, 'e2': {'word': 'system', 'word_index': [(10, 10)], 'id': 'W03-0421.9'}}	This ENTITY presents a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ( NEE ) ENTITYOTHER for the CoNLL - 2003 shared ENTITYUNRELATED ENTITYUNRELATED .
As in the past year edition ( Carreras et al., 2002a ), we have approached the task by treating the two main sub-tasks of the problem, recognition (NER) and classification (NEC), sequentially and independently with separate modules.	recognition	task	part_whole	{'e1': {'word': 'recognition', 'word_index': [(30, 30)], 'id': 'W03-0421.16'}, 'e2': {'word': 'task', 'word_index': [(19, 19)], 'id': 'W03-0421.13'}}	As in the past year edition ( Carreras et al. , 2002 a ) , we have ENTITYUNRELATED the ENTITYOTHER by treating the two ENTITYUNRELATED sub-tasks of the ENTITYUNRELATED , ENTITY ( NER ) and ENTITYUNRELATED ( NEC ) , sequentially and independently with separate ENTITYUNRELATED .
Both modules are machine learning based systems, which make use of binary and multiclass AdaBoost classifiers.	classifiers	systems	usage	{'e1': {'word': 'classifiers', 'word_index': [(16, 16)], 'id': 'W03-0421.23'}, 'e2': {'word': 'systems', 'word_index': [(6, 6)], 'id': 'W03-0421.22'}}	Both ENTITYUNRELATED are ENTITYUNRELATED learning ENTITYUNRELATED ENTITYOTHER , which make use of binary and multiclass AdaBoost ENTITY .
Named Entity recognition is performed as a greedy sequence tagging procedure under the well-known BIO labelling scheme.	procedure	recognition	usage	{'e1': {'word': 'procedure', 'word_index': [(10, 10)], 'id': 'W03-0421.30'}, 'e2': {'word': 'recognition', 'word_index': [(2, 2)], 'id': 'W03-0421.26'}}	ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER is ENTITYUNRELATED as a greedy ENTITYUNRELATED ENTITYUNRELATED ENTITY under the well - known BIO labelling ENTITYUNRELATED .
This tagging process makes use of three binary classifiers trained to be experts</abstract>	classifiers	process	usage	{'e1': {'word': 'classifiers', 'word_index': [(8, 8)], 'id': 'W03-0421.34'}, 'e2': {'word': 'process', 'word_index': [(2, 2)], 'id': 'W03-0421.33'}}	This ENTITYUNRELATED ENTITYOTHER makes use of three binary ENTITY ENTITYUNRELATED to be ENTITYUNRELATED < / abstract >
WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components	Infrastructure	Integration	usage	{'e1': {'word': 'Infrastructure', 'word_index': [(6, 6)], 'id': 'W03-0802.1'}, 'e2': {'word': 'Integration', 'word_index': [(9, 9)], 'id': 'W03-0802.2'}}	WHAT : An XSLT - Based ENTITY For The ENTITYOTHER Of ENTITYUNRELATED Components
The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components.	project	system	result	{'e1': {'word': 'project', 'word_index': [(1, 1)], 'id': 'W03-0802.9'}, 'e2': {'word': 'system', 'word_index': [(10, 10)], 'id': 'W03-0802.10'}}	The ENTITY came up with the first fully integrated hybrid ENTITYOTHER consisting of a fast HPSG ENTITYUNRELATED that utilizes tokenization , PoS , ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED and ( for German ) topological ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED from shallow ENTITYUNRELATED .
The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components.	morphology	parser	usage	{'e1': {'word': 'morphology', 'word_index': [(23, 23)], 'id': 'W03-0802.12'}, 'e2': {'word': 'parser', 'word_index': [(16, 16)], 'id': 'W03-0802.11'}}	The ENTITYUNRELATED came up with the first fully integrated hybrid ENTITYUNRELATED consisting of a fast HPSG ENTITYOTHER that utilizes tokenization , PoS , ENTITY , ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED and ( for German ) topological ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED from shallow ENTITYUNRELATED .
This integration increases robustness, directs the search space and hence reduces processing time of the deep parser.	integration	robustness	result	{'e1': {'word': 'integration', 'word_index': [(1, 1)], 'id': 'W03-0802.22'}, 'e2': {'word': 'robustness', 'word_index': [(3, 3)], 'id': 'W03-0802.24'}}	This ENTITY ENTITYUNRELATED ENTITYOTHER , directs the ENTITYUNRELATED and hence reduces ENTITYUNRELATED ENTITYUNRELATED of the deep ENTITYUNRELATED .
In this paper, we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture.	paper	integration	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W03-0802.29'}, 'e2': {'word': 'integration', 'word_index': [(11, 11)], 'id': 'W03-0802.31'}}	In this ENTITY , we ENTITYUNRELATED on one of the central ENTITYOTHER facilities , the XSLT - based Whiteboard Annotation Transformer ( WHAT ) , ENTITYUNRELATED on the ENTITYUNRELATED of XSLT - based NLP ENTITYUNRELATED ENTITYUNRELATED , and present ENTITYUNRELATED of XSL ENTITYUNRELATED of shallow and deep annotations used in the integrated ENTITYUNRELATED .
In this paper, we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture.	integration	benefits	result	{'e1': {'word': 'integration', 'word_index': [(35, 35)], 'id': 'W03-0802.35'}, 'e2': {'word': 'benefits', 'word_index': [(28, 28)], 'id': 'W03-0802.33'}}	In this ENTITYUNRELATED , we ENTITYUNRELATED on one of the central ENTITYUNRELATED facilities , the XSLT - based Whiteboard Annotation Transformer ( WHAT ) , ENTITYUNRELATED on the ENTITYOTHER of XSLT - based NLP ENTITYUNRELATED ENTITY , and present ENTITYUNRELATED of XSL ENTITYUNRELATED of shallow and deep annotations used in the integrated ENTITYUNRELATED .
The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications.	infrastructure	development	usage	{'e1': {'word': 'infrastructure', 'word_index': [(1, 1)], 'id': 'W03-0802.39'}, 'e2': {'word': 'development', 'word_index': [(16, 16)], 'id': 'W03-0802.40'}}	The ENTITY is open , portable and well suited for , but not restricted to the ENTITYOTHER of hybrid NLP ENTITYUNRELATED as well as ENTITYUNRELATED .
This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments, enhancing common notions of text similarity.	paper	task	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W99-0907.4'}, 'e2': {'word': 'task', 'word_index': [(5, 5)], 'id': 'W99-0907.5'}}	This ENTITY addresses a novel ENTITYOTHER of detecting ENTITYUNRELATED ENTITYUNRELATED in a ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED , enhancing ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED .
This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments, enhancing common notions of text similarity.	correspondence	fragments	part_whole	{'e1': {'word': 'correspondence', 'word_index': [(9, 9)], 'id': 'W99-0907.7'}, 'e2': {'word': 'fragments', 'word_index': [(15, 15)], 'id': 'W99-0907.10'}}	This ENTITYUNRELATED addresses a novel ENTITYUNRELATED of detecting ENTITYUNRELATED ENTITY in a ENTITYUNRELATED of ENTITYUNRELATED ENTITYOTHER , enhancing ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED .
This task is addressed by coupling corresponding term subsets through bipartite clustering.	clustering	task	usage	{'e1': {'word': 'clustering', 'word_index': [(11, 11)], 'id': 'W99-0907.17'}, 'e2': {'word': 'task', 'word_index': [(1, 1)], 'id': 'W99-0907.15'}}	This ENTITYOTHER is addressed by coupling corresponding ENTITYUNRELATED subsets through bipartite ENTITY .
The paper presents a cost-based clustering scheme and compares it with a bipartite version of the single-link method, providing illustrating results.	paper	scheme	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W99-0907.18'}, 'e2': {'word': 'scheme', 'word_index': [(6, 6)], 'id': 'W99-0907.21'}}	The ENTITY presents a ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER and compares it with a bipartite ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED illustrating ENTITYUNRELATED .
This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL.	paper	submissions	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W08-0327.2'}, 'e2': {'word': 'submissions', 'word_index': [(4, 4)], 'id': 'W08-0327.3'}}	This ENTITY describes SYSTRAN ENTITYOTHER for the shared ENTITYUNRELATED of the third ENTITYUNRELATED on ENTITYUNRELATED at ACL .
This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL.	Workshop	Statistical Machine Translation	topic	{'e1': {'word': 'Workshop', 'word_index': [(12, 12)], 'id': 'W08-0327.5'}, 'e2': {'word': 'Statistical Machine Translation', 'word_index': [(14, 14)], 'id': 'W08-0327.6'}}	This ENTITYUNRELATED describes SYSTRAN ENTITYUNRELATED for the shared ENTITYUNRELATED of the third ENTITY on ENTITYOTHER at ACL .
Our main contribution consists in a French-English statistical model trained without the use of any human-translated parallel corpus.	contribution	statistical model	topic	{'e1': {'word': 'contribution', 'word_index': [(2, 2)], 'id': 'W08-0327.8'}, 'e2': {'word': 'statistical model', 'word_index': [(9, 9)], 'id': 'W08-0327.10'}}	Our ENTITYUNRELATED ENTITY consists in a French - ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED without the use of any human-translated ENTITYUNRELATED .
In substitution, we translated a monolingual corpus with SYSTRAN rule-based translation engine to produce the parallel corpus.	engine	parallel corpus	usage	{'e1': {'word': 'engine', 'word_index': [(12, 12)], 'id': 'W08-0327.18'}, 'e2': {'word': 'parallel corpus', 'word_index': [(16, 16)], 'id': 'W08-0327.19'}}	In ENTITYUNRELATED , we ENTITYUNRELATED a monolingual ENTITYUNRELATED with SYSTRAN ENTITYUNRELATED ENTITYUNRELATED ENTITY to produce the ENTITYOTHER .
The dichotomy of topic and focus, based, in the Praguean Functional Generative Description, on the scale of communicative dynamism, is relevant not only for a possible placement of the sentence in a context, but also for its semantic interpretation.	semantic interpretation	sentence	model-feature	{'e1': {'word': 'semantic interpretation', 'word_index': [(42, 42)], 'id': 'J95-1004.13'}, 'e2': {'word': 'sentence', 'word_index': [(33, 33)], 'id': 'J95-1004.11'}}	The dichotomy of ENTITYUNRELATED and ENTITYUNRELATED , ENTITYUNRELATED , in the Praguean Functional Generative ENTITYUNRELATED , on the ENTITYUNRELATED of communicative dynamism , is relevant not only for a possible placement of the ENTITYOTHER in a ENTITYUNRELATED , but also for its ENTITY .
An automatic identification of topic and focus may use the input information on word order, on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus), on definiteness, and on lexical semantic properties of words.	information	identification	usage	{'e1': {'word': 'information', 'word_index': [(11, 11)], 'id': 'J95-1004.19'}, 'e2': {'word': 'identification', 'word_index': [(2, 2)], 'id': 'J95-1004.15'}}	An ENTITYUNRELATED ENTITYOTHER of ENTITYUNRELATED and ENTITYUNRELATED may use the ENTITYUNRELATED ENTITY on ENTITYUNRELATED ENTITYUNRELATED , on the systemic ENTITYUNRELATED of ENTITYUNRELATED of complementations ( reflected by the underlying ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED in the ENTITYUNRELATED ) , on definiteness , and on ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED .
An automatic identification of topic and focus may use the input information on word order, on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus), on definiteness, and on lexical semantic properties of words.	order	items	model-feature	{'e1': {'word': 'order', 'word_index': [(29, 29)], 'id': 'J95-1004.24'}, 'e2': {'word': 'items', 'word_index': [(32, 32)], 'id': 'J95-1004.25'}}	An ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED may use the ENTITYUNRELATED ENTITYUNRELATED on ENTITYUNRELATED ENTITYUNRELATED , on the systemic ENTITYUNRELATED of ENTITYUNRELATED of complementations ( reflected by the underlying ENTITY of the ENTITYOTHER ENTITYUNRELATED in the ENTITYUNRELATED ) , on definiteness , and on ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED .
An automatic identification of topic and focus may use the input information on word order, on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus), on definiteness, and on lexical semantic properties of words.	semantic properties	words	model-feature	{'e1': {'word': 'semantic properties', 'word_index': [(45, 45)], 'id': 'J95-1004.29'}, 'e2': {'word': 'words', 'word_index': [(47, 47)], 'id': 'J95-1004.30'}}	An ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED may use the ENTITYUNRELATED ENTITYUNRELATED on ENTITYUNRELATED ENTITYUNRELATED , on the systemic ENTITYUNRELATED of ENTITYUNRELATED of complementations ( reflected by the underlying ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED in the ENTITYUNRELATED ) , on definiteness , and on ENTITYUNRELATED ENTITY of ENTITYOTHER .
An algorithm for the analysis of English sentences has been implemented and is discussed and illustrated on several examples.	algorithm	analysis	usage	{'e1': {'word': 'algorithm', 'word_index': [(1, 1)], 'id': 'J95-1004.31'}, 'e2': {'word': 'analysis', 'word_index': [(4, 4)], 'id': 'J95-1004.32'}}	An ENTITY for the ENTITYOTHER of ENTITYUNRELATED ENTITYUNRELATED has been ENTITYUNRELATED and is discussed and illustrated on several ENTITYUNRELATED .
Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules	conversion	languages	usage	{'e1': {'word': 'conversion', 'word_index': [(1, 1)], 'id': 'J96-3003.5'}, 'e2': {'word': 'languages', 'word_index': [(11, 11)], 'id': 'J96-3003.6'}}	ENTITYUNRELATED ENTITY ( GTPC ) has been achieved in most European ENTITYOTHER by ENTITYUNRELATED look - up or using ENTITYUNRELATED
The application of these methods, however, in the reverse process, (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems, especially in inflectionally rich languages.	methods	process	usage	{'e1': {'word': 'methods', 'word_index': [(4, 4)], 'id': 'J96-3003.10'}, 'e2': {'word': 'process', 'word_index': [(11, 11)], 'id': 'J96-3003.11'}}	The ENTITYUNRELATED of these ENTITY , however , in the reverse ENTITYOTHER , ( i.e. , in ENTITYUNRELATED ENTITYUNRELATED [ PTGC ] ) creates serious ENTITYUNRELATED , especially in inflectionally rich ENTITYUNRELATED .
The application of these methods, however, in the reverse process, (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems, especially in inflectionally rich languages.	problems	languages	model-feature	{'e1': {'word': 'problems', 'word_index': [(25, 25)], 'id': 'J96-3003.14'}, 'e2': {'word': 'languages', 'word_index': [(31, 31)], 'id': 'J96-3003.15'}}	The ENTITYUNRELATED of these ENTITYUNRELATED , however , in the reverse ENTITYUNRELATED , ( i.e. , in ENTITYUNRELATED ENTITYUNRELATED [ PTGC ] ) creates serious ENTITY , especially in inflectionally rich ENTITYOTHER .
In this paper the PTGC problem is approached from a completely different point of view.	paper	problem	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'J96-3003.16'}, 'e2': {'word': 'problem', 'word_index': [(5, 5)], 'id': 'J96-3003.17'}}	In this ENTITY the PTGC ENTITYOTHER is ENTITYUNRELATED from a completely different ENTITYUNRELATED .
Instead of rules or a dictionary, the statistics of language connecting pronunciation to spelling are exploited.	rules	statistics	compare	{'e1': {'word': 'rules', 'word_index': [(2, 2)], 'id': 'J96-3003.20'}, 'e2': {'word': 'statistics', 'word_index': [(8, 8)], 'id': 'J96-3003.22'}}	Instead of ENTITY or a ENTITYUNRELATED , the ENTITYOTHER of ENTITYUNRELATED connecting ENTITYUNRELATED to ENTITYUNRELATED are exploited .
The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm.	Markov models	features	model-feature	{'e1': {'word': 'Markov models', 'word_index': [(14, 14)], 'id': 'J96-3003.31'}, 'e2': {'word': 'features', 'word_index': [(8, 8)], 'id': 'J96-3003.29'}}	The ENTITYUNRELATED lies in ENTITYUNRELATED the ENTITYUNRELATED intraword ENTITYOTHER using the ENTITYUNRELATED of hidden ENTITY ( HMM ) and ENTITYUNRELATED the ENTITYUNRELATED using the Viterbi ENTITYUNRELATED .
The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm.	algorithm	conversion	usage	{'e1': {'word': 'algorithm', 'word_index': [(25, 25)], 'id': 'J96-3003.34'}, 'e2': {'word': 'conversion', 'word_index': [(21, 21)], 'id': 'J96-3003.33'}}	The ENTITYUNRELATED lies in ENTITYUNRELATED the ENTITYUNRELATED intraword ENTITYUNRELATED using the ENTITYUNRELATED of hidden ENTITYUNRELATED ( HMM ) and ENTITYUNRELATED the ENTITYOTHER using the Viterbi ENTITY .
The PTGC system has been established and tested on various multilingual corpora.	system	corpora	usage	{'e1': {'word': 'system', 'word_index': [(2, 2)], 'id': 'J96-3003.35'}, 'e2': {'word': 'corpora', 'word_index': [(11, 11)], 'id': 'J96-3003.37'}}	The PTGC ENTITY has been established and ENTITYUNRELATED on various multilingual ENTITYOTHER .
Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word.	algorithm	transcription	usage	{'e1': {'word': 'algorithm', 'word_index': [(9, 9)], 'id': 'J96-3003.40'}, 'e2': {'word': 'transcription', 'word_index': [(16, 16)], 'id': 'J96-3003.41'}}	Initially , the ENTITYUNRELATED HMM and the ENTITYUNRELATED Viterbi ENTITY were used to obtain a single ENTITYOTHER for each ENTITYUNRELATED .
Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones).	transcriptions	input	model-feature	{'e1': {'word': 'transcriptions', 'word_index': [(19, 19)], 'id': 'J96-3003.48'}, 'e2': {'word': 'input', 'word_index': [(23, 23)], 'id': 'J96-3003.50'}}	Afterwards , the ENTITYUNRELATED HMM and the N-best ENTITYUNRELATED ENTITYUNRELATED to PTGC were ENTITYUNRELATED to ENTITYUNRELATED one or more ENTITY for each ENTITYUNRELATED ENTITYOTHER ( homophones ) .
This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates) for most of the seven languages it was tested on (Dutch, English, French, German, Greek, Italian, and Spanish).	system	words	result	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'J96-3003.51'}, 'e2': {'word': 'words', 'word_index': [(13, 13)], 'id': 'J96-3003.52'}}	This ENTITY gave an average score of more than 99 % correctly transcribed ENTITYOTHER ( overall ENTITYUNRELATED in the first four ENTITYUNRELATED ) for most of the seven ENTITYUNRELATED it was ENTITYUNRELATED on ( Dutch , ENTITYUNRELATED , French , German , ENTITYUNRELATED , Italian , and Spanish ) .
The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems.	system	language	usage	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'J96-3003.59'}, 'e2': {'word': 'language', 'word_index': [(8, 8)], 'id': 'J96-3003.61'}}	The ENTITY can be ENTITYUNRELATED to almost any ENTITYOTHER with little ENTITYUNRELATED and can be ENTITYUNRELATED in hardware to serve in ENTITYUNRELATED ENTITYUNRELATED .
The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS.	paper	interface	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'P98-2171.6'}, 'e2': {'word': 'interface', 'word_index': [(4, 4)], 'id': 'P98-2171.7'}}	The ENTITY describes an ENTITYOTHER between ENTITYUNRELATED and synthesizer of the German ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED VieCtoS.
The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS.	generator	system	part_whole	{'e1': {'word': 'generator', 'word_index': [(6, 6)], 'id': 'P98-2171.8'}, 'e2': {'word': 'system', 'word_index': [(14, 14)], 'id': 'P98-2171.11'}}	The ENTITYUNRELATED describes an ENTITYUNRELATED between ENTITY and synthesizer of the German ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER VieCtoS.
It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies (projection of information structure into syntax) and prosodie context (performance-related modifications to intonation patterns).	phenomena	intonation	part_whole	{'e1': {'word': 'phenomena', 'word_index': [(2, 2)], 'id': 'P98-2171.12'}, 'e2': {'word': 'intonation', 'word_index': [(5, 5)], 'id': 'P98-2171.13'}}	It discusses ENTITY in German ENTITYOTHER that depend on the ENTITYUNRELATED between grammatical ENTITYUNRELATED ( ENTITYUNRELATED of ENTITYUNRELATED into ENTITYUNRELATED ) and prosodie ENTITYUNRELATED ( ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED ) .
It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies (projection of information structure into syntax) and prosodie context (performance-related modifications to intonation patterns).	interaction	dependencies	model-feature	{'e1': {'word': 'interaction', 'word_index': [(10, 10)], 'id': 'P98-2171.14'}, 'e2': {'word': 'dependencies', 'word_index': [(13, 13)], 'id': 'P98-2171.15'}}	It discusses ENTITYUNRELATED in German ENTITYUNRELATED that depend on the ENTITY between grammatical ENTITYOTHER ( ENTITYUNRELATED of ENTITYUNRELATED into ENTITYUNRELATED ) and prosodie ENTITYUNRELATED ( ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED ) .
Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation.	dimensions	processing	part_whole	{'e1': {'word': 'dimensions', 'word_index': [(11, 11)], 'id': 'P98-2171.26'}, 'e2': {'word': 'processing', 'word_index': [(1, 1)], 'id': 'P98-2171.24'}}	Phonological ENTITYOTHER in our ENTITYUNRELATED comprises segmental as well as suprasegmental ENTITY such as syllabification , ENTITYUNRELATED of ENTITYUNRELATED stress positions , and a symbolic encoding of ENTITYUNRELATED .
Phonological phenomena often touch upon more than one of these dimensions, so that mutual accessibility of thedata structures on each dimension had to be ensured.	dimensions	phenomena	model-feature	{'e1': {'word': 'dimensions', 'word_index': [(10, 10)], 'id': 'P98-2171.31'}, 'e2': {'word': 'phenomena', 'word_index': [(1, 1)], 'id': 'P98-2171.30'}}	Phonological ENTITYOTHER often touch upon more than one of these ENTITY , so that mutual accessibility of the ENTITYUNRELATED ENTITYUNRELATED on each ENTITYUNRELATED had to be ensured .
We present a linear representation of the multidimensional phonologicaldata based on a straightforward linearization convention, which suffices to bring this conceptually multilineardata set under the scope of the well-known processing techniques for two-level morphology.	representation	data	model-feature	{'e1': {'word': 'representation', 'word_index': [(4, 4)], 'id': 'P98-2171.35'}, 'e2': {'word': 'data', 'word_index': [(9, 9)], 'id': 'P98-2171.36'}}	We present a linear ENTITY of the multidimensional phonological ENTITYOTHER ENTITYUNRELATED on a straightforward linearization ENTITYUNRELATED , which suffices to bring this conceptually multilinear ENTITYUNRELATED set under the ENTITYUNRELATED of the well - known ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED .
We present a linear representation of the multidimensional phonologicaldata based on a straightforward linearization convention, which suffices to bring this conceptually multilineardata set under the scope of the well-known processing techniques for two-level morphology.	morphology	data	usage	{'e1': {'word': 'morphology', 'word_index': [(38, 38)], 'id': 'P98-2171.44'}, 'e2': {'word': 'data', 'word_index': [(24, 24)], 'id': 'P98-2171.39'}}	We present a linear ENTITYUNRELATED of the multidimensional phonological ENTITYUNRELATED ENTITYUNRELATED on a straightforward linearization ENTITYUNRELATED , which suffices to bring this conceptually multilinear ENTITYOTHER set under the ENTITYUNRELATED of the well - known ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITY .
An open, extendible multi-dictionary system is introduced in the paper.	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(10, 10)], 'id': 'P98-2175.5'}, 'e2': {'word': 'system', 'word_index': [(5, 5)], 'id': 'P98-2175.4'}}	An open , extendible ENTITYUNRELATED ENTITYOTHER is introduced in the ENTITY .
It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora.	entries	dictionaries	part_whole	{'e1': {'word': 'entries', 'word_index': [(7, 7)], 'id': 'P98-2175.9'}, 'e2': {'word': 'dictionaries', 'word_index': [(13, 13)], 'id': 'P98-2175.10'}}	It ENTITYUNRELATED the ENTITYUNRELATED in ENTITYUNRELATED adequate ENTITY of various bi- and monolingual ENTITYOTHER and ENTITYUNRELATED ENTITYUNRELATED from ENTITYUNRELATED .
It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora.	examples	parallel corpora	part_whole	{'e1': {'word': 'examples', 'word_index': [(16, 16)], 'id': 'P98-2175.12'}, 'e2': {'word': 'parallel corpora', 'word_index': [(18, 18)], 'id': 'P98-2175.13'}}	It ENTITYUNRELATED the ENTITYUNRELATED in ENTITYUNRELATED adequate ENTITYUNRELATED of various bi- and monolingual ENTITYUNRELATED and ENTITYUNRELATED ENTITY from ENTITYOTHER .
The implemented system (called MoBiDic) knows morphological rules of the dictionaries' languages.	rules	system	usage	{'e1': {'word': 'rules', 'word_index': [(9, 9)], 'id': 'P98-2175.25'}, 'e2': {'word': 'system', 'word_index': [(2, 2)], 'id': 'P98-2175.23'}}	The ENTITYUNRELATED ENTITYOTHER ( ENTITYUNRELATED MoBiDic ) knows morphological ENTITY of the ENTITYUNRELATED ' ENTITYUNRELATED .
Thus, never the actual (inflected) words, but always their lemmas - that is, the right dictionary entries - are looked up.	lemmas	words	model-feature	{'e1': {'word': 'lemmas', 'word_index': [(13, 13)], 'id': 'P98-2175.29'}, 'e2': {'word': 'words', 'word_index': [(8, 8)], 'id': 'P98-2175.28'}}	Thus , never the actual ( inflected ) ENTITYOTHER , but always their ENTITY - that is , the right ENTITYUNRELATED ENTITYUNRELATED - are looked up .
MoBiDic has an open, multimedial architecture, thus it is suitable for handling not only textual, but speaking or picture dictionaries, as well.	architecture	dictionaries	usage	{'e1': {'word': 'architecture', 'word_index': [(6, 6)], 'id': 'P98-2175.32'}, 'e2': {'word': 'dictionaries', 'word_index': [(22, 22)], 'id': 'P98-2175.33'}}	MoBiDic has an open , multimedial ENTITY , thus it is suitable for handling not only textual , but speaking or picture ENTITYOTHER , as well .
The same system is also able to find words and expressions in corpora, dynamically providing the translators with examples from their earlier translations or other translators' works.	expressions	corpora	part_whole	{'e1': {'word': 'expressions', 'word_index': [(10, 10)], 'id': 'P98-2175.36'}, 'e2': {'word': 'corpora', 'word_index': [(12, 12)], 'id': 'P98-2175.37'}}	The same ENTITYUNRELATED is also able to find ENTITYUNRELATED and ENTITY in ENTITYOTHER , dynamically ENTITYUNRELATED the ENTITYUNRELATED with ENTITYUNRELATED from their earlier ENTITYUNRELATED or other ENTITYUNRELATED ' works .
The same system is also able to find words and expressions in corpora, dynamically providing the translators with examples from their earlier translations or other translators' works.	translations	examples	part_whole	{'e1': {'word': 'translations', 'word_index': [(23, 23)], 'id': 'P98-2175.41'}, 'e2': {'word': 'examples', 'word_index': [(19, 19)], 'id': 'P98-2175.40'}}	The same ENTITYUNRELATED is also able to find ENTITYUNRELATED and ENTITYUNRELATED in ENTITYUNRELATED , dynamically ENTITYUNRELATED the ENTITYUNRELATED with ENTITYOTHER from their earlier ENTITY or other ENTITYUNRELATED ' works .
Identifying the original forms of content words is crucial for natural language processing and information retrieval.	forms	content words	model-feature	{'e1': {'word': 'forms', 'word_index': [(3, 3)], 'id': 'I08-1001.7'}, 'e2': {'word': 'content words', 'word_index': [(5, 5)], 'id': 'I08-1001.8'}}	ENTITYUNRELATED the original ENTITY of ENTITYOTHER is crucial for ENTITYUNRELATED and ENTITYUNRELATED .
We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval.	method	indexing	usage	{'e1': {'word': 'method', 'word_index': [(11, 11)], 'id': 'I08-1001.14'}, 'e2': {'word': 'indexing', 'word_index': [(13, 13)], 'id': 'I08-1001.15'}}	We ENTITYUNRELATED a lemmatization ENTITYUNRELATED for Modern Mongolian and ENTITYUNRELATED our ENTITY to ENTITYOTHER for ENTITYUNRELATED .
We use technical abstracts to show the effectiveness of our method experimentally.	method	abstracts	usage	{'e1': {'word': 'method', 'word_index': [(10, 10)], 'id': 'I08-1001.19'}, 'e2': {'word': 'abstracts', 'word_index': [(3, 3)], 'id': 'I08-1001.17'}}	We use technical ENTITYOTHER to show the ENTITYUNRELATED of our ENTITY experimentally .
Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers.	parsers	parsers	compare	{'e1': {'word': 'parsers', 'word_index': [(2, 2)], 'id': 'W04-2003.7'}, 'e2': {'word': 'parsers', 'word_index': [(20, 20)], 'id': 'W04-2003.10'}}	Modern ENTITYUNRELATED ENTITY are ENTITYUNRELATED and quite fast , but their ENTITYUNRELATED is relatively shallow when compared to formal grammar ENTITYOTHER .
We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser.	complexity	parser	model-feature	{'e1': {'word': 'complexity', 'word_index': [(20, 20)], 'id': 'W04-2003.16'}, 'e2': {'word': 'parser', 'word_index': [(24, 24)], 'id': 'W04-2003.18'}}	We suggest to extend ENTITYUNRELATED ENTITYUNRELATED to a more ENTITYUNRELATED while at the same ENTITYUNRELATED keeping the ENTITYUNRELATED and low ENTITY of a ENTITYUNRELATED ENTITYOTHER .
With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article.	parser	applications	usage	{'e1': {'word': 'parser', 'word_index': [(20, 20)], 'id': 'W04-2003.35'}, 'e2': {'word': 'applications', 'word_index': [(28, 28)], 'id': 'W04-2003.38'}}	With its ENTITYUNRELATED ENTITYUNRELATED of about 300,000 ENTITYUNRELATED per hour and state - of - the - art ENTITYUNRELATED the ENTITY is reliable for a ENTITYUNRELATED of ENTITYUNRELATED ENTITYOTHER discussed in the article .
GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platform's efficiency through the development of two different applications based on this platform: EG-Banking, a voice-portal for high-quality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction.	research project	development	topic	{'e1': {'word': 'research project', 'word_index': [(14, 14)], 'id': 'W04-2306.8'}, 'e2': {'word': 'development', 'word_index': [(25, 25)], 'id': 'W04-2306.11'}}	GEMINI ( Generic ENTITYUNRELATED for Multilingual Interactive ENTITYUNRELATED Interfaces ) is an EC funded ENTITY , which has two ENTITYUNRELATED ENTITYUNRELATED : First , the ENTITYOTHER of a flexible ENTITYUNRELATED able to produce ENTITYUNRELATED interactive multilingual and multi-modal ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED with a ENTITYUNRELATED of human ENTITYUNRELATED , and , second , the ENTITYUNRELATED of the ENTITYUNRELATED 's ENTITYUNRELATED through the ENTITYUNRELATED of two different ENTITYUNRELATED ENTITYUNRELATED on this ENTITYUNRELATED : EG - Banking , a voice - portal for ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED , and CitizenCare , an e-government ENTITYUNRELATED ENTITYUNRELATED for citizen-to-administration ENTITYUNRELATED which are available for spoken and web - based ENTITYUNRELATED ENTITYUNRELATED .
GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platform's efficiency through the development of two different applications based on this platform: EG-Banking, a voice-portal for high-quality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction.	platform	dialogue	usage	{'e1': {'word': 'platform', 'word_index': [(29, 29)], 'id': 'W04-2306.12'}, 'e2': {'word': 'dialogue', 'word_index': [(38, 38)], 'id': 'W04-2306.14'}}	GEMINI ( Generic ENTITYUNRELATED for Multilingual Interactive ENTITYUNRELATED Interfaces ) is an EC funded ENTITYUNRELATED , which has two ENTITYUNRELATED ENTITYUNRELATED : First , the ENTITYUNRELATED of a flexible ENTITY able to produce ENTITYUNRELATED interactive multilingual and multi-modal ENTITYOTHER ENTITYUNRELATED to ENTITYUNRELATED with a ENTITYUNRELATED of human ENTITYUNRELATED , and , second , the ENTITYUNRELATED of the ENTITYUNRELATED 's ENTITYUNRELATED through the ENTITYUNRELATED of two different ENTITYUNRELATED ENTITYUNRELATED on this ENTITYUNRELATED : EG - Banking , a voice - portal for ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED , and CitizenCare , an e-government ENTITYUNRELATED ENTITYUNRELATED for citizen-to-administration ENTITYUNRELATED which are available for spoken and web - based ENTITYUNRELATED ENTITYUNRELATED .
GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platform's efficiency through the development of two different applications based on this platform: EG-Banking, a voice-portal for high-quality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction.	interfaces	databases	part_whole	{'e1': {'word': 'interfaces', 'word_index': [(39, 39)], 'id': 'W04-2306.15'}, 'e2': {'word': 'databases', 'word_index': [(41, 41)], 'id': 'W04-2306.16'}}	GEMINI ( Generic ENTITYUNRELATED for Multilingual Interactive ENTITYUNRELATED Interfaces ) is an EC funded ENTITYUNRELATED , which has two ENTITYUNRELATED ENTITYUNRELATED : First , the ENTITYUNRELATED of a flexible ENTITYUNRELATED able to produce ENTITYUNRELATED interactive multilingual and multi-modal ENTITYUNRELATED ENTITY to ENTITYOTHER with a ENTITYUNRELATED of human ENTITYUNRELATED , and , second , the ENTITYUNRELATED of the ENTITYUNRELATED 's ENTITYUNRELATED through the ENTITYUNRELATED of two different ENTITYUNRELATED ENTITYUNRELATED on this ENTITYUNRELATED : EG - Banking , a voice - portal for ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED , and CitizenCare , an e-government ENTITYUNRELATED ENTITYUNRELATED for citizen-to-administration ENTITYUNRELATED which are available for spoken and web - based ENTITYUNRELATED ENTITYUNRELATED .
GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platform's efficiency through the development of two different applications based on this platform: EG-Banking, a voice-portal for high-quality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction.	efficiency	platform	model-feature	{'e1': {'word': 'efficiency', 'word_index': [(59, 59)], 'id': 'W04-2306.21'}, 'e2': {'word': 'platform', 'word_index': [(57, 57)], 'id': 'W04-2306.20'}}	GEMINI ( Generic ENTITYUNRELATED for Multilingual Interactive ENTITYUNRELATED Interfaces ) is an EC funded ENTITYUNRELATED , which has two ENTITYUNRELATED ENTITYUNRELATED : First , the ENTITYUNRELATED of a flexible ENTITYUNRELATED able to produce ENTITYUNRELATED interactive multilingual and multi-modal ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED with a ENTITYUNRELATED of human ENTITYUNRELATED , and , second , the ENTITYUNRELATED of the ENTITYOTHER 's ENTITY through the ENTITYUNRELATED of two different ENTITYUNRELATED ENTITYUNRELATED on this ENTITYUNRELATED : EG - Banking , a voice - portal for ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED , and CitizenCare , an e-government ENTITYUNRELATED ENTITYUNRELATED for citizen-to-administration ENTITYUNRELATED which are available for spoken and web - based ENTITYUNRELATED ENTITYUNRELATED .
GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platform's efficiency through the development of two different applications based on this platform: EG-Banking, a voice-portal for high-quality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction.	platform	applications	usage	{'e1': {'word': 'platform', 'word_index': [(70, 70)], 'id': 'W04-2306.25'}, 'e2': {'word': 'applications', 'word_index': [(66, 66)], 'id': 'W04-2306.23'}}	GEMINI ( Generic ENTITYUNRELATED for Multilingual Interactive ENTITYUNRELATED Interfaces ) is an EC funded ENTITYUNRELATED , which has two ENTITYUNRELATED ENTITYUNRELATED : First , the ENTITYUNRELATED of a flexible ENTITYUNRELATED able to produce ENTITYUNRELATED interactive multilingual and multi-modal ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED with a ENTITYUNRELATED of human ENTITYUNRELATED , and , second , the ENTITYUNRELATED of the ENTITYUNRELATED 's ENTITYUNRELATED through the ENTITYUNRELATED of two different ENTITYOTHER ENTITYUNRELATED on this ENTITY : EG - Banking , a voice - portal for ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED , and CitizenCare , an e-government ENTITYUNRELATED ENTITYUNRELATED for citizen-to-administration ENTITYUNRELATED which are available for spoken and web - based ENTITYUNRELATED ENTITYUNRELATED .
GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platform's efficiency through the development of two different applications based on this platform: EG-Banking, a voice-portal for high-quality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction.	framework	interaction	usage	{'e1': {'word': 'framework', 'word_index': [(93, 93)], 'id': 'W04-2306.31'}, 'e2': {'word': 'interaction', 'word_index': [(96, 96)], 'id': 'W04-2306.32'}}	GEMINI ( Generic ENTITYUNRELATED for Multilingual Interactive ENTITYUNRELATED Interfaces ) is an EC funded ENTITYUNRELATED , which has two ENTITYUNRELATED ENTITYUNRELATED : First , the ENTITYUNRELATED of a flexible ENTITYUNRELATED able to produce ENTITYUNRELATED interactive multilingual and multi-modal ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED with a ENTITYUNRELATED of human ENTITYUNRELATED , and , second , the ENTITYUNRELATED of the ENTITYUNRELATED 's ENTITYUNRELATED through the ENTITYUNRELATED of two different ENTITYUNRELATED ENTITYUNRELATED on this ENTITYUNRELATED : EG - Banking , a voice - portal for ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED , and CitizenCare , an e-government ENTITYUNRELATED ENTITY for citizen-to-administration ENTITYOTHER which are available for spoken and web - based ENTITYUNRELATED ENTITYUNRELATED .
Strategies For Advanced Question Answering	Strategies	Question Answering	usage	{'e1': {'word': 'Strategies', 'word_index': [(0, 0)], 'id': 'W04-2501.1'}, 'e2': {'word': 'Question Answering', 'word_index': [(3, 3)], 'id': 'W04-2501.2'}}	ENTITY For Advanced ENTITYOTHER
Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity; (2) enhancing the precision of question interpretation and answer extraction; and (3) question decomposition and answer fusion	strategies	Question Answering	usage	{'e1': {'word': 'strategies', 'word_index': [(12, 12)], 'id': 'W04-2501.5'}, 'e2': {'word': 'Question Answering', 'word_index': [(2, 2)], 'id': 'W04-2501.4'}}	ENTITYUNRELATED in ENTITYOTHER can be achieved by ( 1 ) combining multiple ENTITY that optimally resolve different ENTITYUNRELATED ENTITYUNRELATED of various ENTITYUNRELATED of ENTITYUNRELATED ; ( 2 ) enhancing the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED and answer ENTITYUNRELATED ; and ( 3 ) ENTITYUNRELATED ENTITYUNRELATED and answer ENTITYUNRELATED
Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity; (2) enhancing the precision of question interpretation and answer extraction; and (3) question decomposition and answer fusion	complexity	classes	model-feature	{'e1': {'word': 'complexity', 'word_index': [(23, 23)], 'id': 'W04-2501.9'}, 'e2': {'word': 'classes', 'word_index': [(18, 18)], 'id': 'W04-2501.7'}}	ENTITYUNRELATED in ENTITYUNRELATED can be achieved by ( 1 ) combining multiple ENTITYUNRELATED that optimally resolve different ENTITYUNRELATED ENTITYOTHER of various ENTITYUNRELATED of ENTITY ; ( 2 ) enhancing the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED and answer ENTITYUNRELATED ; and ( 3 ) ENTITYUNRELATED ENTITYUNRELATED and answer ENTITYUNRELATED
Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity; (2) enhancing the precision of question interpretation and answer extraction; and (3) question decomposition and answer fusion	interpretation	precision	result	{'e1': {'word': 'interpretation', 'word_index': [(33, 33)], 'id': 'W04-2501.12'}, 'e2': {'word': 'precision', 'word_index': [(30, 30)], 'id': 'W04-2501.10'}}	ENTITYUNRELATED in ENTITYUNRELATED can be achieved by ( 1 ) combining multiple ENTITYUNRELATED that optimally resolve different ENTITYUNRELATED ENTITYUNRELATED of various ENTITYUNRELATED of ENTITYUNRELATED ; ( 2 ) enhancing the ENTITYOTHER of ENTITYUNRELATED ENTITY and answer ENTITYUNRELATED ; and ( 3 ) ENTITYUNRELATED ENTITYUNRELATED and answer ENTITYUNRELATED
In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.	paper	impact	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W04-2501.17'}, 'e2': {'word': 'impact', 'word_index': [(7, 7)], 'id': 'W04-2501.18'}}	In this ENTITY we also present the ENTITYOTHER of ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED on Q/A and discuss the pragmatics pf ENTITYUNRELATED ENTITYUNRELATED in Q/A.
Like previous work in this area, our approach exploits orthographic regularities in a search for possible morphological segmentation points.	regularities	approach	usage	{'e1': {'word': 'regularities', 'word_index': [(11, 11)], 'id': 'W05-0617.10'}, 'e2': {'word': 'approach', 'word_index': [(8, 8)], 'id': 'W05-0617.9'}}	Like previous work in this ENTITYUNRELATED , our ENTITYOTHER exploits orthographic ENTITY in a ENTITYUNRELATED for possible morphological segmentation points .
Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from thedata.	rules	correspondences	model-feature	{'e1': {'word': 'rules', 'word_index': [(11, 11)], 'id': 'W05-0617.14'}, 'e2': {'word': 'correspondences', 'word_index': [(14, 14)], 'id': 'W05-0617.15'}}	Instead of affixes , however , we ENTITYUNRELATED for affix ENTITYUNRELATED ENTITY that express ENTITYOTHER between ENTITYUNRELATED ENTITYUNRELATED induced from the ENTITYUNRELATED .
Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from thedata.	clusters	data	model-feature	{'e1': {'word': 'clusters', 'word_index': [(17, 17)], 'id': 'W05-0617.17'}, 'e2': {'word': 'data', 'word_index': [(21, 21)], 'id': 'W05-0617.18'}}	Instead of affixes , however , we ENTITYUNRELATED for affix ENTITYUNRELATED ENTITYUNRELATED that express ENTITYUNRELATED between ENTITYUNRELATED ENTITY induced from the ENTITYOTHER .
This focuses the system on substrings having syntactic function, and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately.	system	forms	usage	{'e1': {'word': 'system', 'word_index': [(17, 17)], 'id': 'W05-0617.26'}, 'e2': {'word': 'forms', 'word_index': [(22, 22)], 'id': 'W05-0617.28'}}	This ENTITYUNRELATED the ENTITYUNRELATED on substrings having ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED which enable the ENTITY to ENTITYUNRELATED unknown morphological ENTITYOTHER of known ENTITYUNRELATED accurately .
We evaluate our approach using the CELEX database.	database	approach	usage	{'e1': {'word': 'database', 'word_index': [(7, 7)], 'id': 'W05-0617.35'}, 'e2': {'word': 'approach', 'word_index': [(3, 3)], 'id': 'W05-0617.34'}}	We ENTITYUNRELATED our ENTITYOTHER using the CELEX ENTITY .
In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking.	method	parsing	usage	{'e1': {'word': 'method', 'word_index': [(20, 20)], 'id': 'W05-1514.13'}, 'e2': {'word': 'parsing', 'word_index': [(7, 7)], 'id': 'W05-1514.8'}}	In this ENTITYUNRELATED we show that ENTITYUNRELATED ENTITYOTHER can ENTITYUNRELATED significantly better than previously ENTITYUNRELATED by using a ENTITYUNRELATED ENTITYUNRELATED ENTITY and ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED in each ENTITYUNRELATED of ENTITYUNRELATED .
In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking.	classifiers	recognition	usage	{'e1': {'word': 'classifiers', 'word_index': [(23, 23)], 'id': 'W05-1514.15'}, 'e2': {'word': 'recognition', 'word_index': [(26, 26)], 'id': 'W05-1514.17'}}	In this ENTITYUNRELATED we show that ENTITYUNRELATED ENTITYUNRELATED can ENTITYUNRELATED significantly better than previously ENTITYUNRELATED by using a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED ENTITY for ENTITYUNRELATED ENTITYOTHER in each ENTITYUNRELATED of ENTITYUNRELATED .
Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence).	parser	outputs	result	{'e1': {'word': 'parser', 'word_index': [(10, 10)], 'id': 'W05-1514.25'}, 'e2': {'word': 'outputs', 'word_index': [(15, 15)], 'id': 'W05-1514.28'}}	ENTITYUNRELATED ENTITYUNRELATED with the ENTITYUNRELATED ENTITYUNRELATED show that our ENTITYUNRELATED ENTITY can give ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER with very high ENTITYUNRELATED ( 14 msec / ENTITYUNRELATED ) .
We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy.	method	searching	usage	{'e1': {'word': 'method', 'word_index': [(5, 5)], 'id': 'W05-1514.32'}, 'e2': {'word': 'searching', 'word_index': [(7, 7)], 'id': 'W05-1514.33'}}	We also present a ENTITYUNRELATED ENTITY for ENTITYOTHER the best ENTITYUNRELATED by considering the ENTITYUNRELATED ENTITYUNRELATED by the ENTITYUNRELATED ENTITYUNRELATED , and show that the ENTITYUNRELATED ENTITYUNRELATED can further ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED .
We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy.	method	accuracy	result	{'e1': {'word': 'method', 'word_index': [(26, 26)], 'id': 'W05-1514.40'}, 'e2': {'word': 'accuracy', 'word_index': [(32, 32)], 'id': 'W05-1514.43'}}	We also present a ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED the best ENTITYUNRELATED by considering the ENTITYUNRELATED ENTITYUNRELATED by the ENTITYUNRELATED ENTITYUNRELATED , and show that the ENTITYUNRELATED ENTITY can further ENTITYUNRELATED the ENTITYUNRELATED ENTITYOTHER .
Automatic Knowledge Representation Using A Graph-Based Algorithm For Language-Independent Lexical Chainin	Algorithm	Knowledge Representation	usage	{'e1': {'word': 'Algorithm', 'word_index': [(7, 7)], 'id': 'W06-0205.4'}, 'e2': {'word': 'Knowledge Representation', 'word_index': [(1, 1)], 'id': 'W06-0205.2'}}	ENTITYUNRELATED ENTITYOTHER Using A Graph - ENTITYUNRELATED ENTITY For ENTITYUNRELATED Independent ENTITYUNRELATED Chainin
Lexical Chains are powerful representations of documents	Lexical Chains	documents	model-feature	{'e1': {'word': 'Lexical Chains', 'word_index': [(0, 0)], 'id': 'W06-0205.7'}, 'e2': {'word': 'documents', 'word_index': [(5, 5)], 'id': 'W06-0205.9'}}	ENTITY are powerful ENTITYUNRELATED of ENTITYOTHER
However, until now, Lexical Chaining algorithms have only been proposed for English.	algorithms	English	usage	{'e1': {'word': 'algorithms', 'word_index': [(7, 7)], 'id': 'W06-0205.14'}, 'e2': {'word': 'English', 'word_index': [(13, 13)], 'id': 'W06-0205.16'}}	However , until now , ENTITYUNRELATED Chaining ENTITY have only been ENTITYUNRELATED for ENTITYOTHER .
In this paper, we propose a greedy Language-Independent algorithm that automatically extracts Lexical Chains from texts.	paper	algorithm	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W06-0205.17'}, 'e2': {'word': 'algorithm', 'word_index': [(10, 10)], 'id': 'W06-0205.20'}}	In this ENTITY , we ENTITYUNRELATED a greedy ENTITYUNRELATED Independent ENTITYOTHER that automatically ENTITYUNRELATED ENTITYUNRELATED from ENTITYUNRELATED .
In this paper, we propose a greedy Language-Independent algorithm that automatically extracts Lexical Chains from texts.	Lexical Chains	texts	part_whole	{'e1': {'word': 'Lexical Chains', 'word_index': [(14, 14)], 'id': 'W06-0205.22'}, 'e2': {'word': 'texts', 'word_index': [(16, 16)], 'id': 'W06-0205.23'}}	In this ENTITYUNRELATED , we ENTITYUNRELATED a greedy ENTITYUNRELATED Independent ENTITYUNRELATED that automatically ENTITYUNRELATED ENTITY from ENTITYOTHER .
For that purpose, we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole-Based Overlapping Clustering Algorithm.	base	texts	part_whole	{'e1': {'word': 'base', 'word_index': [(9, 9)], 'id': 'W06-0205.26'}, 'e2': {'word': 'texts', 'word_index': [(14, 14)], 'id': 'W06-0205.28'}}	For that ENTITYUNRELATED , we build a hierarchical ENTITYUNRELATED ENTITY from a ENTITYUNRELATED of ENTITYOTHER by using the Pole - ENTITYUNRELATED Overlapping ENTITYUNRELATED .
As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.	methodology	language	usage	{'e1': {'word': 'methodology', 'word_index': [(5, 5)], 'id': 'W06-0205.31'}, 'e2': {'word': 'language', 'word_index': [(11, 11)], 'id': 'W06-0205.33'}}	As a consequence , our ENTITY can be ENTITYUNRELATED to any ENTITYOTHER and ENTITYUNRELATED a ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED Chainers .
The role of lexical resources is often understated in NLP research.	research	lexical resources	topic	{'e1': {'word': 'research', 'word_index': [(9, 9)], 'id': 'W06-1002.7'}, 'e2': {'word': 'lexical resources', 'word_index': [(3, 3)], 'id': 'W06-1002.6'}}	The ENTITYUNRELATED of ENTITYOTHER is often understated in NLP ENTITY .
The complexity of Chinese, Japanese and Korean (CJK) poses special challenges to developers of NLP tools, especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT).	complexity	Chinese	model-feature	{'e1': {'word': 'complexity', 'word_index': [(1, 1)], 'id': 'W06-1002.8'}, 'e2': {'word': 'Chinese', 'word_index': [(3, 3)], 'id': 'W06-1002.9'}}	The ENTITY of ENTITYOTHER , ENTITYUNRELATED and Korean ( CJK ) poses special ENTITYUNRELATED to ENTITYUNRELATED of NLP ENTITYUNRELATED , especially in the ENTITYUNRELATED of ENTITYUNRELATED ( WS ) , ENTITYUNRELATED ( IR ) , ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ( NER ) , and ENTITYUNRELATED ( MT ) .
These difficulties are exacerbated by the lack of comprehensive lexical resources, especially for proper nouns, and the lack of a standardized orthography, especially in Japanese.	lack	Japanese	model-feature	{'e1': {'word': 'lack', 'word_index': [(18, 18)], 'id': 'W06-1002.25'}, 'e2': {'word': 'Japanese', 'word_index': [(26, 26)], 'id': 'W06-1002.26'}}	These ENTITYUNRELATED are exacerbated by the ENTITYUNRELATED of comprehensive ENTITYUNRELATED , especially for proper ENTITYUNRELATED , and the ENTITY of a standardized orthography , especially in ENTITYOTHER .
This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources, and discusses the central role such resources should play in enhancing the accuracy of NLP tools.	paper	issues	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W06-1002.27'}, 'e2': {'word': 'issues', 'word_index': [(8, 8)], 'id': 'W06-1002.28'}}	This ENTITY summarizes some of the major linguistic ENTITYOTHER in the ENTITYUNRELATED ENTITYUNRELATED that are dependent on ENTITYUNRELATED , and discusses the central ENTITYUNRELATED such ENTITYUNRELATED should play in enhancing the ENTITYUNRELATED of NLP ENTITYUNRELATED .
This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources, and discusses the central role such resources should play in enhancing the accuracy of NLP tools.	lexical resources	NLP applications	usage	{'e1': {'word': 'lexical resources', 'word_index': [(17, 17)], 'id': 'W06-1002.31'}, 'e2': {'word': 'NLP applications', 'word_index': [(12, 12)], 'id': 'W06-1002.30'}}	This ENTITYUNRELATED summarizes some of the major linguistic ENTITYUNRELATED in the ENTITYUNRELATED ENTITYOTHER that are dependent on ENTITY , and discusses the central ENTITYUNRELATED such ENTITYUNRELATED should play in enhancing the ENTITYUNRELATED of NLP ENTITYUNRELATED .
This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources, and discusses the central role such resources should play in enhancing the accuracy of NLP tools.	resources	accuracy	result	{'e1': {'word': 'resources', 'word_index': [(25, 25)], 'id': 'W06-1002.33'}, 'e2': {'word': 'accuracy', 'word_index': [(31, 31)], 'id': 'W06-1002.34'}}	This ENTITYUNRELATED summarizes some of the major linguistic ENTITYUNRELATED in the ENTITYUNRELATED ENTITYUNRELATED that are dependent on ENTITYUNRELATED , and discusses the central ENTITYUNRELATED such ENTITY should play in enhancing the ENTITYOTHER of NLP ENTITYUNRELATED .
Although traditionally seen as a language-independent task, collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g., lemmatization, POS tagging, chunking or parsing) prior to the application of statistical measures.	POS tagging	extraction	usage	{'e1': {'word': 'POS tagging', 'word_index': [(26, 26)], 'id': 'W06-1006.9'}, 'e2': {'word': 'extraction', 'word_index': [(9, 9)], 'id': 'W06-1006.7'}}	Although traditionally seen as a ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED ENTITYOTHER relies nowadays more and more on the linguistic preprocessing of ENTITYUNRELATED ( e.g. , lemmatization , ENTITY , ENTITYUNRELATED or ENTITYUNRELATED ) prior to the ENTITYUNRELATED of ENTITYUNRELATED measures .
This paper provides a language-oriented review of the existing extraction work.	paper	review	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W06-1006.14'}, 'e2': {'word': 'review', 'word_index': [(5, 5)], 'id': 'W06-1006.17'}}	This ENTITY ENTITYUNRELATED a ENTITYUNRELATED ENTITYOTHER of the existing ENTITYUNRELATED work .
It points out several language-specific issues related to extraction and proposes a strategy for coping with them.	issues	extraction	model-feature	{'e1': {'word': 'issues', 'word_index': [(5, 5)], 'id': 'W06-1006.20'}, 'e2': {'word': 'extraction', 'word_index': [(8, 8)], 'id': 'W06-1006.21'}}	It points out several ENTITYUNRELATED ENTITY related to ENTITYOTHER and ENTITYUNRELATED a ENTITYUNRELATED for coping with them .
It then describes a hybrid extraction system based on a multilingual parser.	parser	extraction system	usage	{'e1': {'word': 'parser', 'word_index': [(10, 10)], 'id': 'W06-1006.26'}, 'e2': {'word': 'extraction system', 'word_index': [(5, 5)], 'id': 'W06-1006.24'}}	It then describes a hybrid ENTITYOTHER ENTITYUNRELATED on a multilingual ENTITY .
Finally, it presents a case-study on the performance of an association measure across a number of languages.	case-study	performance	topic	{'e1': {'word': 'case-study', 'word_index': [(5, 5)], 'id': 'W06-1006.27'}, 'e2': {'word': 'performance', 'word_index': [(8, 8)], 'id': 'W06-1006.28'}}	Finally , it presents a ENTITY on the ENTITYOTHER of an ENTITYUNRELATED measure across a ENTITYUNRELATED of ENTITYUNRELATED .
In this paper, we describe methods for building and evaluation of limited domain question-answering characters.	paper	methods	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W06-1303.3'}, 'e2': {'word': 'methods', 'word_index': [(6, 6)], 'id': 'W06-1303.4'}}	In this ENTITY , we describe ENTITYOTHER for ENTITYUNRELATED and ENTITYUNRELATED of limited ENTITYUNRELATED ENTITYUNRELATED characters .
Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate.	support vector machines	text classification	usage	{'e1': {'word': 'support vector machines', 'word_index': [(9, 9)], 'id': 'W06-1303.14'}, 'e2': {'word': 'text classification', 'word_index': [(7, 7)], 'id': 'W06-1303.13'}}	Several ENTITYUNRELATED ENTITYUNRELATED are ENTITYUNRELATED , ENTITYUNRELATED ENTITYOTHER using ENTITY , ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , with the latter having the highest ENTITYUNRELATED ENTITYUNRELATED .
Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate.	techniques	rate	result	{'e1': {'word': 'techniques', 'word_index': [(18, 18)], 'id': 'W06-1303.20'}, 'e2': {'word': 'rate', 'word_index': [(27, 27)], 'id': 'W06-1303.22'}}	Several ENTITYUNRELATED ENTITYUNRELATED are ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED using ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED ENTITY , with the latter having the highest ENTITYUNRELATED ENTITYOTHER .
We also evaluated the effect of speech recognition errors on performance with users, finding that retrieval is robust until recognition reaches over 50% WER.	errors	performance	result	{'e1': {'word': 'errors', 'word_index': [(7, 7)], 'id': 'W06-1303.26'}, 'e2': {'word': 'performance', 'word_index': [(9, 9)], 'id': 'W06-1303.27'}}	We also ENTITYUNRELATED the ENTITYUNRELATED of ENTITYUNRELATED ENTITY on ENTITYOTHER with ENTITYUNRELATED , finding that ENTITYUNRELATED is ENTITYUNRELATED until ENTITYUNRELATED ENTITYUNRELATED over 50 % WER .
We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts.	relations	anchor texts	part_whole	{'e1': {'word': 'relations', 'word_index': [(13, 13)], 'id': 'I08-2123.20'}, 'e2': {'word': 'anchor texts', 'word_index': [(19, 19)], 'id': 'I08-2123.22'}}	We introduce the ENTITYUNRELATED of a ENTITYUNRELATED ENTITYUNRELATED graph to represent the mutual ENTITY between ENTITYUNRELATED that appear in ENTITYOTHER .
Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url.	nodes	Words	model-feature	{'e1': {'word': 'nodes', 'word_index': [(6, 6)], 'id': 'I08-2123.25'}, 'e2': {'word': 'Words', 'word_index': [(0, 0)], 'id': 'I08-2123.23'}}	ENTITYOTHER in ENTITYUNRELATED are represented as ENTITY in the ENTITYUNRELATED graph and an ENTITYUNRELATED is ENTITYUNRELATED between ENTITYUNRELATED which ENTITYUNRELATED to the same url .
For a given personal name, its neighboring nodes in the graph are considered as candidates of its aliases.	nodes	aliases	model-feature	{'e1': {'word': 'nodes', 'word_index': [(8, 8)], 'id': 'I08-2123.33'}, 'e2': {'word': 'aliases', 'word_index': [(18, 18)], 'id': 'I08-2123.35'}}	For a given personal ENTITYUNRELATED , its ENTITYUNRELATED ENTITY in the graph are considered as ENTITYUNRELATED of its ENTITYOTHER .
We integrate various ranking scores through support vector machines to leverage a robust ranking function and use it to extract aliases for a given name.	support vector machines	function	usage	{'e1': {'word': 'support vector machines', 'word_index': [(6, 6)], 'id': 'I08-2123.42'}, 'e2': {'word': 'function', 'word_index': [(12, 12)], 'id': 'I08-2123.45'}}	We integrate various ranking scores through ENTITY to ENTITYUNRELATED a ENTITYUNRELATED ranking ENTITYOTHER and use it to ENTITYUNRELATED ENTITYUNRELATED for a given ENTITYUNRELATED .
We analyze four different types of document networks with respect to their small world characteristics.	characteristics	networks	model-feature	{'e1': {'word': 'characteristics', 'word_index': [(14, 14)], 'id': 'W06-2801.7'}, 'e2': {'word': 'networks', 'word_index': [(7, 7)], 'id': 'W06-2801.5'}}	We analyze four different ENTITYUNRELATED of ENTITYUNRELATED ENTITYOTHER with ENTITYUNRELATED to their small world ENTITY .
These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks.	wiki-based systems	networks	compare	{'e1': {'word': 'wiki-based systems', 'word_index': [(4, 4)], 'id': 'W06-2801.9'}, 'e2': {'word': 'networks', 'word_index': [(11, 11)], 'id': 'W06-2801.12'}}	These ENTITYUNRELATED allow distinguishing ENTITY from ENTITYUNRELATED and more traditional ENTITYUNRELATED ENTITYOTHER augmented by hyperlinks .
The study provides evidence that a more appropriate network model is needed which better reflects the specifics of wiki systems.	study	evidence	topic	{'e1': {'word': 'study', 'word_index': [(1, 1)], 'id': 'W06-2801.13'}, 'e2': {'word': 'evidence', 'word_index': [(3, 3)], 'id': 'W06-2801.15'}}	The ENTITY ENTITYUNRELATED ENTITYOTHER that a more appropriate ENTITYUNRELATED ENTITYUNRELATED is needed which better reflects the specifics of wiki ENTITYUNRELATED .
It puts emphasize on their topological differences as a result of wiki-related linking compared to other text-based networks.	linking	networks	compare	{'e1': {'word': 'linking', 'word_index': [(12, 12)], 'id': 'W06-2801.22'}, 'e2': {'word': 'networks', 'word_index': [(17, 17)], 'id': 'W06-2801.24'}}	It puts emphasize on their topological ENTITYUNRELATED as a ENTITYUNRELATED of ENTITYUNRELATED ENTITY compared to other ENTITYUNRELATED ENTITYOTHER .
This discussion document concerns the challenges to assessments of reliability posed by wikis and the potential for language processing techniques for aiding readers to decide whether to trust particular text.	document	challenges	topic	{'e1': {'word': 'document', 'word_index': [(2, 2)], 'id': 'W06-2802.2'}, 'e2': {'word': 'challenges', 'word_index': [(5, 5)], 'id': 'W06-2802.4'}}	This ENTITYUNRELATED ENTITY ENTITYUNRELATED the ENTITYOTHER to ENTITYUNRELATED of ENTITYUNRELATED posed by wikis and the potential for ENTITYUNRELATED ENTITYUNRELATED for aiding readers to decide whether to trust particular ENTITYUNRELATED .
"""We present two translation systems experimented for the shared-task of """"Workshop on Statistical Machine Translation,"""" a phrase-based model and a hierarchical phrase-based model."	translation systems	shared-task	usage	{'e1': {'word': 'translation systems', 'word_index': [(4, 4)], 'id': 'W06-3115.4'}, 'e2': {'word': 'shared-task', 'word_index': [(8, 8)], 'id': 'W06-3115.6'}}	""" We present two ENTITY ENTITYUNRELATED for the ENTITYOTHER of "" "" ENTITYUNRELATED on ENTITYUNRELATED , "" "" a ENTITYUNRELATED and a hierarchical ENTITYUNRELATED ."
The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals.	unit	translation	usage	{'e1': {'word': 'unit', 'word_index': [(5, 5)], 'id': 'W06-3115.11'}, 'e2': {'word': 'translation', 'word_index': [(7, 7)], 'id': 'W06-3115.12'}}	The former uses a phrasal ENTITY for ENTITYOTHER , whereas the latter is conceptualized as a synchronous - CFG in which ENTITYUNRELATED are hierarchically combined using non-terminals .
Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model.	phrase-based model	phrase-based model	compare	{'e1': {'word': 'phrase-based model', 'word_index': [(5, 5)], 'id': 'W06-3115.15'}, 'e2': {'word': 'phrase-based model', 'word_index': [(11, 11)], 'id': 'W06-3115.17'}}	ENTITYUNRELATED showed that the hierarchical ENTITY ENTITYUNRELATED very comparable to the ENTITYOTHER .
"We also report a phrase/rule extraction technique differentiating tokenization of corpora. """	technique	corpora	usage	{'e1': {'word': 'technique', 'word_index': [(8, 8)], 'id': 'W06-3115.22'}, 'e2': {'word': 'corpora', 'word_index': [(12, 12)], 'id': 'W06-3115.23'}}	"We also ENTITYUNRELATED a ENTITYUNRELATED / ENTITYUNRELATED ENTITYUNRELATED ENTITY differentiating tokenization of ENTITYOTHER . """
In this paper we present an approach to structure learning in the area of web documents.	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W06-1710.3'}, 'e2': {'word': 'approach', 'word_index': [(6, 6)], 'id': 'W06-1710.4'}}	In this ENTITY we present an ENTITYOTHER to ENTITYUNRELATED learning in the ENTITYUNRELATED of ENTITYUNRELATED .
In this paper we present an approach to structure learning in the area of web documents.	structure	web documents	model-feature	{'e1': {'word': 'structure', 'word_index': [(8, 8)], 'id': 'W06-1710.5'}, 'e2': {'word': 'web documents', 'word_index': [(14, 14)], 'id': 'W06-1710.7'}}	In this ENTITYUNRELATED we present an ENTITYUNRELATED to ENTITY learning in the ENTITYUNRELATED of ENTITYOTHER .
A central outcome of the paper is that purely structure oriented approaches to web document classification provide an information gain which may be utilized in combined approaches of web content and structure analysis.	approaches	information gain	result	{'e1': {'word': 'approaches', 'word_index': [(11, 11)], 'id': 'W06-1710.18'}, 'e2': {'word': 'information gain', 'word_index': [(17, 17)], 'id': 'W06-1710.22'}}	A central ENTITYUNRELATED of the ENTITYUNRELATED is that purely ENTITYUNRELATED oriented ENTITY to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED an ENTITYOTHER which may be utilized in combined ENTITYUNRELATED of web ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED .
A central outcome of the paper is that purely structure oriented approaches to web document classification provide an information gain which may be utilized in combined approaches of web content and structure analysis.	approaches	analysis	usage	{'e1': {'word': 'approaches', 'word_index': [(24, 24)], 'id': 'W06-1710.23'}, 'e2': {'word': 'analysis', 'word_index': [(30, 30)], 'id': 'W06-1710.26'}}	A central ENTITYUNRELATED of the ENTITYUNRELATED is that purely ENTITYUNRELATED oriented ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED an ENTITYUNRELATED which may be utilized in combined ENTITY of web ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER .
BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier</title>	Classifier	Classification	usage	{'e1': {'word': 'Classifier', 'word_index': [(12, 12)], 'id': 'W06-1906.5'}, 'e2': {'word': 'Classification', 'word_index': [(3, 3)], 'id': 'W06-1906.2'}}	BRUJA : ENTITYUNRELATED ENTITYOTHER For Spanish Using ENTITYUNRELATED Translation and An ENTITYUNRELATED ENTITY < / title >
Question Classification is an important task in Question Answering Systems	Classification	Systems	part_whole	{'e1': {'word': 'Classification', 'word_index': [(1, 1)], 'id': 'W06-1906.7'}, 'e2': {'word': 'Systems', 'word_index': [(8, 8)], 'id': 'W06-1906.10'}}	ENTITYUNRELATED ENTITY is an important ENTITYUNRELATED in ENTITYUNRELATED ENTITYOTHER
This paper presents a Spanish Question Classifier based on machine learning, automatic online translators and different language features.	machine learning	Classifier	usage	{'e1': {'word': 'machine learning', 'word_index': [(9, 9)], 'id': 'W06-1906.15'}, 'e2': {'word': 'Classifier', 'word_index': [(6, 6)], 'id': 'W06-1906.13'}}	This ENTITYUNRELATED presents a Spanish ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED on ENTITY , ENTITYUNRELATED online ENTITYUNRELATED and different ENTITYUNRELATED ENTITYUNRELATED .
Our system works with English collections and bilingual questions (English/Spanish).	system	collections	usage	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'W06-1906.20'}, 'e2': {'word': 'collections', 'word_index': [(5, 5)], 'id': 'W06-1906.22'}}	Our ENTITY works with ENTITYUNRELATED ENTITYOTHER and bilingual ENTITYUNRELATED ( ENTITYUNRELATED / Spanish ) .
We have made experiments using lexical, syntactic and semantic features to test which ones made a better performance.	semantic features	performance	result	{'e1': {'word': 'semantic features', 'word_index': [(9, 9)], 'id': 'W06-1906.32'}, 'e2': {'word': 'performance', 'word_index': [(17, 17)], 'id': 'W06-1906.34'}}	We have made ENTITYUNRELATED using ENTITYUNRELATED , ENTITYUNRELATED and ENTITY to ENTITYUNRELATED which ones made a better ENTITYOTHER .
The obtained results show that our system makes good classifications, over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems.	system	classifications	result	{'e1': {'word': 'system', 'word_index': [(6, 6)], 'id': 'W06-1906.36'}, 'e2': {'word': 'classifications', 'word_index': [(9, 9)], 'id': 'W06-1906.37'}}	The obtained ENTITYUNRELATED show that our ENTITY makes good ENTITYOTHER , over a 80 % in ENTITYUNRELATED of ENTITYUNRELATED using the original ENTITYUNRELATED ENTITYUNRELATED and over a 65 % using Spanish ENTITYUNRELATED and ENTITYUNRELATED .
Our conclusion about the features is that a lexical, syntactic and semantic features combination obtains the best result.	combination	result	result	{'e1': {'word': 'combination', 'word_index': [(13, 13)], 'id': 'W06-1906.49'}, 'e2': {'word': 'result', 'word_index': [(17, 17)], 'id': 'W06-1906.50'}}	Our ENTITYUNRELATED about the ENTITYUNRELATED is that a ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED ENTITY obtains the best ENTITYOTHER .
We created a new Chinese morphological analyzer, Achilles , by integrating rule-based, dictionary-based, and statistical machine learning method, conditional random fields (CRF).	method	analyzer	usage	{'e1': {'word': 'method', 'word_index': [(20, 20)], 'id': 'I08-4033.9'}, 'e2': {'word': 'analyzer', 'word_index': [(6, 6)], 'id': 'I08-4033.4'}}	We created a new ENTITYUNRELATED morphological ENTITYOTHER , Achilles , by integrating ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED learning ENTITY , ENTITYUNRELATED ( CRF ) .
The rule-based method is used to recognize regular expressions: numbers, time and alphabets.	rule-based method	expressions	usage	{'e1': {'word': 'rule-based method', 'word_index': [(1, 1)], 'id': 'I08-4033.11'}, 'e2': {'word': 'expressions', 'word_index': [(7, 7)], 'id': 'I08-4033.12'}}	The ENTITY is used to recognize regular ENTITYOTHER : ENTITYUNRELATED , ENTITYUNRELATED and alphabets .
The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs.	dictionary-based method	words	usage	{'e1': {'word': 'dictionary-based method', 'word_index': [(1, 1)], 'id': 'I08-4033.15'}, 'e2': {'word': 'words', 'word_index': [(10, 10)], 'id': 'I08-4033.17'}}	The ENTITY is used to find ENTITYUNRELATED ( IV ) ENTITYOTHER while ENTITYUNRELATED ( OOV ) ENTITYUNRELATED are detected by the CRFs .
At last, confidence measure based approach is used to weigh all the results and output the best ones.	approach	results	usage	{'e1': {'word': 'approach', 'word_index': [(6, 6)], 'id': 'I08-4033.22'}, 'e2': {'word': 'results', 'word_index': [(13, 13)], 'id': 'I08-4033.23'}}	At last , ENTITYUNRELATED measure ENTITYUNRELATED ENTITY is used to weigh all the ENTITYOTHER and ENTITYUNRELATED the best ones .
We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus.	tagging	corpus	usage	{'e1': {'word': 'tagging', 'word_index': [(9, 9)], 'id': 'I08-4033.28'}, 'e2': {'word': 'corpus', 'word_index': [(14, 14)], 'id': 'I08-4033.30'}}	We participated the closed tracks of ENTITYUNRELATED and ENTITYUNRELATED ENTITY for all the ENTITYUNRELATED ENTITYOTHER .
In spite of an unexpected file encoding errors, the system exhibited a top level performance.	system	performance	result	{'e1': {'word': 'system', 'word_index': [(10, 10)], 'id': 'I08-4033.33'}, 'e2': {'word': 'performance', 'word_index': [(15, 15)], 'id': 'I08-4033.35'}}	In ENTITYUNRELATED of an unexpected file encoding ENTITYUNRELATED , the ENTITY exhibited a top ENTITYUNRELATED ENTITYOTHER .
A higher word segmentation accuracy for the corpus ckip and ncc were achieved.	word segmentation	corpus	usage	{'e1': {'word': 'word segmentation', 'word_index': [(2, 2)], 'id': 'I08-4033.36'}, 'e2': {'word': 'corpus', 'word_index': [(6, 6)], 'id': 'I08-4033.38'}}	A higher ENTITY ENTITYUNRELATED for the ENTITYOTHER ckip and ncc were achieved .
Achilles uses a feature combined approach for part-of-speech tagging.	approach	tagging	usage	{'e1': {'word': 'approach', 'word_index': [(5, 5)], 'id': 'I08-4033.43'}, 'e2': {'word': 'tagging', 'word_index': [(8, 8)], 'id': 'I08-4033.45'}}	Achilles uses a ENTITYUNRELATED combined ENTITY for ENTITYUNRELATED ENTITYOTHER .
Our post-evaluation results prove the effectiveness of this approach for POS tagging.	approach	post-evaluation results	result	{'e1': {'word': 'approach', 'word_index': [(7, 7)], 'id': 'I08-4033.48'}, 'e2': {'word': 'post-evaluation results', 'word_index': [(1, 1)], 'id': 'I08-4033.46'}}	Our ENTITYOTHER prove the ENTITYUNRELATED of this ENTITY for ENTITYUNRELATED .
Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets	Training	Enhancement	usage	{'e1': {'word': 'Training', 'word_index': [(2, 2)], 'id': 'P07-1078.1'}, 'e2': {'word': 'Enhancement', 'word_index': [(4, 4)], 'id': 'P07-1078.2'}}	Self - ENTITY for ENTITYOTHER and ENTITYUNRELATED of ENTITYUNRELATED Parsers ENTITYUNRELATED on Small Datasets
Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and testdata are taken from different domains.	data	parsers	usage	{'e1': {'word': 'data', 'word_index': [(5, 5)], 'id': 'P07-1078.7'}, 'e2': {'word': 'parsers', 'word_index': [(10, 10)], 'id': 'P07-1078.10'}}	Creating large ENTITYUNRELATED of annotated ENTITY to ENTITYUNRELATED ENTITYUNRELATED PCFG ENTITYOTHER is expensive , and the ENTITYUNRELATED of such ENTITYUNRELATED declines when ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED are taken from different ENTITYUNRELATED .
Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and testdata are taken from different domains.	domains	performance	result	{'e1': {'word': 'domains', 'word_index': [(30, 30)], 'id': 'P07-1078.16'}, 'e2': {'word': 'performance', 'word_index': [(16, 16)], 'id': 'P07-1078.11'}}	Creating large ENTITYUNRELATED of annotated ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED PCFG ENTITYUNRELATED is expensive , and the ENTITYOTHER of such ENTITYUNRELATED declines when ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED are taken from different ENTITY .
In this paper we use self-training in order to improve the quality of a parser and to adapt it to a different domain, using only small amounts of manually annotated seeddata.	self-training	parser	usage	{'e1': {'word': 'self-training', 'word_index': [(5, 5)], 'id': 'P07-1078.18'}, 'e2': {'word': 'parser', 'word_index': [(14, 14)], 'id': 'P07-1078.22'}}	In this ENTITYUNRELATED we use ENTITY in ENTITYUNRELATED to ENTITYUNRELATED the ENTITYUNRELATED of a ENTITYOTHER and to ENTITYUNRELATED it to a different ENTITYUNRELATED , using only small ENTITYUNRELATED of manually annotated ENTITYUNRELATED ENTITYUNRELATED .
This is the first time that self-training with small labeled datasets is applied successfully to these tasks.	self-training	tasks	usage	{'e1': {'word': 'self-training', 'word_index': [(6, 6)], 'id': 'P07-1078.46'}, 'e2': {'word': 'tasks', 'word_index': [(16, 16)], 'id': 'P07-1078.48'}}	This is the first ENTITYUNRELATED that ENTITY with small labeled datasets is ENTITYUNRELATED successfully to these ENTITYOTHER .
With OpenCCG, language models may be used to select realizations with preferred word orders, promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search.	language models	realizations	usage	{'e1': {'word': 'language models', 'word_index': [(3, 3)], 'id': 'W05-1104.8'}, 'e2': {'word': 'realizations', 'word_index': [(9, 9)], 'id': 'W05-1104.9'}}	With OpenCCG , ENTITY may be used to select ENTITYOTHER with preferred ENTITYUNRELATED ENTITYUNRELATED , promote ENTITYUNRELATED with a conversational partner , avoid repetitive ENTITYUNRELATED use , and ENTITYUNRELATED the ENTITYUNRELATED of the best - first anytime ENTITYUNRELATED .
With OpenCCG, language models may be used to select realizations with preferred word orders, promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search.	speed	search	model-feature	{'e1': {'word': 'speed', 'word_index': [(30, 30)], 'id': 'W05-1104.15'}, 'e2': {'word': 'search', 'word_index': [(37, 37)], 'id': 'W05-1104.16'}}	With OpenCCG , ENTITYUNRELATED may be used to select ENTITYUNRELATED with preferred ENTITYUNRELATED ENTITYUNRELATED , promote ENTITYUNRELATED with a conversational partner , avoid repetitive ENTITYUNRELATED use , and ENTITYUNRELATED the ENTITY of the best - first anytime ENTITYOTHER .
"The n-gram models may be of any order, operate in reverse (""""right-to-left""""), and selectively replace certain words with their semantic classes."	order	n-gram models	model-feature	{'e1': {'word': 'order', 'word_index': [(6, 6)], 'id': 'W05-1104.23'}, 'e2': {'word': 'n-gram models', 'word_index': [(1, 1)], 'id': 'W05-1104.22'}}	"The ENTITYOTHER may be of any ENTITY , operate in reverse ( "" "" right - to - left "" "" ) , and selectively replace certain ENTITYUNRELATED with their ENTITYUNRELATED ."
"The n-gram models may be of any order, operate in reverse (""""right-to-left""""), and selectively replace certain words with their semantic classes."	semantic classes	words	model-feature	{'e1': {'word': 'semantic classes', 'word_index': [(30, 30)], 'id': 'W05-1104.25'}, 'e2': {'word': 'words', 'word_index': [(27, 27)], 'id': 'W05-1104.24'}}	"The ENTITYUNRELATED may be of any ENTITYUNRELATED , operate in reverse ( "" "" right - to - left "" "" ) , and selectively replace certain ENTITYOTHER with their ENTITY ."
"Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form, pitch accent, stem, part of speech, supertag, and semantic class. """	factors	words	model-feature	{'e1': {'word': 'factors', 'word_index': [(16, 16)], 'id': 'W05-1104.29'}, 'e2': {'word': 'words', 'word_index': [(11, 11)], 'id': 'W05-1104.28'}}	"Factored ENTITYUNRELATED with ENTITYUNRELATED backoff may also be employed , over ENTITYOTHER represented as bundles of ENTITY such as ENTITYUNRELATED , ENTITYUNRELATED accent , ENTITYUNRELATED , ENTITYUNRELATED , supertag , and ENTITYUNRELATED . """
With the information overload in the life sciences there is an increasing need for annotated corpora, particularly with biological and biomedical entities, which is the driving force for data-driven language processing applications and the empirical approach to language study.	entities	corpora	part_whole	{'e1': {'word': 'entities', 'word_index': [(22, 22)], 'id': 'L08-1237.6'}, 'e2': {'word': 'corpora', 'word_index': [(15, 15)], 'id': 'L08-1237.5'}}	With the ENTITYUNRELATED overload in the life ENTITYUNRELATED there is an ENTITYUNRELATED need for annotated ENTITYOTHER , particularly with biological and biomedical ENTITY , which is the driving force for ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and the empirical ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED .
With the information overload in the life sciences there is an increasing need for annotated corpora, particularly with biological and biomedical entities, which is the driving force for data-driven language processing applications and the empirical approach to language study.	approach	study	usage	{'e1': {'word': 'approach', 'word_index': [(36, 36)], 'id': 'L08-1237.10'}, 'e2': {'word': 'study', 'word_index': [(39, 39)], 'id': 'L08-1237.12'}}	With the ENTITYUNRELATED overload in the life ENTITYUNRELATED there is an ENTITYUNRELATED need for annotated ENTITYUNRELATED , particularly with biological and biomedical ENTITYUNRELATED , which is the driving force for ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and the empirical ENTITY to ENTITYUNRELATED ENTITYOTHER .
Ontology Search with the OntoSelect Ontology Library	Library	Search	usage	{'e1': {'word': 'Library', 'word_index': [(6, 6)], 'id': 'L08-1265.4'}, 'e2': {'word': 'Search', 'word_index': [(1, 1)], 'id': 'L08-1265.2'}}	ENTITYUNRELATED ENTITYOTHER with the OntoSelect ENTITYUNRELATED ENTITY
OntoSelect allows searching as well as browsing of ontologies according to size (number of classes, properties), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies) and human languages used for class- and object property-labels.	size	ontologies	model-feature	{'e1': {'word': 'size', 'word_index': [(12, 12)], 'id': 'L08-1265.11'}, 'e2': {'word': 'ontologies', 'word_index': [(9, 9)], 'id': 'L08-1265.10'}}	Onto Select allows ENTITYUNRELATED as well as browsing of ENTITYOTHER according to ENTITY ( ENTITYUNRELATED of ENTITYUNRELATED , ENTITYUNRELATED ) , ENTITYUNRELATED ENTITYUNRELATED ( DAML , RDFS , OWL ) , connectedness ( score over the ENTITYUNRELATED of ENTITYUNRELATED and referring ENTITYUNRELATED ) and ENTITYUNRELATED used for ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED .
OntoSelect allows searching as well as browsing of ontologies according to size (number of classes, properties), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies) and human languages used for class- and object property-labels.	number	ontologies	model-feature	{'e1': {'word': 'number', 'word_index': [(36, 36)], 'id': 'L08-1265.17'}, 'e2': {'word': 'ontologies', 'word_index': [(41, 41)], 'id': 'L08-1265.19'}}	Onto Select allows ENTITYUNRELATED as well as browsing of ENTITYUNRELATED according to ENTITYUNRELATED ( ENTITYUNRELATED of ENTITYUNRELATED , ENTITYUNRELATED ) , ENTITYUNRELATED ENTITYUNRELATED ( DAML , RDFS , OWL ) , connectedness ( score over the ENTITY of ENTITYUNRELATED and referring ENTITYOTHER ) and ENTITYUNRELATED used for ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED .
OntoSelect allows searching as well as browsing of ontologies according to size (number of classes, properties), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies) and human languages used for class- and object property-labels.	human languages	property-labels	usage	{'e1': {'word': 'human languages', 'word_index': [(44, 44)], 'id': 'L08-1265.20'}, 'e2': {'word': 'property-labels', 'word_index': [(50, 50)], 'id': 'L08-1265.23'}}	Onto Select allows ENTITYUNRELATED as well as browsing of ENTITYUNRELATED according to ENTITYUNRELATED ( ENTITYUNRELATED of ENTITYUNRELATED , ENTITYUNRELATED ) , ENTITYUNRELATED ENTITYUNRELATED ( DAML , RDFS , OWL ) , connectedness ( score over the ENTITYUNRELATED of ENTITYUNRELATED and referring ENTITYUNRELATED ) and ENTITY used for ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER .
Ontology search in OntoSelect is based on a combined measure of coverage, structure connectedness.	coverage	search	usage	{'e1': {'word': 'coverage', 'word_index': [(11, 11)], 'id': 'L08-1265.27'}, 'e2': {'word': 'search', 'word_index': [(1, 1)], 'id': 'L08-1265.25'}}	ENTITYUNRELATED ENTITYOTHER in OntoSelect is ENTITYUNRELATED on a combined measure of ENTITY , ENTITYUNRELATED connectedness .
Speech Recognition And The Frequency Of Recently Used Words: A Modified Markov Model For Natural Language	Frequency	Words	model-feature	{'e1': {'word': 'Frequency', 'word_index': [(3, 3)], 'id': 'C88-1071.2'}, 'e2': {'word': 'Words', 'word_index': [(7, 7)], 'id': 'C88-1071.3'}}	ENTITYUNRELATED And The ENTITY Of Recently Used ENTITYOTHER : A Modified ENTITYUNRELATED For ENTITYUNRELATED
Speech Recognition And The Frequency Of Recently Used Words: A Modified Markov Model For Natural Language	Markov Model	Natural Language	usage	{'e1': {'word': 'Markov Model', 'word_index': [(11, 11)], 'id': 'C88-1071.4'}, 'e2': {'word': 'Natural Language', 'word_index': [(13, 13)], 'id': 'C88-1071.5'}}	ENTITYUNRELATED And The ENTITYUNRELATED Of Recently Used ENTITYUNRELATED : A Modified ENTITY For ENTITYOTHER
Speech recognition systems incorporate a language model which, at each stage of the recognition task, assigns a probability of occurrence to each word in the vocabulary	language model	Speech recognition systems	part_whole	{'e1': {'word': 'language model', 'word_index': [(3, 3)], 'id': 'C88-1071.7'}, 'e2': {'word': 'Speech recognition systems', 'word_index': [(0, 0)], 'id': 'C88-1071.6'}}	ENTITYOTHER incorporate a ENTITY which , at each stage of the ENTITYUNRELATED ENTITYUNRELATED , assigns a ENTITYUNRELATED of ENTITYUNRELATED to each ENTITYUNRELATED in the ENTITYUNRELATED
Speech recognition systems incorporate a language model which, at each stage of the recognition task, assigns a probability of occurrence to each word in the vocabulary	probability	word	model-feature	{'e1': {'word': 'probability', 'word_index': [(16, 16)], 'id': 'C88-1071.10'}, 'e2': {'word': 'word', 'word_index': [(21, 21)], 'id': 'C88-1071.12'}}	ENTITYUNRELATED incorporate a ENTITYUNRELATED which , at each stage of the ENTITYUNRELATED ENTITYUNRELATED , assigns a ENTITY of ENTITYUNRELATED to each ENTITYOTHER in the ENTITYUNRELATED
A class of Markov language models identified by Jelinek has achieved consider able success in this domain.	language models	success	result	{'e1': {'word': 'language models', 'word_index': [(4, 4)], 'id': 'C88-1071.15'}, 'e2': {'word': 'success', 'word_index': [(12, 12)], 'id': 'C88-1071.16'}}	A ENTITYUNRELATED of Markov ENTITY identified by Jelinek has achieved consider able ENTITYOTHER in this ENTITYUNRELATED .
A modification of the Markov approach, which assigns higher probabilities to recently used words, is proposed and tested against a pure Markov model.	probabilities	words	model-feature	{'e1': {'word': 'probabilities', 'word_index': [(10, 10)], 'id': 'C88-1071.20'}, 'e2': {'word': 'words', 'word_index': [(14, 14)], 'id': 'C88-1071.21'}}	A ENTITYUNRELATED of the Markov ENTITYUNRELATED , which assigns higher ENTITY to recently used ENTITYOTHER , is ENTITYUNRELATED and ENTITYUNRELATED against a pure ENTITYUNRELATED .
Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English.	English	Corpus	part_whole	{'e1': {'word': 'English', 'word_index': [(18, 18)], 'id': 'C88-1071.31'}, 'e2': {'word': 'Corpus', 'word_index': [(14, 14)], 'id': 'C88-1071.29'}}	ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED of the two ENTITYUNRELATED both involve use of the LOB ENTITYOTHER of ENTITYUNRELATED modern ENTITY .
This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence, and tries to explore what preferences exist among the factors related to the two tasks.	paper	interaction	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'A00-3001.10'}, 'e2': {'word': 'interaction', 'word_index': [(5, 5)], 'id': 'A00-3001.12'}}	This ENTITY ENTITYUNRELATED on the ENTITYOTHER between a specific ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED planning , in particular , maintaining local ENTITYUNRELATED , and tries to explore what ENTITYUNRELATED exist among the ENTITYUNRELATED related to the two ENTITYUNRELATED .
This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence, and tries to explore what preferences exist among the factors related to the two tasks.	factors	tasks	model-feature	{'e1': {'word': 'factors', 'word_index': [(32, 32)], 'id': 'A00-3001.18'}, 'e2': {'word': 'tasks', 'word_index': [(37, 37)], 'id': 'A00-3001.19'}}	This ENTITYUNRELATED ENTITYUNRELATED on the ENTITYUNRELATED between a specific ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED planning , in particular , maintaining local ENTITYUNRELATED , and tries to explore what ENTITYUNRELATED exist among the ENTITY related to the two ENTITYOTHER .
The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text.	preferences	quality	result	{'e1': {'word': 'preferences', 'word_index': [(7, 7)], 'id': 'A00-3001.21'}, 'e2': {'word': 'quality', 'word_index': [(11, 11)], 'id': 'A00-3001.22'}}	The ENTITYUNRELATED shows that it is these ENTITY that decide the ENTITYOTHER of the ENTITYUNRELATED ENTITYUNRELATED and capturing them properly in a ENTITYUNRELATED could lead to coherent ENTITYUNRELATED .
The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text.	generation system	text	result	{'e1': {'word': 'generation system', 'word_index': [(22, 22)], 'id': 'A00-3001.25'}, 'e2': {'word': 'text', 'word_index': [(27, 27)], 'id': 'A00-3001.26'}}	The ENTITYUNRELATED shows that it is these ENTITYUNRELATED that decide the ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED and capturing them properly in a ENTITY could lead to coherent ENTITYOTHER .
Quantitative Modeling Of Segmental Duration</title>	Modeling	Duration	usage	{'e1': {'word': 'Modeling', 'word_index': [(1, 1)], 'id': 'H93-1065.1'}, 'e2': {'word': 'Duration', 'word_index': [(4, 4)], 'id': 'H93-1065.2'}}	Quantitative ENTITY Of Segmental ENTITYOTHER < / title >
In natural speech, durations of phonetic segments are strongly dependent on contextual factors.	durations	segments	model-feature	{'e1': {'word': 'durations', 'word_index': [(4, 4)], 'id': 'H93-1065.5'}, 'e2': {'word': 'segments', 'word_index': [(7, 7)], 'id': 'H93-1065.6'}}	In ENTITYUNRELATED ENTITYUNRELATED , ENTITY of phonetic ENTITYOTHER are strongly dependent on contextual ENTITYUNRELATED .
Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech  recognition.	descriptions	effects	topic	{'e1': {'word': 'descriptions', 'word_index': [(1, 1)], 'id': 'H93-1065.8'}, 'e2': {'word': 'effects', 'word_index': [(5, 5)], 'id': 'H93-1065.9'}}	Quantitative ENTITY of these contextual ENTITYOTHER have ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED and in ENTITYUNRELATED .
In this paper, we describe a speaker-dependent system for predicting segmental duration from text, with emphasis on the statistical methods used for its construction.	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'H93-1065.14'}, 'e2': {'word': 'system', 'word_index': [(10, 10)], 'id': 'H93-1065.15'}}	In this ENTITY , we describe a speaker - dependent ENTITYOTHER for predicting segmental ENTITYUNRELATED from ENTITYUNRELATED , with emphasis on the ENTITYUNRELATED used for its ENTITYUNRELATED .
In this paper, we describe a speaker-dependent system for predicting segmental duration from text, with emphasis on the statistical methods used for its construction.	text	duration	usage	{'e1': {'word': 'text', 'word_index': [(16, 16)], 'id': 'H93-1065.17'}, 'e2': {'word': 'duration', 'word_index': [(14, 14)], 'id': 'H93-1065.16'}}	In this ENTITYUNRELATED , we describe a speaker - dependent ENTITYUNRELATED for predicting segmental ENTITYOTHER from ENTITY , with emphasis on the ENTITYUNRELATED used for its ENTITYUNRELATED .
In this paper, we describe a speaker-dependent system for predicting segmental duration from text, with emphasis on the statistical methods used for its construction.	statistical methods	construction	usage	{'e1': {'word': 'statistical methods', 'word_index': [(22, 22)], 'id': 'H93-1065.18'}, 'e2': {'word': 'construction', 'word_index': [(26, 26)], 'id': 'H93-1065.19'}}	In this ENTITYUNRELATED , we describe a speaker - dependent ENTITYUNRELATED for predicting segmental ENTITYUNRELATED from ENTITYUNRELATED , with emphasis on the ENTITY used for its ENTITYOTHER .
We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes.	experiment	results	result	{'e1': {'word': 'experiment', 'word_index': [(8, 8)], 'id': 'H93-1065.22'}, 'e2': {'word': 'results', 'word_index': [(3, 3)], 'id': 'H93-1065.21'}}	We also ENTITYUNRELATED ENTITYOTHER of a subjective listening ENTITY ENTITYUNRELATED an ENTITYUNRELATED of this ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes.	implementation	synthesis	usage	{'e1': {'word': 'implementation', 'word_index': [(11, 11)], 'id': 'H93-1065.24'}, 'e2': {'word': 'synthesis', 'word_index': [(17, 17)], 'id': 'H93-1065.27'}}	We also ENTITYUNRELATED ENTITYUNRELATED of a subjective listening ENTITYUNRELATED ENTITYUNRELATED an ENTITY of this ENTITYUNRELATED for ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED .
We present a method to realize flexible mixed-initiative dialogue, in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors.	method	dialogue	usage	{'e1': {'word': 'method', 'word_index': [(3, 3)], 'id': 'C00-1068.8'}, 'e2': {'word': 'dialogue', 'word_index': [(8, 8)], 'id': 'C00-1068.10'}}	We present a ENTITY to realize flexible ENTITYUNRELATED ENTITYOTHER , in which the ENTITYUNRELATED can make effective ENTITYUNRELATED and guidance using ENTITYUNRELATED ENTITYUNRELATED measures ( CMs ) derived from ENTITYUNRELATED recognizer ENTITYUNRELATED in ENTITYUNRELATED to handle ENTITYUNRELATED ENTITYUNRELATED .
We present a method to realize flexible mixed-initiative dialogue, in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors.	system	confirmation	usage	{'e1': {'word': 'system', 'word_index': [(13, 13)], 'id': 'C00-1068.11'}, 'e2': {'word': 'confirmation', 'word_index': [(17, 17)], 'id': 'C00-1068.12'}}	We present a ENTITYUNRELATED to realize flexible ENTITYUNRELATED ENTITYUNRELATED , in which the ENTITY can make effective ENTITYOTHER and guidance using ENTITYUNRELATED ENTITYUNRELATED measures ( CMs ) derived from ENTITYUNRELATED recognizer ENTITYUNRELATED in ENTITYUNRELATED to handle ENTITYUNRELATED ENTITYUNRELATED .
We present a method to realize flexible mixed-initiative dialogue, in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors.	output	confidence	usage	{'e1': {'word': 'output', 'word_index': [(31, 31)], 'id': 'C00-1068.16'}, 'e2': {'word': 'confidence', 'word_index': [(22, 22)], 'id': 'C00-1068.14'}}	We present a ENTITYUNRELATED to realize flexible ENTITYUNRELATED ENTITYUNRELATED , in which the ENTITYUNRELATED can make effective ENTITYUNRELATED and guidance using ENTITYUNRELATED ENTITYOTHER measures ( CMs ) derived from ENTITYUNRELATED recognizer ENTITY in ENTITYUNRELATED to handle ENTITYUNRELATED ENTITYUNRELATED .
Less confident interpretations are given to confirmation process.	process	interpretations	usage	{'e1': {'word': 'process', 'word_index': [(7, 7)], 'id': 'C00-1068.31'}, 'e2': {'word': 'interpretations', 'word_index': [(2, 2)], 'id': 'C00-1068.29'}}	Less confident ENTITYOTHER are given to ENTITYUNRELATED ENTITY .
The strategy improved the interpretation accuracy by 11.5%.	strategy	accuracy	result	{'e1': {'word': 'strategy', 'word_index': [(1, 1)], 'id': 'C00-1068.32'}, 'e2': {'word': 'accuracy', 'word_index': [(5, 5)], 'id': 'C00-1068.35'}}	The ENTITY ENTITYUNRELATED the ENTITYUNRELATED ENTITYOTHER by 11.5 %.
We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus.	frequencies	frames	model-feature	{'e1': {'word': 'frequencies', 'word_index': [(12, 12)], 'id': 'W93-0109.8'}, 'e2': {'word': 'frames', 'word_index': [(9, 9)], 'id': 'W93-0109.7'}}	We describe a ENTITYUNRELATED for automatically acquiring ENTITYUNRELATED subcategorization ENTITYOTHER and their ENTITY in a large ENTITYUNRELATED .
A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus.	noun phrases	corpus	part_whole	{'e1': {'word': 'noun phrases', 'word_index': [(9, 9)], 'id': 'W93-0109.12'}, 'e2': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'W93-0109.11'}}	A ENTITYUNRELATED ENTITYOTHER is first partially parsed to identify ENTITY and then a linear grammar is used to estimate the appropriate subcategorization ENTITYUNRELATED for each ENTITYUNRELATED token in the ENTITYUNRELATED .
A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus.	verb	corpus	part_whole	{'e1': {'word': 'verb', 'word_index': [(25, 25)], 'id': 'W93-0109.14'}, 'e2': {'word': 'corpus', 'word_index': [(29, 29)], 'id': 'W93-0109.15'}}	A ENTITYUNRELATED ENTITYUNRELATED is first partially parsed to identify ENTITYUNRELATED and then a linear grammar is used to estimate the appropriate subcategorization ENTITYUNRELATED for each ENTITY token in the ENTITYOTHER .
In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy.	system	accuracy	result	{'e1': {'word': 'system', 'word_index': [(14, 14)], 'id': 'W93-0109.20'}, 'e2': {'word': 'accuracy', 'word_index': [(20, 20)], 'id': 'W93-0109.21'}}	In an ENTITYUNRELATED involving the ENTITYUNRELATED of six fixed subcategorization ENTITYUNRELATED , our ENTITYUNRELATED ENTITY showed more than 80 % ENTITYOTHER .
In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.	approach	accuracy	result	{'e1': {'word': 'approach', 'word_index': [(6, 6)], 'id': 'W93-0109.24'}, 'e2': {'word': 'accuracy', 'word_index': [(10, 10)], 'id': 'W93-0109.26'}}	In ENTITYUNRELATED , a new ENTITYUNRELATED ENTITY substantially ENTITYUNRELATED the ENTITYOTHER of the ENTITYUNRELATED ENTITYUNRELATED .
The Semantic Representation Of Spatial Configurations: A Conceptual Motivation For Generation In Machine Translation</title>	Generation	Machine Translation	usage	{'e1': {'word': 'Generation', 'word_index': [(10, 10)], 'id': 'C90-3053.3'}, 'e2': {'word': 'Machine Translation', 'word_index': [(12, 12)], 'id': 'C90-3053.4'}}	The ENTITYUNRELATED Of Spatial Configurations : A Conceptual ENTITYUNRELATED For ENTITY In ENTITYOTHER < / title >
This paper deals with the automatic translation of prepositions, which are highly polysemous.	paper	translation	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C90-3053.5'}, 'e2': {'word': 'translation', 'word_index': [(6, 6)], 'id': 'C90-3053.8'}}	This ENTITY ENTITYUNRELATED with the ENTITYUNRELATED ENTITYOTHER of ENTITYUNRELATED , which are highly polysemous .
Moreover, the same real situation is often expressed by different prepositions in different languages.	prepositions	languages	part_whole	{'e1': {'word': 'prepositions', 'word_index': [(11, 11)], 'id': 'C90-3053.11'}, 'e2': {'word': 'languages', 'word_index': [(14, 14)], 'id': 'C90-3053.12'}}	Moreover , the same real ENTITYUNRELATED is often expressed by different ENTITY in different ENTITYOTHER .
Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy.	principles	process	usage	{'e1': {'word': 'principles', 'word_index': [(2, 2)], 'id': 'C90-3053.18'}, 'e2': {'word': 'process', 'word_index': [(11, 11)], 'id': 'C90-3053.21'}}	Following cognitive ENTITY of spatial conceptualization , we ENTITYUNRELATED a ENTITYUNRELATED ENTITYOTHER for spatial ENTITYUNRELATED in which our ENTITYUNRELATED uses ENTITYUNRELATED derived from a ENTITYUNRELATED sort hierarchy .
Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy.	semantic features	translation system	usage	{'e1': {'word': 'semantic features', 'word_index': [(20, 20)], 'id': 'C90-3053.24'}, 'e2': {'word': 'translation system', 'word_index': [(18, 18)], 'id': 'C90-3053.23'}}	Following cognitive ENTITYUNRELATED of spatial conceptualization , we ENTITYUNRELATED a ENTITYUNRELATED ENTITYUNRELATED for spatial ENTITYUNRELATED in which our ENTITYOTHER uses ENTITY derived from a ENTITYUNRELATED sort hierarchy .
Thus we can differentiate subtle distinctions between spatially significant configurations.	distinctions	configurations	model-feature	{'e1': {'word': 'distinctions', 'word_index': [(5, 5)], 'id': 'C90-3053.26'}, 'e2': {'word': 'configurations', 'word_index': [(9, 9)], 'id': 'C90-3053.27'}}	Thus we can differentiate subtle ENTITY between spatially significant ENTITYOTHER .
We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters.	approach	tagging	usage	{'e1': {'word': 'approach', 'word_index': [(3, 3)], 'id': 'W97-0210.4'}, 'e2': {'word': 'tagging', 'word_index': [(5, 5)], 'id': 'W97-0210.5'}}	We present an ENTITY for ENTITYOTHER ENTITYUNRELATED ENTITYUNRELATED that combines a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED on subcategorization and ENTITYUNRELATED with a ENTITYUNRELATED ENTITYUNRELATED utilizing statistically ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters.	alternations	method	usage	{'e1': {'word': 'alternations', 'word_index': [(17, 17)], 'id': 'W97-0210.11'}, 'e2': {'word': 'method', 'word_index': [(12, 12)], 'id': 'W97-0210.9'}}	We present an ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED that combines a ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED on subcategorization and ENTITY with a ENTITYUNRELATED ENTITYUNRELATED utilizing statistically ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters.	clusters	method	usage	{'e1': {'word': 'clusters', 'word_index': [(26, 26)], 'id': 'W97-0210.16'}, 'e2': {'word': 'method', 'word_index': [(21, 21)], 'id': 'W97-0210.13'}}	We present an ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED that combines a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED on subcategorization and ENTITYUNRELATED with a ENTITYUNRELATED ENTITYOTHER utilizing statistically ENTITYUNRELATED ENTITYUNRELATED ENTITY .
Building A Sense Tagged Corpus With Open Mind Word Expert</title>	Sense	Corpus	model-feature	{'e1': {'word': 'Sense', 'word_index': [(2, 2)], 'id': 'W02-0817.1'}, 'e2': {'word': 'Corpus', 'word_index': [(4, 4)], 'id': 'W02-0817.2'}}	Building A ENTITY Tagged ENTITYOTHER With Open Mind ENTITYUNRELATED ENTITYUNRELATED < / title >
Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web.	system	tagging	usage	{'e1': {'word': 'system', 'word_index': [(8, 8)], 'id': 'W02-0817.9'}, 'e2': {'word': 'tagging', 'word_index': [(12, 12)], 'id': 'W02-0817.11'}}	Open Mind ENTITYUNRELATED ENTITYUNRELATED is an ENTITYUNRELATED ENTITYUNRELATED ENTITY for collecting ENTITYUNRELATED ENTITYOTHER from the general public over the Web .
We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers.	system	data	result	{'e1': {'word': 'system', 'word_index': [(3, 3)], 'id': 'W02-0817.12'}, 'e2': {'word': 'data', 'word_index': [(12, 12)], 'id': 'W02-0817.17'}}	We expect the ENTITY to ENTITYUNRELATED a large ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER at a much lower ENTITYUNRELATED than the traditional ENTITYUNRELATED of hiring lexicographers .
This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning.	paper	interactions	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W98-1126.4'}, 'e2': {'word': 'interactions', 'word_index': [(3, 3)], 'id': 'W98-1126.5'}}	This ENTITY investigates ENTITYOTHER between collocational ENTITYUNRELATED and ENTITYUNRELATED for organizing them into ENTITYUNRELATED for ENTITYUNRELATED .
This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning.	properties	features	usage	{'e1': {'word': 'properties', 'word_index': [(6, 6)], 'id': 'W98-1126.6'}, 'e2': {'word': 'features', 'word_index': [(13, 13)], 'id': 'W98-1126.8'}}	This ENTITYUNRELATED investigates ENTITYUNRELATED between collocational ENTITY and ENTITYUNRELATED for organizing them into ENTITYOTHER for ENTITYUNRELATED .
In experiments performing an event categorization task, Wiebe et al. (1997a) found that different organizations are best for different properties.	experiments	task	usage	{'e1': {'word': 'experiments', 'word_index': [(1, 1)], 'id': 'W98-1126.10'}, 'e2': {'word': 'task', 'word_index': [(6, 6)], 'id': 'W98-1126.14'}}	In ENTITY ENTITYUNRELATED an ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER , Wiebe et al. ( 1997a ) found that different ENTITYUNRELATED are best for different ENTITYUNRELATED .
In experiments performing an event categorization task, Wiebe et al. (1997a) found that different organizations are best for different properties.	organizations	properties	usage	{'e1': {'word': 'organizations', 'word_index': [(17, 17)], 'id': 'W98-1126.15'}, 'e2': {'word': 'properties', 'word_index': [(22, 22)], 'id': 'W98-1126.16'}}	In ENTITYUNRELATED ENTITYUNRELATED an ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , Wiebe et al. ( 1997a ) found that different ENTITY are best for different ENTITYOTHER .
This paper presents a statistical analysis of the results across different machine learning algorithms.	paper	analysis	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W98-1126.17'}, 'e2': {'word': 'analysis', 'word_index': [(5, 5)], 'id': 'W98-1126.19'}}	This ENTITY presents a ENTITYUNRELATED ENTITYOTHER of the ENTITYUNRELATED across different ENTITYUNRELATED learning ENTITYUNRELATED .
This paper presents a statistical analysis of the results across different machine learning algorithms.	algorithms	results	result	{'e1': {'word': 'algorithms', 'word_index': [(13, 13)], 'id': 'W98-1126.22'}, 'e2': {'word': 'results', 'word_index': [(8, 8)], 'id': 'W98-1126.20'}}	This ENTITYUNRELATED presents a ENTITYUNRELATED ENTITYUNRELATED of the ENTITYOTHER across different ENTITYUNRELATED learning ENTITY .
This prompted further analysis of this relationship, and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments.	analysis	relationship	topic	{'e1': {'word': 'analysis', 'word_index': [(3, 3)], 'id': 'W98-1126.28'}, 'e2': {'word': 'relationship', 'word_index': [(6, 6)], 'id': 'W98-1126.29'}}	This prompted further ENTITY of this ENTITYOTHER , and an ENTITYUNRELATED of ENTITYUNRELATED for recognizing beneficial ways to ENTITYUNRELATED collocational ENTITYUNRELATED in ENTITYUNRELATED learning ENTITYUNRELATED .
This prompted further analysis of this relationship, and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments.	investigation	criteria	topic	{'e1': {'word': 'investigation', 'word_index': [(10, 10)], 'id': 'W98-1126.30'}, 'e2': {'word': 'criteria', 'word_index': [(12, 12)], 'id': 'W98-1126.31'}}	This prompted further ENTITYUNRELATED of this ENTITYUNRELATED , and an ENTITY of ENTITYOTHER for recognizing beneficial ways to ENTITYUNRELATED collocational ENTITYUNRELATED in ENTITYUNRELATED learning ENTITYUNRELATED .
This prompted further analysis of this relationship, and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments.	properties	experiments	usage	{'e1': {'word': 'properties', 'word_index': [(20, 20)], 'id': 'W98-1126.33'}, 'e2': {'word': 'experiments', 'word_index': [(24, 24)], 'id': 'W98-1126.35'}}	This prompted further ENTITYUNRELATED of this ENTITYUNRELATED , and an ENTITYUNRELATED of ENTITYUNRELATED for recognizing beneficial ways to ENTITYUNRELATED collocational ENTITY in ENTITYUNRELATED learning ENTITYOTHER .
While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.	properties	features	usage	{'e1': {'word': 'properties', 'word_index': [(5, 5)], 'id': 'W98-1126.37'}, 'e2': {'word': 'features', 'word_index': [(12, 12)], 'id': 'W98-1126.39'}}	While many ENTITYUNRELATED of collocational ENTITY and ENTITYUNRELATED of organizing them into ENTITYOTHER have been used in NLP , systematic ENTITYUNRELATED of their ENTITYUNRELATED are rare .
While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.	investigations	interaction	topic	{'e1': {'word': 'investigations', 'word_index': [(20, 20)], 'id': 'W98-1126.40'}, 'e2': {'word': 'interaction', 'word_index': [(23, 23)], 'id': 'W98-1126.41'}}	While many ENTITYUNRELATED of collocational ENTITYUNRELATED and ENTITYUNRELATED of organizing them into ENTITYUNRELATED have been used in NLP , systematic ENTITY of their ENTITYOTHER are rare .
KnoFusius: a New Knowledge Fusion System for Interpretation of Gene Expression Data</title>	System	Interpretation	usage	{'e1': {'word': 'System', 'word_index': [(6, 6)], 'id': 'L08-1203.3'}, 'e2': {'word': 'Interpretation', 'word_index': [(8, 8)], 'id': 'L08-1203.4'}}	KnoFusius : a New ENTITYUNRELATED ENTITYUNRELATED ENTITY for ENTITYOTHER of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED < / title >
This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand .	paper	architecture	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'L08-1203.8'}, 'e2': {'word': 'architecture', 'word_index': [(5, 5)], 'id': 'L08-1203.9'}}	This ENTITY introduces a new ENTITYOTHER that aims at combining molecular ENTITYUNRELATED ENTITYUNRELATED with ENTITYUNRELATED automatically ENTITYUNRELATED from relevant ENTITYUNRELATED ( using ENTITYUNRELATED mining ENTITYUNRELATED on PubMed ENTITYUNRELATED and fulltext ENTITYUNRELATED ) to ENTITYUNRELATED biomedical ENTITYUNRELATED to interpret ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED .
This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand .	information	scientific literature	part_whole	{'e1': {'word': 'information', 'word_index': [(14, 14)], 'id': 'L08-1203.12'}, 'e2': {'word': 'scientific literature', 'word_index': [(19, 19)], 'id': 'L08-1203.14'}}	This ENTITYUNRELATED introduces a new ENTITYUNRELATED that aims at combining molecular ENTITYUNRELATED ENTITYUNRELATED with ENTITY automatically ENTITYUNRELATED from relevant ENTITYOTHER ( using ENTITYUNRELATED mining ENTITYUNRELATED on PubMed ENTITYUNRELATED and fulltext ENTITYUNRELATED ) to ENTITYUNRELATED biomedical ENTITYUNRELATED to interpret ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED .
This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand .	techniques	abstracts	usage	{'e1': {'word': 'techniques', 'word_index': [(24, 24)], 'id': 'L08-1203.16'}, 'e2': {'word': 'abstracts', 'word_index': [(27, 27)], 'id': 'L08-1203.17'}}	This ENTITYUNRELATED introduces a new ENTITYUNRELATED that aims at combining molecular ENTITYUNRELATED ENTITYUNRELATED with ENTITYUNRELATED automatically ENTITYUNRELATED from relevant ENTITYUNRELATED ( using ENTITYUNRELATED mining ENTITY on PubMed ENTITYOTHER and fulltext ENTITYUNRELATED ) to ENTITYUNRELATED biomedical ENTITYUNRELATED to interpret ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED .
The infrastructural level bears on semantic-web technologies and standards that facilitate the actual fusion of the multi-source knowledge .	standards	fusion	usage	{'e1': {'word': 'standards', 'word_index': [(8, 8)], 'id': 'L08-1203.27'}, 'e2': {'word': 'fusion', 'word_index': [(13, 13)], 'id': 'L08-1203.28'}}	The infrastructural ENTITYUNRELATED bears on ENTITYUNRELATED ENTITYUNRELATED and ENTITY that facilitate the actual ENTITYOTHER of the ENTITYUNRELATED ENTITYUNRELATED .
In this paper , we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic  properties of the nouns influence the results .	paper	vector-based models	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'L08-1204.20'}, 'e2': {'word': 'vector-based models', 'word_index': [(7, 7)], 'id': 'L08-1204.21'}}	In this ENTITY , we use three ENTITYOTHER to retrieve semantically related ENTITYUNRELATED for a set of Dutch ENTITYUNRELATED and we analyse whether three linguistic ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED the ENTITYUNRELATED .
In this paper , we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic  properties of the nouns influence the results .	properties	results	result	{'e1': {'word': 'properties', 'word_index': [(25, 25)], 'id': 'L08-1204.24'}, 'e2': {'word': 'results', 'word_index': [(31, 31)], 'id': 'L08-1204.27'}}	In this ENTITYUNRELATED , we use three ENTITYUNRELATED to retrieve semantically related ENTITYUNRELATED for a set of Dutch ENTITYUNRELATED and we analyse whether three linguistic ENTITY of the ENTITYUNRELATED ENTITYUNRELATED the ENTITYOTHER .
In particular, we compare results from a dependency-based model  with those from a 1st and 2nd order bag-of-words model and we examine the effect of the nouns ' frequency , semantic speficity and semantic class .	dependency-based model	results	result	{'e1': {'word': 'dependency-based model', 'word_index': [(8, 8)], 'id': 'L08-1204.29'}, 'e2': {'word': 'results', 'word_index': [(5, 5)], 'id': 'L08-1204.28'}}	In particular , we compare ENTITYOTHER from a ENTITY with those from a 1st and 2nd ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and we examine the ENTITYUNRELATED of the ENTITYUNRELATED ' ENTITYUNRELATED , ENTITYUNRELATED speficity and ENTITYUNRELATED .
This database contains recorded , transcribed and annotated read speech  (42 GB or 130 hours) of 400 Dutch speaking elementary school children with or without reading difficulties .	speech 	database	part_whole	{'e1': {'word': 'speech ', 'word_index': [(9, 9)], 'id': 'L08-1205.12'}, 'e2': {'word': 'database', 'word_index': [(1, 1)], 'id': 'L08-1205.10'}}	This ENTITYOTHER contains ENTITYUNRELATED , transcribed and annotated read ENTITY ( 42 GB or 130 hours ) of 400 Dutch speaking elementary school children with or without reading ENTITYUNRELATED .
Analyses of inter- and intra-annotator agreement are carried out in order to investigate the  consistency  with which reading errors are detected, orthographic and phonetic transcriptions are made, and reading errors and reading strategies are labeled.	Analyses	consistency	topic	{'e1': {'word': 'Analyses', 'word_index': [(0, 0)], 'id': 'L08-1205.14'}, 'e2': {'word': 'consistency', 'word_index': [(15, 15)], 'id': 'L08-1205.17'}}	ENTITY of inter - and intra-annotator ENTITYUNRELATED are carried out in ENTITYUNRELATED to investigate the ENTITYOTHER with which reading ENTITYUNRELATED are detected , orthographic and phonetic ENTITYUNRELATED are made , and reading ENTITYUNRELATED and reading ENTITYUNRELATED are labeled .
School type and reading  type seem to account for systematic differences in % agreement , but these differences disappear when kappa values are calculated that correct for chance agreement .	type	differences	result	{'e1': {'word': 'type', 'word_index': [(4, 4)], 'id': 'L08-1205.32'}, 'e2': {'word': 'differences', 'word_index': [(10, 10)], 'id': 'L08-1205.33'}}	School ENTITYUNRELATED and reading ENTITY seem to account for systematic ENTITYOTHER in % ENTITYUNRELATED , but these ENTITYUNRELATED disappear when ENTITYUNRELATED values are calculated that correct for chance ENTITYUNRELATED .
To conclude, an  analysis of the annotation differences  with respect to the '*s' label (i.e.	analysis	differences	topic	{'e1': {'word': 'analysis', 'word_index': [(4, 4)], 'id': 'L08-1205.38'}, 'e2': {'word': 'differences', 'word_index': [(8, 8)], 'id': 'L08-1205.39'}}	To conclude , an ENTITY of the annotation ENTITYOTHER with ENTITYUNRELATED to the '* s ' label ( i.e.
A Bilingual  Corpus of Inter-linked Events</title>	Events	Corpus	part_whole	{'e1': {'word': 'Events', 'word_index': [(5, 5)], 'id': 'L08-1206.2'}, 'e2': {'word': 'Corpus', 'word_index': [(2, 2)], 'id': 'L08-1206.1'}}	A Bilingual ENTITYOTHER of Inter-linked ENTITY < / title >
This paper describes the creation  of a bilingual corpus  of inter-linked events  for Italian and English .	paper	creation	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'L08-1206.3'}, 'e2': {'word': 'creation', 'word_index': [(4, 4)], 'id': 'L08-1206.4'}}	This ENTITY describes the ENTITYOTHER of a bilingual ENTITYUNRELATED of inter-linked ENTITYUNRELATED for Italian and ENTITYUNRELATED .
This paper describes the creation  of a bilingual corpus  of inter-linked events  for Italian and English .	events	corpus	part_whole	{'e1': {'word': 'events', 'word_index': [(11, 11)], 'id': 'L08-1206.6'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'L08-1206.5'}}	This ENTITYUNRELATED describes the ENTITYUNRELATED of a bilingual ENTITYOTHER of inter-linked ENTITY for Italian and ENTITYUNRELATED .
The availability of this resource , on the one hand , enables  contrastive analysis  of the linguistic phenomena  surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts .	analysis	phenomena	topic	{'e1': {'word': 'analysis', 'word_index': [(13, 13)], 'id': 'L08-1206.14'}, 'e2': {'word': 'phenomena', 'word_index': [(17, 17)], 'id': 'L08-1206.15'}}	The ENTITYUNRELATED of this ENTITYUNRELATED , on the one ENTITYUNRELATED , enables contrastive ENTITY of the linguistic ENTITYOTHER surrounding ENTITYUNRELATED in both ENTITYUNRELATED , and on the other ENTITYUNRELATED , can be used to ENTITYUNRELATED multilingual temporal ENTITYUNRELATED of ENTITYUNRELATED .
The availability of this resource , on the one hand , enables  contrastive analysis  of the linguistic phenomena  surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts .	analysis	texts	topic	{'e1': {'word': 'analysis', 'word_index': [(37, 37)], 'id': 'L08-1206.20'}, 'e2': {'word': 'texts', 'word_index': [(39, 39)], 'id': 'L08-1206.21'}}	The ENTITYUNRELATED of this ENTITYUNRELATED , on the one ENTITYUNRELATED , enables contrastive ENTITYUNRELATED of the linguistic ENTITYUNRELATED surrounding ENTITYUNRELATED in both ENTITYUNRELATED , and on the other ENTITYUNRELATED , can be used to ENTITYUNRELATED multilingual temporal ENTITY of ENTITYOTHER .
In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense .	methodology	construction	usage	{'e1': {'word': 'methodology', 'word_index': [(5, 5)], 'id': 'L08-1206.23'}, 'e2': {'word': 'construction', 'word_index': [(7, 7)], 'id': 'L08-1206.24'}}	In ENTITYUNRELATED to describing the ENTITY for ENTITYOTHER of the inter-linked ENTITYUNRELATED and the ENTITYUNRELATED of the ENTITYUNRELATED collected , we demonstrate that the ILI could potentially be used to ENTITYUNRELATED the ENTITYUNRELATED of comparable ENTITYUNRELATED by exporting ENTITYUNRELATED of annotation for ENTITYUNRELATED that have the same ENTITYUNRELATED .
The first phase the program focuses on translation of handwritten Arabic documents .	translation	documents	usage	{'e1': {'word': 'translation', 'word_index': [(7, 7)], 'id': 'L08-1207.18'}, 'e2': {'word': 'documents', 'word_index': [(11, 11)], 'id': 'L08-1207.19'}}	The first ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED on ENTITY of handwritten Arabic ENTITYOTHER .
Evaluation data will be carefully selected from the available data pools and high quality references will be produced, which can be used to compare MADCAT system performance against the human-produced gold standard .	performance	gold standard	compare	{'e1': {'word': 'performance', 'word_index': [(26, 26)], 'id': 'L08-1207.59'}, 'e2': {'word': 'gold standard', 'word_index': [(32, 32)], 'id': 'L08-1207.60'}}	ENTITYUNRELATED ENTITYUNRELATED will be carefully selected from the available ENTITYUNRELATED pools and ENTITYUNRELATED ENTITYUNRELATED will be produced , which can be used to compare MADCAT ENTITYUNRELATED ENTITY against the human - produced ENTITYOTHER .
This question is tightly related to estimating the classifier performance after a certain amount of data has already been annotated.	question	performance	result	{'e1': {'word': 'question', 'word_index': [(1, 1)], 'id': 'L08-1208.15'}, 'e2': {'word': 'performance', 'word_index': [(9, 9)], 'id': 'L08-1208.17'}}	This ENTITY is tightly related to estimating the ENTITYUNRELATED ENTITYOTHER after a certain ENTITYUNRELATED of ENTITYUNRELATED has already been annotated .
This  method relies on a separate, unlabeled corpus and is thus well suited for situations where a labeled gold standard is not available or would be too expensive to obtain.	method	corpus	usage	{'e1': {'word': 'method', 'word_index': [(1, 1)], 'id': 'L08-1208.32'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'L08-1208.33'}}	This ENTITY relies on a separate , unlabeled ENTITYOTHER and is thus well suited for ENTITYUNRELATED where a labeled ENTITYUNRELATED is not available or would be too expensive to obtain .
Considering named entity recognition as a test case we provide empirical evidence that this approach works well under simulation as well as under real-world annotation conditions.	approach	recognition	usage	{'e1': {'word': 'approach', 'word_index': [(14, 14)], 'id': 'L08-1208.43'}, 'e2': {'word': 'recognition', 'word_index': [(3, 3)], 'id': 'L08-1208.38'}}	Considering ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER as a ENTITYUNRELATED ENTITYUNRELATED we ENTITYUNRELATED empirical ENTITYUNRELATED that this ENTITY works well under ENTITYUNRELATED as well as under real - world annotation conditions .
Lexicon schemas and their use are discussed in this paper from the perspective of lexicographers and field linguists 	paper	schemas	topic	{'e1': {'word': 'paper', 'word_index': [(9, 9)], 'id': 'L08-1209.7'}, 'e2': {'word': 'schemas', 'word_index': [(1, 1)], 'id': 'L08-1209.6'}}	ENTITYUNRELATED ENTITYOTHER and their use are discussed in this ENTITY from the ENTITYUNRELATED of lexicographers and ENTITYUNRELATED ENTITYUNRELATED
A variety of lexicon schemas have been developed , with goals ranging from computational lexicography (DATR) through archiving (LIFT, TEI) to standardization (LMF, FSR).	schemas	archiving	usage	{'e1': {'word': 'schemas', 'word_index': [(4, 4)], 'id': 'L08-1209.13'}, 'e2': {'word': 'archiving', 'word_index': [(19, 19)], 'id': 'L08-1209.17'}}	A ENTITYUNRELATED of ENTITYUNRELATED ENTITY have been ENTITYUNRELATED , with ENTITYUNRELATED ranging from ENTITYUNRELATED lexicography ( DATR ) through ENTITYOTHER ( LIFT , TEI ) to standardization ( LMF , FSR ) .
Arabic WordNet: Semi-automatic Extensions using Bayesian Inference</title>	Inference	Extensions	usage	{'e1': {'word': 'Inference', 'word_index': [(7, 7)], 'id': 'L08-1211.3'}, 'e2': {'word': 'Extensions', 'word_index': [(4, 4)], 'id': 'L08-1211.2'}}	Arabic WordNet : ENTITYUNRELATED ENTITYOTHER using Bayesian ENTITY < / title >
This presentation focuses on the semi-automatic extension of Arabic WordNet (AWN) using lexical and morphological rules and applying Bayesian inference 	presentation	extension	topic	{'e1': {'word': 'presentation', 'word_index': [(1, 1)], 'id': 'L08-1211.4'}, 'e2': {'word': 'extension', 'word_index': [(6, 6)], 'id': 'L08-1211.7'}}	This ENTITY ENTITYUNRELATED on the ENTITYUNRELATED ENTITYOTHER of Arabic WordNet ( AWN ) using ENTITYUNRELATED and morphological ENTITYUNRELATED and ENTITYUNRELATED Bayesian ENTITYUNRELATED
The application of this set of rules , combined with the use of bilingual Arabic- English resources and Princeton's WordNet, allows the generation of a graph representing the semantic neighbourhood of the original word .	rules	generation	usage	{'e1': {'word': 'rules', 'word_index': [(6, 6)], 'id': 'L08-1211.24'}, 'e2': {'word': 'generation', 'word_index': [(25, 25)], 'id': 'L08-1211.27'}}	The ENTITYUNRELATED of this set of ENTITY , combined with the use of bilingual Arabic - ENTITYUNRELATED ENTITYUNRELATED and Princeton 's WordNet , allows the ENTITYOTHER of a graph representing the ENTITYUNRELATED neighbourhood of the original ENTITYUNRELATED .
Here, a novel approach to extending AWN is presented whereby a Bayesian Network is automatically built from the graph and then the net is used as an inferencing mechanism for scoring the set of candidate associations .	Network	mechanism	usage	{'e1': {'word': 'Network', 'word_index': [(13, 13)], 'id': 'L08-1211.36'}, 'e2': {'word': 'mechanism', 'word_index': [(29, 29)], 'id': 'L08-1211.38'}}	Here , a novel ENTITYUNRELATED to extending AWN is presented whereby a Bayesian ENTITY is automatically built from the graph and then the net is used as an ENTITYUNRELATED ENTITYOTHER for scoring the set of ENTITYUNRELATED ENTITYUNRELATED .
Both on its own and in combination with the previous technique , this new approach has led to improved results .	approach	results	result	{'e1': {'word': 'approach', 'word_index': [(14, 14)], 'id': 'L08-1211.43'}, 'e2': {'word': 'results', 'word_index': [(19, 19)], 'id': 'L08-1211.45'}}	Both on its own and in ENTITYUNRELATED with the previous ENTITYUNRELATED , this new ENTITY has led to ENTITYUNRELATED ENTITYOTHER .
This paper describes the evaluation process of an emotional speech database recorded for standard Basque, in order to determine its adequacy for the analysis of emotional models and its use in speech synthesis .	paper	process	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'L08-1212.4'}, 'e2': {'word': 'process', 'word_index': [(5, 5)], 'id': 'L08-1212.6'}}	This ENTITY describes the ENTITYUNRELATED ENTITYOTHER of an emotional ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED Basque , in ENTITYUNRELATED to determine its ENTITYUNRELATED for the ENTITYUNRELATED of emotional ENTITYUNRELATED and its use in ENTITYUNRELATED .
The corpus consists of seven hundred semantically neutral sentences that were recorded for the Big Six emotions and neutral style, by two professional actors.	sentences	corpus	part_whole	{'e1': {'word': 'sentences', 'word_index': [(8, 8)], 'id': 'L08-1212.17'}, 'e2': {'word': 'corpus', 'word_index': [(1, 1)], 'id': 'L08-1212.16'}}	The ENTITYOTHER consists of seven hundred semantically neutral ENTITY that were ENTITYUNRELATED for the Big Six ENTITYUNRELATED and neutral style , by two professional actors .
Therefore the database is a valid linguistic resource for the research and development purposes it was designed for.	database	purposes	usage	{'e1': {'word': 'database', 'word_index': [(2, 2)], 'id': 'L08-1212.24'}, 'e2': {'word': 'purposes', 'word_index': [(12, 12)], 'id': 'L08-1212.28'}}	Therefore the ENTITY is a valid ENTITYUNRELATED for the ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER it was ENTITYUNRELATED for .
The testsuite provides a well thought-out error classification , which enables us to compare parser output for parsers trained on treebanks with different encoding schemes and provides interesting insights into the impact of treebank annotation schemes on specific constructions like PP attachment or non-constituent coordination .	schemes	constructions	result	{'e1': {'word': 'schemes', 'word_index': [(37, 37)], 'id': 'L08-1213.46'}, 'e2': {'word': 'constructions', 'word_index': [(40, 40)], 'id': 'L08-1213.47'}}	The testsuite ENTITYUNRELATED a well thought - out ENTITYUNRELATED ENTITYUNRELATED , which enables us to compare ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED on treebanks with different encoding ENTITYUNRELATED and ENTITYUNRELATED interesting ENTITYUNRELATED into the ENTITYUNRELATED of treebank annotation ENTITY on specific ENTITYOTHER like PP ENTITYUNRELATED or ENTITYUNRELATED ENTITYUNRELATED .
The corpus is built from a collection of about 1,4 millions newswires (10 GB) in three languages , Arabic, English and French provided by Agence France Press (AFP) and selected from a 3 years period.	collection	corpus	part_whole	{'e1': {'word': 'collection', 'word_index': [(6, 6)], 'id': 'L08-1214.15'}, 'e2': {'word': 'corpus', 'word_index': [(1, 1)], 'id': 'L08-1214.14'}}	The ENTITYOTHER is built from a ENTITY of about 1,4 millions newswires ( 10 GB ) in three ENTITYUNRELATED , Arabic , ENTITYUNRELATED and French ENTITYUNRELATED by Agence France Press ( AFP ) and selected from a 3 years period .
The profiles corpus is made of 50 profiles from which 30 concern general news and events (national and international affairs, politics, sports ...) and 20 concern scientific and technical subjects.	events	corpus	part_whole	{'e1': {'word': 'events', 'word_index': [(15, 15)], 'id': 'L08-1214.22'}, 'e2': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'L08-1214.19'}}	The profiles ENTITYOTHER is made of 50 profiles from which 30 ENTITYUNRELATED general ENTITYUNRELATED and ENTITY ( national and international affairs , politics , ENTITYUNRELATED ... ) and 20 ENTITYUNRELATED scientific and technical subjects .
In languages that use diacritical characters, if these special signs are stripped-off from a word , the resulted string of characters may not exist in the language , and therefore its normative form is, in general, easy to recover.	string	language	part_whole	{'e1': {'word': 'string', 'word_index': [(21, 21)], 'id': 'L08-1215.5'}, 'e2': {'word': 'language', 'word_index': [(29, 29)], 'id': 'L08-1215.6'}}	In ENTITYUNRELATED that use diacritical characters , if these special signs are stripped - off from a ENTITYUNRELATED , the ENTITYUNRELATED ENTITY of characters may not exist in the ENTITYOTHER , and therefore its normative ENTITYUNRELATED is , in general , easy to recover .
This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data .	paper	project	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'L08-1216.3'}, 'e2': {'word': 'project', 'word_index': [(5, 5)], 'id': 'L08-1216.4'}}	This ENTITY describes an ongoing ENTITYOTHER in which we are collecting a learner ENTITYUNRELATED of Arabic , ENTITYUNRELATED a tagset for ENTITYUNRELATED annotation and ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ( CEA ) on the ENTITYUNRELATED .
This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data .	Error Analysis	data	usage	{'e1': {'word': 'Error Analysis', 'word_index': [(26, 26)], 'id': 'L08-1216.10'}, 'e2': {'word': 'data', 'word_index': [(32, 32)], 'id': 'L08-1216.11'}}	This ENTITYUNRELATED describes an ongoing ENTITYUNRELATED in which we are collecting a learner ENTITYUNRELATED of Arabic , ENTITYUNRELATED a tagset for ENTITYUNRELATED annotation and ENTITYUNRELATED ENTITYUNRELATED ENTITY ( CEA ) on the ENTITYOTHER .
We chose FRIDA in order to follow a known standard and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the distance between the two languages with respect to learner difficulty .	difficulty	languages	model-feature	{'e1': {'word': 'difficulty', 'word_index': [(42, 42)], 'id': 'L08-1216.20'}, 'e2': {'word': 'languages', 'word_index': [(37, 37)], 'id': 'L08-1216.18'}}	We chose FRIDA in ENTITYUNRELATED to follow a known ENTITYUNRELATED and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the ENTITYUNRELATED between the two ENTITYOTHER with ENTITYUNRELATED to learner ENTITY .
The current collection of texts , which is constantly growing, contains intermediate and advanced-level student writings .	advanced-level	collection	part_whole	{'e1': {'word': 'advanced-level', 'word_index': [(14, 14)], 'id': 'L08-1216.24'}, 'e2': {'word': 'collection', 'word_index': [(2, 2)], 'id': 'L08-1216.22'}}	The ENTITYUNRELATED ENTITYOTHER of ENTITYUNRELATED , which is constantly growing , contains intermediate and ENTITY student writings .
We describe the need for such corpora , the learner data we have collected and the tagset we have developed .	data	corpora	part_whole	{'e1': {'word': 'data', 'word_index': [(10, 10)], 'id': 'L08-1216.26'}, 'e2': {'word': 'corpora', 'word_index': [(6, 6)], 'id': 'L08-1216.25'}}	We describe the need for such ENTITYOTHER , the learner ENTITY we have collected and the tagset we have ENTITYUNRELATED .
All, and only, the Errors: more Complete and Consistent Spelling and OCR-Error Correction Evaluation</title>	Correction	OCR-Error	usage	{'e1': {'word': 'Correction', 'word_index': [(15, 15)], 'id': 'L08-1217.2'}, 'e2': {'word': 'OCR-Error', 'word_index': [(14, 14)], 'id': 'L08-1217.1'}}	All , and only , the Errors : more Complete and Consistent Spelling and ENTITYOTHER ENTITY ENTITYUNRELATED < / title >
Some time in the future, some spelling error correction system will correct all the errors , and only the errors .	system	errors	usage	{'e1': {'word': 'system', 'word_index': [(10, 10)], 'id': 'L08-1217.8'}, 'e2': {'word': 'errors', 'word_index': [(15, 15)], 'id': 'L08-1217.9'}}	Some ENTITYUNRELATED in the future , some ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITY will correct all the ENTITYOTHER , and only the ENTITYUNRELATED .
We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal .	evaluation	current	topic	{'e1': {'word': 'evaluation', 'word_index': [(10, 10)], 'id': 'L08-1217.17'}, 'e2': {'word': 'current', 'word_index': [(3, 3)], 'id': 'L08-1217.14'}}	We ENTITYUNRELATED the ENTITYOTHER ENTITYUNRELATED in the ENTITYUNRELATED of the ENTITY ENTITYUNRELATED of the latest major publication on ENTITYUNRELATED ENTITYUNRELATED in a leading ENTITYUNRELATED .
We finally contrast our preferred metrics to Accuracy , which is widely used in this field to this day and to the Area- Under-the-Curve, which is increasingly finding acceptance in other fields .	metrics	Accuracy	compare	{'e1': {'word': 'metrics', 'word_index': [(5, 5)], 'id': 'L08-1217.40'}, 'e2': {'word': 'Accuracy', 'word_index': [(7, 7)], 'id': 'L08-1217.41'}}	We finally ENTITYUNRELATED our preferred ENTITY to ENTITYOTHER , which is widely used in this ENTITYUNRELATED to this day and to the ENTITYUNRELATED Under-the - Curve , which is increasingly finding acceptance in other ENTITYUNRELATED .
This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'L08-1218.3'}, 'e2': {'word': 'method', 'word_index': [(4, 4)], 'id': 'L08-1218.4'}}	This ENTITY presents a ENTITYOTHER for compiling a ENTITYUNRELATED bilingual ENTITYUNRELATED from a ENTITYUNRELATED of movie subtitles .
This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles .	database	corpus	part_whole	{'e1': {'word': 'database', 'word_index': [(13, 13)], 'id': 'L08-1218.7'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'L08-1218.6'}}	This ENTITYUNRELATED presents a ENTITYUNRELATED for compiling a ENTITYUNRELATED bilingual ENTITYOTHER from a ENTITY of movie subtitles .
To create the corpus , we propose an algorithm based on Gale and Church's sentence alignment algorithm (1993).	algorithm	algorithm	usage	{'e1': {'word': 'algorithm', 'word_index': [(17, 17)], 'id': 'L08-1218.15'}, 'e2': {'word': 'algorithm', 'word_index': [(8, 8)], 'id': 'L08-1218.10'}}	To create the ENTITYUNRELATED , we ENTITYUNRELATED an ENTITYOTHER ENTITYUNRELATED on ENTITYUNRELATED and Church 's ENTITYUNRELATED ENTITYUNRELATED ENTITY ( 1993 ) .
However, our algorithm not only relies on character length information , but also uses subtitle-timing information , which is encoded in the subtitle files.	information	algorithm	usage	{'e1': {'word': 'information', 'word_index': [(10, 10)], 'id': 'L08-1218.18'}, 'e2': {'word': 'algorithm', 'word_index': [(3, 3)], 'id': 'L08-1218.16'}}	However , our ENTITYOTHER not only relies on character ENTITYUNRELATED ENTITY , but also uses ENTITYUNRELATED ENTITYUNRELATED , which is encoded in the subtitle files .
However, the absolute time values can't be used for alignment , since the timing is usually specified by frame numbers and not by real time , and converting it to real time values is not always possible, hence we  use normalized subtitle duration instead .	time	alignment	usage	{'e1': {'word': 'time', 'word_index': [(4, 4)], 'id': 'L08-1218.26'}, 'e2': {'word': 'alignment', 'word_index': [(11, 11)], 'id': 'L08-1218.27'}}	However , the absolute ENTITY values ca n't be used for ENTITYOTHER , since the ENTITYUNRELATED is usually specified by ENTITYUNRELATED ENTITYUNRELATED and not by ENTITYUNRELATED , and converting it to ENTITYUNRELATED values is not always possible , hence we use normalized subtitle ENTITYUNRELATED instead .
Corpora of multi-modal conversational speech are rare and frequently difficult to use due to privacy and copyright restrictions .	speech	Corpora	part_whole	{'e1': {'word': 'speech', 'word_index': [(4, 4)], 'id': 'L08-1219.13'}, 'e2': {'word': 'Corpora', 'word_index': [(0, 0)], 'id': 'L08-1219.12'}}	ENTITYOTHER of multi-modal conversational ENTITY are rare and frequently difficult to use ENTITYUNRELATED to privacy and copyright ENTITYUNRELATED .
A freely available annotated corpus is presented, gratis and libre, of high quality video recordings of face-to-face conversational speech .	video	corpus	part_whole	{'e1': {'word': 'video', 'word_index': [(14, 14)], 'id': 'L08-1219.18'}, 'e2': {'word': 'corpus', 'word_index': [(4, 4)], 'id': 'L08-1219.16'}}	A freely available annotated ENTITYOTHER is presented , gratis and libre , of ENTITYUNRELATED ENTITY ENTITYUNRELATED of face - to - face conversational ENTITYUNRELATED .
"From our experiences we would like to advocate the formulation of """"best practises"""" for both legal handling and database storage of recordings and annotations. """	recordings	database	part_whole	{'e1': {'word': 'recordings', 'word_index': [(25, 25)], 'id': 'L08-1219.32'}, 'e2': {'word': 'database', 'word_index': [(22, 22)], 'id': 'L08-1219.30'}}	"From our ENTITYUNRELATED we would like to advocate the ENTITYUNRELATED of "" "" best practises "" "" for both legal handling and ENTITYOTHER ENTITYUNRELATED of ENTITY and annotations . """
This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories 	paper	data	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'L08-1220.3'}, 'e2': {'word': 'data', 'word_index': [(6, 6)], 'id': 'L08-1220.4'}}	This ENTITY describes a multichannel acoustic ENTITYOTHER ENTITYUNRELATED ENTITYUNRELATED under the European DICIT ENTITYUNRELATED , during the Wizard of Oz ( WOZ ) ENTITYUNRELATED carried out at FAU and FBK - irst ENTITYUNRELATED
The scenario is a distant-talking interface for interactive control of a TV.	interface	control	usage	{'e1': {'word': 'interface', 'word_index': [(7, 7)], 'id': 'L08-1220.11'}, 'e2': {'word': 'control', 'word_index': [(10, 10)], 'id': 'L08-1220.12'}}	The ENTITYUNRELATED is a distant - talking ENTITY for interactive ENTITYOTHER of a TV.
In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments .	scenarios	experiments	usage	{'e1': {'word': 'scenarios', 'word_index': [(5, 5)], 'id': 'L08-1220.22'}, 'e2': {'word': 'experiments', 'word_index': [(22, 22)], 'id': 'L08-1220.25'}}	In this way , realistic ENTITY can be simulated at a preliminary stage , instead of ENTITYUNRELATED ENTITYUNRELATED , allowing for repeatable ENTITYOTHER .
Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels .	events	database	part_whole	{'e1': {'word': 'events', 'word_index': [(12, 12)], 'id': 'L08-1220.37'}, 'e2': {'word': 'database', 'word_index': [(6, 6)], 'id': 'L08-1220.35'}}	Besides the ENTITYUNRELATED ENTITYUNRELATED , the ENTITYOTHER also contains ENTITYUNRELATED related acoustic ENTITY , room impulse response measurements and ENTITYUNRELATED data , the latter used to ENTITYUNRELATED 3D labels .
Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels .	video	compute	usage	{'e1': {'word': 'video', 'word_index': [(19, 19)], 'id': 'L08-1220.38'}, 'e2': {'word': 'compute', 'word_index': [(26, 26)], 'id': 'L08-1220.39'}}	Besides the ENTITYUNRELATED ENTITYUNRELATED , the ENTITYUNRELATED also contains ENTITYUNRELATED related acoustic ENTITYUNRELATED , room impulse response measurements and ENTITY data , the latter used to ENTITYOTHER 3D labels .
Process Model for Composing High-quality Text Corpora	Process	Corpora	usage	{'e1': {'word': 'Process', 'word_index': [(0, 0)], 'id': 'L08-1221.1'}, 'e2': {'word': 'Corpora', 'word_index': [(6, 6)], 'id': 'L08-1221.5'}}	ENTITY ENTITYUNRELATED for Composing ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER
The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research 	model	corpus	usage	{'e1': {'word': 'model', 'word_index': [(4, 4)], 'id': 'L08-1221.7'}, 'e2': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'L08-1221.6'}}	The Teko ENTITYOTHER composing ENTITY offers a decentralized , dynamic way of collecting ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED for linguistic ENTITYUNRELATED
The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research 	corpora	research	usage	{'e1': {'word': 'corpora', 'word_index': [(15, 15)], 'id': 'L08-1221.10'}, 'e2': {'word': 'research', 'word_index': [(18, 18)], 'id': 'L08-1221.11'}}	The Teko ENTITYUNRELATED composing ENTITYUNRELATED offers a decentralized , dynamic way of collecting ENTITYUNRELATED ENTITYUNRELATED ENTITY for linguistic ENTITYOTHER
The  resulting corpus consists of independent text sets.	text	corpus	part_whole	{'e1': {'word': 'text', 'word_index': [(6, 6)], 'id': 'L08-1221.14'}, 'e2': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'L08-1221.13'}}	The ENTITYUNRELATED ENTITYOTHER consists of independent ENTITY sets .
Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project .	software	extracting	usage	{'e1': {'word': 'software', 'word_index': [(2, 2)], 'id': 'L08-1221.24'}, 'e2': {'word': 'extracting', 'word_index': [(4, 4)], 'id': 'L08-1221.25'}}	Furthermore , ENTITY for ENTITYOTHER ENTITYUNRELATED quantitative ENTITYUNRELATED from the ENTITYUNRELATED sets has been created during the ENTITYUNRELATED .
The paper describes the project , and estimates its benefits and problems .	paper	project	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'L08-1221.30'}, 'e2': {'word': 'project', 'word_index': [(4, 4)], 'id': 'L08-1221.31'}}	The ENTITY describes the ENTITYOTHER , and estimates its ENTITYUNRELATED and ENTITYUNRELATED .
It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .	overview	qualities	topic	{'e1': {'word': 'overview', 'word_index': [(4, 4)], 'id': 'L08-1221.34'}, 'e2': {'word': 'qualities', 'word_index': [(8, 8)], 'id': 'L08-1221.35'}}	It also gives an ENTITY of the technical ENTITYOTHER of the ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED connected to the Teko ENTITYUNRELATED .
This paper presents AnCora, a multilingual corpus annotated at different linguistic levels consisting of 500,000 words in Catalan (AnCora-Ca) and in Spanish (AnCora-Es).	words	corpus	part_whole	{'e1': {'word': 'words', 'word_index': [(16, 16)], 'id': 'L08-1222.5'}, 'e2': {'word': 'corpus', 'word_index': [(7, 7)], 'id': 'L08-1222.3'}}	This ENTITYUNRELATED presents AnCora , a multilingual ENTITYOTHER annotated at different linguistic ENTITYUNRELATED consisting of 500,000 ENTITY in Catalan ( AnCora - Ca ) and in Spanish ( AnCora - Es ) .
At present AnCora is the largest multilayer annotated corpus of these languages freely available from http://clic.	languages	corpus	part_whole	{'e1': {'word': 'languages', 'word_index': [(11, 11)], 'id': 'L08-1222.7'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'L08-1222.6'}}	At present AnCora is the largest multilayer annotated ENTITYOTHER of these ENTITY freely available from http://clic.
The two corpora consist mainly of newspaper texts annotated at different levels of linguistic description : morphological (PoS and lemmas ), syntactic ( constituents and functions ), and semantic ( argument structures , thematic roles , semantic verb classes , named entities , and WordNet nominal senses).	texts	corpora	part_whole	{'e1': {'word': 'texts', 'word_index': [(7, 7)], 'id': 'L08-1222.10'}, 'e2': {'word': 'corpora', 'word_index': [(2, 2)], 'id': 'L08-1222.8'}}	The two ENTITYOTHER consist mainly of ENTITYUNRELATED ENTITY annotated at different ENTITYUNRELATED of ENTITYUNRELATED : morphological ( PoS and ENTITYUNRELATED ) , ENTITYUNRELATED ( ENTITYUNRELATED and ENTITYUNRELATED ) , and ENTITYUNRELATED ( ENTITYUNRELATED , thematic ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED , and WordNet nominal senses ) .
This paper describes the use of a computational environment (SIDGrid) that facilitates interdisciplinary instruction by providing support for students with little computational background as well as extending the scale of projects accessible to students with more advanced computational skills.	environment	instruction	usage	{'e1': {'word': 'environment', 'word_index': [(8, 8)], 'id': 'W08-0213.10'}, 'e2': {'word': 'instruction', 'word_index': [(15, 15)], 'id': 'W08-0213.11'}}	This ENTITYUNRELATED describes the use of a ENTITYUNRELATED ENTITY ( SIDGrid ) that facilitates interdisciplinary ENTITYOTHER by ENTITYUNRELATED ENTITYUNRELATED for students with little ENTITYUNRELATED ENTITYUNRELATED as well as extending the ENTITYUNRELATED of ENTITYUNRELATED accessible to students with more ENTITYUNRELATED ENTITYUNRELATED skills .
The environment facilitates the use of hands-on exercises and is being applied to interdisciplinary instruction in Discourse and Dialogue .	environment	instruction	usage	{'e1': {'word': 'environment', 'word_index': [(1, 1)], 'id': 'W08-0213.20'}, 'e2': {'word': 'instruction', 'word_index': [(16, 16)], 'id': 'W08-0213.22'}}	The ENTITY facilitates the use of hands - on exercises and is being ENTITYUNRELATED to interdisciplinary ENTITYOTHER in ENTITYUNRELATED and ENTITYUNRELATED .
We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.	system	challenge	usage	{'e1': {'word': 'system', 'word_index': [(3, 3)], 'id': 'W08-2128.5'}, 'e2': {'word': 'challenge', 'word_index': [(8, 8)], 'id': 'W08-2128.6'}}	We describe the ENTITY submitted to the closed ENTITYOTHER of the CoNLL -2008 shared ENTITYUNRELATED on joint ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED .
We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.	parsing	dependencies	usage	{'e1': {'word': 'parsing', 'word_index': [(17, 17)], 'id': 'W08-2128.8'}, 'e2': {'word': 'dependencies', 'word_index': [(22, 22)], 'id': 'W08-2128.11'}}	We describe the ENTITYUNRELATED submitted to the closed ENTITYUNRELATED of the CoNLL -2008 shared ENTITYUNRELATED on joint ENTITY of ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER .
Syntactic dependencies are processed with the Malt-Parser 0.4.	Parser	dependencies	usage	{'e1': {'word': 'Parser', 'word_index': [(8, 8)], 'id': 'W08-2128.15'}, 'e2': {'word': 'dependencies', 'word_index': [(1, 1)], 'id': 'W08-2128.13'}}	ENTITYUNRELATED ENTITYOTHER are ENTITYUNRELATED with the Malt - ENTITY 0.4.
Semantic dependencies are processed with a combination of memory-based classifiers.	classifiers	dependencies	usage	{'e1': {'word': 'classifiers', 'word_index': [(9, 9)], 'id': 'W08-2128.21'}, 'e2': {'word': 'dependencies', 'word_index': [(1, 1)], 'id': 'W08-2128.17'}}	ENTITYUNRELATED ENTITYOTHER are ENTITYUNRELATED with a ENTITYUNRELATED of ENTITYUNRELATED ENTITY .
The system achieves 78.43 labeled macro Fl for the complete problem, 86.07 labeled attachment score for syntactic dependencies, and 70.51 labeled Fl for semantic dependencies.	system	problem	usage	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'W08-2128.22'}, 'e2': {'word': 'problem', 'word_index': [(10, 10)], 'id': 'W08-2128.23'}}	The ENTITY achieves 78.43 labeled macro Fl for the complete ENTITYOTHER , 86.07 labeled ENTITYUNRELATED score for ENTITYUNRELATED ENTITYUNRELATED , and 70.51 labeled Fl for ENTITYUNRELATED ENTITYUNRELATED .
Letter-to-phoneme conversion generally requires aligned trainingdata of letters and phonemes	phonemes	data	part_whole	{'e1': {'word': 'phonemes', 'word_index': [(10, 10)], 'id': 'N07-1047.10'}, 'e2': {'word': 'data', 'word_index': [(6, 6)], 'id': 'N07-1047.9'}}	ENTITYUNRELATED ENTITYUNRELATED generally ENTITYUNRELATED aligned ENTITYUNRELATED ENTITYOTHER of letters and ENTITY
We present a novel technique of training with many-to-many alignments.	alignments	training	usage	{'e1': {'word': 'alignments', 'word_index': [(13, 13)], 'id': 'N07-1047.16'}, 'e2': {'word': 'training', 'word_index': [(6, 6)], 'id': 'N07-1047.15'}}	We present a novel ENTITYUNRELATED of ENTITYOTHER with many - to - many ENTITY .
A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists.	prediction	phonemes	usage	{'e1': {'word': 'prediction', 'word_index': [(4, 4)], 'id': 'N07-1047.18'}, 'e2': {'word': 'phonemes', 'word_index': [(10, 10)], 'id': 'N07-1047.19'}}	A letter ENTITYUNRELATED bigram ENTITY manages double letters and double ENTITYOTHER automatically as opposed to preprocessing with fixed ENTITYUNRELATED .
We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word.	classification model	sequence	usage	{'e1': {'word': 'classification model', 'word_index': [(11, 11)], 'id': 'N07-1047.24'}, 'e2': {'word': 'sequence', 'word_index': [(17, 17)], 'id': 'N07-1047.26'}}	We also ENTITYUNRELATED an HMM ENTITYUNRELATED in ENTITYUNRELATED with a local ENTITY to predict a global ENTITYUNRELATED ENTITYOTHER given a ENTITYUNRELATED .
The many-to-many alignments result in significant improvements over the traditional one-to-one approach.	alignments	approach	compare	{'e1': {'word': 'alignments', 'word_index': [(6, 6)], 'id': 'N07-1047.28'}, 'e2': {'word': 'approach', 'word_index': [(19, 19)], 'id': 'N07-1047.31'}}	The many - to - many ENTITY ENTITYUNRELATED in significant ENTITYUNRELATED over the traditional one - to - one ENTITYOTHER .
Our system achieves state-of-the-art performance on several languages anddata sets.	system	performance	result	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'N07-1047.32'}, 'e2': {'word': 'performance', 'word_index': [(10, 10)], 'id': 'N07-1047.33'}}	Our ENTITY achieves state - of - the - art ENTITYOTHER on several ENTITYUNRELATED and ENTITYUNRELATED sets .
We describe ConQuest, an open-source, reusable spoken dialog system that provides technical program information during conferences.	dialog system	information	usage	{'e1': {'word': 'dialog system', 'word_index': [(9, 9)], 'id': 'N07-2003.4'}, 'e2': {'word': 'information', 'word_index': [(14, 14)], 'id': 'N07-2003.7'}}	We describe ConQuest , an ENTITYUNRELATED , reusable spoken ENTITY that ENTITYUNRELATED technical ENTITYUNRELATED ENTITYOTHER during conferences .
The system uses a transparent, modular and open infrastructure, and aims to enable applied research in spoken language interfaces.	infrastructure	system	usage	{'e1': {'word': 'infrastructure', 'word_index': [(9, 9)], 'id': 'N07-2003.9'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'N07-2003.8'}}	The ENTITYOTHER uses a transparent , modular and open ENTITY , and aims to enable ENTITYUNRELATED ENTITYUNRELATED in spoken ENTITYUNRELATED ENTITYUNRELATED .
The system uses a transparent, modular and open infrastructure, and aims to enable applied research in spoken language interfaces.	research	interfaces	topic	{'e1': {'word': 'research', 'word_index': [(16, 16)], 'id': 'N07-2003.11'}, 'e2': {'word': 'interfaces', 'word_index': [(20, 20)], 'id': 'N07-2003.13'}}	The ENTITYUNRELATED uses a transparent , modular and open ENTITYUNRELATED , and aims to enable ENTITYUNRELATED ENTITY in spoken ENTITYUNRELATED ENTITYOTHER .
The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base.	domain	research	usage	{'e1': {'word': 'domain', 'word_index': [(2, 2)], 'id': 'N07-2003.14'}, 'e2': {'word': 'research', 'word_index': [(9, 9)], 'id': 'N07-2003.17'}}	The conference ENTITY is a good ENTITYUNRELATED for ENTITYUNRELATED ENTITYOTHER since it permits periodical redeployments and ENTITYUNRELATED with a real ENTITYUNRELATED .
In this paper, we describe the system's functionality, overall architecture, and we discuss two initial deployments.	paper	functionality	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'N07-2003.20'}, 'e2': {'word': 'functionality', 'word_index': [(9, 9)], 'id': 'N07-2003.22'}}	In this ENTITY , we describe the ENTITYUNRELATED 's ENTITYOTHER , overall ENTITYUNRELATED , and we discuss two initial ENTITYUNRELATED .
We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling.	technique	parsing	usage	{'e1': {'word': 'technique', 'word_index': [(3, 3)], 'id': 'P04-1006.3'}, 'e2': {'word': 'parsing', 'word_index': [(10, 10)], 'id': 'P04-1006.7'}}	We present a ENTITY that ENTITYUNRELATED the ENTITYUNRELATED of ENTITYUNRELATED ENTITYOTHER as used in ENTITYUNRELATED ENTITYUNRELATED .
Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the word-lattice.	parser	technique	usage	{'e1': {'word': 'parser', 'word_index': [(5, 5)], 'id': 'P04-1006.12'}, 'e2': {'word': 'technique', 'word_index': [(1, 1)], 'id': 'P04-1006.10'}}	Our ENTITYOTHER ENTITYUNRELATED a probabilistic ENTITY iteratively where on each ENTITYUNRELATED it ENTITYUNRELATED on a different subset of the ENTITYUNRELATED .
The parser's attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited.	word-lattice	parser	usage	{'e1': {'word': 'word-lattice', 'word_index': [(7, 7)], 'id': 'P04-1006.18'}, 'e2': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'P04-1006.16'}}	The ENTITYOTHER 's attention is ENTITYUNRELATED towards ENTITY subsets for which there are few or no ENTITYUNRELATED posited .
This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model.	technique	increase	result	{'e1': {'word': 'technique', 'word_index': [(4, 4)], 'id': 'P04-1006.20'}, 'e2': {'word': 'increase', 'word_index': [(10, 10)], 'id': 'P04-1006.22'}}	This attention - shifting ENTITY ENTITYUNRELATED a six - times ENTITYOTHER in ENTITYUNRELATED ( measured as the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ) while ENTITYUNRELATED equivalently when used as the first -stage of a multi-stage ENTITYUNRELATED ENTITYUNRELATED .
We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge.	approach	Word Sense Disambiguation	usage	{'e1': {'word': 'approach', 'word_index': [(5, 5)], 'id': 'P06-3010.4'}, 'e2': {'word': 'Word Sense Disambiguation', 'word_index': [(7, 7)], 'id': 'P06-3010.5'}}	We present a novel hybrid ENTITY for ENTITYOTHER ( WSD ) which makes use of a ENTITYUNRELATED ENTITYUNRELATED to represent ENTITYUNRELATED and ENTITYUNRELATED .
We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge.	formalism	instances	model-feature	{'e1': {'word': 'formalism', 'word_index': [(17, 17)], 'id': 'P06-3010.7'}, 'e2': {'word': 'instances', 'word_index': [(20, 20)], 'id': 'P06-3010.8'}}	We present a novel hybrid ENTITYUNRELATED for ENTITYUNRELATED ( WSD ) which makes use of a ENTITYUNRELATED ENTITY to represent ENTITYOTHER and ENTITYUNRELATED .
It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process, producing a rule-based WSD model.	techniques	evidence	usage	{'e1': {'word': 'techniques', 'word_index': [(6, 6)], 'id': 'P06-3010.11'}, 'e2': {'word': 'evidence', 'word_index': [(9, 9)], 'id': 'P06-3010.12'}}	It is built using Inductive ENTITYUNRELATED ENTITY to combine ENTITYOTHER coming from both ENTITYUNRELATED during the learning ENTITYUNRELATED , producing a ENTITYUNRELATED WSD ENTITYUNRELATED .
It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process, producing a rule-based WSD model.	process	model	result	{'e1': {'word': 'process', 'word_index': [(17, 17)], 'id': 'P06-3010.14'}, 'e2': {'word': 'model', 'word_index': [(23, 23)], 'id': 'P06-3010.16'}}	It is built using Inductive ENTITYUNRELATED ENTITYUNRELATED to combine ENTITYUNRELATED coming from both ENTITYUNRELATED during the learning ENTITY , producing a ENTITYUNRELATED WSD ENTITYOTHER .
We experimented with this approach to disambiguate 7 highly ambiguous verbs in English-Portuguese translation.	approach	verbs	usage	{'e1': {'word': 'approach', 'word_index': [(4, 4)], 'id': 'P06-3010.18'}, 'e2': {'word': 'verbs', 'word_index': [(10, 10)], 'id': 'P06-3010.20'}}	We ENTITYUNRELATED with this ENTITY to ENTITYUNRELATED 7 highly ambiguous ENTITYOTHER in ENTITYUNRELATED Portuguese ENTITYUNRELATED .
Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%).	approach	accuracy	result	{'e1': {'word': 'approach', 'word_index': [(4, 4)], 'id': 'P06-3010.24'}, 'e2': {'word': 'accuracy', 'word_index': [(11, 11)], 'id': 'P06-3010.25'}}	ENTITYUNRELATED showed that the ENTITY is promising , achieving an average ENTITYOTHER of 75 % , which outperforms the other ENTITYUNRELATED learning ENTITYUNRELATED investigated ( 66 % ) .
"""Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf."	approach	approach	compare	{'e1': {'word': 'approach', 'word_index': [(11, 11)], 'id': 'C82-1028.5'}, 'e2': {'word': 'approach', 'word_index': [(15, 15)], 'id': 'C82-1028.7'}}	""" Usually two ENTITYUNRELATED to ENTITYUNRELATED are distinguished : the interlingual ENTITY and the ENTITYUNRELATED ENTITYOTHER ( cf."
In the interlingual approach translation is a two-stage process: from source language to interlingua and from interlingua to target language.	process	approach	part_whole	{'e1': {'word': 'process', 'word_index': [(10, 10)], 'id': 'C82-1028.10'}, 'e2': {'word': 'approach', 'word_index': [(3, 3)], 'id': 'C82-1028.8'}}	In the interlingual ENTITYOTHER ENTITYUNRELATED is a two - stage ENTITY : from ENTITYUNRELATED to interlingua and from interlingua to ENTITYUNRELATED .
In the transfer approach there are three stages: source language analysis, transfer and target language generation.	analysis	approach	part_whole	{'e1': {'word': 'analysis', 'word_index': [(10, 10)], 'id': 'C82-1028.16'}, 'e2': {'word': 'approach', 'word_index': [(3, 3)], 'id': 'C82-1028.14'}}	In the ENTITYUNRELATED ENTITYOTHER there are three stages : ENTITYUNRELATED ENTITY , ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED .
The approach advanced in this paper is a variant of the interlingual one.	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(5, 5)], 'id': 'C82-1028.22'}, 'e2': {'word': 'approach', 'word_index': [(1, 1)], 'id': 'C82-1028.20'}}	The ENTITYOTHER ENTITYUNRELATED in this ENTITY is a ENTITYUNRELATED of the interlingual one .
The syntactic rules of these grammars must correspond with logical operations, in accordance with the compositionality principle of Montague grammar.	principle	rules	usage	{'e1': {'word': 'principle', 'word_index': [(17, 17)], 'id': 'C82-1028.30'}, 'e2': {'word': 'rules', 'word_index': [(2, 2)], 'id': 'C82-1028.28'}}	The ENTITYUNRELATED ENTITYOTHER of these grammars must correspond with logical ENTITYUNRELATED , in accordance with the compositionality ENTITY of Montague grammar .
Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation, the other grammars must contain rules corresponding with the same operation.	operation	rule	model-feature	{'e1': {'word': 'operation', 'word_index': [(24, 24)], 'id': 'C82-1028.32'}, 'e2': {'word': 'rule', 'word_index': [(18, 18)], 'id': 'C82-1028.31'}}	Moreover , the grammars must be attuned to each other as follows : if one grammar contains a ENTITYOTHER corresponding with a particular logical ENTITY , the other grammars must contain ENTITYUNRELATED corresponding with the same ENTITYUNRELATED .
Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation, the other grammars must contain rules corresponding with the same operation.	operation	rules	model-feature	{'e1': {'word': 'operation', 'word_index': [(36, 36)], 'id': 'C82-1028.34'}, 'e2': {'word': 'rules', 'word_index': [(31, 31)], 'id': 'C82-1028.33'}}	Moreover , the grammars must be attuned to each other as follows : if one grammar contains a ENTITYUNRELATED corresponding with a particular logical ENTITYUNRELATED , the other grammars must contain ENTITYOTHER corresponding with the same ENTITY .
If the grammars are attuned to each other in this way,  'logical derivation trees', representations of both the syntactical and the logical structure of sentences, can be used as intermediate expressions.	trees	structure	model-feature	{'e1': {'word': 'trees', 'word_index': [(15, 15)], 'id': 'C82-1028.37'}, 'e2': {'word': 'structure', 'word_index': [(26, 26)], 'id': 'C82-1028.39'}}	If the grammars are attuned to each other in this way , ' logical ENTITYUNRELATED ENTITY ' , ENTITYUNRELATED of both the syntactical and the logical ENTITYOTHER of ENTITYUNRELATED , can be used as intermediate ENTITYUNRELATED .
In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced.	section	concepts	topic	{'e1': {'word': 'section', 'word_index': [(1, 1)], 'id': 'C82-1028.43'}, 'e2': {'word': 'concepts', 'word_index': [(5, 5)], 'id': 'C82-1028.44'}}	In ENTITY 2 the relevant ENTITYOTHER of Montague grammar and the ENTITYUNRELATED ' logically isomorphic grammars ' are introduced .
In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals.	section	version	topic	{'e1': {'word': 'section', 'word_index': [(1, 1)], 'id': 'C82-1028.46'}, 'e2': {'word': 'version', 'word_index': [(4, 4)], 'id': 'C82-1028.47'}}	In ENTITY 3 a ENTITYOTHER of Montague grammar is described , ENTITYUNRELATED M-grammar , which is more suitable for ENTITYUNRELATED use than Montague 's original ENTITYUNRELATED .
"In section 4 the design of the Rosetta translation system, based on this approach, is outlined, followed by a brief discussion in section 5. """	section	design	topic	{'e1': {'word': 'section', 'word_index': [(1, 1)], 'id': 'C82-1028.52'}, 'e2': {'word': 'design', 'word_index': [(4, 4)], 'id': 'C82-1028.53'}}	"In ENTITY 4 the ENTITYOTHER of the Rosetta ENTITYUNRELATED , ENTITYUNRELATED on this ENTITYUNRELATED , is ENTITYUNRELATED , followed by a brief ENTITYUNRELATED in ENTITYUNRELATED 5 . """
"In section 4 the design of the Rosetta translation system, based on this approach, is outlined, followed by a brief discussion in section 5. """	approach	translation system	usage	{'e1': {'word': 'approach', 'word_index': [(13, 13)], 'id': 'C82-1028.56'}, 'e2': {'word': 'translation system', 'word_index': [(8, 8)], 'id': 'C82-1028.54'}}	"In ENTITYUNRELATED 4 the ENTITYUNRELATED of the Rosetta ENTITYOTHER , ENTITYUNRELATED on this ENTITY , is ENTITYUNRELATED , followed by a brief ENTITYUNRELATED in ENTITYUNRELATED 5 . """
This paper presents a model-theoretic semantics for directional modifiers in English.	paper	semantics	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C86-1082.3'}, 'e2': {'word': 'semantics', 'word_index': [(5, 5)], 'id': 'C86-1082.5'}}	This ENTITY presents a ENTITYUNRELATED ENTITYOTHER for directional ENTITYUNRELATED in ENTITYUNRELATED .
This paper presents a model-theoretic semantics for directional modifiers in English.	modifiers	English	part_whole	{'e1': {'word': 'modifiers', 'word_index': [(8, 8)], 'id': 'C86-1082.6'}, 'e2': {'word': 'English', 'word_index': [(10, 10)], 'id': 'C86-1082.7'}}	This ENTITYUNRELATED presents a ENTITYUNRELATED ENTITYUNRELATED for directional ENTITY in ENTITYOTHER .
The semantic theory presupposed for the analysis is that of Montague Grammar (cf.	theory	analysis	usage	{'e1': {'word': 'theory', 'word_index': [(2, 2)], 'id': 'C86-1082.9'}, 'e2': {'word': 'analysis', 'word_index': [(6, 6)], 'id': 'C86-1082.10'}}	The ENTITYUNRELATED ENTITY presupposed for the ENTITYOTHER is that of Montague Grammar ( cf.
Such a treatment has significant computational advantages over case-based treatments of directional modifiers that are advocated in the Al literature.	treatment	treatments	compare	{'e1': {'word': 'treatment', 'word_index': [(2, 2)], 'id': 'C86-1082.14'}, 'e2': {'word': 'treatments', 'word_index': [(9, 9)], 'id': 'C86-1082.18'}}	Such a ENTITY has significant ENTITYUNRELATED ENTITYUNRELATED over ENTITYUNRELATED ENTITYOTHER of directional ENTITYUNRELATED that are advocated in the Al ENTITYUNRELATED .
Our stylistic grammar is a branching stratificational model, built upon a foundation dealing with lexical, syntactic, and semantic stylistic realizations.	model	realizations	model-feature	{'e1': {'word': 'model', 'word_index': [(7, 7)], 'id': 'C88-1031.8'}, 'e2': {'word': 'realizations', 'word_index': [(22, 22)], 'id': 'C88-1031.13'}}	Our stylistic grammar is a branching stratificational ENTITY , built upon a foundation ENTITYUNRELATED with ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED stylistic ENTITYOTHER .
Its central level uses a vocabulary of constituent stylistic elements common to both English and French, while the top level correlates stylistic goals, such as clarity and concreteness, with patterns of these elements.	vocabulary	level	usage	{'e1': {'word': 'vocabulary', 'word_index': [(5, 5)], 'id': 'C88-1031.15'}, 'e2': {'word': 'level', 'word_index': [(2, 2)], 'id': 'C88-1031.14'}}	Its central ENTITYOTHER uses a ENTITY of ENTITYUNRELATED stylistic elements ENTITYUNRELATED to both ENTITYUNRELATED and French , while the top ENTITYUNRELATED correlates stylistic ENTITYUNRELATED , such as clarity and concreteness , with ENTITYUNRELATED of these elements .
Overall, we are implementing a computational schema of stylistics in French-to-English translation.	schema	translation	model-feature	{'e1': {'word': 'schema', 'word_index': [(7, 7)], 'id': 'C88-1031.24'}, 'e2': {'word': 'translation', 'word_index': [(15, 15)], 'id': 'C88-1031.26'}}	Overall , we are ENTITYUNRELATED a ENTITYUNRELATED ENTITY of stylistics in French -to - ENTITYUNRELATED ENTITYOTHER .
We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output.	analysis	machine translation systems	part_whole	{'e1': {'word': 'analysis', 'word_index': [(7, 7)], 'id': 'C88-1031.27'}, 'e2': {'word': 'machine translation systems', 'word_index': [(9, 9)], 'id': 'C88-1031.28'}}	We believe that the incorporation of stylistic ENTITY into ENTITYOTHER will significantly reduce the ENTITYUNRELATED reliance on human post-editing and ENTITYUNRELATED the ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED .
We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output.	systems'	quality	result	"{'e1': {'word': ""systems'"", 'word_index': [(25, 25)], 'id': 'C88-1031.32'}, 'e2': {'word': 'quality', 'word_index': [(22, 22)], 'id': 'C88-1031.31'}}"	We believe that the incorporation of stylistic ENTITYUNRELATED into ENTITYUNRELATED will significantly reduce the ENTITYUNRELATED reliance on human post-editing and ENTITYUNRELATED the ENTITYOTHER of the ENTITY ENTITYUNRELATED .
This paper is on dividing non-separated language sentences (whose words are not separated from each other with a space or other separater	words	sentences	part_whole	{'e1': {'word': 'words', 'word_index': [(10, 10)], 'id': 'C94-1036.7'}, 'e2': {'word': 'sentences', 'word_index': [(7, 7)], 'id': 'C94-1036.6'}}	This ENTITYUNRELATED is on dividing non-separated ENTITYUNRELATED ENTITYOTHER ( whose ENTITY are not separated from each other with a ENTITYUNRELATED or other separater
) into morphemes using statistical information, not grammatical information which is often used in NLP.	information	information	compare	{'e1': {'word': 'information', 'word_index': [(5, 5)], 'id': 'C94-1036.10'}, 'e2': {'word': 'information', 'word_index': [(9, 9)], 'id': 'C94-1036.11'}}	) into morphemes using ENTITYUNRELATED ENTITY , not grammatical ENTITYOTHER which is often used in NLP .
In this paper we describe our method and experimental result on Japanese and Chinese sentences.	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'C94-1036.12'}, 'e2': {'word': 'method', 'word_index': [(6, 6)], 'id': 'C94-1036.13'}}	In this ENTITY we describe our ENTITYOTHER and ENTITYUNRELATED ENTITYUNRELATED on ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED .
In this paper we describe our method and experimental result on Japanese and Chinese sentences.	sentences	result	result	{'e1': {'word': 'sentences', 'word_index': [(14, 14)], 'id': 'C94-1036.18'}, 'e2': {'word': 'result', 'word_index': [(9, 9)], 'id': 'C94-1036.15'}}	In this ENTITYUNRELATED we describe our ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER on ENTITYUNRELATED and ENTITYUNRELATED ENTITY .
As will be seen in the body of this paper, the result shows that this system is efficient for most of the sentences.	system	result	result	{'e1': {'word': 'system', 'word_index': [(16, 16)], 'id': 'C94-1036.21'}, 'e2': {'word': 'result', 'word_index': [(12, 12)], 'id': 'C94-1036.20'}}	As will be seen in the body of this ENTITYUNRELATED , the ENTITYOTHER shows that this ENTITY is efficient for most of the ENTITYUNRELATED .
This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases.	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C94-2106.8'}, 'e2': {'word': 'system', 'word_index': [(4, 4)], 'id': 'C94-2106.10'}}	This ENTITY ENTITYUNRELATED a ENTITYOTHER of 97 ENTITYUNRELATED ENTITYUNRELATED attributes for ENTITYUNRELATED ENTITYUNRELATED which considers both dynamic ENTITYUNRELATED and the ENTITYUNRELATED of ENTITYUNRELATED to ENTITYUNRELATED .
This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases.	characteristics	verbs	model-feature	{'e1': {'word': 'characteristics', 'word_index': [(17, 17)], 'id': 'C94-2106.15'}, 'e2': {'word': 'verbs', 'word_index': [(12, 12)], 'id': 'C94-2106.14'}}	This ENTITYUNRELATED ENTITYUNRELATED a ENTITYUNRELATED of 97 ENTITYUNRELATED ENTITYUNRELATED attributes for ENTITYUNRELATED ENTITYOTHER which considers both dynamic ENTITY and the ENTITYUNRELATED of ENTITYUNRELATED to ENTITYUNRELATED .
These attribute values are used to disambiguate the meanings of all Japanese and English pattern pairs in a Japanese to English transfer pattern dictionary consisting of 15,000 pairs of Japanese valence patterns and equivalent English syntactic structures.	patterns	dictionary	part_whole	{'e1': {'word': 'patterns', 'word_index': [(31, 31)], 'id': 'C94-2106.32'}, 'e2': {'word': 'dictionary', 'word_index': [(23, 23)], 'id': 'C94-2106.28'}}	These attribute values are used to ENTITYUNRELATED the meanings of all ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED in a ENTITYUNRELATED to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER consisting of 15,000 ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITY and equivalent ENTITYUNRELATED ENTITYUNRELATED .
This paper describes the analysis component of the language processing system PLAIN from the viewpoint of unification grammars.	component	language processing system	part_whole	{'e1': {'word': 'component', 'word_index': [(5, 5)], 'id': 'C86-1046.5'}, 'e2': {'word': 'language processing system', 'word_index': [(8, 8)], 'id': 'C86-1046.6'}}	This ENTITYUNRELATED describes the ENTITYUNRELATED ENTITY of the ENTITYOTHER PLAIN from the viewpoint of ENTITYUNRELATED grammars .
A unification-based parsing procedure is part of the formalism.	procedure	formalism	part_whole	{'e1': {'word': 'procedure', 'word_index': [(3, 3)], 'id': 'C86-1046.17'}, 'e2': {'word': 'formalism', 'word_index': [(8, 8)], 'id': 'C86-1046.19'}}	A ENTITYUNRELATED ENTITYUNRELATED ENTITY is ENTITYUNRELATED of the ENTITYOTHER .
We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interactiondata.	knowledge	dialog system	usage	{'e1': {'word': 'knowledge', 'word_index': [(9, 9)], 'id': 'D08-1100.10'}, 'e2': {'word': 'dialog system', 'word_index': [(15, 15)], 'id': 'D08-1100.13'}}	We describe an ENTITYUNRELATED for acquiring the ENTITYUNRELATED ENTITYUNRELATED ENTITY ENTITYUNRELATED to configure a ENTITYUNRELATED ENTITYOTHER that uses human - human ENTITYUNRELATED ENTITYUNRELATED .
The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs.	design	representation	model-feature	{'e1': {'word': 'design', 'word_index': [(8, 8)], 'id': 'D08-1100.18'}, 'e2': {'word': 'representation', 'word_index': [(13, 13)], 'id': 'D08-1100.21'}}	The key ENTITYUNRELATED of this ENTITYUNRELATED are the ENTITY of a ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER and a learning ENTITYUNRELATED that ENTITYUNRELATED capture of ENTITYUNRELATED ENTITYUNRELATED from ENTITYUNRELATED ENTITYUNRELATED .
The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs.	approach	information	usage	{'e1': {'word': 'approach', 'word_index': [(17, 17)], 'id': 'D08-1100.22'}, 'e2': {'word': 'information', 'word_index': [(23, 23)], 'id': 'D08-1100.25'}}	The key ENTITYUNRELATED of this ENTITYUNRELATED are the ENTITYUNRELATED of a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and a learning ENTITY that ENTITYUNRELATED capture of ENTITYUNRELATED ENTITYOTHER from ENTITYUNRELATED ENTITYUNRELATED .
Information extraction from largedata repositories is critical to Information Management solutions.	Information extraction	solutions	usage	{'e1': {'word': 'Information extraction', 'word_index': [(0, 0)], 'id': 'L08-1189.4'}, 'e2': {'word': 'solutions', 'word_index': [(10, 10)], 'id': 'L08-1189.9'}}	ENTITY from large ENTITYUNRELATED ENTITYUNRELATED is critical to ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
In addition to prerequisite corpus analysis, to determine domain-specific characteristics of text resources, developing, refining and evaluating analytics entails a complex and lengthy process, typically requiring more than just domain expertise.	characteristics	resources	model-feature	{'e1': {'word': 'characteristics', 'word_index': [(10, 10)], 'id': 'L08-1189.14'}, 'e2': {'word': 'resources', 'word_index': [(13, 13)], 'id': 'L08-1189.16'}}	In ENTITYUNRELATED to prerequisite ENTITYUNRELATED ENTITYUNRELATED , to determine ENTITYUNRELATED ENTITY of ENTITYUNRELATED ENTITYOTHER , ENTITYUNRELATED , refining and ENTITYUNRELATED analytics entails a ENTITYUNRELATED and lengthy ENTITYUNRELATED , typically ENTITYUNRELATED more than just ENTITYUNRELATED expertise .
In addition to prerequisite corpus analysis, to determine domain-specific characteristics of text resources, developing, refining and evaluating analytics entails a complex and lengthy process, typically requiring more than just domain expertise.	process	evaluating	model-feature	{'e1': {'word': 'process', 'word_index': [(26, 26)], 'id': 'L08-1189.20'}, 'e2': {'word': 'evaluating', 'word_index': [(19, 19)], 'id': 'L08-1189.18'}}	In ENTITYUNRELATED to prerequisite ENTITYUNRELATED ENTITYUNRELATED , to determine ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED , refining and ENTITYOTHER analytics entails a ENTITYUNRELATED and lengthy ENTITY , typically ENTITYUNRELATED more than just ENTITYUNRELATED expertise .
Modern architectures for text processing, while facilitating reuse and (re-)composition of analytical pipelines, place additional constraints upon the analytics development, as domain experts need not only configure individual annotator components, but situate these within a fully functional annotator pipeline.	architectures	text processing	usage	{'e1': {'word': 'architectures', 'word_index': [(1, 1)], 'id': 'L08-1189.23'}, 'e2': {'word': 'text processing', 'word_index': [(3, 3)], 'id': 'L08-1189.24'}}	Modern ENTITY for ENTITYOTHER , while facilitating reuse and ( re - ) composition of analytical ENTITYUNRELATED , place additional ENTITYUNRELATED upon the analytics ENTITYUNRELATED , as ENTITYUNRELATED ENTITYUNRELATED need not only configure ENTITYUNRELATED annotator ENTITYUNRELATED , but situate these within a fully functional annotator ENTITYUNRELATED .
Modern architectures for text processing, while facilitating reuse and (re-)composition of analytical pipelines, place additional constraints upon the analytics development, as domain experts need not only configure individual annotator components, but situate these within a fully functional annotator pipeline.	components	pipeline	part_whole	{'e1': {'word': 'components', 'word_index': [(35, 35)], 'id': 'L08-1189.31'}, 'e2': {'word': 'pipeline', 'word_index': [(45, 45)], 'id': 'L08-1189.32'}}	Modern ENTITYUNRELATED for ENTITYUNRELATED , while facilitating reuse and ( re - ) composition of analytical ENTITYUNRELATED , place additional ENTITYUNRELATED upon the analytics ENTITYUNRELATED , as ENTITYUNRELATED ENTITYUNRELATED need not only configure ENTITYUNRELATED annotator ENTITY , but situate these within a fully functional annotator ENTITYOTHER .
We present the design, and current status, of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details, pipeline composition constraints, anddata management.	status	tool	model-feature	{'e1': {'word': 'status', 'word_index': [(7, 7)], 'id': 'L08-1189.35'}, 'e2': {'word': 'tool', 'word_index': [(11, 11)], 'id': 'L08-1189.36'}}	We present the ENTITYUNRELATED , and ENTITYUNRELATED ENTITY , of a ENTITYOTHER for configuring ENTITYUNRELATED annotators , which ENTITYUNRELATED away from annotator ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED composition ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED .
Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition, to model development and testing, to large scale evaluation, to easy and rapid composition of text applications deploying these concept models.	tool	cycle	usage	{'e1': {'word': 'tool', 'word_index': [(3, 3)], 'id': 'L08-1189.45'}, 'e2': {'word': 'cycle', 'word_index': [(13, 13)], 'id': 'L08-1189.50'}}	Instead , the ENTITY embodies ENTITYUNRELATED for all stages of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER - from ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED , to ENTITYUNRELATED ENTITYUNRELATED and testing , to large ENTITYUNRELATED ENTITYUNRELATED , to easy and rapid composition of ENTITYUNRELATED ENTITYUNRELATED deploying these ENTITYUNRELATED ENTITYUNRELATED .
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task.	method	task	usage	{'e1': {'word': 'method', 'word_index': [(6, 6)], 'id': 'L08-1002.5'}, 'e2': {'word': 'task', 'word_index': [(23, 23)], 'id': 'L08-1002.14'}}	In this article we present a ENTITY for combining different ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED to ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED in a ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
Then the system is evaluated on testdata.	system	data	usage	{'e1': {'word': 'system', 'word_index': [(2, 2)], 'id': 'L08-1002.19'}, 'e2': {'word': 'data', 'word_index': [(7, 7)], 'id': 'L08-1002.22'}}	Then the ENTITY is ENTITYUNRELATED on ENTITYUNRELATED ENTITYOTHER .
The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many recognition errors.	task	spontaneous speech	usage	{'e1': {'word': 'task', 'word_index': [(1, 1)], 'id': 'L08-1002.23'}, 'e2': {'word': 'spontaneous speech', 'word_index': [(12, 12)], 'id': 'L08-1002.26'}}	The ENTITY is particularly difficult because the ENTITYUNRELATED ENTITYUNRELATED is automatically transcribed ENTITYOTHER , with many ENTITYUNRELATED ENTITYUNRELATED .
Also, the topics are real information needs, difficult to satisfy.	topics	information	model-feature	{'e1': {'word': 'topics', 'word_index': [(3, 3)], 'id': 'L08-1002.29'}, 'e2': {'word': 'information', 'word_index': [(6, 6)], 'id': 'L08-1002.30'}}	Also , the ENTITY are real ENTITYOTHER needs , difficult to satisfy .
Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included.	Information Retrieval systems	results	result	{'e1': {'word': 'Information Retrieval systems', 'word_index': [(0, 0)], 'id': 'L08-1002.31'}, 'e2': {'word': 'results', 'word_index': [(7, 7)], 'id': 'L08-1002.32'}}	ENTITY are not able to obtain good ENTITYOTHER on this ENTITYUNRELATED set , except for the ENTITYUNRELATED when ENTITYUNRELATED ENTITYUNRELATED are ENTITYUNRELATED .
Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included.	summaries	data	part_whole	{'e1': {'word': 'summaries', 'word_index': [(19, 19)], 'id': 'L08-1002.36'}, 'e2': {'word': 'data', 'word_index': [(10, 10)], 'id': 'L08-1002.33'}}	ENTITYUNRELATED are not able to obtain good ENTITYUNRELATED on this ENTITYOTHER set , except for the ENTITYUNRELATED when ENTITYUNRELATED ENTITY are ENTITYUNRELATED .
These treebanks usually consist of sentences addressing different subdomains (e.g. sports, politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use.	statistics	parsers	usage	{'e1': {'word': 'statistics', 'word_index': [(22, 22)], 'id': 'L08-1022.10'}, 'e2': {'word': 'parsers', 'word_index': [(27, 27)], 'id': 'L08-1022.13'}}	These treebanks usually consist of ENTITYUNRELATED addressing different subdomains ( e.g. ENTITYUNRELATED , politics , music ) , which implies that the ENTITY gathered by ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER are ENTITYUNRELATED of subdomains of ENTITYUNRELATED use .
In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser.	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'L08-1022.16'}, 'e2': {'word': 'method', 'word_index': [(6, 6)], 'id': 'L08-1022.17'}}	In this ENTITY we present a ENTITYOTHER that exploits raw subdomain ENTITYUNRELATED gathered from the web to introduce subdomain ENTITYUNRELATED into a given ENTITYUNRELATED .
In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser.	sensitivity	parser	model-feature	{'e1': {'word': 'sensitivity', 'word_index': [(19, 19)], 'id': 'L08-1022.19'}, 'e2': {'word': 'parser', 'word_index': [(23, 23)], 'id': 'L08-1022.20'}}	In this ENTITYUNRELATED we present a ENTITYUNRELATED that exploits raw subdomain ENTITYUNRELATED gathered from the web to introduce subdomain ENTITY into a given ENTITYOTHER .
We employ statistical techniques for creating an ensemble of domain sensitive parsers, and explore methods for amalgamating their predictions.	techniques	parsers	usage	{'e1': {'word': 'techniques', 'word_index': [(3, 3)], 'id': 'L08-1022.22'}, 'e2': {'word': 'parsers', 'word_index': [(11, 11)], 'id': 'L08-1022.24'}}	We employ ENTITYUNRELATED ENTITY for creating an ensemble of ENTITYUNRELATED sensitive ENTITYOTHER , and explore ENTITYUNRELATED for amalgamating their ENTITYUNRELATED .
We employ statistical techniques for creating an ensemble of domain sensitive parsers, and explore methods for amalgamating their predictions.	methods	predictions	usage	{'e1': {'word': 'methods', 'word_index': [(15, 15)], 'id': 'L08-1022.25'}, 'e2': {'word': 'predictions', 'word_index': [(19, 19)], 'id': 'L08-1022.26'}}	We employ ENTITYUNRELATED ENTITYUNRELATED for creating an ensemble of ENTITYUNRELATED sensitive ENTITYUNRELATED , and explore ENTITY for amalgamating their ENTITYOTHER .
Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.	corpora	sensitivity	usage	{'e1': {'word': 'corpora', 'word_index': [(10, 10)], 'id': 'L08-1022.30'}, 'e2': {'word': 'sensitivity', 'word_index': [(6, 6)], 'id': 'L08-1022.29'}}	Our ENTITYUNRELATED show that introducing ENTITYUNRELATED ENTITYOTHER by exploiting raw ENTITY can ENTITYUNRELATED over a tough , state - of - the - art baseline .
We investigate the controversial issue about the upper bound of interjudge agreement in the use of a low-level grammatical representation.	agreement	representation	model-feature	{'e1': {'word': 'agreement', 'word_index': [(11, 11)], 'id': 'E99-1027.7'}, 'e2': {'word': 'representation', 'word_index': [(19, 19)], 'id': 'E99-1027.9'}}	We investigate the controversial ENTITYUNRELATED about the upper bound of interjudge ENTITY in the use of a ENTITYUNRELATED grammatical ENTITYOTHER .
Pessimistic views suggest that several percent of words in running text are undecidable in terms of part-of-speech categories.	words	text	part_whole	{'e1': {'word': 'words', 'word_index': [(7, 7)], 'id': 'E99-1027.10'}, 'e2': {'word': 'text', 'word_index': [(10, 10)], 'id': 'E99-1027.11'}}	Pessimistic views suggest that several percent of ENTITY in running ENTITYOTHER are undecidable in ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED .
Our experiments with 55kWdata give reason for optimism: linguists with only 30 hours' training apply the EngCG-2 morphological tags with almost 100% interjudge agreement.	training	linguists	model-feature	{'e1': {'word': 'training', 'word_index': [(17, 17)], 'id': 'E99-1027.19'}, 'e2': {'word': 'linguists', 'word_index': [(11, 11)], 'id': 'E99-1027.18'}}	Our ENTITYUNRELATED with 55 kW ENTITYUNRELATED give ENTITYUNRELATED for optimism : ENTITYOTHER with only 30 hours ' ENTITY ENTITYUNRELATED the EngCG - 2 morphological ENTITYUNRELATED with almost 100 % interjudge ENTITYUNRELATED .
Our experiments with 55kWdata give reason for optimism: linguists with only 30 hours' training apply the EngCG-2 morphological tags with almost 100% interjudge agreement.	agreement	tags	model-feature	{'e1': {'word': 'agreement', 'word_index': [(30, 30)], 'id': 'E99-1027.22'}, 'e2': {'word': 'tags', 'word_index': [(24, 24)], 'id': 'E99-1027.21'}}	Our ENTITYUNRELATED with 55 kW ENTITYUNRELATED give ENTITYUNRELATED for optimism : ENTITYUNRELATED with only 30 hours ' ENTITYUNRELATED ENTITYUNRELATED the EngCG - 2 morphological ENTITYOTHER with almost 100 % interjudge ENTITY .
Although there is an extensive body of research concerned with anaphora resolution (e.g. ( Fox, 1987 ; Grosz et al., 1995 )), event anaphora has been widely neglected.	research	anaphora resolution	topic	{'e1': {'word': 'research', 'word_index': [(7, 7)], 'id': 'E99-1049.2'}, 'e2': {'word': 'anaphora resolution', 'word_index': [(10, 10)], 'id': 'E99-1049.4'}}	Although there is an extensive body of ENTITY ENTITYUNRELATED with ENTITYOTHER ( e.g. ( Fox , 1987 ; Grosz et al. , 1995 ) ) , ENTITYUNRELATED anaphora has been widely neglected .
This paper describes the results of an empirical study regarding event reference.	paper	results	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'E99-1049.6'}, 'e2': {'word': 'results', 'word_index': [(4, 4)], 'id': 'E99-1049.7'}}	This ENTITY describes the ENTITYOTHER of an empirical ENTITYUNRELATED regarding ENTITYUNRELATED ENTITYUNRELATED .
This paper describes the results of an empirical study regarding event reference.	study	reference	topic	{'e1': {'word': 'study', 'word_index': [(8, 8)], 'id': 'E99-1049.8'}, 'e2': {'word': 'reference', 'word_index': [(11, 11)], 'id': 'E99-1049.10'}}	This ENTITYUNRELATED describes the ENTITYUNRELATED of an empirical ENTITY regarding ENTITYUNRELATED ENTITYOTHER .
The experiment investigated event anaphora in narrative discourse via a sentence completion task.	task	experiment	usage	{'e1': {'word': 'task', 'word_index': [(12, 12)], 'id': 'E99-1049.16'}, 'e2': {'word': 'experiment', 'word_index': [(1, 1)], 'id': 'E99-1049.11'}}	The ENTITYOTHER investigated ENTITYUNRELATED anaphora in narrative ENTITYUNRELATED via a ENTITYUNRELATED ENTITYUNRELATED ENTITY .
From a syntactic treebank of spoken Dutch we extract instances of the attachment of prepositional phrases to either a governing verb or noun.	attachment	prepositional phrases	model-feature	{'e1': {'word': 'attachment', 'word_index': [(12, 12)], 'id': 'E03-1051.8'}, 'e2': {'word': 'prepositional phrases', 'word_index': [(14, 14)], 'id': 'E03-1051.9'}}	From a ENTITYUNRELATED treebank of spoken Dutch we ENTITYUNRELATED ENTITYUNRELATED of the ENTITY of ENTITYOTHER to either a governing ENTITYUNRELATED or ENTITYUNRELATED .
Using cross-validated parameter and feature selection, we train two learning algorithms, iBl and RIPPER, on making this distinction, based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts.	feature selection	algorithms	usage	{'e1': {'word': 'feature selection', 'word_index': [(4, 4)], 'id': 'E03-1051.14'}, 'e2': {'word': 'algorithms', 'word_index': [(10, 10)], 'id': 'E03-1051.16'}}	Using ENTITYUNRELATED ENTITYUNRELATED and ENTITY , we ENTITYUNRELATED two learning ENTITYOTHER , i Bl and RIPPER , on making this ENTITYUNRELATED , ENTITYUNRELATED on unigram and bigram ENTITYUNRELATED and a cooccurrence ENTITYUNRELATED derived from WWW counts .
Using cross-validated parameter and feature selection, we train two learning algorithms, iBl and RIPPER, on making this distinction, based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts.	lexical features	distinction	usage	{'e1': {'word': 'lexical features', 'word_index': [(27, 27)], 'id': 'E03-1051.19'}, 'e2': {'word': 'distinction', 'word_index': [(20, 20)], 'id': 'E03-1051.17'}}	Using ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED , we ENTITYUNRELATED two learning ENTITYUNRELATED , i Bl and RIPPER , on making this ENTITYOTHER , ENTITYUNRELATED on unigram and bigram ENTITY and a cooccurrence ENTITYUNRELATED derived from WWW counts .
When used as a filter for prosodic phrasing, using attachment decisions from IBl yields the best improvement on precision (by six points to 71) on phrase boundary placement.	decisions	improvement	result	{'e1': {'word': 'decisions', 'word_index': [(11, 11)], 'id': 'E03-1051.32'}, 'e2': {'word': 'improvement', 'word_index': [(17, 17)], 'id': 'E03-1051.34'}}	When used as a filter for prosodic phrasing , using ENTITYUNRELATED ENTITY from IBl ENTITYUNRELATED the best ENTITYOTHER on ENTITYUNRELATED ( by six points to 71 ) on ENTITYUNRELATED ENTITYUNRELATED placement .
Most Natural Language Processing systems use a sequential architecture embodying classical linguistic layers.	layers	Natural Language Processing systems	part_whole	{'e1': {'word': 'layers', 'word_index': [(9, 9)], 'id': 'C96-2210.9'}, 'e2': {'word': 'Natural Language Processing systems', 'word_index': [(1, 1)], 'id': 'C96-2210.7'}}	Most ENTITYOTHER use a sequential ENTITYUNRELATED embodying classical linguistic ENTITY .
When one works with a general language; and not a sublanguage, there are different cases of ambiguities at different classical levels; and more particularly when one works on complex language phenomena analysis (coordination, ellipsis, negation...) it is difficult to take into account all the different types of these constructions with a general grammar.	ambiguities	language	part_whole	{'e1': {'word': 'ambiguities', 'word_index': [(18, 18)], 'id': 'C96-2210.12'}, 'e2': {'word': 'language', 'word_index': [(6, 6)], 'id': 'C96-2210.10'}}	When one works with a general ENTITYOTHER ; and not a sublanguage , there are different ENTITYUNRELATED of ENTITY at different classical ENTITYUNRELATED ; and more particularly when one works on ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ( ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED ... ) it is difficult to take into account all the different ENTITYUNRELATED of these ENTITYUNRELATED with a general grammar .
Indeed, the inconvenience of this approach is the possible risk of a combinatory explosion.	explosion	approach	model-feature	{'e1': {'word': 'explosion', 'word_index': [(14, 14)], 'id': 'C96-2210.24'}, 'e2': {'word': 'approach', 'word_index': [(6, 6)], 'id': 'C96-2210.23'}}	Indeed , the inconvenience of this ENTITYOTHER is the possible risk of a combinatory ENTITY .
So, we have defined the TALISMAN architecture that includes linguistic agents that correspond either to classical levels in linguistics (morphology, syntax, semantic) or to complex language phenomena analysis.	agents	architecture	part_whole	{'e1': {'word': 'agents', 'word_index': [(11, 11)], 'id': 'C96-2210.27'}, 'e2': {'word': 'architecture', 'word_index': [(7, 7)], 'id': 'C96-2210.25'}}	So , we have defined the TALISMAN ENTITYOTHER that ENTITYUNRELATED linguistic ENTITY that correspond either to classical ENTITYUNRELATED in ENTITYUNRELATED ( ENTITYUNRELATED , ENTITYUNRELATED , ENTITYUNRELATED ) or to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
In this paper, we propose a method of generating bilingual keyword clusters or thesauri from parallel or comparable bilingual corpora.	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'C00-1058.4'}, 'e2': {'word': 'method', 'word_index': [(7, 7)], 'id': 'C00-1058.6'}}	In this ENTITY , we ENTITYUNRELATED a ENTITYOTHER of ENTITYUNRELATED bilingual ENTITYUNRELATED ENTITYUNRELATED or ENTITYUNRELATED from parallel or comparable bilingual ENTITYUNRELATED .
The method combines morphological and lexical processing, bilingual word alignment, and graph-theoretic cluster generation.	processing	method	part_whole	{'e1': {'word': 'processing', 'word_index': [(6, 6)], 'id': 'C00-1058.14'}, 'e2': {'word': 'method', 'word_index': [(1, 1)], 'id': 'C00-1058.12'}}	The ENTITYOTHER combines morphological and ENTITYUNRELATED ENTITY , bilingual ENTITYUNRELATED , and graph-theoretic ENTITYUNRELATED ENTITYUNRELATED .
This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to-English cross-language information retrieval.	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C02-1086.7'}, 'e2': {'word': 'method', 'word_index': [(4, 4)], 'id': 'C02-1086.8'}}	This ENTITY presents a ENTITYOTHER to implicitly resolve ENTITYUNRELATED using dynamic incremental ENTITYUNRELATED in Korean -to - ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to-English cross-language information retrieval.	clustering	ambiguities	usage	{'e1': {'word': 'clustering', 'word_index': [(12, 12)], 'id': 'C02-1086.10'}, 'e2': {'word': 'ambiguities', 'word_index': [(8, 8)], 'id': 'C02-1086.9'}}	This ENTITYUNRELATED presents a ENTITYUNRELATED to implicitly resolve ENTITYOTHER using dynamic incremental ENTITY in Korean -to - ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
In the framework we propose, a query in Korean is first translated into English by looking up Korean-English dictionary, then documents are retrieved based on the vector space retrieval for the translated query terms.	dictionary	query	usage	{'e1': {'word': 'dictionary', 'word_index': [(21, 21)], 'id': 'C02-1086.20'}, 'e2': {'word': 'query', 'word_index': [(7, 7)], 'id': 'C02-1086.16'}}	In the ENTITYUNRELATED we ENTITYUNRELATED , a ENTITYOTHER in Korean is first ENTITYUNRELATED into ENTITYUNRELATED by looking up Korean - ENTITYUNRELATED ENTITY , then ENTITYUNRELATED are retrieved ENTITYUNRELATED on the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED for the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
In the framework we propose, a query in Korean is first translated into English by looking up Korean-English dictionary, then documents are retrieved based on the vector space retrieval for the translated query terms.	retrieval	documents	usage	{'e1': {'word': 'retrieval', 'word_index': [(32, 32)], 'id': 'C02-1086.25'}, 'e2': {'word': 'documents', 'word_index': [(24, 24)], 'id': 'C02-1086.21'}}	In the ENTITYUNRELATED we ENTITYUNRELATED , a ENTITYUNRELATED in Korean is first ENTITYUNRELATED into ENTITYUNRELATED by looking up Korean - ENTITYUNRELATED ENTITYUNRELATED , then ENTITYOTHER are retrieved ENTITYUNRELATED on the ENTITYUNRELATED ENTITYUNRELATED ENTITY for the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
For the top-ranked retrieved documents, query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters.	clusters	documents	model-feature	{'e1': {'word': 'clusters', 'word_index': [(9, 9)], 'id': 'C02-1086.32'}, 'e2': {'word': 'documents', 'word_index': [(5, 5)], 'id': 'C02-1086.29'}}	For the top- ranked retrieved ENTITYOTHER , ENTITYUNRELATED ENTITYUNRELATED ENTITY are incrementally created and the ENTITYUNRELATED of each retrieved ENTITYUNRELATED is re-calculated by using ENTITYUNRELATED .
For the top-ranked retrieved documents, query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters.	clusters	weight	usage	{'e1': {'word': 'clusters', 'word_index': [(24, 24)], 'id': 'C02-1086.35'}, 'e2': {'word': 'weight', 'word_index': [(15, 15)], 'id': 'C02-1086.33'}}	For the top- ranked retrieved ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED are incrementally created and the ENTITYOTHER of each retrieved ENTITYUNRELATED is re-calculated by using ENTITY .
In experiment on TREC-6 CLIR test collection, our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries.	method	performance improvement	result	{'e1': {'word': 'method', 'word_index': [(9, 9)], 'id': 'C02-1086.39'}, 'e2': {'word': 'performance improvement', 'word_index': [(13, 13)], 'id': 'C02-1086.40'}}	In ENTITYUNRELATED on TREC-6 CLIR ENTITYUNRELATED ENTITYUNRELATED , our ENTITY achieved 28.29 % ENTITYOTHER for ENTITYUNRELATED ENTITYUNRELATED without ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED .
When we combine our method with query ambiguity resolution, our method even outperforms the monolingual retrieval.	method	retrieval	compare	{'e1': {'word': 'method', 'word_index': [(11, 11)], 'id': 'C02-1086.52'}, 'e2': {'word': 'retrieval', 'word_index': [(16, 16)], 'id': 'C02-1086.53'}}	When we combine our ENTITYUNRELATED with ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , our ENTITY even outperforms the monolingual ENTITYOTHER .
In this paper, we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility.	framework	problem	usage	{'e1': {'word': 'framework', 'word_index': [(9, 9)], 'id': 'C04-1091.10'}, 'e2': {'word': 'problem', 'word_index': [(14, 14)], 'id': 'C04-1091.12'}}	In this ENTITYUNRELATED , we ENTITYUNRELATED a new algorithmic ENTITY for ENTITYUNRELATED the decoding ENTITYOTHER and demonstrate its ENTITYUNRELATED .
The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems.	maximization	problem	model-feature	{'e1': {'word': 'maximization', 'word_index': [(18, 18)], 'id': 'C04-1091.20'}, 'e2': {'word': 'problem', 'word_index': [(12, 12)], 'id': 'C04-1091.19'}}	The key idea behind the ENTITYUNRELATED is the ENTITYUNRELATED of the decoding ENTITYOTHER as one that involves alternating ENTITY of two relatively simpler subproblems .
A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations.	techniques	algorithms	result	{'e1': {'word': 'techniques', 'word_index': [(13, 13)], 'id': 'C04-1091.27'}, 'e2': {'word': 'algorithms', 'word_index': [(6, 6)], 'id': 'C04-1091.25'}}	A family of provably fast decoding ENTITYOTHER can be derived from the ENTITYUNRELATED ENTITY underlying the ENTITYUNRELATED and we present a few illustrations .
Our first algorithm is a prov-ably linear time search algorithm.	search algorithm	algorithm	model-feature	{'e1': {'word': 'search algorithm', 'word_index': [(8, 8)], 'id': 'C04-1091.31'}, 'e2': {'word': 'algorithm', 'word_index': [(2, 2)], 'id': 'C04-1091.29'}}	Our first ENTITYOTHER is a prov-ably linear ENTITYUNRELATED ENTITY .
We use this algorithm as a subroutine in the other algorithms.	algorithm	algorithms	usage	{'e1': {'word': 'algorithm', 'word_index': [(3, 3)], 'id': 'C04-1091.32'}, 'e2': {'word': 'algorithms', 'word_index': [(10, 10)], 'id': 'C04-1091.33'}}	We use this ENTITY as a subroutine in the other ENTITYOTHER .
We believe that decoding algorithms derived from our framework can be of practical significance.	algorithms	significance	result	{'e1': {'word': 'algorithms', 'word_index': [(4, 4)], 'id': 'C04-1091.34'}, 'e2': {'word': 'significance', 'word_index': [(13, 13)], 'id': 'C04-1091.36'}}	We believe that decoding ENTITY derived from our ENTITYUNRELATED can be of practical ENTITYOTHER .
This paper presents work on the task of constructing an example base from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT).	schema	base	usage	{'e1': {'word': 'schema', 'word_index': [(21, 21)], 'id': 'C04-1155.13'}, 'e2': {'word': 'base', 'word_index': [(11, 11)], 'id': 'C04-1155.10'}}	This ENTITYUNRELATED presents work on the ENTITYUNRELATED of ENTITYUNRELATED an ENTITYUNRELATED ENTITYOTHER from a given bilingual ENTITYUNRELATED ENTITYUNRELATED on the annotation ENTITY of ENTITYUNRELATED Corresponding ENTITYUNRELATED ( TCT ) .
It represents the syntactic structure of source language sentence, and more importantly is the facility to specify the correspondences between string (both the source and target sentences) and the representation tree.	syntactic structure	sentence	model-feature	{'e1': {'word': 'syntactic structure', 'word_index': [(3, 3)], 'id': 'C04-1155.20'}, 'e2': {'word': 'sentence', 'word_index': [(6, 6)], 'id': 'C04-1155.22'}}	It represents the ENTITY of ENTITYUNRELATED ENTITYOTHER , and more importantly is the facility to specify the ENTITYUNRELATED between ENTITYUNRELATED ( both the ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED ) and the ENTITYUNRELATED ENTITYUNRELATED .
It represents the syntactic structure of source language sentence, and more importantly is the facility to specify the correspondences between string (both the source and target sentences) and the representation tree.	tree	string	model-feature	{'e1': {'word': 'tree', 'word_index': [(31, 31)], 'id': 'C04-1155.29'}, 'e2': {'word': 'string', 'word_index': [(19, 19)], 'id': 'C04-1155.24'}}	It represents the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED , and more importantly is the facility to specify the ENTITYUNRELATED between ENTITYOTHER ( both the ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED ) and the ENTITYUNRELATED ENTITY .
Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages.	node	representation	part_whole	{'e1': {'word': 'node', 'word_index': [(10, 10)], 'id': 'C04-1155.32'}, 'e2': {'word': 'representation', 'word_index': [(14, 14)], 'id': 'C04-1155.33'}}	Furthermore , ENTITYUNRELATED ENTITYUNRELATED clues are also encapsulated at each ENTITY in the TCT ENTITYOTHER to capture the differentiation of grammatical ENTITYUNRELATED between the ENTITYUNRELATED and ENTITYUNRELATED .
Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages.	structure	target languages	model-feature	{'e1': {'word': 'structure', 'word_index': [(21, 21)], 'id': 'C04-1155.34'}, 'e2': {'word': 'target languages', 'word_index': [(26, 26)], 'id': 'C04-1155.36'}}	Furthermore , ENTITYUNRELATED ENTITYUNRELATED clues are also encapsulated at each ENTITYUNRELATED in the TCT ENTITYUNRELATED to capture the differentiation of grammatical ENTITY between the ENTITYUNRELATED and ENTITYOTHER .
With this annotation schema, translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system.	examples	database	part_whole	{'e1': {'word': 'examples', 'word_index': [(6, 6)], 'id': 'C04-1155.39'}, 'e2': {'word': 'database', 'word_index': [(16, 16)], 'id': 'C04-1155.41'}}	With this annotation ENTITYUNRELATED , ENTITYUNRELATED ENTITY are effectively represented and organized in the bilingual ENTITYUNRELATED ENTITYOTHER that we need for the Portuguese to ENTITYUNRELATED ENTITYUNRELATED .
This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations.	accuracy	parsing	model-feature	{'e1': {'word': 'accuracy', 'word_index': [(4, 4)], 'id': 'C04-1204.7'}, 'e2': {'word': 'parsing', 'word_index': [(7, 7)], 'id': 'C04-1204.8'}}	This ENTITYUNRELATED ENTITYUNRELATED the ENTITY of HPSG ENTITYOTHER in ENTITYUNRELATED of the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED .
We could directly compare the output of HPSG parsing with Prop-Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation.	output	Bank	compare	{'e1': {'word': 'output', 'word_index': [(5, 5)], 'id': 'C04-1204.13'}, 'e2': {'word': 'Bank', 'word_index': [(11, 11)], 'id': 'C04-1204.16'}}	We could directly compare the ENTITY of HPSG ENTITYUNRELATED with ENTITYUNRELATED ENTITYOTHER annotations , by assuming a unique ENTITYUNRELATED from HPSG ENTITYUNRELATED into PropBank annotation .
We could directly compare the output of HPSG parsing with Prop-Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation.	mapping	semantic representation	model-feature	{'e1': {'word': 'mapping', 'word_index': [(18, 18)], 'id': 'C04-1204.17'}, 'e2': {'word': 'semantic representation', 'word_index': [(21, 21)], 'id': 'C04-1204.18'}}	We could directly compare the ENTITYUNRELATED of HPSG ENTITYUNRELATED with ENTITYUNRELATED ENTITYUNRELATED annotations , by assuming a unique ENTITY from HPSG ENTITYOTHER into PropBank annotation .
Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations.	parser	studies	compare	{'e1': {'word': 'parser', 'word_index': [(16, 16)], 'id': 'C04-1204.22'}, 'e2': {'word': 'studies', 'word_index': [(23, 23)], 'id': 'C04-1204.24'}}	Even though PropBank was not used for the ENTITYUNRELATED of a ENTITYUNRELATED ENTITYUNRELATED , an HPSG ENTITY achieved the ENTITYUNRELATED competitive with existing ENTITYOTHER on the ENTITYUNRELATED of identifying PropBank annotations .
"This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the """"best"""" parse of a sentence."	paper	algorithm	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'E91-1004.2'}, 'e2': {'word': 'algorithm', 'word_index': [(6, 6)], 'id': 'E91-1004.5'}}	"This ENTITY describes a ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER for unrestricted ENTITYUNRELATED which uses a ENTITYUNRELATED scoring ENTITYUNRELATED to select the "" "" best "" "" ENTITYUNRELATED of a ENTITYUNRELATED ."
"This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the """"best"""" parse of a sentence."	parse	sentence	model-feature	{'e1': {'word': 'parse', 'word_index': [(24, 24)], 'id': 'E91-1004.9'}, 'e2': {'word': 'sentence', 'word_index': [(27, 27)], 'id': 'E91-1004.10'}}	"This ENTITYUNRELATED describes a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED for unrestricted ENTITYUNRELATED which uses a ENTITYUNRELATED scoring ENTITYUNRELATED to select the "" "" best "" "" ENTITY of a ENTITYOTHER ."
The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context, of the sentence predicts that interpretation.	prediction	parser	usage	{'e1': {'word': 'prediction', 'word_index': [(17, 17)], 'id': 'E91-1004.15'}, 'e2': {'word': 'parser', 'word_index': [(12, 12)], 'id': 'E91-1004.13'}}	The ENTITYUNRELATED , Pearl , is a ENTITYUNRELATED bottom - up chart ENTITYOTHER with ENTITYUNRELATED top- down ENTITY which pursues the highest - scoring ENTITYUNRELATED in the chart , where the score of a ENTITYUNRELATED represents the ENTITYUNRELATED to which the ENTITYUNRELATED , of the ENTITYUNRELATED predicts that ENTITYUNRELATED .
The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context, of the sentence predicts that interpretation.	interpretation	extent	model-feature	{'e1': {'word': 'interpretation', 'word_index': [(48, 48)], 'id': 'E91-1004.21'}, 'e2': {'word': 'extent', 'word_index': [(37, 37)], 'id': 'E91-1004.18'}}	The ENTITYUNRELATED , Pearl , is a ENTITYUNRELATED bottom - up chart ENTITYUNRELATED with ENTITYUNRELATED top- down ENTITYUNRELATED which pursues the highest - scoring ENTITYUNRELATED in the chart , where the score of a ENTITYUNRELATED represents the ENTITYOTHER to which the ENTITYUNRELATED , of the ENTITYUNRELATED predicts that ENTITY .
This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context, to predict, likelihood.	parser	parsers	compare	{'e1': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'E91-1004.22'}, 'e2': {'word': 'parsers', 'word_index': [(8, 8)], 'id': 'E91-1004.23'}}	This ENTITY differs from previous attempts at stochastic ENTITYOTHER in that it uses a richer ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED on ENTITYUNRELATED , to predict , likelihood .
This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context, to predict, likelihood.	context	conditional probabilities	usage	{'e1': {'word': 'context', 'word_index': [(20, 20)], 'id': 'E91-1004.27'}, 'e2': {'word': 'conditional probabilities', 'word_index': [(17, 17)], 'id': 'E91-1004.25'}}	This ENTITYUNRELATED differs from previous attempts at stochastic ENTITYUNRELATED in that it uses a richer ENTITYUNRELATED of ENTITYOTHER ENTITYUNRELATED on ENTITY , to predict , likelihood .
'Pearl also provides a framework for incorporating the results of previous work in pait-ol'-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture.	results	tool	usage	{'e1': {'word': 'results', 'word_index': [(9, 9)], 'id': 'E91-1004.30'}, 'e2': {'word': 'tool', 'word_index': [(32, 32)], 'id': 'E91-1004.37'}}	' Pearl also ENTITYUNRELATED a ENTITYUNRELATED for incorporating the ENTITY of previous work in pait - ol '- ENTITYUNRELATED ENTITYUNRELATED , unknown ENTITYUNRELATED , and other ENTITYUNRELATED of ENTITYUNRELATED into one ENTITYUNRELATED ENTITYOTHER , interleaving these ENTITYUNRELATED instead of using the traditional ENTITYUNRELATED ENTITYUNRELATED .
"In preliminary tests, 'Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar."""	categories	unknown words	model-feature	{'e1': {'word': 'categories', 'word_index': [(21, 21)], 'id': 'E91-1004.46'}, 'e2': {'word': 'unknown words', 'word_index': [(23, 23)], 'id': 'E91-1004.47'}}	"In preliminary ENTITYUNRELATED , ' Pearl has been successful at resolving ENTITYUNRELATED and ENTITYUNRELATED ( in ENTITYUNRELATED ) ENTITYUNRELATED , determining ENTITY for ENTITYOTHER , and selecting correct ENTITYUNRELATED first using a very loosely fitting covering grammar . """
For analysis of the features of nursing activities, we built nursing corpora from actual nursing conversation sets collected in hospitals that involve various information about nursing activities.	analysis	features	topic	{'e1': {'word': 'analysis', 'word_index': [(1, 1)], 'id': 'L08-1227.7'}, 'e2': {'word': 'features', 'word_index': [(4, 4)], 'id': 'L08-1227.8'}}	For ENTITY of the ENTITYOTHER of nursing activities , we built nursing ENTITYUNRELATED from actual nursing ENTITYUNRELATED sets collected in hospitals that involve various ENTITYUNRELATED about nursing activities .
For analysis of the features of nursing activities, we built nursing corpora from actual nursing conversation sets collected in hospitals that involve various information about nursing activities.	conversation	corpora	part_whole	{'e1': {'word': 'conversation', 'word_index': [(16, 16)], 'id': 'L08-1227.10'}, 'e2': {'word': 'corpora', 'word_index': [(12, 12)], 'id': 'L08-1227.9'}}	For ENTITYUNRELATED of the ENTITYUNRELATED of nursing activities , we built nursing ENTITYOTHER from actual nursing ENTITY sets collected in hospitals that involve various ENTITYUNRELATED about nursing activities .
Ex-nurses manually assigned nursing activity information to the nursing conversations in the corpora .	information	conversations	model-feature	{'e1': {'word': 'information', 'word_index': [(5, 5)], 'id': 'L08-1227.12'}, 'e2': {'word': 'conversations', 'word_index': [(9, 9)], 'id': 'L08-1227.13'}}	Ex-nurses manually assigned nursing activity ENTITY to the nursing ENTITYOTHER in the ENTITYUNRELATED .
In this paper , we adopted a maximum entropy approach for learning.	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'L08-1227.22'}, 'e2': {'word': 'approach', 'word_index': [(8, 8)], 'id': 'L08-1227.24'}}	In this ENTITY , we adopted a ENTITYUNRELATED ENTITYOTHER for learning .
"In this paper , we investigate the applicability of distributional and WordNet-based <entity id=""D08-1048.10"">models</entity> on the <entity id=""D08-1048.11"">task</entity> of <entity id=""D08-1048.12"">lexical</entity> <entity id=""D08-1048.13"">unit</entity> <entity id=""D08-1048.14"">induction</entity> ,"	paper	applicability	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'D08-1048.7'}, 'e2': {'word': 'applicability', 'word_index': [(7, 7)], 'id': 'D08-1048.8'}}	"In this ENTITY , we investigate the ENTITYOTHER of distributional and WordNet -based < entity id = "" D08-1048.10 "" > models < / entity > on the < entity id = "" D08-1048.11 "" > task < / entity > of < entity id = "" D08-1048.12 "" > lexical < / entity > < entity id = "" D08-1048.13 "" >unit < /entity > < entity id = "" D08-1048.14 "" > induction < / entity > ,"
Classification of Multiple- Sentence Questions	Classification	Questions	model-feature	{'e1': {'word': 'Classification', 'word_index': [(0, 0)], 'id': 'I05-1038.1'}, 'e2': {'word': 'Questions', 'word_index': [(5, 5)], 'id': 'I05-1038.3'}}	ENTITY of Multiple - ENTITYUNRELATED ENTITYOTHER
Using the Structure of a Conceptual Network in Computing Semantic Relatedness	Structure	Computing	usage	{'e1': {'word': 'Structure', 'word_index': [(2, 2)], 'id': 'I05-1067.1'}, 'e2': {'word': 'Computing', 'word_index': [(8, 8)], 'id': 'I05-1067.3'}}	Using the ENTITY of a Conceptual ENTITYUNRELATED in ENTITYOTHER ENTITYUNRELATED ENTITYUNRELATED
In this paper we propose a methodology for investigating the relationship between architectures of natural language generation (NLG)	paper	methodology	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'E99-1033.2'}, 'e2': {'word': 'methodology', 'word_index': [(6, 6)], 'id': 'E99-1033.4'}}	In this ENTITY we ENTITYUNRELATED a ENTITYOTHER for investigating the ENTITYUNRELATED between ENTITYUNRELATED of ENTITYUNRELATED ( NLG )
The main goal of this paper is to propose automatic schemes for the translation paired comparison method .	paper	schemes	topic	{'e1': {'word': 'paper', 'word_index': [(5, 5)], 'id': 'E03-1010.8'}, 'e2': {'word': 'schemes', 'word_index': [(10, 10)], 'id': 'E03-1010.11'}}	The ENTITYUNRELATED ENTITYUNRELATED of this ENTITY is to ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER for the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
This method was proposed to precisely evaluate a speech translation system 's capability .	method	speech translation	usage	{'e1': {'word': 'method', 'word_index': [(1, 1)], 'id': 'E03-1010.16'}, 'e2': {'word': 'speech translation', 'word_index': [(8, 8)], 'id': 'E03-1010.19'}}	This ENTITY was ENTITYUNRELATED to precisely ENTITYUNRELATED a ENTITYOTHER ENTITYUNRELATED 's ENTITYUNRELATED .
This research note reports on the work in progress which regards automatic transformation of phrase-structure syntactic trees of Arabic into dependency-driven analytical ones.	research	transformation	topic	{'e1': {'word': 'research', 'word_index': [(1, 1)], 'id': 'E03-1014.4'}, 'e2': {'word': 'transformation', 'word_index': [(12, 12)], 'id': 'E03-1014.9'}}	This ENTITY ENTITYUNRELATED ENTITYUNRELATED on the work in ENTITYUNRELATED which regards ENTITYUNRELATED ENTITYOTHER of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED of Arabic into ENTITYUNRELATED analytical ones .
Guidelines for these descriptions have been developed at the Linguistic Data Consortium, University of Pennsylvania, and at the Faculty of Mathematics and Physics and the Faculty of Arts, Charles University in Prague, respectively.	Guidelines	descriptions	usage	{'e1': {'word': 'Guidelines', 'word_index': [(0, 0)], 'id': 'E03-1014.14'}, 'e2': {'word': 'descriptions', 'word_index': [(3, 3)], 'id': 'E03-1014.15'}}	ENTITY for these ENTITYOTHER have been ENTITYUNRELATED at the Linguistic ENTITYUNRELATED Consortium , ENTITYUNRELATED of Pennsylvania , and at the ENTITYUNRELATED of Mathematics and Physics and the ENTITYUNRELATED of Arts , Charles ENTITYUNRELATED in Prague , respectively .
"The transformation consists of (i) a recursive function translating the topology of a phrase tree into a corresponding dependency tree , and (ii) a procedure assigning analytical functions to the nodes of the dependency <entity id=""E03-1014.33"">tree</entity> ."	translating	tree	usage	{'e1': {'word': 'translating', 'word_index': [(10, 10)], 'id': 'E03-1014.24'}, 'e2': {'word': 'tree', 'word_index': [(16, 16)], 'id': 'E03-1014.27'}}	"The ENTITYUNRELATED consists of ( i ) a recursive ENTITYUNRELATED ENTITY the ENTITYUNRELATED of a ENTITYUNRELATED ENTITYOTHER into a corresponding ENTITYUNRELATED , and ( ii ) a ENTITYUNRELATED assigning analytical ENTITYUNRELATED to the ENTITYUNRELATED of the dependency < entity id = "" E03-1014.33 "" > tree < / entity > ."
Apart from an outline of the annotation schemes and a deeper insight into these procedures , model application of the transformation is given herein.	model	transformation	model-feature	{'e1': {'word': 'model', 'word_index': [(16, 16)], 'id': 'E03-1014.38'}, 'e2': {'word': 'transformation', 'word_index': [(20, 20)], 'id': 'E03-1014.40'}}	Apart from an ENTITYUNRELATED of the annotation ENTITYUNRELATED and a deeper ENTITYUNRELATED into these ENTITYUNRELATED , ENTITY ENTITYUNRELATED of the ENTITYOTHER is given herein .
"In this paper we present an investigation into the use of LSA in language <entity id=""N03-1008.23"">modeling</entity> for conversational <entity id=""N03-1008.24"">speech <entity id=""N03-1008.25"">recognition</entity></entity> ."	paper	investigation	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'N03-1008.20'}, 'e2': {'word': 'investigation', 'word_index': [(6, 6)], 'id': 'N03-1008.21'}}	"In this ENTITY we present an ENTITYOTHER into the use of LSA in language < entity id = "" N03-1008.23 "" > modeling < / entity > for conversational < entity id = "" N03-1008.24 "" > speech < entity id = "" N03-1008.25 "" > recognition < / entity > </ entity > ."
"""We describe our approach to the construction and evaluation of a large-scale database called """"CatVar"""" which contains categorial variations of English lexemes."	variations	database	part_whole	{'e1': {'word': 'variations', 'word_index': [(23, 23)], 'id': 'N03-1013.10'}, 'e2': {'word': 'database', 'word_index': [(13, 13)], 'id': 'N03-1013.8'}}	""" We describe our ENTITYUNRELATED to the ENTITYUNRELATED and ENTITYUNRELATED of a ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED "" "" CatVar "" "" which contains categorial ENTITY of ENTITYUNRELATED lexemes ."
Due to the prevalence of cross-language categorial variation in multilingual applications , our categorial-variation resource may serve as an integral part of a diverse range of natural language applications .	resource	applications	part_whole	{'e1': {'word': 'resource', 'word_index': [(14, 14)], 'id': 'N03-1013.17'}, 'e2': {'word': 'applications', 'word_index': [(27, 27)], 'id': 'N03-1013.20'}}	ENTITYUNRELATED to the prevalence of ENTITYUNRELATED categorial ENTITYUNRELATED in multilingual ENTITYUNRELATED , our ENTITYUNRELATED ENTITY may serve as an integral ENTITYUNRELATED of a diverse range of ENTITYUNRELATED ENTITYOTHER .
The analysis of this unexpected result explains some of the details of th e MUC-3 test , and we propose ways of looking at the scores to distinguish different aspects of system performance .	analysis	result	topic	{'e1': {'word': 'analysis', 'word_index': [(1, 1)], 'id': 'M91-1007.10'}, 'e2': {'word': 'result', 'word_index': [(5, 5)], 'id': 'M91-1007.11'}}	The ENTITY of this unexpected ENTITYOTHER explains some of the ENTITYUNRELATED of th e MUC -3 ENTITYUNRELATED , and we ENTITYUNRELATED ways of looking at the scores to distinguish different ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED .
In this paper we examine text filtering in MUC systems with three goals in mind.	paper	text	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'M92-1004.7'}, 'e2': {'word': 'text', 'word_index': [(5, 5)], 'id': 'M92-1004.8'}}	In this ENTITY we examine ENTITYOTHER filtering in MUC ENTITYUNRELATED with three ENTITYUNRELATED in mind .
"Secondly, we discuss the use of text filtering components in MUC-3 and MUC-4 systems , and present a preliminary scheme for classifying data extraction <entity id=""M92-1004.27"">systems</entity> in <entity id=""M92-1004.28"">terms</entity> of the <entity id=""M92-1004.29"">features</entity> over which they do <entity id=""M92-1004.30"">text</entity> filtering."	components	systems	part_whole	{'e1': {'word': 'components', 'word_index': [(9, 9)], 'id': 'M92-1004.22'}, 'e2': {'word': 'systems', 'word_index': [(17, 17)], 'id': 'M92-1004.23'}}	"Secondly , we discuss the use of ENTITYUNRELATED filtering ENTITY in MUC - 3 and MUC -4 ENTITYOTHER , and present a preliminary ENTITYUNRELATED for classifying ENTITYUNRELATED extraction < entity id = "" M92-1004.27 "" > systems < / entity > in < entity id = "" M92-1004.28 "" > terms < / entity > of the < entity id = "" M92-1004.29 "" > features < / entity > over which they do < entity id = "" M92-1004.30 "" > text < / entity > filtering ."
Finally, we examine the text filtering effectiveness of MUC-3 and MUC-4 systems , and introduce some approaches to the evaluation of text filtering systems which may be of interest themselves.	approaches	evaluation	usage	{'e1': {'word': 'approaches', 'word_index': [(20, 20)], 'id': 'M92-1004.34'}, 'e2': {'word': 'evaluation', 'word_index': [(23, 23)], 'id': 'M92-1004.35'}}	Finally , we examine the ENTITYUNRELATED filtering ENTITYUNRELATED of MUC - 3 and MUC -4 ENTITYUNRELATED , and introduce some ENTITY to the ENTITYOTHER of ENTITYUNRELATED filtering ENTITYUNRELATED which may be of interest themselves .
However, with respect to the second question , we present preliminary evidence suggesting that the text filtering precision of MUC systems declines with the generality of the data stream they process , i.e.	generality	precision	result	{'e1': {'word': 'generality', 'word_index': [(25, 25)], 'id': 'M92-1004.60'}, 'e2': {'word': 'precision', 'word_index': [(18, 18)], 'id': 'M92-1004.58'}}	However , with ENTITYUNRELATED to the second ENTITYUNRELATED , we present preliminary ENTITYUNRELATED suggesting that the ENTITYUNRELATED filtering ENTITYOTHER of MUC ENTITYUNRELATED declines with the ENTITY of the ENTITYUNRELATED ENTITYUNRELATED they ENTITYUNRELATED , i.e.
The design of the template for an information extraction application (or exercise) reflects the nature of the task and therefore crucially affects the success of the attempt to capture information from text .	template	application	usage	{'e1': {'word': 'template', 'word_index': [(4, 4)], 'id': 'X93-1015.6'}, 'e2': {'word': 'application', 'word_index': [(8, 8)], 'id': 'X93-1015.8'}}	The ENTITYUNRELATED of the ENTITY for an ENTITYUNRELATED ENTITYOTHER ( or exercise ) reflects the ENTITYUNRELATED of the ENTITYUNRELATED and therefore crucially ENTITYUNRELATED the ENTITYUNRELATED of the attempt to capture ENTITYUNRELATED from ENTITYUNRELATED .
"This paper addresses the template design requirement by discussing the general principles or template definition effort which is explicitly discussed in a Case <entity id=""X93-1015.24"">Study</entity> in the last <entity id=""X93-1015.25"">section</entity> of this <entity id=""X93-1015.26"">paper</entity> ."	paper	principles	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'X93-1015.15'}, 'e2': {'word': 'principles', 'word_index': [(11, 11)], 'id': 'X93-1015.19'}}	"This ENTITY addresses the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED by discussing the general ENTITYOTHER or ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED which is explicitly discussed in a Case < entity id = "" X93-1015.24 "" > Study < / entity > in the last < entity id = "" X93-1015.25 "" > section < / entity > of this < entity id = "" X93-1015.26 "" > paper < /entity > ."
In this paper we examine some issues pertaining to the task of selection in text planning.	paper	issues	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W90-0111.6'}, 'e2': {'word': 'issues', 'word_index': [(6, 6)], 'id': 'W90-0111.7'}}	In this ENTITY we examine some ENTITYOTHER pertaining to the ENTITYUNRELATED of ENTITYUNRELATED in ENTITYUNRELATED planning .
We describe our research on generating bus route descriptions .Keywords: Natural Language Generation , Text Planning, Selection , Salience, Relevance , Coupling, Route Descriptions	research	descriptions	topic	{'e1': {'word': 'research', 'word_index': [(3, 3)], 'id': 'W90-0111.24'}, 'e2': {'word': 'descriptions', 'word_index': [(8, 8)], 'id': 'W90-0111.27'}}	We describe our ENTITY on ENTITYUNRELATED bus ENTITYUNRELATED ENTITYOTHER . Keywords : ENTITYUNRELATED , ENTITYUNRELATED Planning , ENTITYUNRELATED , Salience , ENTITYUNRELATED , Coupling , ENTITYUNRELATED Descriptions
This proliferation of relations has been pointed out before (eg Hovy [1]), and several methods for justifying a standard set of relations have been proposed : this paper reviews some of these, and presents a new method of justification which overcomes some awkward problems .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(32, 32)], 'id': 'W93-0213.23'}, 'e2': {'word': 'method', 'word_index': [(42, 42)], 'id': 'W93-0213.25'}}	This proliferation of ENTITYUNRELATED has been pointed out before ( eg Hovy [ 1 ] ) , and several ENTITYUNRELATED for justifying a ENTITYUNRELATED set of ENTITYUNRELATED have been ENTITYUNRELATED : this ENTITY ENTITYUNRELATED some of these , and presents a new ENTITYOTHER of ENTITYUNRELATED which overcomes some awkward ENTITYUNRELATED .
It is generally agreed that text has structure (at least, coherent text does).	structure	text	model-feature	{'e1': {'word': 'structure', 'word_index': [(7, 7)], 'id': 'W93-0231.6'}, 'e2': {'word': 'text', 'word_index': [(5, 5)], 'id': 'W93-0231.5'}}	It is generally agreed that ENTITYOTHER has ENTITY ( at least , coherent ENTITYUNRELATED does ) .
Li this paper , I present a case for the importance of domain structure in structuring text , and discuss the role of rhetorical structure and intentionality.	paper	case	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W93-0231.22'}, 'e2': {'word': 'case', 'word_index': [(7, 7)], 'id': 'W93-0231.23'}}	Li this ENTITY , I present a ENTITYOTHER for the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED , and discuss the ENTITYUNRELATED of rhetorical ENTITYUNRELATED and intentionality .
Li this paper , I present a case for the importance of domain structure in structuring text , and discuss the role of rhetorical structure and intentionality.	structure	text	result	{'e1': {'word': 'structure', 'word_index': [(13, 13)], 'id': 'W93-0231.26'}, 'e2': {'word': 'text', 'word_index': [(16, 16)], 'id': 'W93-0231.28'}}	Li this ENTITYUNRELATED , I present a ENTITYUNRELATED for the ENTITYUNRELATED of ENTITYUNRELATED ENTITY in ENTITYUNRELATED ENTITYOTHER , and discuss the ENTITYUNRELATED of rhetorical ENTITYUNRELATED and intentionality .
Corpus- Based Adaptation Mechanisms For Chinese Homophone Disambiguation	Adaptation	Disambiguation	usage	{'e1': {'word': 'Adaptation', 'word_index': [(2, 2)], 'id': 'W93-0311.3'}, 'e2': {'word': 'Disambiguation', 'word_index': [(7, 7)], 'id': 'W93-0311.5'}}	ENTITYUNRELATED ENTITYUNRELATED ENTITY Mechanisms For ENTITYUNRELATED Homophone ENTITYOTHER
Based on the concepts of bidirectional conversion and automatic evaluation , we propose two nser-adapiation mechanisms , character-preference learning and pstlido-word learning , for resolving Chinese homophone ambiguities in syllable-to-character conversion 	ambiguities	conversion	model-feature	{'e1': {'word': 'ambiguities', 'word_index': [(26, 26)], 'id': 'W93-0311.17'}, 'e2': {'word': 'conversion', 'word_index': [(33, 33)], 'id': 'W93-0311.18'}}	ENTITYUNRELATED on the ENTITYUNRELATED of bidirectional ENTITYUNRELATED and ENTITYUNRELATED , we ENTITYUNRELATED two nser-adapiation ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED , for resolving ENTITYUNRELATED homophone ENTITY in syllable - to - character ENTITYOTHER
The 1991  United Daily corpus oj approximately 10 million Chinese characters is used for extraction of 10 reporter-specific article databases and for computation of word frequencies and character bi-grams.	databases	corpus	part_whole	{'e1': {'word': 'databases', 'word_index': [(20, 20)], 'id': 'W93-0311.22'}, 'e2': {'word': 'corpus', 'word_index': [(4, 4)], 'id': 'W93-0311.19'}}	The 1991 United Daily ENTITYOTHER oj approximately 10 million ENTITYUNRELATED characters is used for ENTITYUNRELATED of 10 reporter -specific article ENTITY and for ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED and character bi-grams .
    In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed .	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W95-0109.5'}, 'e2': {'word': 'approach', 'word_index': [(6, 6)], 'id': 'W95-0109.6'}}	In this ENTITY , an unsupervised ENTITYOTHER for ENTITYUNRELATED a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED is ENTITYUNRELATED .
The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus .	dictionary	corpus	part_whole	{'e1': {'word': 'dictionary', 'word_index': [(13, 13)], 'id': 'W95-0109.16'}, 'e2': {'word': 'corpus', 'word_index': [(19, 19)], 'id': 'W95-0109.18'}}	The ENTITYUNRELATED ENTITYUNRELATED is to enable cheap and quick ENTITYUNRELATED of a ENTITYUNRELATED ENTITY from a large untagged ENTITYUNRELATED ENTITYOTHER with the aid of the ENTITYUNRELATED in a small ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus .	information	corpus	part_whole	{'e1': {'word': 'information', 'word_index': [(25, 25)], 'id': 'W95-0109.19'}, 'e2': {'word': 'corpus', 'word_index': [(31, 31)], 'id': 'W95-0109.22'}}	The ENTITYUNRELATED ENTITYUNRELATED is to enable cheap and quick ENTITYUNRELATED of a ENTITYUNRELATED ENTITYUNRELATED from a large untagged ENTITYUNRELATED ENTITYUNRELATED with the aid of the ENTITY in a small ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
The basic model is based on a Viterbi reestimation technique .	technique	model	usage	{'e1': {'word': 'technique', 'word_index': [(9, 9)], 'id': 'W95-0109.26'}, 'e2': {'word': 'model', 'word_index': [(2, 2)], 'id': 'W95-0109.24'}}	The ENTITYUNRELATED ENTITYOTHER is ENTITYUNRELATED on a Viterbi reestimation ENTITY .
The refined parameters are then used to further get a better tagging result .	parameters	tagging	usage	{'e1': {'word': 'parameters', 'word_index': [(2, 2)], 'id': 'W95-0109.36'}, 'e2': {'word': 'tagging', 'word_index': [(11, 11)], 'id': 'W95-0109.37'}}	The refined ENTITY are then used to further get a better ENTITYOTHER ENTITYUNRELATED .
In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance .	classifier	n-gram	usage	{'e1': {'word': 'classifier', 'word_index': [(5, 5)], 'id': 'W95-0109.41'}, 'e2': {'word': 'n-gram', 'word_index': [(13, 13)], 'id': 'W95-0109.42'}}	In ENTITYUNRELATED , a ENTITYUNRELATED ENTITY , which is capable of classifying an ENTITYOTHER either as a ENTITYUNRELATED or a ENTITYUNRELATED , is used in ENTITYUNRELATED with the Viterbi ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED .
The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module .	module	configurations	part_whole	{'e1': {'word': 'module', 'word_index': [(10, 10)], 'id': 'W95-0109.60'}, 'e2': {'word': 'configurations', 'word_index': [(1, 1)], 'id': 'W95-0109.56'}}	The ENTITYOTHER ENTITYUNRELATED ( 1 ) a Viterbi ENTITYUNRELATED ENTITYUNRELATED ENTITY followed by a Viterbi ENTITYUNRELATED ENTITYUNRELATED and ( 2 ) a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED as the postfilter for the above Viterbi ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module .	identification	precision	result	{'e1': {'word': 'identification', 'word_index': [(19, 19)], 'id': 'W95-0109.75'}, 'e2': {'word': 'precision', 'word_index': [(24, 24)], 'id': 'W95-0109.76'}}	With a ENTITYUNRELATED of 1,000 ENTITYUNRELATED and an untagged ENTITYUNRELATED of 311,591 ENTITYUNRELATED , the ENTITYUNRELATED for bigram ENTITYUNRELATED ENTITY is 56.88 % in ENTITYOTHER and 77.37 % in ENTITYUNRELATED when the ENTITYUNRELATED ENTITYUNRELATED is ENTITYUNRELATED to the ENTITYUNRELATED ENTITYUNRELATED suggested by the Viterbi ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module .	classifier	list	usage	{'e1': {'word': 'classifier', 'word_index': [(33, 33)], 'id': 'W95-0109.79'}, 'e2': {'word': 'list', 'word_index': [(39, 39)], 'id': 'W95-0109.82'}}	With a ENTITYUNRELATED of 1,000 ENTITYUNRELATED and an untagged ENTITYUNRELATED of 311,591 ENTITYUNRELATED , the ENTITYUNRELATED for bigram ENTITYUNRELATED ENTITYUNRELATED is 56.88 % in ENTITYUNRELATED and 77.37 % in ENTITYUNRELATED when the ENTITYUNRELATED ENTITY is ENTITYUNRELATED to the ENTITYUNRELATED ENTITYOTHER suggested by the Viterbi ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
A Geometric Approach To Mapping Bitext Correspondence</title>	Approach	Mapping	usage	{'e1': {'word': 'Approach', 'word_index': [(2, 2)], 'id': 'W96-0201.1'}, 'e2': {'word': 'Mapping', 'word_index': [(4, 4)], 'id': 'W96-0201.2'}}	A Geometric ENTITY To ENTITYOTHER Bitext ENTITYUNRELATED < / title >
The first step in most corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation .	map	correspondence	model-feature	{'e1': {'word': 'map', 'word_index': [(13, 13)], 'id': 'W96-0201.8'}, 'e2': {'word': 'correspondence', 'word_index': [(16, 16)], 'id': 'W96-0201.9'}}	The ENTITYUNRELATED in most ENTITYUNRELATED multilingual NLP work is to ENTITYUNRELATED a ENTITYUNRELATED ENTITY of the ENTITYOTHER between a ENTITYUNRELATED and its ENTITYUNRELATED .
This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set .	classifier	test set	usage	{'e1': {'word': 'classifier', 'word_index': [(14, 14)], 'id': 'W96-0210.4'}, 'e2': {'word': 'test set', 'word_index': [(19, 19)], 'id': 'W96-0210.6'}}	This ENTITYUNRELATED describes measures for ENTITYUNRELATED the three determinants of how well a probabilistic ENTITY ENTITYUNRELATED on a given ENTITYOTHER .
These determinants are the appropriateness , for the test set , of the results of (1) feature selection , (2) formulation of the parametric form of the model , and (3) parameter estimation .	appropriateness	results	model-feature	{'e1': {'word': 'appropriateness', 'word_index': [(4, 4)], 'id': 'W96-0210.7'}, 'e2': {'word': 'results', 'word_index': [(12, 12)], 'id': 'W96-0210.9'}}	These determinants are the ENTITY , for the ENTITYUNRELATED , of the ENTITYOTHER of ( 1 ) ENTITYUNRELATED , ( 2 ) ENTITYUNRELATED of the parametric ENTITYUNRELATED of the ENTITYUNRELATED , and ( 3 ) ENTITYUNRELATED .
This paper presents a technique for sentence generation .	paper	technique	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W96-0404.2'}, 'e2': {'word': 'technique', 'word_index': [(4, 4)], 'id': 'W96-0404.3'}}	This ENTITY presents a ENTITYOTHER for ENTITYUNRELATED .
This paper describes the input specification language of the WAG Sentence Generation system .	paper	language	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W96-0405.5'}, 'e2': {'word': 'language', 'word_index': [(6, 6)], 'id': 'W96-0405.8'}}	This ENTITY describes the ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER of the WAG ENTITYUNRELATED ENTITYUNRELATED .
The results of our preliminary experiments show that accent differences cause recognizer performance to degrade .	differences	performance	result	{'e1': {'word': 'differences', 'word_index': [(9, 9)], 'id': 'W97-0406.25'}, 'e2': {'word': 'performance', 'word_index': [(12, 12)], 'id': 'W97-0406.26'}}	The ENTITYUNRELATED of our preliminary ENTITYUNRELATED show that accent ENTITY cause recognizer ENTITYOTHER to degrade .
Our experimental results also show that a common feature set , parameter set, and common algorithm lead to different performance output for Cantonese and English speech recognition modules .	common	output	result	{'e1': {'word': 'common', 'word_index': [(7, 7)], 'id': 'W97-0406.38'}, 'e2': {'word': 'output', 'word_index': [(20, 20)], 'id': 'W97-0406.44'}}	Our ENTITYUNRELATED ENTITYUNRELATED also show that a ENTITY ENTITYUNRELATED , ENTITYUNRELATED set , and ENTITYUNRELATED ENTITYUNRELATED lead to different ENTITYUNRELATED ENTITYOTHER for Cantonese and ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
To infer as much information as possible from the retained sequence of words , we propose a bottom-up syntactico-semantic robust parsing relying on a lexi-calized tree grammar and on integrated repairing strategies .	strategies	parsing	usage	{'e1': {'word': 'strategies', 'word_index': [(34, 34)], 'id': 'W97-0615.26'}, 'e2': {'word': 'parsing', 'word_index': [(22, 22)], 'id': 'W97-0615.23'}}	To infer as much ENTITYUNRELATED as possible from the retained ENTITYUNRELATED of ENTITYUNRELATED , we ENTITYUNRELATED a bottom - up ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER relying on a lexi- calized ENTITYUNRELATED grammar and on integrated ENTITYUNRELATED ENTITY .
A Programmable Multi-Blackboard Architecture For Dialogue Processing Systems</title>	Architecture	Systems	usage	{'e1': {'word': 'Architecture', 'word_index': [(3, 3)], 'id': 'W97-0618.1'}, 'e2': {'word': 'Systems', 'word_index': [(7, 7)], 'id': 'W97-0618.4'}}	A Programmable Multi-Blackboard ENTITY For ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER < / title >
In current Natural Language Processing Systems , different components for different processing tasks and input / output modalities have to be integrated.	components	tasks	usage	{'e1': {'word': 'components', 'word_index': [(6, 6)], 'id': 'W97-0618.8'}, 'e2': {'word': 'tasks', 'word_index': [(10, 10)], 'id': 'W97-0618.10'}}	In ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , different ENTITY for different ENTITYUNRELATED ENTITYOTHER and ENTITYUNRELATED / ENTITYUNRELATED ENTITYUNRELATED have to be integrated .
These rules may contain typed variables .	variables	rules	part_whole	{'e1': {'word': 'variables', 'word_index': [(5, 5)], 'id': 'W97-0618.34'}, 'e2': {'word': 'rules', 'word_index': [(1, 1)], 'id': 'W97-0618.32'}}	These ENTITYOTHER may contain ENTITYUNRELATED ENTITY .
Furthermore, the representations in the blackboards allow to represent partial information and to leave disjunctions unresolved.	representations	information	model-feature	{'e1': {'word': 'representations', 'word_index': [(3, 3)], 'id': 'W97-0618.37'}, 'e2': {'word': 'information', 'word_index': [(11, 11)], 'id': 'W97-0618.39'}}	Furthermore , the ENTITY in the blackboards allow to represent ENTITYUNRELATED ENTITYOTHER and to leave ENTITYUNRELATED unresolved .
Moreover, the conditions of the rule may depend on the specificity of the representations with which the variables are instantiated.	specificity	rule	usage	{'e1': {'word': 'specificity', 'word_index': [(11, 11)], 'id': 'W97-0618.42'}, 'e2': {'word': 'rule', 'word_index': [(6, 6)], 'id': 'W97-0618.41'}}	Moreover , the conditions of the ENTITYOTHER may depend on the ENTITY of the ENTITYUNRELATED with which the ENTITYUNRELATED are instantiated .
In this paper we will present several results concerning phrase structure trees .	paper	results	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'J82-1001.2'}, 'e2': {'word': 'results', 'word_index': [(7, 7)], 'id': 'J82-1001.3'}}	In this ENTITY we will present several ENTITYOTHER ENTITYUNRELATED ENTITYUNRELATED .
We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints .	theorem	constraints	topic	{'e1': {'word': 'theorem', 'word_index': [(18, 18)], 'id': 'J82-1001.11'}, 'e2': {'word': 'constraints', 'word_index': [(21, 21)], 'id': 'J82-1001.12'}}	We have given a brief account of local ENTITYUNRELATED on ENTITYUNRELATED ENTITYUNRELATED and an intuitive proof of a ENTITY about local ENTITYOTHER .
We have compared the local constraints approach to some aspects of Gazdar 's framework and that of Peters and Ritchie and of Karttunen .	approach	framework	compare	{'e1': {'word': 'approach', 'word_index': [(6, 6)], 'id': 'J82-1001.14'}, 'e2': {'word': 'framework', 'word_index': [(13, 13)], 'id': 'J82-1001.16'}}	We have compared the local ENTITYUNRELATED ENTITY to some ENTITYUNRELATED of Gazdar 's ENTITYOTHER and that of Peters and Ritchie and of Karttunen .
A major problem faced by would-be users of computer systems is that computers generally make use of special-purpose languages familiar only to those trained in computer science .	languages	computers	usage	{'e1': {'word': 'languages', 'word_index': [(20, 20)], 'id': 'J82-2002.10'}, 'e2': {'word': 'computers', 'word_index': [(14, 14)], 'id': 'J82-2002.8'}}	A major ENTITYUNRELATED faced by would - be ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED is that ENTITYOTHER generally make use of ENTITYUNRELATED ENTITY familiar only to those ENTITYUNRELATED in ENTITYUNRELATED .
Among these are such systems as those described in the several references at the end of this paper .	paper	references	topic	{'e1': {'word': 'paper', 'word_index': [(17, 17)], 'id': 'J82-2002.33'}, 'e2': {'word': 'references', 'word_index': [(11, 11)], 'id': 'J82-2002.32'}}	Among these are such ENTITYUNRELATED as those described in the several ENTITYOTHER at the end of this ENTITY .
In this paper , we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes .	paper	lexicon	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'P98-1099.15'}, 'e2': {'word': 'lexicon', 'word_index': [(12, 12)], 'id': 'P98-1099.16'}}	In this ENTITY , we describe our work to build such a ENTITYOTHER by combining multiple , heterogeneous ENTITYUNRELATED which have been ENTITYUNRELATED for other ENTITYUNRELATED .
The integration of the lexicon and the architecture is able to effectively improve the system paraphrasing power, minimize the chance of grammatical errors , and simplify the development process substantially.	integration	errors	result	{'e1': {'word': 'integration', 'word_index': [(1, 1)], 'id': 'P98-1099.36'}, 'e2': {'word': 'errors', 'word_index': [(23, 23)], 'id': 'P98-1099.41'}}	The ENTITY of the ENTITYUNRELATED and the ENTITYUNRELATED is able to effectively ENTITYUNRELATED the ENTITYUNRELATED paraphrasing power , minimize the chance of grammatical ENTITYOTHER , and simplify the ENTITYUNRELATED ENTITYUNRELATED substantially .
It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system .	errors	results	model-feature	{'e1': {'word': 'errors', 'word_index': [(6, 6)], 'id': 'P98-1107.7'}, 'e2': {'word': 'results', 'word_index': [(9, 9)], 'id': 'P98-1107.8'}}	It is important to correct the ENTITY in the ENTITYOTHER of ENTITYUNRELATED to ENTITYUNRELATED the ENTITYUNRELATED of a speech ENTITYUNRELATED .
It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system .	translation system	performance	result	{'e1': {'word': 'translation system', 'word_index': [(19, 19)], 'id': 'P98-1107.12'}, 'e2': {'word': 'performance', 'word_index': [(15, 15)], 'id': 'P98-1107.11'}}	It is important to correct the ENTITYUNRELATED in the ENTITYUNRELATED of ENTITYUNRELATED to ENTITYUNRELATED the ENTITYOTHER of a speech ENTITY .
This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method .	method	errors	usage	{'e1': {'word': 'method', 'word_index': [(4, 4)], 'id': 'P98-1107.15'}, 'e2': {'word': 'errors', 'word_index': [(7, 7)], 'id': 'P98-1107.16'}}	This ENTITYUNRELATED ENTITYUNRELATED a ENTITY for correcting ENTITYOTHER using the ENTITYUNRELATED ENTITYUNRELATED of character ENTITYUNRELATED , and ENTITYUNRELATED the ENTITYUNRELATED .
The proposed method comprises two successive correcting processes .	processes	method	part_whole	{'e1': {'word': 'processes', 'word_index': [(7, 7)], 'id': 'P98-1107.24'}, 'e2': {'word': 'method', 'word_index': [(2, 2)], 'id': 'P98-1107.23'}}	The ENTITYUNRELATED ENTITYOTHER comprises two successive correcting ENTITY .
"The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=""P98-1107.31"">recognition</entity> , the second <entity id=""P98-1107.32"">string</entity> is the corresponding <entity id=""P98-1107.33"">section</entity> of the actual <entity id=""P98-1107.34"">utterance</entity> ."	pairs	process	usage	{'e1': {'word': 'pairs', 'word_index': [(4, 4)], 'id': 'P98-1107.26'}, 'e2': {'word': 'process', 'word_index': [(2, 2)], 'id': 'P98-1107.25'}}	"The first ENTITYOTHER uses ENTITY of ENTITYUNRELATED : the first ENTITYUNRELATED is an erroneous substring of the ENTITYUNRELATED predicted by speech < entity id = "" P98-1107.31 "" > recognition < / entity > , the second < entity id = "" P98-1107.32 "" > string < / entity > is the corresponding < entity id = "" P98-1107.33 "" > section < / entity > of the actual < entity id = "" P98-1107.34 "" > utterance < / entity > ."
Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs .	pairs	database	part_whole	{'e1': {'word': 'pairs', 'word_index': [(15, 15)], 'id': 'P98-1107.37'}, 'e2': {'word': 'database', 'word_index': [(8, 8)], 'id': 'P98-1107.35'}}	Errors are detected and corrected according to the ENTITYOTHER learned from erroneous - correct ENTITYUNRELATED ENTITY .
The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors .	string	process	usage	{'e1': {'word': 'string', 'word_index': [(12, 12)], 'id': 'P98-1107.40'}, 'e2': {'word': 'process', 'word_index': [(8, 8)], 'id': 'P98-1107.39'}}	The remaining ENTITYUNRELATED are passed to the posterior ENTITYOTHER which uses a ENTITY in the ENTITYUNRELATED that is similar to the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors .	errors	string	part_whole	{'e1': {'word': 'errors', 'word_index': [(24, 24)], 'id': 'P98-1107.45'}, 'e2': {'word': 'string', 'word_index': [(21, 21)], 'id': 'P98-1107.42'}}	The remaining ENTITYUNRELATED are passed to the posterior ENTITYUNRELATED which uses a ENTITYUNRELATED in the ENTITYUNRELATED that is similar to the ENTITYOTHER ENTITYUNRELATED ENTITYUNRELATED ENTITY .
method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada</abstract>	method	segments	result	{'e1': {'word': 'method', 'word_index': [(0, 0)], 'id': 'P98-1107.56'}, 'e2': {'word': 'segments', 'word_index': [(6, 6)], 'id': 'P98-1107.58'}}	ENTITY also obtains reliably recognized ENTITYUNRELATED ENTITYOTHER of an ENTITYUNRELATED by cooperatively using both grammatical and ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , and uses a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED the grammatical ENTITYUNRELATED described by ENTITYUNRELATED grammar ( Tsukada < / abstract >
Rich mark-up can considerably benefit the process of establishing bitext correspondences , that is, the task of providing correct identification and alignment methods for text segments that are translation equivalences of each other in a parallel corpus .	alignment	segments	usage	{'e1': {'word': 'alignment', 'word_index': [(24, 24)], 'id': 'P98-2134.7'}, 'e2': {'word': 'segments', 'word_index': [(28, 28)], 'id': 'P98-2134.10'}}	Rich mark - up can considerably ENTITYUNRELATED the ENTITYUNRELATED of establishing bitext ENTITYUNRELATED , that is , the ENTITYUNRELATED of ENTITYUNRELATED correct ENTITYUNRELATED and ENTITY ENTITYUNRELATED for ENTITYUNRELATED ENTITYOTHER that are ENTITYUNRELATED ENTITYUNRELATED of each other in a ENTITYUNRELATED .
We present a sentence alignment algorithm that, by taking advantage of previously annotated texts , obtains accuracy rates close to 100%.	algorithm	accuracy	result	{'e1': {'word': 'algorithm', 'word_index': [(5, 5)], 'id': 'P98-2134.16'}, 'e2': {'word': 'accuracy', 'word_index': [(17, 17)], 'id': 'P98-2134.19'}}	We present a ENTITYUNRELATED ENTITYUNRELATED ENTITY that , by taking ENTITYUNRELATED of previously annotated ENTITYUNRELATED , obtains ENTITYOTHER ENTITYUNRELATED close to 100 % .
This paper proposes a method for generating a logical-constraint-based internal representation from a unification grammar formalism with disjunctive information .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'P98-2154.2'}, 'e2': {'word': 'method', 'word_index': [(4, 4)], 'id': 'P98-2154.4'}}	This ENTITY ENTITYUNRELATED a ENTITYOTHER for ENTITYUNRELATED a ENTITYUNRELATED internal ENTITYUNRELATED from a ENTITYUNRELATED grammar ENTITYUNRELATED with disjunctive ENTITYUNRELATED .
This paper proposes a method for generating a logical-constraint-based internal representation from a unification grammar formalism with disjunctive information .	representation	information	part_whole	{'e1': {'word': 'representation', 'word_index': [(10, 10)], 'id': 'P98-2154.7'}, 'e2': {'word': 'information', 'word_index': [(18, 18)], 'id': 'P98-2154.10'}}	This ENTITYUNRELATED ENTITYUNRELATED a ENTITYUNRELATED for ENTITYUNRELATED a ENTITYUNRELATED internal ENTITY from a ENTITYUNRELATED grammar ENTITYUNRELATED with disjunctive ENTITYOTHER .
Unification grammar formalisms based on path equations and lists of pairs of labels and values are better than those based on first-order terms in that the former is easier to describe and to understand.	lists	formalisms	usage	{'e1': {'word': 'lists', 'word_index': [(8, 8)], 'id': 'P98-2154.16'}, 'e2': {'word': 'formalisms', 'word_index': [(2, 2)], 'id': 'P98-2154.12'}}	ENTITYUNRELATED grammar ENTITYOTHER ENTITYUNRELATED on ENTITYUNRELATED ENTITYUNRELATED and ENTITY of ENTITYUNRELATED of labels and values are better than those ENTITYUNRELATED on ENTITYUNRELATED ENTITYUNRELATED in that the former is easier to describe and to understand .
Parsing with term-based internal representations is more efficient than parsing with graph-based representations .	Parsing	parsing	compare	{'e1': {'word': 'Parsing', 'word_index': [(0, 0)], 'id': 'P98-2154.21'}, 'e2': {'word': 'parsing', 'word_index': [(9, 9)], 'id': 'P98-2154.24'}}	ENTITY with ENTITYUNRELATED internal ENTITYUNRELATED is more efficient than ENTITYOTHER with graph - based ENTITYUNRELATED .
Therefore, it is effective to translate unification grammar formalism based on path equations and lists of pairs of labels and values into a term-based internal representation .	lists	formalism	usage	{'e1': {'word': 'lists', 'word_index': [(15, 15)], 'id': 'P98-2154.32'}, 'e2': {'word': 'formalism', 'word_index': [(9, 9)], 'id': 'P98-2154.28'}}	Therefore , it is effective to ENTITYUNRELATED ENTITYUNRELATED grammar ENTITYOTHER ENTITYUNRELATED on ENTITYUNRELATED ENTITYUNRELATED and ENTITY of ENTITYUNRELATED of labels and values into a ENTITYUNRELATED internal ENTITYUNRELATED .
ParaMetric calculates precision and recall scores by comparing the paraphrases discovered by automatic paraphrasing techniques against gold standard alignments of words and phrases within equivalent sentences .	alignments	sentences	model-feature	{'e1': {'word': 'alignments', 'word_index': [(17, 17)], 'id': 'C08-1013.17'}, 'e2': {'word': 'sentences', 'word_index': [(24, 24)], 'id': 'C08-1013.20'}}	ParaMetric calculates ENTITYUNRELATED and ENTITYUNRELATED scores by comparing the paraphrases discovered by ENTITYUNRELATED paraphrasing ENTITYUNRELATED against ENTITYUNRELATED ENTITY of ENTITYUNRELATED and ENTITYUNRELATED within equivalent ENTITYOTHER .
The procedures produce substantial improvements (up to 31.6% error reduction ) on the task of determining subcategorization frames of novel verbs , relative to a smoothed Penn Treebank-trained PCFG.	procedures	improvements	result	{'e1': {'word': 'procedures', 'word_index': [(1, 1)], 'id': 'C08-1025.9'}, 'e2': {'word': 'improvements', 'word_index': [(4, 4)], 'id': 'C08-1025.10'}}	The ENTITY produce substantial ENTITYOTHER ( up to 31.6 % ENTITYUNRELATED ENTITYUNRELATED ) on the ENTITYUNRELATED of determining subcategorization ENTITYUNRELATED of novel ENTITYUNRELATED , ENTITYUNRELATED to a ENTITYUNRELATED ENTITYUNRELATED PCFG .
The procedures produce substantial improvements (up to 31.6% error reduction ) on the task of determining subcategorization frames of novel verbs , relative to a smoothed Penn Treebank-trained PCFG.	frames	Penn Treebank-trained	part_whole	{'e1': {'word': 'frames', 'word_index': [(19, 19)], 'id': 'C08-1025.14'}, 'e2': {'word': 'Penn Treebank-trained', 'word_index': [(28, 28)], 'id': 'C08-1025.18'}}	The ENTITYUNRELATED produce substantial ENTITYUNRELATED ( up to 31.6 % ENTITYUNRELATED ENTITYUNRELATED ) on the ENTITYUNRELATED of determining subcategorization ENTITY of novel ENTITYUNRELATED , ENTITYUNRELATED to a ENTITYUNRELATED ENTITYOTHER PCFG .
In this paper , we propose a method to perform domain adaptation for statistical machine translation , where in-domain bilingual corpora do not exist.	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'C08-1125.12'}, 'e2': {'word': 'method', 'word_index': [(7, 7)], 'id': 'C08-1125.14'}}	In this ENTITY , we ENTITYUNRELATED a ENTITYOTHER to ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED , where ENTITYUNRELATED bilingual ENTITYUNRELATED do not exist .
This method first uses out-of-domain corpora to train a baseline system and then uses in-domain translation dictionaries and in-domain monolingual corpora to improve the indomain performance .	corpora	method	usage	{'e1': {'word': 'corpora', 'word_index': [(5, 5)], 'id': 'C08-1125.22'}, 'e2': {'word': 'method', 'word_index': [(1, 1)], 'id': 'C08-1125.20'}}	This ENTITYOTHER first uses ENTITYUNRELATED ENTITY to ENTITYUNRELATED a ENTITYUNRELATED and then uses ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED monolingual ENTITYUNRELATED to ENTITYUNRELATED the indomain ENTITYUNRELATED .
In our approach , an SVM-based classifier is employed to classify the given Chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences .	classifier	sentences	usage	{'e1': {'word': 'classifier', 'word_index': [(8, 8)], 'id': 'C08-1137.27'}, 'e2': {'word': 'sentences', 'word_index': [(16, 16)], 'id': 'C08-1137.29'}}	In our ENTITYUNRELATED , an SVM - based ENTITY is employed to classify the given ENTITYUNRELATED ENTITYOTHER into three ENTITYUNRELATED : special interrogative ENTITYUNRELATED , other interrogative ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED .
An automatic processor of written French language is described.	processor	language	usage	{'e1': {'word': 'processor', 'word_index': [(2, 2)], 'id': 'C80-1002.5'}, 'e2': {'word': 'language', 'word_index': [(6, 6)], 'id': 'C80-1002.6'}}	An ENTITYUNRELATED ENTITY of written French ENTITYOTHER is described .
An application to the processing of the medical records is then discussed.	processing	records	usage	{'e1': {'word': 'processing', 'word_index': [(4, 4)], 'id': 'C80-1002.21'}, 'e2': {'word': 'records', 'word_index': [(8, 8)], 'id': 'C80-1002.22'}}	An ENTITYUNRELATED to the ENTITY of the medical ENTITYOTHER is then discussed .
Computer- Aided Grammatical Tagging Of Spoken English	Tagging	English	usage	{'e1': {'word': 'Tagging', 'word_index': [(3, 3)], 'id': 'C80-1005.2'}, 'e2': {'word': 'English', 'word_index': [(6, 6)], 'id': 'C80-1005.3'}}	ENTITYUNRELATED Aided Grammatical ENTITY Of Spoken ENTITYOTHER
This paper presents a proposal for the proper treatment of ill-formed input .	paper	proposal	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C80-1008.19'}, 'e2': {'word': 'proposal', 'word_index': [(4, 4)], 'id': 'C80-1008.20'}}	This ENTITY presents a ENTITYOTHER for the proper ENTITYUNRELATED of ill- formed ENTITYUNRELATED .
Meta-rules modifying the rules of normal processing should be used for error identification and recovery .	rules	identification	usage	{'e1': {'word': 'rules', 'word_index': [(3, 3)], 'id': 'C80-1008.27'}, 'e2': {'word': 'identification', 'word_index': [(12, 12)], 'id': 'C80-1008.30'}}	Meta-rules modifying the ENTITY of normal ENTITYUNRELATED should be used for ENTITYUNRELATED ENTITYOTHER and ENTITYUNRELATED .
Chemistry research papers are a primary source of information about chemistry, as in any scientific field .	papers	information	part_whole	{'e1': {'word': 'papers', 'word_index': [(2, 2)], 'id': 'L08-1232.4'}, 'e2': {'word': 'information', 'word_index': [(8, 8)], 'id': 'L08-1232.6'}}	Chemistry ENTITYUNRELATED ENTITY are a primary ENTITYUNRELATED of ENTITYOTHER about chemistry , as in any scientific ENTITYUNRELATED .
At one level , extracting the relevant information from research papers is a text mining task , requiring both extensive language resources and specialised knowledge of the subject domain .	information	papers	part_whole	{'e1': {'word': 'information', 'word_index': [(7, 7)], 'id': 'L08-1232.19'}, 'e2': {'word': 'papers', 'word_index': [(10, 10)], 'id': 'L08-1232.21'}}	At one ENTITYUNRELATED , ENTITYUNRELATED the relevant ENTITY from ENTITYUNRELATED ENTITYOTHER is a ENTITYUNRELATED mining ENTITYUNRELATED , ENTITYUNRELATED both extensive ENTITYUNRELATED and specialised ENTITYUNRELATED of the subject ENTITYUNRELATED .
However, the papers also encode information about the way the research is conducted and the structure of the field itself.	information	papers	part_whole	{'e1': {'word': 'information', 'word_index': [(6, 6)], 'id': 'L08-1232.29'}, 'e2': {'word': 'papers', 'word_index': [(3, 3)], 'id': 'L08-1232.28'}}	However , the ENTITYOTHER also encode ENTITY about the way the ENTITYUNRELATED is conducted and the ENTITYUNRELATED of the ENTITYUNRELATED itself .
The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research .	research	corpus	part_whole	{'e1': {'word': 'research', 'word_index': [(15, 15)], 'id': 'L08-1232.41'}, 'e2': {'word': 'corpus', 'word_index': [(11, 11)], 'id': 'L08-1232.40'}}	The SciBorg ENTITYUNRELATED sets out to ENTITYUNRELATED an extensive , analysed ENTITYOTHER of published chemistry ENTITY .
Large repositories of life science data in the form of domain-specific literature , textual databases and other large specialised textual collections ( corpora ) in electronic form increase on a daily basis to a level beyond the human mind can grasp and interpret.	data	repositories	part_whole	{'e1': {'word': 'data', 'word_index': [(5, 5)], 'id': 'L08-1236.5'}, 'e2': {'word': 'repositories', 'word_index': [(1, 1)], 'id': 'L08-1236.3'}}	Large ENTITYOTHER of life ENTITYUNRELATED ENTITY in the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED , textual ENTITYUNRELATED and other large specialised textual ENTITYUNRELATED ( ENTITYUNRELATED ) in electronic ENTITYUNRELATED ENTITYUNRELATED on a daily ENTITYUNRELATED to a ENTITYUNRELATED beyond the human mind can grasp and interpret .
In this paper , we present an evaluation of how the performance of the RITEL system differs when users interact with it using spoken versus textual input and output .	paper	evaluation	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'L08-1378.19'}, 'e2': {'word': 'evaluation', 'word_index': [(7, 7)], 'id': 'L08-1378.20'}}	In this ENTITY , we present an ENTITYOTHER of how the ENTITYUNRELATED of the RITEL ENTITYUNRELATED differs when ENTITYUNRELATED interact with it using spoken versus textual ENTITYUNRELATED and ENTITYUNRELATED .
In this paper , we present an evaluation of how the performance of the RITEL system differs when users interact with it using spoken versus textual input and output .	system	performance	result	{'e1': {'word': 'system', 'word_index': [(15, 15)], 'id': 'L08-1378.22'}, 'e2': {'word': 'performance', 'word_index': [(11, 11)], 'id': 'L08-1378.21'}}	In this ENTITYUNRELATED , we present an ENTITYUNRELATED of how the ENTITYOTHER of the RITEL ENTITY differs when ENTITYUNRELATED interact with it using spoken versus textual ENTITYUNRELATED and ENTITYUNRELATED .
In this paper , we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains .	paper	technique	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'D08-1093.18'}, 'e2': {'word': 'technique', 'word_index': [(7, 7)], 'id': 'D08-1093.20'}}	In this ENTITY , we ENTITYUNRELATED a ENTITYOTHER that automatically takes into account certain ENTITYUNRELATED of the ENTITYUNRELATED of interest , and accurately predicts ENTITYUNRELATED ENTITYUNRELATED on ENTITYUNRELATED from these new ENTITYUNRELATED .
As a result , we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain .	parser	performance	result	{'e1': {'word': 'parser', 'word_index': [(23, 23)], 'id': 'D08-1093.30'}, 'e2': {'word': 'performance', 'word_index': [(19, 19)], 'id': 'D08-1093.28'}}	As a ENTITYUNRELATED , we have a cheap ( no annotation involved ) and effective recipe for measuring the ENTITYOTHER of a ENTITYUNRELATED ENTITY on any given ENTITYUNRELATED .
French- English Terminology Extraction from Comparable Corpora</title>	Terminology	Corpora	part_whole	{'e1': {'word': 'Terminology', 'word_index': [(3, 3)], 'id': 'I05-1062.2'}, 'e2': {'word': 'Corpora', 'word_index': [(7, 7)], 'id': 'I05-1062.4'}}	French - ENTITYUNRELATED ENTITY ENTITYUNRELATED from Comparable ENTITYOTHER < / title >
In particular, we consider the use of prosodic cues in a tone language , Mandarin Chinese , where variations in pitch height and slope additionally serve to determine word meaning .	variations	pitch	model-feature	{'e1': {'word': 'variations', 'word_index': [(19, 19)], 'id': 'I05-3010.15'}, 'e2': {'word': 'pitch', 'word_index': [(21, 21)], 'id': 'I05-3010.16'}}	In particular , we consider the use of prosodic ENTITYUNRELATED in a tone ENTITYUNRELATED , Mandarin ENTITYUNRELATED , where ENTITY in ENTITYOTHER height and slope additionally serve to determine ENTITYUNRELATED .
Within a corpus of spontaneous Chinese dialogues , we find that turn-unit final syllables are significantly lower in average pitch and intensity than turn-unit initial syllables in both smooth turn changes and segments ended by speaker overlap.	dialogues	corpus	part_whole	{'e1': {'word': 'dialogues', 'word_index': [(6, 6)], 'id': 'I05-3010.20'}, 'e2': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'I05-3010.18'}}	Within a ENTITYOTHER of spontaneous ENTITYUNRELATED ENTITY , we find that ENTITYUNRELATED final syllables are significantly lower in average ENTITYUNRELATED and intensity than ENTITYUNRELATED initial syllables in both ENTITYUNRELATED turn changes and ENTITYUNRELATED ended by speaker overlap .
The paper shows how Combinatory Categorial Grammar (CCG) can be adapted to take advantage of the extra resource-sensitivity provided by the Categorial Type Logic framework .	paper	Categorial Grammar	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'E03-1036.2'}, 'e2': {'word': 'Categorial Grammar', 'word_index': [(5, 5)], 'id': 'E03-1036.3'}}	The ENTITY shows how Combinatory ENTITYOTHER ( CCG ) can be ENTITYUNRELATED to take ENTITYUNRELATED of the extra ENTITYUNRELATED ENTITYUNRELATED by the Categorial ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
Automatic Acquisition Of Script Knowledge From A Text Collection	Knowledge	Collection	part_whole	{'e1': {'word': 'Knowledge', 'word_index': [(4, 4)], 'id': 'E03-1061.4'}, 'e2': {'word': 'Collection', 'word_index': [(8, 8)], 'id': 'E03-1061.6'}}	ENTITYUNRELATED ENTITYUNRELATED Of ENTITYUNRELATED ENTITY From A ENTITYUNRELATED ENTITYOTHER
In this paper , we describe a method for automatic acquisition of script knowledge from a Japanese text collection .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'E03-1061.7'}, 'e2': {'word': 'method', 'word_index': [(7, 7)], 'id': 'E03-1061.8'}}	In this ENTITY , we describe a ENTITYOTHER for ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED from a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
In this paper , we describe a method for automatic acquisition of script knowledge from a Japanese text collection .	knowledge	collection	part_whole	{'e1': {'word': 'knowledge', 'word_index': [(13, 13)], 'id': 'E03-1061.12'}, 'e2': {'word': 'collection', 'word_index': [(18, 18)], 'id': 'E03-1061.15'}}	In this ENTITYUNRELATED , we describe a ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED ENTITY from a ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
We extracted sequences ( pairs ) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences ( pairs ) in terms of the frequency of their occurrence .	sequences	collection	part_whole	{'e1': {'word': 'sequences', 'word_index': [(2, 2)], 'id': 'E03-1061.22'}, 'e2': {'word': 'collection', 'word_index': [(16, 16)], 'id': 'E03-1061.29'}}	We ENTITYUNRELATED ENTITY ( ENTITYUNRELATED ) of ENTITYUNRELATED occurring in ENTITYUNRELATED ENTITYUNRELATED from a ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER and then chose those that were typical of certain ENTITYUNRELATED by ENTITYUNRELATED these ENTITYUNRELATED ( ENTITYUNRELATED ) in ENTITYUNRELATED of the ENTITYUNRELATED of their ENTITYUNRELATED .
We extracted sequences ( pairs ) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences ( pairs ) in terms of the frequency of their occurrence .	frequency	occurrence	model-feature	{'e1': {'word': 'frequency', 'word_index': [(38, 38)], 'id': 'E03-1061.35'}, 'e2': {'word': 'occurrence', 'word_index': [(41, 41)], 'id': 'E03-1061.36'}}	We ENTITYUNRELATED ENTITYUNRELATED ( ENTITYUNRELATED ) of ENTITYUNRELATED occurring in ENTITYUNRELATED ENTITYUNRELATED from a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and then chose those that were typical of certain ENTITYUNRELATED by ENTITYUNRELATED these ENTITYUNRELATED ( ENTITYUNRELATED ) in ENTITYUNRELATED of the ENTITY of their ENTITYOTHER .
Comparing Automatic And Human Evaluation Of NLG Systems</title>	Evaluation	Systems	topic	{'e1': {'word': 'Evaluation', 'word_index': [(4, 4)], 'id': 'E06-1040.2'}, 'e2': {'word': 'Systems', 'word_index': [(7, 7)], 'id': 'E06-1040.3'}}	Comparing ENTITYUNRELATED And Human ENTITY Of NLG ENTITYOTHER < / title >
Bayesian Nets For Syntactic Categorization Of Novel Words</title>	Categorization	Words	usage	{'e1': {'word': 'Categorization', 'word_index': [(4, 4)], 'id': 'N03-2027.2'}, 'e2': {'word': 'Words', 'word_index': [(7, 7)], 'id': 'N03-2027.3'}}	Bayesian Nets For ENTITYUNRELATED ENTITY Of Novel ENTITYOTHER < / title >
This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of- Speech (PoS) tags to novel text .	paper	application	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'N03-2027.4'}, 'e2': {'word': 'application', 'word_index': [(4, 4)], 'id': 'N03-2027.5'}}	This ENTITY presents an ENTITYOTHER of a Dynamic Bayesian ENTITYUNRELATED ( DBN ) to the ENTITYUNRELATED of assigning ENTITYUNRELATED ENTITYUNRELATED ( PoS ) ENTITYUNRELATED to novel ENTITYUNRELATED .
This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of- Speech (PoS) tags to novel text .	tags	text	usage	{'e1': {'word': 'tags', 'word_index': [(23, 23)], 'id': 'N03-2027.10'}, 'e2': {'word': 'text', 'word_index': [(26, 26)], 'id': 'N03-2027.11'}}	This ENTITYUNRELATED presents an ENTITYUNRELATED of a Dynamic Bayesian ENTITYUNRELATED ( DBN ) to the ENTITYUNRELATED of assigning ENTITYUNRELATED ENTITYUNRELATED ( PoS ) ENTITY to novel ENTITYOTHER .
"This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information <entity id=""N04-1018.20"">sources</entity> ."	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'N04-1018.12'}, 'e2': {'word': 'system', 'word_index': [(4, 4)], 'id': 'N04-1018.13'}}	"This ENTITY presents a ENTITYOTHER that detects various ENTITYUNRELATED of disfluencies and other ENTITYUNRELATED ENTITYUNRELATED with ENTITYUNRELATED obtained from ENTITYUNRELATED and prosodic information < entity id = "" N04-1018.20 "" > sources < / entity > ."
Results are reported on human and automatic transcripts of conversational telephone speech .	transcripts	speech	part_whole	{'e1': {'word': 'transcripts', 'word_index': [(7, 7)], 'id': 'N04-1018.32'}, 'e2': {'word': 'speech', 'word_index': [(11, 11)], 'id': 'N04-1018.34'}}	ENTITYUNRELATED are ENTITYUNRELATED on human and ENTITYUNRELATED ENTITY of conversational ENTITYUNRELATED ENTITYOTHER .
We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks .	networks	model	usage	{'e1': {'word': 'networks', 'word_index': [(11, 11)], 'id': 'N04-4021.27'}, 'e2': {'word': 'model', 'word_index': [(7, 7)], 'id': 'N04-4021.26'}}	We have ENTITYUNRELATED a flexible ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER using dynamic Bayesian ENTITY .
In this paper , we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus .	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'N04-4021.28'}, 'e2': {'word': 'approach', 'word_index': [(7, 7)], 'id': 'N04-4021.29'}}	In this ENTITY , we describe our ENTITYOTHER and ENTITYUNRELATED on a ENTITYUNRELATED ENTITYUNRELATED using phonetic ENTITYUNRELATED of ENTITYUNRELATED from the Switchboard ENTITYUNRELATED .
In this paper , we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus .	transcriptions	corpus	part_whole	{'e1': {'word': 'transcriptions', 'word_index': [(16, 16)], 'id': 'N04-4021.33'}, 'e2': {'word': 'corpus', 'word_index': [(22, 22)], 'id': 'N04-4021.35'}}	In this ENTITYUNRELATED , we describe our ENTITYUNRELATED and ENTITYUNRELATED on a ENTITYUNRELATED ENTITYUNRELATED using phonetic ENTITY of ENTITYUNRELATED from the Switchboard ENTITYOTHER .
Confidence Estimation For Information Extraction	Confidence	Estimation	result	{'e1': {'word': 'Confidence', 'word_index': [(0, 0)], 'id': 'N04-4028.1'}, 'e2': {'word': 'Estimation', 'word_index': [(1, 1)], 'id': 'N04-4028.2'}}	ENTITY ENTITYOTHER For ENTITYUNRELATED
Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents 	structured	databases	model-feature	{'e1': {'word': 'structured', 'word_index': [(4, 4)], 'id': 'N04-4028.6'}, 'e2': {'word': 'databases', 'word_index': [(5, 5)], 'id': 'N04-4028.7'}}	ENTITYUNRELATED ENTITYUNRELATED automatically create ENTITY ENTITYOTHER from unstructured ENTITYUNRELATED ENTITYUNRELATED , such as the Web or newswire ENTITYUNRELATED
Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents 	databases	sources	part_whole	{'e1': {'word': 'databases', 'word_index': [(5, 5)], 'id': 'N04-4028.7'}, 'e2': {'word': 'sources', 'word_index': [(9, 9)], 'id': 'N04-4028.9'}}	ENTITYUNRELATED ENTITYUNRELATED automatically create ENTITYUNRELATED ENTITY from unstructured ENTITYUNRELATED ENTITYOTHER , such as the Web or newswire ENTITYUNRELATED
Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents 	sources	data	usage	{'e1': {'word': 'sources', 'word_index': [(9, 9)], 'id': 'N04-4028.9'}, 'e2': {'word': 'data', 'word_index': [(8, 8)], 'id': 'N04-4028.8'}}	ENTITYUNRELATED ENTITYUNRELATED automatically create ENTITYUNRELATED ENTITYUNRELATED from unstructured ENTITYOTHER ENTITY , such as the Web or newswire ENTITYUNRELATED
For many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field .	confidence	system	model-feature	{'e1': {'word': 'confidence', 'word_index': [(12, 12)], 'id': 'N04-4028.15'}, 'e2': {'word': 'system', 'word_index': [(14, 14)], 'id': 'N04-4028.16'}}	For many ENTITYUNRELATED , it is highly desirable to accurately estimate the ENTITY the ENTITYOTHER has in the ENTITYUNRELATED of each ENTITYUNRELATED ENTITYUNRELATED .
We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records .	techniques	precision	result	{'e1': {'word': 'techniques', 'word_index': [(3, 3)], 'id': 'N04-4028.35'}, 'e2': {'word': 'precision', 'word_index': [(20, 20)], 'id': 'N04-4028.41'}}	We ENTITYUNRELATED several ENTITY to estimate the ENTITYUNRELATED of both ENTITYUNRELATED ENTITYUNRELATED and entire ENTITYUNRELATED ENTITYUNRELATED , obtaining an average ENTITYOTHER of 98 % for retrieving correct ENTITYUNRELATED and 87 % for ENTITYUNRELATED ENTITYUNRELATED .
PRC Inc: Description Of The PAKTUS System Used For MUC-3	Description	System	model-feature	{'e1': {'word': 'Description', 'word_index': [(3, 3)], 'id': 'M91-1029.1'}, 'e2': {'word': 'System', 'word_index': [(7, 7)], 'id': 'M91-1029.2'}}	PRC Inc : ENTITY Of The PAKTUS ENTITYOTHER Used For MUC - 3
The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) has been under development as an Independent Research and Development project at PRC since 1984.	Text	Understanding System	usage	{'e1': {'word': 'Text', 'word_index': [(4, 4)], 'id': 'M91-1029.4'}, 'e2': {'word': 'Understanding System', 'word_index': [(5, 5)], 'id': 'M91-1029.5'}}	The PRC Adaptive ENTITYUNRELATED ENTITY ENTITYOTHER ( PAKTUS ) has been under ENTITYUNRELATED as an Independent ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED at PRC since 1984 .
The objective is a generic system of tools , including a core English lexicon , grammar, and concept representations , for building natural language processing (NLP) systems for text understanding .	lexicon	system	part_whole	{'e1': {'word': 'lexicon', 'word_index': [(13, 13)], 'id': 'M91-1029.16'}, 'e2': {'word': 'system', 'word_index': [(5, 5)], 'id': 'M91-1029.11'}}	The ENTITYUNRELATED is a generic ENTITYOTHER of ENTITYUNRELATED , ENTITYUNRELATED a ENTITYUNRELATED ENTITYUNRELATED ENTITY , grammar , and ENTITYUNRELATED ENTITYUNRELATED , for ENTITYUNRELATED ENTITYUNRELATED ( NLP ) ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED .
The objective is a generic system of tools , including a core English lexicon , grammar, and concept representations , for building natural language processing (NLP) systems for text understanding .	systems	understanding	usage	{'e1': {'word': 'systems', 'word_index': [(27, 27)], 'id': 'M91-1029.21'}, 'e2': {'word': 'understanding', 'word_index': [(30, 30)], 'id': 'M91-1029.23'}}	The ENTITYUNRELATED is a generic ENTITYUNRELATED of ENTITYUNRELATED , ENTITYUNRELATED a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , grammar , and ENTITYUNRELATED ENTITYUNRELATED , for ENTITYUNRELATED ENTITYUNRELATED ( NLP ) ENTITY for ENTITYUNRELATED ENTITYOTHER .
Input to the NLP system is typically derived from an existing electronic message stream , such as a news wire.	Input	stream	part_whole	{'e1': {'word': 'Input', 'word_index': [(0, 0)], 'id': 'M91-1029.32'}, 'e2': {'word': 'stream', 'word_index': [(12, 12)], 'id': 'M91-1029.35'}}	ENTITY to the ENTITYUNRELATED is typically derived from an existing electronic ENTITYUNRELATED ENTITYOTHER , such as a ENTITYUNRELATED wire .
The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	system	discourses	usage	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'M91-1029.56'}, 'e2': {'word': 'discourses', 'word_index': [(14, 14)], 'id': 'M91-1029.59'}}	The ENTITYUNRELATED ENTITYUNRELATED is a ENTITY that can ENTITYUNRELATED the ENTITYUNRELATED of relatively long ENTITYOTHER in ENTITYUNRELATED that are fairly broad with a high ENTITYUNRELATED of ENTITYUNRELATED .
The paper describes models for representation and methods to handle lexicographic structures supplied by the	paper	models	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'A92-1041.4'}, 'e2': {'word': 'models', 'word_index': [(3, 3)], 'id': 'A92-1041.5'}}	The ENTITY describes ENTITYOTHER for ENTITYUNRELATED and ENTITYUNRELATED to handle lexicographic ENTITYUNRELATED supplied by the
In this paper , we describe the user needs analysis we performed and how it influenced the development of PLANDoc.	paper	analysis	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'A94-1002.16'}, 'e2': {'word': 'analysis', 'word_index': [(9, 9)], 'id': 'A94-1002.18'}}	In this ENTITY , we describe the ENTITYUNRELATED needs ENTITYOTHER we ENTITYUNRELATED and how it ENTITYUNRELATED the ENTITYUNRELATED of PLANDoc.
The system , designed for large vocabulary natural language tasks , makes use of phonetic Hidden Markov models (HMM) and incorporates acoustic, phonetic, and linguistic sources of knowledge to achieve high recognition performance .	system	tasks	usage	{'e1': {'word': 'system', 'word_index': [(1, 1)], 'id': 'H90-1017.9'}, 'e2': {'word': 'tasks', 'word_index': [(8, 8)], 'id': 'H90-1017.13'}}	The ENTITY , ENTITYUNRELATED for large ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER , makes use of phonetic ENTITYUNRELATED ( HMM ) and incorporates acoustic , phonetic , and linguistic ENTITYUNRELATED of ENTITYUNRELATED to achieve high ENTITYUNRELATED ENTITYUNRELATED .
It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context .	evaluation	input	usage	{'e1': {'word': 'evaluation', 'word_index': [(6, 6)], 'id': 'H90-1028.19'}, 'e2': {'word': 'input', 'word_index': [(16, 16)], 'id': 'H90-1028.23'}}	It was decided that the first ENTITY would be limited in ENTITYUNRELATED to ENTITYUNRELATED with ENTITYUNRELATED ENTITYOTHER only , and to cover only ENTITYUNRELATED that could be understood unambiguously out of ENTITYUNRELATED .
It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context .	sentences	context	model-feature	{'e1': {'word': 'sentences', 'word_index': [(23, 23)], 'id': 'H90-1028.24'}, 'e2': {'word': 'context', 'word_index': [(31, 31)], 'id': 'H90-1028.25'}}	It was decided that the first ENTITYUNRELATED would be limited in ENTITYUNRELATED to ENTITYUNRELATED with ENTITYUNRELATED ENTITYUNRELATED only , and to cover only ENTITY that could be understood unambiguously out of ENTITYOTHER .
This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set .	paper	progress	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'H90-1028.41'}, 'e2': {'word': 'progress', 'word_index': [(4, 4)], 'id': 'H90-1028.42'}}	This ENTITY describes our ENTITYOTHER to ENTITYUNRELATED on this ENTITYUNRELATED , ENTITYUNRELATED an ENTITYUNRELATED of the ENTITYUNRELATED of the ENTITYUNRELATED on the recently released designated DARPA ENTITYUNRELATED .
This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set .	evaluation	performance	topic	{'e1': {'word': 'evaluation', 'word_index': [(13, 13)], 'id': 'H90-1028.46'}, 'e2': {'word': 'performance', 'word_index': [(16, 16)], 'id': 'H90-1028.47'}}	This ENTITYUNRELATED describes our ENTITYUNRELATED to ENTITYUNRELATED on this ENTITYUNRELATED , ENTITYUNRELATED an ENTITY of the ENTITYOTHER of the ENTITYUNRELATED on the recently released designated DARPA ENTITYUNRELATED .
First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task .	description	system	model-feature	{'e1': {'word': 'description', 'word_index': [(6, 6)], 'id': 'H90-1028.52'}, 'e2': {'word': 'system', 'word_index': [(9, 9)], 'id': 'H90-1028.53'}}	First we will give a general ENTITY of the ENTITYOTHER we are ENTITYUNRELATED , emphasizing those ENTITYUNRELATED that differ from the ENTITYUNRELATED general conception of the ENTITYUNRELATED ENTITYUNRELATED .
This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment .	section	parser	topic	{'e1': {'word': 'section', 'word_index': [(5, 5)], 'id': 'H90-1028.68'}, 'e2': {'word': 'parser', 'word_index': [(11, 11)], 'id': 'H90-1028.69'}}	This is followed by a ENTITY describing changes made in the ENTITYOTHER , in the ENTITYUNRELATED of ENTITYUNRELATED , the ENTITYUNRELATED with the back - end , and a preliminary ENTITYUNRELATED ENTITYUNRELATED .
This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences .	section	discussion	topic	{'e1': {'word': 'section', 'word_index': [(1, 1)], 'id': 'H90-1028.75'}, 'e2': {'word': 'discussion', 'word_index': [(6, 6)], 'id': 'H90-1028.77'}}	This ENTITY also ENTITYUNRELATED a brief ENTITYOTHER of some interesting ENTITYUNRELATED that occurred in the ENTITYUNRELATED ENTITYUNRELATED .
This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences .	phenomena	sentences	part_whole	{'e1': {'word': 'phenomena', 'word_index': [(10, 10)], 'id': 'H90-1028.78'}, 'e2': {'word': 'sentences', 'word_index': [(16, 16)], 'id': 'H90-1028.80'}}	This ENTITYUNRELATED also ENTITYUNRELATED a brief ENTITYUNRELATED of some interesting ENTITY that occurred in the ENTITYUNRELATED ENTITYOTHER .
An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system .	section	performance	topic	{'e1': {'word': 'section', 'word_index': [(2, 2)], 'id': 'H90-1028.82'}, 'e2': {'word': 'performance', 'word_index': [(9, 9)], 'id': 'H90-1028.84'}}	An ENTITYUNRELATED ENTITY follows , discussing our ENTITYUNRELATED 's ENTITYOTHER on both ENTITYUNRELATED and ENTITYUNRELATED data , as well as a preliminary ENTITYUNRELATED of the ENTITYUNRELATED of the ENTITYUNRELATED .
"We conclude with a summary of our results and our position on the nature of the common task . """	summary	results	topic	{'e1': {'word': 'summary', 'word_index': [(4, 4)], 'id': 'H90-1028.90'}, 'e2': {'word': 'results', 'word_index': [(7, 7)], 'id': 'H90-1028.91'}}	"We conclude with a ENTITY of our ENTITYOTHER and our position on the ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED . """
 Advanced parsing methodologies , including augmented-LR compilation where knowledge sources ( syntactic grammars, lexicons , and semantic ontologies ) can be defined and maintained separately but are jointly compiled to apply simultaneously at run time , both in parsing and in generation .	compilation	methodologies	part_whole	{'e1': {'word': 'compilation', 'word_index': [(7, 7)], 'id': 'H91-1023.22'}, 'e2': {'word': 'methodologies', 'word_index': [(2, 2)], 'id': 'H91-1023.20'}}	Advanced ENTITYUNRELATED ENTITYOTHER , ENTITYUNRELATED augmented -LR ENTITY where ENTITYUNRELATED ( ENTITYUNRELATED grammars , ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED ) can be defined and maintained separately but are jointly compiled to ENTITYUNRELATED simultaneously at run ENTITYUNRELATED , both in ENTITYUNRELATED and in ENTITYUNRELATED .
 Automated corpus analysis tools , statistical and other means of extracting useful information from large bi- or multi-lingual corpora , including collocations , transfers , and contextual cues for disambiguation .	information	corpora	part_whole	{'e1': {'word': 'information', 'word_index': [(12, 12)], 'id': 'H91-1023.42'}, 'e2': {'word': 'corpora', 'word_index': [(18, 18)], 'id': 'H91-1023.44'}}	Automated ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED and other means of ENTITYUNRELATED useful ENTITY from large bi- or ENTITYUNRELATED ENTITYOTHER , ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED , and contextual ENTITYUNRELATED for ENTITYUNRELATED .
Calculating The Probability Of A Partial Parse Of A Sentence</title>	Probability	Sentence	model-feature	{'e1': {'word': 'Probability', 'word_index': [(2, 2)], 'id': 'H91-1045.1'}, 'e2': {'word': 'Sentence', 'word_index': [(9, 9)], 'id': 'H91-1045.4'}}	Calculating The ENTITY Of A ENTITYUNRELATED ENTITYUNRELATED Of A ENTITYOTHER < / title >
This paper describes our recent efforts to define and construct a model of discourse interaction that handle dialogs that are rich in these natural dialog-related phenomena .	paper	efforts	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'H91-1064.22'}, 'e2': {'word': 'efforts', 'word_index': [(5, 5)], 'id': 'H91-1064.23'}}	This ENTITY describes our recent ENTITYOTHER to define and ENTITYUNRELATED a ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED that handle ENTITYUNRELATED that are rich in these ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
This paper describes our recent efforts to define and construct a model of discourse interaction that handle dialogs that are rich in these natural dialog-related phenomena .	model	discourse	model-feature	{'e1': {'word': 'model', 'word_index': [(11, 11)], 'id': 'H91-1064.25'}, 'e2': {'word': 'discourse', 'word_index': [(13, 13)], 'id': 'H91-1064.26'}}	This ENTITYUNRELATED describes our recent ENTITYUNRELATED to define and ENTITYUNRELATED a ENTITY of ENTITYOTHER ENTITYUNRELATED that handle ENTITYUNRELATED that are rich in these ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
This paper describes our recent efforts to define and construct a model of discourse interaction that handle dialogs that are rich in these natural dialog-related phenomena .	phenomena	dialogs	part_whole	{'e1': {'word': 'phenomena', 'word_index': [(25, 25)], 'id': 'H91-1064.31'}, 'e2': {'word': 'dialogs', 'word_index': [(17, 17)], 'id': 'H91-1064.28'}}	This ENTITYUNRELATED describes our recent ENTITYUNRELATED to define and ENTITYUNRELATED a ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED that handle ENTITYOTHER that are rich in these ENTITYUNRELATED ENTITYUNRELATED ENTITY .
This paper provides a brief introduction to prosody research in the context of human-computer communication and an overview of the contributions of the papers in the session.	paper	introduction	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'H93-1063.2'}, 'e2': {'word': 'introduction', 'word_index': [(5, 5)], 'id': 'H93-1063.4'}}	This ENTITY ENTITYUNRELATED a brief ENTITYOTHER to ENTITYUNRELATED ENTITYUNRELATED in the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED and an ENTITYUNRELATED of the ENTITYUNRELATED of the ENTITYUNRELATED in the session .
"Specifically, the goal is to build a prototype , real-time system capable of processing radio communication between air traffic controllers and pilots , identifying dialogs and extracting their """"gist"""" (e.g., identifying flights, determining whether they are landing or taking off), and producing a continuous output stream with that information ."	system	processing	usage	{'e1': {'word': 'system', 'word_index': [(11, 11)], 'id': 'H93-1078.13'}, 'e2': {'word': 'processing', 'word_index': [(14, 14)], 'id': 'H93-1078.14'}}	"Specifically , the ENTITYUNRELATED is to build a ENTITYUNRELATED , ENTITYUNRELATED ENTITY capable of ENTITYOTHER radio ENTITYUNRELATED between air traffic controllers and ENTITYUNRELATED , identifying ENTITYUNRELATED and ENTITYUNRELATED their "" "" gist "" "" ( e.g. , identifying flights , determining whether they are landing or taking off ) , and producing a continuous ENTITYUNRELATED ENTITYUNRELATED with that ENTITYUNRELATED ."
The system is built upon state-of-the-art techniques in speech recognition , speaker identification , natural language analysis , and topic statistical classification .	techniques	system	usage	{'e1': {'word': 'techniques', 'word_index': [(12, 12)], 'id': 'H93-1078.25'}, 'e2': {'word': 'system', 'word_index': [(1, 1)], 'id': 'H93-1078.24'}}	The ENTITYOTHER is built upon state - of - the - art ENTITY in ENTITYUNRELATED , speaker ENTITYUNRELATED , ENTITYUNRELATED , and ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
The most significant change from earlier versions is a new set of modules that produce a draft translation of the document for the user to refer to or modify.	modules	translation	result	{'e1': {'word': 'modules', 'word_index': [(12, 12)], 'id': 'H94-1029.10'}, 'e2': {'word': 'translation', 'word_index': [(17, 17)], 'id': 'H94-1029.11'}}	The most significant change from earlier ENTITYUNRELATED is a new set of ENTITY that produce a draft ENTITYOTHER of the ENTITYUNRELATED for the ENTITYUNRELATED to refer to or modify .
This paper describes these modules , with special emphasis on an automatically trained lexicalized grammar used in the parsing module .	paper	modules	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'H94-1029.14'}, 'e2': {'word': 'modules', 'word_index': [(4, 4)], 'id': 'H94-1029.15'}}	This ENTITY describes these ENTITYOTHER , with special emphasis on an automatically ENTITYUNRELATED lexicalized grammar used in the ENTITYUNRELATED ENTITYUNRELATED .
Contextual spelling errors are denned as the use of an incorrect, though valid, word in a particular sentence or context 	word	sentence	model-feature	{'e1': {'word': 'word', 'word_index': [(15, 15)], 'id': 'A97-1025.6'}, 'e2': {'word': 'sentence', 'word_index': [(19, 19)], 'id': 'A97-1025.7'}}	Contextual ENTITYUNRELATED ENTITYUNRELATED are denned as the use of an incorrect , though valid , ENTITY in a particular ENTITYOTHER or ENTITYUNRELATED
Traditional spelling checkers flag misspelled words , but they do not typically attempt to identify words that are used incorrectly in a sentence .	words	sentence	part_whole	{'e1': {'word': 'words', 'word_index': [(15, 15)], 'id': 'A97-1025.11'}, 'e2': {'word': 'sentence', 'word_index': [(22, 22)], 'id': 'A97-1025.12'}}	Traditional ENTITYUNRELATED checkers flag misspelled ENTITYUNRELATED , but they do not typically attempt to identify ENTITY that are used incorrectly in a ENTITYOTHER .
The test design and test collection used for document detection in TIPSTER was also used in TREC.	collection	detection	usage	{'e1': {'word': 'collection', 'word_index': [(5, 5)], 'id': 'X96-1046.28'}, 'e2': {'word': 'detection', 'word_index': [(9, 9)], 'id': 'X96-1046.30'}}	The ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED ENTITY used for ENTITYUNRELATED ENTITYOTHER in TIPSTER was also used in TREC .
"The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or """"right answers"""" to those topics ."	documents	collection	part_whole	{'e1': {'word': 'documents', 'word_index': [(8, 8)], 'id': 'X96-1046.40'}, 'e2': {'word': 'collection', 'word_index': [(2, 2)], 'id': 'X96-1046.39'}}	"The ENTITYUNRELATED ENTITYOTHER consists of over 1 million ENTITY from diverse ENTITYUNRELATED ENTITYUNRELATED , 250 ENTITYUNRELATED , and the set of relevant ENTITYUNRELATED or "" "" right answers "" "" to those ENTITYUNRELATED ."
A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics .	topics	collection	part_whole	{'e1': {'word': 'topics', 'word_index': [(22, 22)], 'id': 'X96-1046.47'}, 'e2': {'word': 'collection', 'word_index': [(2, 2)], 'id': 'X96-1046.46'}}	A Spanish ENTITYOTHER has been built and used during TREC - 3 and TREC - 4 , with a total of 50 ENTITY .
TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ).	size	collection	model-feature	{'e1': {'word': 'size', 'word_index': [(15, 15)], 'id': 'X96-1046.52'}, 'e2': {'word': 'collection', 'word_index': [(19, 19)], 'id': 'X96-1046.54'}}	TREC-1 ENTITYUNRELATED significant ENTITYUNRELATED rebuilding by most groups ENTITYUNRELATED to the huge ENTITYUNRELATED in the ENTITY of the ENTITYUNRELATED ENTITYOTHER ( from a traditional ENTITYUNRELATED ENTITYUNRELATED of several megabytes in ENTITYUNRELATED to the 2 gigabyte TIPSTER ENTITYUNRELATED ) .
TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ).	size	collection	model-feature	{'e1': {'word': 'size', 'word_index': [(30, 30)], 'id': 'X96-1046.57'}, 'e2': {'word': 'collection', 'word_index': [(25, 25)], 'id': 'X96-1046.56'}}	TREC-1 ENTITYUNRELATED significant ENTITYUNRELATED rebuilding by most groups ENTITYUNRELATED to the huge ENTITYUNRELATED in the ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED ( from a traditional ENTITYUNRELATED ENTITYOTHER of several megabytes in ENTITY to the 2 gigabyte TIPSTER ENTITYUNRELATED ) .
The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries .	techniques	experiments	part_whole	{'e1': {'word': 'techniques', 'word_index': [(12, 12)], 'id': 'X96-1046.74'}, 'e2': {'word': 'experiments', 'word_index': [(2, 2)], 'id': 'X96-1046.69'}}	The major ENTITYOTHER in TREC -3 ENTITYUNRELATED the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITY , the use of passages or sub-documents to ENTITYUNRELATED the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED , and the use of the ENTITYUNRELATED ENTITYUNRELATED to select only the best ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED .
These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure.	techniques	results	usage	{'e1': {'word': 'techniques', 'word_index': [(35, 35)], 'id': 'X96-1046.112'}, 'e2': {'word': 'results', 'word_index': [(38, 38)], 'id': 'X96-1046.113'}}	These were added to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED on certain known ENTITYUNRELATED ENTITYUNRELATED , and ENTITYUNRELATED such ENTITYUNRELATED as investigating ENTITYUNRELATED as an interactive ENTITYUNRELATED by examining the ENTITYUNRELATED as well as the ENTITYUNRELATED , investigating ENTITY for merging ENTITYOTHER from the various TREC subcollections , examining the ENTITYUNRELATED of corrupted data , and ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED using a specific ENTITYUNRELATED measure .
Acquiring Predicate- Argument Mapping Information From Multilingual Texts</title>	Information	Texts	part_whole	{'e1': {'word': 'Information', 'word_index': [(5, 5)], 'id': 'W93-0110.3'}, 'e2': {'word': 'Texts', 'word_index': [(8, 8)], 'id': 'W93-0110.4'}}	Acquiring Predicate - ENTITYUNRELATED ENTITYUNRELATED ENTITY From Multilingual ENTITYOTHER < / title >
This paper discusses automatic acquisition of predicate-argument mapping information from multilingual texts .	paper	acquisition	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W93-0110.5'}, 'e2': {'word': 'acquisition', 'word_index': [(4, 4)], 'id': 'W93-0110.7'}}	This ENTITY discusses ENTITYUNRELATED ENTITYOTHER of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED from multilingual ENTITYUNRELATED .
This paper discusses automatic acquisition of predicate-argument mapping information from multilingual texts .	information	texts	part_whole	{'e1': {'word': 'information', 'word_index': [(8, 8)], 'id': 'W93-0110.10'}, 'e2': {'word': 'texts', 'word_index': [(11, 11)], 'id': 'W93-0110.11'}}	This ENTITYUNRELATED discusses ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITY from multilingual ENTITYOTHER .
In achieving this result , I offer a characterisation of Underspecification Theory and Optimality Theory in terms of their methods for ordering defaults .	methods	defaults	usage	{'e1': {'word': 'methods', 'word_index': [(19, 19)], 'id': 'W94-0203.10'}, 'e2': {'word': 'defaults', 'word_index': [(22, 22)], 'id': 'W94-0203.12'}}	In achieving this ENTITYUNRELATED , I offer a characterisation of Underspecification ENTITYUNRELATED and Optimality ENTITYUNRELATED in ENTITYUNRELATED of their ENTITY for ENTITYUNRELATED ENTITYOTHER .
In this paper , a treatment of Czech phonological rules in two-level morphology approach is described .	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W97-1106.2'}, 'e2': {'word': 'approach', 'word_index': [(13, 13)], 'id': 'W97-1106.7'}}	In this ENTITY , a ENTITYUNRELATED of Czech phonological ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER is described .
This paper summarizes several initiatives at MITRE that are investigating the visualization of a range of content .	paper	initiatives	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W98-0208.3'}, 'e2': {'word': 'initiatives', 'word_index': [(4, 4)], 'id': 'W98-0208.4'}}	This ENTITY summarizes several ENTITYOTHER at MITRE that are investigating the ENTITYUNRELATED of a range of ENTITYUNRELATED .
tonymie expressions with significant accuracy , both for a pre-deterrnined inventory of metonymie types and for previously unseen cases .	types	inventory	part_whole	{'e1': {'word': 'types', 'word_index': [(13, 13)], 'id': 'W98-0613.14'}, 'e2': {'word': 'inventory', 'word_index': [(10, 10)], 'id': 'W98-0613.13'}}	tonymie ENTITYUNRELATED with significant ENTITYUNRELATED , both for a pre-deterrnined ENTITYOTHER of metonymie ENTITY and for previously unseen ENTITYUNRELATED .
This paper argues for the usefulness of multimodal spoken language corpora and specifies components of a platform for the creation , maintenance and exploitation of such corpora .	paper	corpora	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W98-0802.3'}, 'e2': {'word': 'corpora', 'word_index': [(10, 10)], 'id': 'W98-0802.6'}}	This ENTITY argues for the ENTITYUNRELATED of multimodal spoken ENTITYUNRELATED ENTITYOTHER and specifies ENTITYUNRELATED of a ENTITYUNRELATED for the ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED of such ENTITYUNRELATED .
This paper argues for the usefulness of multimodal spoken language corpora and specifies components of a platform for the creation , maintenance and exploitation of such corpora .	components	platform	part_whole	{'e1': {'word': 'components', 'word_index': [(13, 13)], 'id': 'W98-0802.7'}, 'e2': {'word': 'platform', 'word_index': [(16, 16)], 'id': 'W98-0802.8'}}	This ENTITYUNRELATED argues for the ENTITYUNRELATED of multimodal spoken ENTITYUNRELATED ENTITYUNRELATED and specifies ENTITY of a ENTITYOTHER for the ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED of such ENTITYUNRELATED .
TransTool is a transcription editor meant to facilitate and partially automate the task of a human transcriber, while SyncTool is a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations (e.g., text , audio, video , acoustic analysis ).	tool	transcriptions	usage	{'e1': {'word': 'tool', 'word_index': [(23, 23)], 'id': 'W98-0802.19'}, 'e2': {'word': 'transcriptions', 'word_index': [(28, 28)], 'id': 'W98-0802.21'}}	Trans Tool is a ENTITYUNRELATED editor meant to facilitate and partially automate the ENTITYUNRELATED of a human transcriber , while SyncTool is a ENTITY for aligning the ENTITYUNRELATED ENTITYOTHER with a digitized audio and ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED to allow synchronized ENTITYUNRELATED of different ENTITYUNRELATED ( e.g. , ENTITYUNRELATED , audio , ENTITYUNRELATED , acoustic ENTITYUNRELATED ) .
Finally, a brief comparison is made between these tools and other programs developed for similar purposes .	tools	programs	compare	{'e1': {'word': 'tools', 'word_index': [(9, 9)], 'id': 'W98-0802.31'}, 'e2': {'word': 'programs', 'word_index': [(12, 12)], 'id': 'W98-0802.32'}}	Finally , a brief ENTITYUNRELATED is made between these ENTITY and other ENTITYOTHER ENTITYUNRELATED for similar ENTITYUNRELATED .
Discovering Lexical Information By Tagging Arabic Newspaper Text</title>	Tagging	Text	usage	{'e1': {'word': 'Tagging', 'word_index': [(3, 3)], 'id': 'W98-1001.2'}, 'e2': {'word': 'Text', 'word_index': [(6, 6)], 'id': 'W98-1001.4'}}	Discovering ENTITYUNRELATED By ENTITY Arabic ENTITYUNRELATED ENTITYOTHER < / title >
In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text .	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W98-1001.5'}, 'e2': {'word': 'system', 'word_index': [(6, 6)], 'id': 'W98-1001.6'}}	In this ENTITY we describe a ENTITYOTHER for building an Arabic ENTITYUNRELATED automatically by ENTITYUNRELATED Arabic ENTITYUNRELATED ENTITYUNRELATED .
In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text .	tagging	text	usage	{'e1': {'word': 'tagging', 'word_index': [(14, 14)], 'id': 'W98-1001.8'}, 'e2': {'word': 'text', 'word_index': [(17, 17)], 'id': 'W98-1001.10'}}	In this ENTITYUNRELATED we describe a ENTITYUNRELATED for building an Arabic ENTITYUNRELATED automatically by ENTITY Arabic ENTITYUNRELATED ENTITYOTHER .
In this system we are using several techniques for tagging the words in the text and figuring out their types and their features .	techniques	system	part_whole	{'e1': {'word': 'techniques', 'word_index': [(7, 7)], 'id': 'W98-1001.12'}, 'e2': {'word': 'system', 'word_index': [(2, 2)], 'id': 'W98-1001.11'}}	In this ENTITYOTHER we are using several ENTITY for ENTITYUNRELATED the ENTITYUNRELATED in the ENTITYUNRELATED and ENTITYUNRELATED out their ENTITYUNRELATED and their ENTITYUNRELATED .
In this system we are using several techniques for tagging the words in the text and figuring out their types and their features .	tagging	words	usage	{'e1': {'word': 'tagging', 'word_index': [(9, 9)], 'id': 'W98-1001.13'}, 'e2': {'word': 'words', 'word_index': [(11, 11)], 'id': 'W98-1001.14'}}	In this ENTITYUNRELATED we are using several ENTITYUNRELATED for ENTITY the ENTITYOTHER in the ENTITYUNRELATED and ENTITYUNRELATED out their ENTITYUNRELATED and their ENTITYUNRELATED .
The paper describes a new formal framework for comparison , design and standardization of annotation schemes for dialogue acts .	paper	framework	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W99-0310.5'}, 'e2': {'word': 'framework', 'word_index': [(6, 6)], 'id': 'W99-0310.6'}}	The ENTITY describes a new formal ENTITYOTHER for ENTITYUNRELATED , ENTITYUNRELATED and standardization of annotation ENTITYUNRELATED for ENTITYUNRELATED .
Mani , Inderjeet; House, David ; Klein , Gary ; Hirschman , Lynette ; Firmin Hand , Therese ; Sundheim , Beth M. ,The TIPSTER SUMMAC Text Summarization Evaluation ,Conference Of The European Association For Computation al Linguistics ,1999 *** Nagao , Katashi; Hasida, Koiti, Automatic Text Summarization Based on the Global Document Annotation,COLING-ACL,1998***Power, Richard ; Scott , Donia R.,Multilingual Authoring using Feedback Texts ,COLING-ACL,1998***Mani, Inderjeet; Gates , Barbara ; Bloedorn , Eric ,Improving Summaries By Revising Them,Annual Meeting Of The Association For Computation al Linguistics ,1999	Document	Automatic	usage	{'e1': {'word': 'Document', 'word_index': [(58, 58)], 'id': 'W00-0409.12'}, 'e2': {'word': 'Automatic', 'word_index': [(52, 52)], 'id': 'W00-0409.9'}}	Mani , Inderjeet ; House , David ; Klein , Gary ; Hirschman , Lynette ; Firmin ENTITYUNRELATED , Therese ; Sundheim , Beth M. , The TIPSTER SUMMAC ENTITYUNRELATED ENTITYUNRELATED , Conference Of The European ENTITYUNRELATED For ENTITYUNRELATED al ENTITYUNRELATED , 1999 *** Nagao , Katashi ; Hasida , Koiti , ENTITYOTHER ENTITYUNRELATED ENTITYUNRELATED on the Global ENTITY Annotation , COLING-ACL ,1998 *** Power , Richard ; Scott , Donia R. , Multilingual Authoring using ENTITYUNRELATED ENTITYUNRELATED , COLING -ACL , 1998 *** Mani , Inderjeet ; Gates , Barbara ; Bloedorn , Eric , Improving Summaries By Revising Them , Annual Meeting Of The ENTITYUNRELATED For ENTITYUNRELATED al ENTITYUNRELATED , 1999
We introduce CST ( cross-document structure theory ), a paradigm for multi-document analysis .	paradigm	analysis	usage	{'e1': {'word': 'paradigm', 'word_index': [(10, 10)], 'id': 'W00-1009.14'}, 'e2': {'word': 'analysis', 'word_index': [(13, 13)], 'id': 'W00-1009.16'}}	We introduce CST ( ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ) , a ENTITY for ENTITYUNRELATED ENTITYOTHER .
CST takes into account the rhetorical structure of clusters of related textual documents .	clusters	documents	model-feature	{'e1': {'word': 'clusters', 'word_index': [(8, 8)], 'id': 'W00-1009.18'}, 'e2': {'word': 'documents', 'word_index': [(12, 12)], 'id': 'W00-1009.19'}}	CST takes into account the rhetorical ENTITYUNRELATED of ENTITY of related textual ENTITYOTHER .
We present a taxonomy of cross-document relationships .	taxonomy	relationships	model-feature	{'e1': {'word': 'taxonomy', 'word_index': [(3, 3)], 'id': 'W00-1009.20'}, 'e2': {'word': 'relationships', 'word_index': [(6, 6)], 'id': 'W00-1009.22'}}	We present a ENTITY of ENTITYUNRELATED ENTITYOTHER .
Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types , 93.60% and 94.64% for noun phrases , and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types , 96.49% and 96.99% for noun phrases , and 97.13% and 97.36% for verb phrases .	technique	rates	result	{'e1': {'word': 'technique', 'word_index': [(4, 4)], 'id': 'W00-1309.27'}, 'e2': {'word': 'rates', 'word_index': [(10, 10)], 'id': 'W00-1309.30'}}	ENTITYUNRELATED show that this ENTITY achieves overall ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER of 93.40 % and 93.95 % for all ENTITYUNRELATED ENTITYUNRELATED , 93.60 % and 94.64 % for ENTITYUNRELATED , and 94.64 % and 94.75 % for ENTITYUNRELATED ENTITYUNRELATED when ENTITYUNRELATED on PENN WSJ TreeBank ENTITYUNRELATED 00 - 19 and ENTITYUNRELATED on ENTITYUNRELATED 20 - 24 , while 25 - fold ENTITYUNRELATED ENTITYUNRELATED of PENN WSJ TreeBank show overall ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED of 96.40 % and 96.47 % for all ENTITYUNRELATED ENTITYUNRELATED , 96.49 % and 96.99 % for ENTITYUNRELATED , and 97.13 % and 97.36 % for ENTITYUNRELATED ENTITYUNRELATED .
Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types , 93.60% and 94.64% for noun phrases , and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types , 96.49% and 96.99% for noun phrases , and 97.13% and 97.36% for verb phrases .	experiments	rates	result	{'e1': {'word': 'experiments', 'word_index': [(62, 62)], 'id': 'W00-1309.41'}, 'e2': {'word': 'rates', 'word_index': [(72, 72)], 'id': 'W00-1309.44'}}	ENTITYUNRELATED show that this ENTITYUNRELATED achieves overall ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED of 93.40 % and 93.95 % for all ENTITYUNRELATED ENTITYUNRELATED , 93.60 % and 94.64 % for ENTITYUNRELATED , and 94.64 % and 94.75 % for ENTITYUNRELATED ENTITYUNRELATED when ENTITYUNRELATED on PENN WSJ TreeBank ENTITYUNRELATED 00 - 19 and ENTITYUNRELATED on ENTITYUNRELATED 20 - 24 , while 25 - fold ENTITYUNRELATED ENTITY of PENN WSJ TreeBank show overall ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER of 96.40 % and 96.47 % for all ENTITYUNRELATED ENTITYUNRELATED , 96.49 % and 96.99 % for ENTITYUNRELATED , and 97.13 % and 97.36 % for ENTITYUNRELATED ENTITYUNRELATED .
This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora .	paper	hypothesis	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W00-1326.5'}, 'e2': {'word': 'hypothesis', 'word_index': [(8, 8)], 'id': 'W00-1326.8'}}	This ENTITY revisits the one ENTITYUNRELATED per ENTITYUNRELATED ENTITYOTHER using fine - grained ENTITYUNRELATED ENTITYUNRELATED and two different ENTITYUNRELATED .
We also show that one sense per collocation does hold across corpora , but that collocations vary from one corpus to the other, following genre and topic variations .	genre	collocations	result	{'e1': {'word': 'genre', 'word_index': [(25, 25)], 'id': 'W00-1326.22'}, 'e2': {'word': 'collocations', 'word_index': [(15, 15)], 'id': 'W00-1326.20'}}	We also show that one ENTITYUNRELATED per ENTITYUNRELATED does hold across ENTITYUNRELATED , but that ENTITYOTHER vary from one ENTITYUNRELATED to the other , following ENTITY and ENTITYUNRELATED ENTITYUNRELATED .
This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm ( Brill, 1995 ) to be applied to multiple classification tasks by training jointly and simultaneously on all fields .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W01-0701.3'}, 'e2': {'word': 'method', 'word_index': [(5, 5)], 'id': 'W01-0701.4'}}	This ENTITY presents a novel ENTITYOTHER that allows a ENTITYUNRELATED learning ENTITYUNRELATED following the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ( Brill , 1995 ) to be ENTITYUNRELATED to multiple ENTITYUNRELATED by ENTITYUNRELATED jointly and simultaneously on all ENTITYUNRELATED .
This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm ( Brill, 1995 ) to be applied to multiple classification tasks by training jointly and simultaneously on all fields .	paradigm	algorithm	usage	{'e1': {'word': 'paradigm', 'word_index': [(16, 16)], 'id': 'W01-0701.9'}, 'e2': {'word': 'algorithm', 'word_index': [(11, 11)], 'id': 'W01-0701.6'}}	This ENTITYUNRELATED presents a novel ENTITYUNRELATED that allows a ENTITYUNRELATED learning ENTITYOTHER following the ENTITYUNRELATED ENTITYUNRELATED ENTITY ( Brill , 1995 ) to be ENTITYUNRELATED to multiple ENTITYUNRELATED by ENTITYUNRELATED jointly and simultaneously on all ENTITYUNRELATED .
The proposed algorithm is evaluated in two experiments : in one, the system is used to jointly predict the part-of-speech and text chunks /baseNP chunks of an English corpus ; and in the second it is used to learn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese .	system	corpus	usage	{'e1': {'word': 'system', 'word_index': [(13, 13)], 'id': 'W01-0701.32'}, 'e2': {'word': 'corpus', 'word_index': [(30, 30)], 'id': 'W01-0701.38'}}	The ENTITYUNRELATED ENTITYUNRELATED is ENTITYUNRELATED in two ENTITYUNRELATED : in one , the ENTITY is used to jointly predict the ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED / baseNP ENTITYUNRELATED of an ENTITYUNRELATED ENTITYOTHER ; and in the second it is used to learn the joint ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED .
The proposed algorithm is evaluated in two experiments : in one, the system is used to jointly predict the part-of-speech and text chunks /baseNP chunks of an English corpus ; and in the second it is used to learn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese .	tagging	Chinese	usage	{'e1': {'word': 'tagging', 'word_index': [(50, 50)], 'id': 'W01-0701.44'}, 'e2': {'word': 'Chinese', 'word_index': [(52, 52)], 'id': 'W01-0701.45'}}	The ENTITYUNRELATED ENTITYUNRELATED is ENTITYUNRELATED in two ENTITYUNRELATED : in one , the ENTITYUNRELATED is used to jointly predict the ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED / baseNP ENTITYUNRELATED of an ENTITYUNRELATED ENTITYUNRELATED ; and in the second it is used to learn the joint ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED ENTITY for ENTITYOTHER .
We studied contrast and variability in a corpus of gene names to identify potential heuristics for use in performing entity identification in the molecular biology domain .	names	corpus	part_whole	{'e1': {'word': 'names', 'word_index': [(10, 10)], 'id': 'W02-0303.9'}, 'e2': {'word': 'corpus', 'word_index': [(7, 7)], 'id': 'W02-0303.7'}}	We ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED in a ENTITYOTHER of ENTITYUNRELATED ENTITY to identify potential heuristics for use in ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED in the molecular ENTITYUNRELATED ENTITYUNRELATED .
We describe the use of a suite of highly flexible xml-based nlp tools in a project for processing and interpreting text in the medical domain .	processing	text	usage	{'e1': {'word': 'processing', 'word_index': [(19, 19)], 'id': 'W02-1706.5'}, 'e2': {'word': 'text', 'word_index': [(22, 22)], 'id': 'W02-1706.6'}}	We describe the use of a ENTITYUNRELATED of highly flexible xml - based nlp ENTITYUNRELATED in a ENTITYUNRELATED for ENTITY and interpreting ENTITYOTHER in the medical ENTITYUNRELATED .
The main aim of the paper is to demonstrate the central role that xml mark-up and xml nlp tools have played in the analysis process and to describe the resultant annotated corpus of medline abstracts .	paper	tools	topic	{'e1': {'word': 'paper', 'word_index': [(5, 5)], 'id': 'W02-1706.9'}, 'e2': {'word': 'tools', 'word_index': [(20, 20)], 'id': 'W02-1706.11'}}	The ENTITYUNRELATED aim of the ENTITY is to demonstrate the central ENTITYUNRELATED that xml mark - up and xml nlp ENTITYOTHER have played in the ENTITYUNRELATED ENTITYUNRELATED and to describe the resultant annotated ENTITYUNRELATED of medline ENTITYUNRELATED .
The main aim of the paper is to demonstrate the central role that xml mark-up and xml nlp tools have played in the analysis process and to describe the resultant annotated corpus of medline abstracts .	abstracts	corpus	part_whole	{'e1': {'word': 'abstracts', 'word_index': [(36, 36)], 'id': 'W02-1706.15'}, 'e2': {'word': 'corpus', 'word_index': [(33, 33)], 'id': 'W02-1706.14'}}	The ENTITYUNRELATED aim of the ENTITYUNRELATED is to demonstrate the central ENTITYUNRELATED that xml mark - up and xml nlp ENTITYUNRELATED have played in the ENTITYUNRELATED ENTITYUNRELATED and to describe the resultant annotated ENTITYOTHER of medline ENTITY .
In addition to the xml tools , we have succeeded in integrating a variety of non-xml 'off the shelf' nlp tools into our pipelines , so that their output is added into the mark-up.	tools	pipelines	part_whole	{'e1': {'word': 'tools', 'word_index': [(23, 23)], 'id': 'W02-1706.19'}, 'e2': {'word': 'pipelines', 'word_index': [(26, 26)], 'id': 'W02-1706.20'}}	In ENTITYUNRELATED to the xml ENTITYUNRELATED , we have succeeded in integrating a ENTITYUNRELATED of non - xml 'off the shelf ' nlp ENTITY into our ENTITYOTHER , so that their ENTITYUNRELATED is added into the mark - up .
This paper presents the experimental results of our attemps to reduce the size of the parameter space in word alignment algorithm .	paper	results	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W03-0305.4'}, 'e2': {'word': 'results', 'word_index': [(5, 5)], 'id': 'W03-0305.6'}}	This ENTITY presents the ENTITYUNRELATED ENTITYOTHER of our attemps to reduce the ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED .
"""We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction , focusing initially on the Brown corpus ."	extracting	texts	usage	{'e1': {'word': 'extracting', 'word_index': [(7, 7)], 'id': 'W03-0902.9'}, 'e2': {'word': 'texts', 'word_index': [(12, 12)], 'id': 'W03-0902.11'}}	""" We have been ENTITYUNRELATED ENTITYUNRELATED for ENTITY general ENTITYUNRELATED from miscellaneous ENTITYOTHER by a ENTITYUNRELATED of approximate ENTITYUNRELATED and ENTITYUNRELATED , ENTITYUNRELATED initially on the ENTITYUNRELATED ."
"We apply interpretive rules to clausal patterns and patterns of modification , and concurrently abstract general """"possi-bilistic"""" propositions from the resulting formulas ."	rules	patterns	usage	{'e1': {'word': 'rules', 'word_index': [(3, 3)], 'id': 'W03-0902.18'}, 'e2': {'word': 'patterns', 'word_index': [(6, 6)], 'id': 'W03-0902.19'}}	"We ENTITYUNRELATED interpretive ENTITY to clausal ENTITYOTHER and ENTITYUNRELATED of ENTITYUNRELATED , and concurrently ENTITYUNRELATED general "" "" possi-bilistic "" "" propositions from the ENTITYUNRELATED ENTITYUNRELATED ."
The second topic of this paper is classifier combination .	paper	combination	topic	{'e1': {'word': 'paper', 'word_index': [(5, 5)], 'id': 'W03-1026.26'}, 'e2': {'word': 'combination', 'word_index': [(8, 8)], 'id': 'W03-1026.28'}}	The second ENTITYUNRELATED of this ENTITY is ENTITYUNRELATED ENTITYOTHER .
We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs .	classifiers	recognition	usage	{'e1': {'word': 'classifiers', 'word_index': [(5, 5)], 'id': 'W03-1026.29'}, 'e2': {'word': 'recognition', 'word_index': [(10, 10)], 'id': 'W03-1026.33'}}	We present and describe four ENTITY for ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER and describe various ENTITYUNRELATED for combining their ENTITYUNRELATED .
The results demonstrate that classifier combination is an effective technique of improving system performance : experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error .	combination	improving	usage	{'e1': {'word': 'combination', 'word_index': [(5, 5)], 'id': 'W03-1026.38'}, 'e2': {'word': 'improving', 'word_index': [(11, 11)], 'id': 'W03-1026.40'}}	The ENTITYUNRELATED demonstrate that ENTITYUNRELATED ENTITY is an effective ENTITYUNRELATED of ENTITYOTHER ENTITYUNRELATED ENTITYUNRELATED : ENTITYUNRELATED over a large annotated ENTITYUNRELATED of fine - grained ENTITYUNRELATED exhibit a 10 % ENTITYUNRELATED ENTITYUNRELATED in F-measure ENTITYUNRELATED .
The results demonstrate that classifier combination is an effective technique of improving system performance : experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error .	experiments	reduction	result	{'e1': {'word': 'experiments', 'word_index': [(15, 15)], 'id': 'W03-1026.43'}, 'e2': {'word': 'reduction', 'word_index': [(31, 31)], 'id': 'W03-1026.47'}}	The ENTITYUNRELATED demonstrate that ENTITYUNRELATED ENTITYUNRELATED is an effective ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED : ENTITY over a large annotated ENTITYUNRELATED of fine - grained ENTITYUNRELATED exhibit a 10 % ENTITYUNRELATED ENTITYOTHER in F-measure ENTITYUNRELATED .
In this paper we study the effectiveness of applying sentence compression on an extraction based multi-document summarization system .	paper	compression	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W03-1101.7'}, 'e2': {'word': 'compression', 'word_index': [(10, 10)], 'id': 'W03-1101.12'}}	In this ENTITY we ENTITYUNRELATED the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER on an ENTITYUNRELATED ENTITYUNRELATED multi-document ENTITYUNRELATED .
Our results show that pure syntactic-based compression does not improve system performance .	compression	performance	result	{'e1': {'word': 'compression', 'word_index': [(6, 6)], 'id': 'W03-1101.18'}, 'e2': {'word': 'performance', 'word_index': [(11, 11)], 'id': 'W03-1101.21'}}	Our ENTITYUNRELATED show that pure ENTITYUNRELATED ENTITY does not ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
In an evaluation on the MUC-6 coreference resolution corpus , the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation .	evaluation	corpus	usage	{'e1': {'word': 'evaluation', 'word_index': [(2, 2)], 'id': 'W99-0611.11'}, 'e2': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'W99-0611.13'}}	In an ENTITY on the MUC -6 ENTITYUNRELATED ENTITYOTHER , the ENTITYUNRELATED achieves an F-measure of 53.6 % , placing it firmly between the worst ( 40 % ) and best ( 65 % ) ENTITYUNRELATED in the MUC -6 ENTITYUNRELATED .
More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem .	approach	system	compare	{'e1': {'word': 'approach', 'word_index': [(5, 5)], 'id': 'W99-0611.18'}, 'e2': {'word': 'system', 'word_index': [(11, 11)], 'id': 'W99-0611.19'}}	More importantly , the ENTITYUNRELATED ENTITY outperforms the only MUC -6 ENTITYOTHER to treat ENTITYUNRELATED as a learning ENTITYUNRELATED .
The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes .	preferences	partitioning	usage	{'e1': {'word': 'preferences', 'word_index': [(19, 19)], 'id': 'W99-0611.30'}, 'e2': {'word': 'partitioning', 'word_index': [(22, 22)], 'id': 'W99-0611.31'}}	The ENTITYUNRELATED ENTITYUNRELATED appears to ENTITYUNRELATED a flexible ENTITYUNRELATED for coordinating the ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED and ENTITY for accurate ENTITYOTHER of ENTITYUNRELATED into coreference ENTITYUNRELATED .
A Plan- Based Analysis Of Indirect Speech Act</title>	Analysis	Speech Act	topic	{'e1': {'word': 'Analysis', 'word_index': [(4, 4)], 'id': 'J80-3003.2'}, 'e2': {'word': 'Speech Act', 'word_index': [(7, 7)], 'id': 'J80-3003.3'}}	A Plan - ENTITYUNRELATED ENTITY Of Indirect ENTITYOTHER < / title >
"	""Schubert , Lenhart K.; Pelletier , Francis Jeffry ,From English To Logic : Context- Free Computation Of """"Conventional"""" Logical Translation ,American Journal Of Computation al Linguistics ,1982 *** Hobbs , Jerry R. ,An Improper Treatment Of Quantification In Ordinary English ,Annual Meeting Of The Association For Computation al Linguistics ,1983 *** Pollack , Martha E. ; Pereira , Fernando ,An Integrated Framework For Semantic And Pragmatic Interpretation ,Annual Meeting Of The Association For Computation al Linguistics ,1988 *** Alshawi , Hiyan ; Van Eijck , Jan,Logical Forms In The Core Language Engine ,Annual Meeting Of The Association For Computation al Linguistics ,1989 """	Framework	Interpretation	usage	{'e1': {'word': 'Framework', 'word_index': [(74, 74)], 'id': 'J92-4002.17'}, 'e2': {'word': 'Interpretation', 'word_index': [(79, 79)], 'id': 'J92-4002.19'}}	""" Schubert , Lenhart K. ; Pelletier , Francis Jeffry , From ENTITYUNRELATED To ENTITYUNRELATED : ENTITYUNRELATED Free ENTITYUNRELATED Of "" "" Conventional "" "" Logical ENTITYUNRELATED , American ENTITYUNRELATED Of ENTITYUNRELATED al ENTITYUNRELATED , 1982 *** Hobbs , Jerry R. , An Improper ENTITYUNRELATED Of ENTITYUNRELATED In Ordinary ENTITYUNRELATED , Annual Meeting Of The ENTITYUNRELATED For ENTITYUNRELATED al ENTITYUNRELATED , 1983 *** Pollack , Martha E. ; Pereira , Fernando , An Integrated ENTITY For ENTITYUNRELATED And Pragmatic ENTITYOTHER , Annual Meeting Of The ENTITYUNRELATED For ENTITYUNRELATED al ENTITYUNRELATED , 1988 *** Alshawi , Hiyan ; ENTITYUNRELATED Eijck , Jan , Logical Forms In The ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , Annual Meeting Of The ENTITYUNRELATED For ENTITYUNRELATED al ENTITYUNRELATED , 1989 """
Extracting The Lowest- Frequency Words : Pitfalls And Possibilitie	Extracting	Words	usage	{'e1': {'word': 'Extracting', 'word_index': [(0, 0)], 'id': 'J00-3001.1'}, 'e2': {'word': 'Words', 'word_index': [(5, 5)], 'id': 'J00-3001.3'}}	ENTITY The Lowest - ENTITYUNRELATED ENTITYOTHER : ENTITYUNRELATED And Possibilitie
Tested accuracy of our approach on this corpus exceeds 92% , demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample .	approach	accuracy	result	{'e1': {'word': 'approach', 'word_index': [(4, 4)], 'id': 'P98-2228.30'}, 'e2': {'word': 'accuracy', 'word_index': [(1, 1)], 'id': 'P98-2228.29'}}	Tested ENTITYOTHER of our ENTITY on this ENTITYUNRELATED exceeds 92 % , demonstrating the viability of ENTITYUNRELATED ENTITYUNRELATED rather than restricting oneself to a small ENTITYUNRELATED .
For realisation , it turns out that in some cases higher attribute selection accuracy leads to larger differences between system-generated and human descriptions .	selection	differences	result	{'e1': {'word': 'selection', 'word_index': [(12, 12)], 'id': 'W08-1138.17'}, 'e2': {'word': 'differences', 'word_index': [(17, 17)], 'id': 'W08-1138.19'}}	For ENTITYUNRELATED , it turns out that in some ENTITYUNRELATED higher attribute ENTITY ENTITYUNRELATED leads to larger ENTITYOTHER between ENTITYUNRELATED and human ENTITYUNRELATED .
In this paper we attempt to apply the IBM algorithm , BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output .	algorithm	output	usage	{'e1': {'word': 'algorithm', 'word_index': [(9, 9)], 'id': 'W03-2805.3'}, 'e2': {'word': 'output', 'word_index': [(15, 15)], 'id': 'W03-2805.4'}}	In this ENTITYUNRELATED we attempt to ENTITYUNRELATED the IBM ENTITY , BLEU , to the ENTITYOTHER of four different summarizers in ENTITYUNRELATED to ENTITYUNRELATED an intrinsic ENTITYUNRELATED of their ENTITYUNRELATED .
In this paper we attempt to apply the IBM algorithm , BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output .	evaluation	output	topic	{'e1': {'word': 'evaluation', 'word_index': [(26, 26)], 'id': 'W03-2805.7'}, 'e2': {'word': 'output', 'word_index': [(29, 29)], 'id': 'W03-2805.8'}}	In this ENTITYUNRELATED we attempt to ENTITYUNRELATED the IBM ENTITYUNRELATED , BLEU , to the ENTITYUNRELATED of four different summarizers in ENTITYUNRELATED to ENTITYUNRELATED an intrinsic ENTITY of their ENTITYOTHER .
Many acoustic approaches to prosodic labeling in English have employed only local classifiers , although text-based classification has employed some sequential models .	models	classification	usage	{'e1': {'word': 'models', 'word_index': [(21, 21)], 'id': 'I08-1029.8'}, 'e2': {'word': 'classification', 'word_index': [(16, 16)], 'id': 'I08-1029.7'}}	Many acoustic ENTITYUNRELATED to prosodic labeling in ENTITYUNRELATED have employed only local ENTITYUNRELATED , although ENTITYUNRELATED ENTITYOTHER has employed some sequential ENTITY .
"Integration of lexical and prosodic features improves pitch accent prediction over either feature set alone, and for lower accuracy feature sets , factorial CRF <entity id=""I08-1029.28"">models</entity> can <entity id=""I08-1029.29"">improve</entity> over linear <entity id=""I08-1029.30"">chain</entity> <entity id=""I08-1029.31"">based</entity> <entity id=""I08-1029.32"">prediction</entity> of <entity id=""I08-1029.33"">pitch</entity> accent."	Integration	prediction	result	{'e1': {'word': 'Integration', 'word_index': [(0, 0)], 'id': 'I08-1029.18'}, 'e2': {'word': 'prediction', 'word_index': [(9, 9)], 'id': 'I08-1029.23'}}	"ENTITY of ENTITYUNRELATED and prosodic ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED accent ENTITYOTHER over either ENTITYUNRELATED alone , and for lower ENTITYUNRELATED ENTITYUNRELATED , factorial CRF < entity id = "" I08-1029.28 "" > models < / entity > can < entity id = "" I08-1029.29 "" > improve < / entity > over linear < entity id = "" I08-1029.30 "" > chain < / entity > < entity id = "" I08-1029.31 "" > based < / entity > < entity id = "" I08-1029.32 "" > prediction < / entity > of < entity id = "" I08-1029.33 "" > pitch < /entity > accent ."
This paper describes a hybrid system for WSD, presented to the English all-words and lexical-sample tasks , that relies on two different unsupervised approaches .	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W04-0829.4'}, 'e2': {'word': 'system', 'word_index': [(5, 5)], 'id': 'W04-0829.5'}}	This ENTITY describes a hybrid ENTITYOTHER for WSD , presented to the ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED , that relies on two different unsupervised ENTITYUNRELATED .
This patterns are matched against the disambiguation contexts .	patterns	contexts	compare	{'e1': {'word': 'patterns', 'word_index': [(1, 1)], 'id': 'W04-0829.19'}, 'e2': {'word': 'contexts', 'word_index': [(7, 7)], 'id': 'W04-0829.22'}}	This ENTITY are ENTITYUNRELATED against the ENTITYUNRELATED ENTITYOTHER .
"""This paper describes the component models and combination model built as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST."	paper	models	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W04-0863.3'}, 'e2': {'word': 'models', 'word_index': [(6, 6)], 'id': 'W04-0863.5'}}	""" This ENTITY describes the ENTITYUNRELATED ENTITYOTHER and ENTITYUNRELATED ENTITYUNRELATED built as a joint ENTITYUNRELATED between Swarthmore College , ENTITYUNRELATED Kong PolyU , and HKUST ."
"Though other models described elsewhere contributed to the final combination model , this paper focuses solely on the joint contributions to the """"Swat-HK"""" effort . """	paper	contributions	topic	{'e1': {'word': 'paper', 'word_index': [(13, 13)], 'id': 'W04-0863.13'}, 'e2': {'word': 'contributions', 'word_index': [(19, 19)], 'id': 'W04-0863.15'}}	"Though other ENTITYUNRELATED described elsewhere contributed to the final ENTITYUNRELATED ENTITYUNRELATED , this ENTITY ENTITYUNRELATED solely on the joint ENTITYOTHER to the "" "" Swat - HK "" "" ENTITYUNRELATED . """
"""The paper deals with the latest application of natural language processing (NLP), specifically of ontological semantics (ONSE) to natural language information assurance and security (NL IAS)."	paper	semantics	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W04-0914.6'}, 'e2': {'word': 'semantics', 'word_index': [(17, 17)], 'id': 'W04-0914.10'}}	""" The ENTITY ENTITYUNRELATED with the latest ENTITYUNRELATED of ENTITYUNRELATED ( NLP ) , specifically of ontological ENTITYOTHER ( ONSE ) to ENTITYUNRELATED ENTITYUNRELATED assurance and security ( NL IAS ) ."
It demonstrates how the existing ideas, methods , and resources of ontological semantics can be applied to detect deception in NL text (and, eventually, in data and other media as well).	resources	text	usage	{'e1': {'word': 'resources', 'word_index': [(10, 10)], 'id': 'W04-0914.14'}, 'e2': {'word': 'text', 'word_index': [(22, 22)], 'id': 'W04-0914.17'}}	It demonstrates how the existing ideas , ENTITYUNRELATED , and ENTITY of ontological ENTITYUNRELATED can be ENTITYUNRELATED to detect deception in NL ENTITYOTHER ( and , eventually , in ENTITYUNRELATED and other media as well ) .
"After stating the problem , the paper proceeds to a brief introduction to ONSE, followed by an equally brief survey of our 5-year-old effort in """"colonizing"""" IAS."	paper	introduction	topic	{'e1': {'word': 'paper', 'word_index': [(6, 6)], 'id': 'W04-0914.20'}, 'e2': {'word': 'introduction', 'word_index': [(11, 11)], 'id': 'W04-0914.22'}}	"After stating the ENTITYUNRELATED , the ENTITY ENTITYUNRELATED to a brief ENTITYOTHER to ONSE , followed by an equally brief ENTITYUNRELATED of our 5 - year - old ENTITYUNRELATED in "" "" colonizing "" "" IAS ."
So, while clearly dealing with a new application , the paper addresses theoretical and methodological extensions of ONSE, as defined currently, that will be useful for other applications as well.	paper	extensions	topic	{'e1': {'word': 'paper', 'word_index': [(11, 11)], 'id': 'W04-0914.65'}, 'e2': {'word': 'extensions', 'word_index': [(16, 16)], 'id': 'W04-0914.66'}}	So , while clearly ENTITYUNRELATED with a new ENTITYUNRELATED , the ENTITY addresses theoretical and methodological ENTITYOTHER of ONSE , as defined currently , that will be useful for other ENTITYUNRELATED as well .
Exploiting Context For Biomedical Entity Recognition : From Syntax To The Web	Context	Recognition	usage	{'e1': {'word': 'Context', 'word_index': [(1, 1)], 'id': 'W04-1217.1'}, 'e2': {'word': 'Recognition', 'word_index': [(5, 5)], 'id': 'W04-1217.3'}}	Exploiting ENTITY For Biomedical ENTITYUNRELATED ENTITYOTHER : From ENTITYUNRELATED To The Web
We describe a machine learning system for the recognition of names in biomedical texts .	system	recognition	usage	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'W04-1217.6'}, 'e2': {'word': 'recognition', 'word_index': [(8, 8)], 'id': 'W04-1217.7'}}	We describe a ENTITYUNRELATED learning ENTITY for the ENTITYOTHER of ENTITYUNRELATED in biomedical ENTITYUNRELATED .
We describe a machine learning system for the recognition of names in biomedical texts .	names	texts	part_whole	{'e1': {'word': 'names', 'word_index': [(10, 10)], 'id': 'W04-1217.8'}, 'e2': {'word': 'texts', 'word_index': [(13, 13)], 'id': 'W04-1217.9'}}	We describe a ENTITYUNRELATED learning ENTITYUNRELATED for the ENTITYUNRELATED of ENTITY in biomedical ENTITYOTHER .
It achieves an F-score of 70% on the Coling 2004  NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus .	entities	corpus	part_whole	{'e1': {'word': 'entities', 'word_index': [(21, 21)], 'id': 'W04-1217.19'}, 'e2': {'word': 'corpus', 'word_index': [(25, 25)], 'id': 'W04-1217.20'}}	It achieves an F-score of 70 % on the Coling 2004 NLPBA / BioNLP shared ENTITYUNRELATED of identifying five biomedical ENTITYUNRELATED ENTITY in the GENIA ENTITYOTHER .
Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets	Feature	Recognition	usage	{'e1': {'word': 'Feature', 'word_index': [(10, 10)], 'id': 'W04-1221.4'}, 'e2': {'word': 'Recognition', 'word_index': [(3, 3)], 'id': 'W04-1221.3'}}	Biomedical ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER Using Conditional Random Fields And Rich ENTITY Sets
This paper presents a framework for simultaneously recognizing occurrences of PROTEIN , DNA, RNA, CELL-LINE, CELL-TYPE F1	paper	framework	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W04-1221.30'}, 'e2': {'word': 'framework', 'word_index': [(4, 4)], 'id': 'W04-1221.31'}}	This ENTITY presents a ENTITYOTHER for simultaneously recognizing ENTITYUNRELATED of ENTITYUNRELATED , DNA , RNA , CELL-LINE , ENTITYUNRELATED F1
This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news , teleconferences, and meetings .	paper	efforts	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W04-2903.3'}, 'e2': {'word': 'efforts', 'word_index': [(7, 7)], 'id': 'W04-2903.5'}}	This ENTITY ENTITYUNRELATED our on - going ENTITYOTHER to exploit multiple ENTITYUNRELATED derived from an audio ENTITYUNRELATED using ENTITYUNRELATED material such as ENTITYUNRELATED , teleconferences , and ENTITYUNRELATED .
This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news , teleconferences, and meetings .	features	stream	model-feature	{'e1': {'word': 'features', 'word_index': [(11, 11)], 'id': 'W04-2903.6'}, 'e2': {'word': 'stream', 'word_index': [(16, 16)], 'id': 'W04-2903.7'}}	This ENTITYUNRELATED ENTITYUNRELATED our on - going ENTITYUNRELATED to exploit multiple ENTITY derived from an audio ENTITYOTHER using ENTITYUNRELATED material such as ENTITYUNRELATED , teleconferences , and ENTITYUNRELATED .
These features are derived from algorithms including automatic speech recognition , automatic speech indexing , speaker identification , prosodic and audio feature extraction .	algorithms	features	result	{'e1': {'word': 'algorithms', 'word_index': [(5, 5)], 'id': 'W04-2903.12'}, 'e2': {'word': 'features', 'word_index': [(1, 1)], 'id': 'W04-2903.11'}}	These ENTITYOTHER are derived from ENTITY ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , speaker ENTITYUNRELATED , prosodic and audio ENTITYUNRELATED ENTITYUNRELATED .
We describe our research prototype - the Audio Hot Spotting System -that allows users to query and retrieve data from multimedia sources utilizing these multiple features .	data	sources	part_whole	{'e1': {'word': 'data', 'word_index': [(19, 19)], 'id': 'W04-2903.26'}, 'e2': {'word': 'sources', 'word_index': [(22, 22)], 'id': 'W04-2903.27'}}	We describe our ENTITYUNRELATED ENTITYUNRELATED - the Audio Hot Spotting ENTITYUNRELATED - that allows ENTITYUNRELATED to ENTITYUNRELATED and retrieve ENTITY from multimedia ENTITYOTHER utilizing these multiple ENTITYUNRELATED .
"""This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems ."	research	errors	topic	{'e1': {'word': 'research', 'word_index': [(5, 5)], 'id': 'W04-3006.7'}, 'e2': {'word': 'errors', 'word_index': [(15, 15)], 'id': 'W04-3006.11'}}	""" This ENTITYUNRELATED describes our ENTITY on both the ENTITYUNRELATED and subsequent ENTITYUNRELATED of ENTITYUNRELATED ENTITYOTHER in spoken ENTITYUNRELATED ."
The paper consists of two major components .	paper	components	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'W04-3006.13'}, 'e2': {'word': 'components', 'word_index': [(6, 6)], 'id': 'W04-3006.14'}}	The ENTITY consists of two major ENTITYOTHER .
The first half concerns the design of the error detection mechanism for resolving city names in our mercury flight reservation system , and an investigation of the behavioral patterns of users in subsequent subdialogues involving keypad entry for disambiguation .	investigation	patterns	topic	{'e1': {'word': 'investigation', 'word_index': [(24, 24)], 'id': 'W04-3006.22'}, 'e2': {'word': 'patterns', 'word_index': [(28, 28)], 'id': 'W04-3006.23'}}	The first half ENTITYUNRELATED the ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED for resolving city ENTITYUNRELATED in our mercury flight reservation ENTITYUNRELATED , and an ENTITY of the behavioral ENTITYOTHER of ENTITYUNRELATED in subsequent subdialogues involving keypad ENTITYUNRELATED for ENTITYUNRELATED .
A novelty of our work is the introduction of a speech synthesizer to simulate the user , which facilitates development and evaluation of our proposed strategy .	evaluation	strategy	topic	{'e1': {'word': 'evaluation', 'word_index': [(21, 21)], 'id': 'W04-3006.44'}, 'e2': {'word': 'strategy', 'word_index': [(25, 25)], 'id': 'W04-3006.46'}}	A ENTITYUNRELATED of our work is the ENTITYUNRELATED of a ENTITYUNRELATED synthesizer to simulate the ENTITYUNRELATED , which facilitates ENTITYUNRELATED and ENTITY of our ENTITYUNRELATED ENTITYOTHER .
We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words .	words	corpus	part_whole	{'e1': {'word': 'words', 'word_index': [(23, 23)], 'id': 'W05-0707.7'}, 'e2': {'word': 'corpus', 'word_index': [(20, 20)], 'id': 'W05-0707.6'}}	We ENTITYUNRELATED Conditional Random Fields ( CRFs ) to the ENTITYUNRELATED of Amharic ENTITYUNRELATED and ENTITYUNRELATED using a small annotated ENTITYOTHER of 1000 ENTITY .
"Given the size of the data and the large number of unknown <entity id=""W05-0707.12"">words</entity> in the <entity id=""W05-0707.13"">test</entity> <entity id=""W05-0707.14"">corpus</entity> (80%), an <entity id=""W05-0707.15"">accuracy</entity> of 84% for Amharic <entity id=""W05-0707.16"">word segmentation</entity> and 74% for <entity id=""W05-0707.17"">POS tagging</entity> is encouraging, indicating the <entity id=""W05-0707.18"">applicability</entity> of CRFs for a morphologically <entity id=""W05-0707.19"">complex</entity> <entity id=""W05-0707.20"">language</entity> like Amharic."	size	data	model-feature	{'e1': {'word': 'size', 'word_index': [(2, 2)], 'id': 'W05-0707.8'}, 'e2': {'word': 'data', 'word_index': [(5, 5)], 'id': 'W05-0707.9'}}	"Given the ENTITY of the ENTITYOTHER and the large ENTITYUNRELATED of unknown < entity id = "" W05-0707.12 "" > words < / entity > in the < entity id = "" W05-0707.13 "" > test < /entity > < entity id = "" W05-0707.14 "" > corpus < /entity > ( 80 % ) , an < entity id = "" W05-0707.15 "" > accuracy < / entity > of 84 % for Amharic < entity id = "" W05-0707.16 "" > word segmentation < / entity > and 74 % for < entity id = "" W05-0707.17 "" > POS tagging < / entity > is encouraging , indicating the < entity id = "" W05-0707.18 "" > applicability < / entity > of CRFs for a morphologically < entity id = "" W05-0707.19 "" > complex < / entity > < entity id = "" W05-0707.20 "" > language < / entity > like Amharic ."
In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics .	statistics	algorithms	usage	{'e1': {'word': 'statistics', 'word_index': [(35, 35)], 'id': 'W05-0801.26'}, 'e2': {'word': 'algorithms', 'word_index': [(19, 19)], 'id': 'W05-0801.22'}}	In this ENTITYUNRELATED we show that it is possible to ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED using ENTITYOTHER that are much faster , and in some ways simpler , ENTITYUNRELATED on ENTITYUNRELATED ENTITYUNRELATED ENTITY .
"""An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of """"atomic abbreviation pairs """" from a large text corpus ."	paper	Model	topic	{'e1': {'word': 'paper', 'word_index': [(16, 16)], 'id': 'W06-0103.10'}, 'e2': {'word': 'Model', 'word_index': [(11, 11)], 'id': 'W06-0103.8'}}	""" An HMM - based Single Character ENTITYUNRELATED ( SCR ) ENTITYOTHER is ENTITYUNRELATED in this ENTITY to ENTITYUNRELATED a large set of "" "" atomic ENTITYUNRELATED ENTITYUNRELATED "" "" from a large ENTITYUNRELATED ENTITYUNRELATED ."
"""An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of """"atomic abbreviation pairs """" from a large text corpus ."	pairs	corpus	part_whole	{'e1': {'word': 'pairs', 'word_index': [(27, 27)], 'id': 'W06-0103.13'}, 'e2': {'word': 'corpus', 'word_index': [(34, 34)], 'id': 'W06-0103.15'}}	""" An HMM - based Single Character ENTITYUNRELATED ( SCR ) ENTITYUNRELATED is ENTITYUNRELATED in this ENTITYUNRELATED to ENTITYUNRELATED a large set of "" "" atomic ENTITYUNRELATED ENTITY "" "" from a large ENTITYUNRELATED ENTITYOTHER ."
"With only a few training iterations , the acquisition accuracy of the proposed SCR model achieves 62% and 50 % precision for training set and test set , respectively, from the ASWSC-2001 corpus . """	model	precision	result	{'e1': {'word': 'model', 'word_index': [(14, 14)], 'id': 'W06-0103.40'}, 'e2': {'word': 'precision', 'word_index': [(21, 21)], 'id': 'W06-0103.41'}}	"With only a few ENTITYUNRELATED ENTITYUNRELATED , the ENTITYUNRELATED ENTITYUNRELATED of the ENTITYUNRELATED SCR ENTITY achieves 62 % and 50 % ENTITYOTHER for ENTITYUNRELATED and ENTITYUNRELATED , respectively , from the ASWSC - 2001 ENTITYUNRELATED . """
In this paper , we present the results of experiments aiming to validate a two-dimensional typology of affective states as a suitable basis for affective classification of texts .	paper	results	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W06-0308.4'}, 'e2': {'word': 'results', 'word_index': [(7, 7)], 'id': 'W06-0308.5'}}	In this ENTITY , we present the ENTITYOTHER of ENTITYUNRELATED aiming to validate a two -dimensional ENTITYUNRELATED of affective states as a suitable ENTITYUNRELATED for affective ENTITYUNRELATED of ENTITYUNRELATED .
"Using a corpus of English weblog posts, annotated for mood by their authors, we trained support <entity id=""W06-0308.16"">vector <entity id=""W06-0308.17"">machine</entity></entity> binary <entity id=""W06-0308.18"">classifiers</entity> to distinguish <entity id=""W06-0308.19"">texts</entity> on the <entity id=""W06-0308.20"">basis</entity> of their affiliation with one <entity id=""W06-0308.21"">region</entity> of the <entity id=""W06-0308.22"">space</entity> ."	corpus	mood	model-feature	{'e1': {'word': 'corpus', 'word_index': [(2, 2)], 'id': 'W06-0308.11'}, 'e2': {'word': 'mood', 'word_index': [(10, 10)], 'id': 'W06-0308.13'}}	"Using a ENTITY of ENTITYUNRELATED weblog posts , annotated for ENTITYOTHER by their authors , we ENTITYUNRELATED support < entity id = "" W06-0308.16 "" > vector < entity id = "" W06-0308.17 "" > machine < / entity > < / entity > binary < entity id = "" W06-0308.18 "" > classifiers < / entity > to distinguish < entity id = "" W06-0308.19 "" > texts < / entity > on the < entity id = "" W06-0308.20 "" > basis < / entity > of their affiliation with one < entity id = "" W06-0308.21 "" > region < / entity > of the < entity id = "" W06-0308.22 "" > space < / entity > ."
The frequency of occurrence of words in natural languages exhibits a periodic and a non-periodic component when analysed as a time series .	occurrence	words	model-feature	{'e1': {'word': 'occurrence', 'word_index': [(3, 3)], 'id': 'W06-0903.5'}, 'e2': {'word': 'words', 'word_index': [(5, 5)], 'id': 'W06-0903.6'}}	The ENTITYUNRELATED of ENTITY of ENTITYOTHER in ENTITYUNRELATED exhibits a periodic and a non-periodic ENTITYUNRELATED when analysed as a ENTITYUNRELATED ENTITYUNRELATED .
This work presents an unsupervised method of extracting periodicity information from text , enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns .	method	extracting	usage	{'e1': {'word': 'method', 'word_index': [(5, 5)], 'id': 'W06-0903.11'}, 'e2': {'word': 'extracting', 'word_index': [(7, 7)], 'id': 'W06-0903.12'}}	This work presents an unsupervised ENTITY of ENTITYOTHER periodicity ENTITYUNRELATED from ENTITYUNRELATED , enabling ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and filtering to be used in the ENTITYUNRELATED of sophisticated ENTITYUNRELATED that can discern between repetitive ENTITYUNRELATED and non-repetitive writing ENTITYUNRELATED .
This work presents an unsupervised method of extracting periodicity information from text , enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns .	information	text	part_whole	{'e1': {'word': 'information', 'word_index': [(9, 9)], 'id': 'W06-0903.13'}, 'e2': {'word': 'text', 'word_index': [(11, 11)], 'id': 'W06-0903.14'}}	This work presents an unsupervised ENTITYUNRELATED of ENTITYUNRELATED periodicity ENTITY from ENTITYOTHER , enabling ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and filtering to be used in the ENTITYUNRELATED of sophisticated ENTITYUNRELATED that can discern between repetitive ENTITYUNRELATED and non-repetitive writing ENTITYUNRELATED .
Evaluation Of Several Phonetic Similarity Algorithms On The Task Of Cognate Identification	Evaluation	Algorithms	topic	{'e1': {'word': 'Evaluation', 'word_index': [(0, 0)], 'id': 'W06-1107.1'}, 'e2': {'word': 'Algorithms', 'word_index': [(5, 5)], 'id': 'W06-1107.3'}}	ENTITY Of Several Phonetic ENTITYUNRELATED ENTITYOTHER On The ENTITYUNRELATED Of Cognate ENTITYUNRELATED
In this paper , we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs).	paper	regularity	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W04-0402.15'}, 'e2': {'word': 'regularity', 'word_index': [(9, 9)], 'id': 'W04-0402.16'}}	In this ENTITY , we attempt to capture the ENTITYOTHER underlying these ENTITYUNRELATED of paraphrases , ENTITYUNRELATED on the paraphrasing of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ( LVCs ) .
We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements.	Structures	model	usage	{'e1': {'word': 'Structures', 'word_index': [(15, 15)], 'id': 'W04-0402.26'}, 'e2': {'word': 'model', 'word_index': [(4, 4)], 'id': 'W04-0402.23'}}	We ENTITYUNRELATED a paraphrasing ENTITYOTHER for LVCs that is ENTITYUNRELATED on transforming the ENTITYUNRELATED Conceptual ENTITY ( LCSs ) of ENTITYUNRELATED elements .
We present a standoff annotation framework for the integration of NLP components , currently implemented in the context of the DELPH-IN tools</abstract>	framework	integration	usage	{'e1': {'word': 'framework', 'word_index': [(5, 5)], 'id': 'W06-2718.2'}, 'e2': {'word': 'integration', 'word_index': [(8, 8)], 'id': 'W06-2718.3'}}	We present a standoff annotation ENTITY for the ENTITYOTHER of NLP ENTITYUNRELATED , currently ENTITYUNRELATED in the ENTITYUNRELATED of the DELPH-IN ENTITYUNRELATED < / abstract >
We have implemented a translation module for conversational texts using this formalism , and applied it to an experimental automatic interpretation system (speech translation system ).	module	texts	usage	{'e1': {'word': 'module', 'word_index': [(5, 5)], 'id': 'C00-2134.17'}, 'e2': {'word': 'texts', 'word_index': [(8, 8)], 'id': 'C00-2134.18'}}	We have ENTITYUNRELATED a ENTITYUNRELATED ENTITY for conversational ENTITYOTHER using this ENTITYUNRELATED , and ENTITYUNRELATED it to an ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ( speech ENTITYUNRELATED ) .
Most statistical parsers have used the grammar induction approach , in which a stochastic grammar is induced from a treebank.	approach	parsers	usage	{'e1': {'word': 'approach', 'word_index': [(7, 7)], 'id': 'W04-3203.5'}, 'e2': {'word': 'parsers', 'word_index': [(2, 2)], 'id': 'W04-3203.3'}}	Most ENTITYUNRELATED ENTITYOTHER have used the ENTITYUNRELATED ENTITY , in which a stochastic grammar is induced from a treebank .
The resulting parsers are surprisingly accurate and robust , considering their speed and simplicity .	robust	parsers	model-feature	{'e1': {'word': 'robust', 'word_index': [(7, 7)], 'id': 'W04-3203.15'}, 'e2': {'word': 'parsers', 'word_index': [(2, 2)], 'id': 'W04-3203.14'}}	The ENTITYUNRELATED ENTITYOTHER are surprisingly accurate and ENTITY , considering their ENTITYUNRELATED and ENTITYUNRELATED .
We also describe Markov parsing models , a general framework for parser modeling and control , of which the parsers reported here are a special case .	framework	modeling	usage	{'e1': {'word': 'framework', 'word_index': [(9, 9)], 'id': 'W04-3203.24'}, 'e2': {'word': 'modeling', 'word_index': [(12, 12)], 'id': 'W04-3203.26'}}	We also describe Markov ENTITYUNRELATED ENTITYUNRELATED , a general ENTITY for ENTITYUNRELATED ENTITYOTHER and ENTITYUNRELATED , of which the ENTITYUNRELATED ENTITYUNRELATED here are a ENTITYUNRELATED .
In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.	paper	algorithm	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'W01-0511.17'}, 'e2': {'word': 'algorithm', 'word_index': [(7, 7)], 'id': 'W01-0511.19'}}	In this ENTITY we describe a ENTITYUNRELATED ENTITYOTHER for identifying ENTITYUNRELATED between ENTITYUNRELATED ENTITYUNRELATED compounds .
We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances , performing better on previously unseen words than a baseline consisting of training on the words themselves.	algorithm	approach	usage	{'e1': {'word': 'algorithm', 'word_index': [(11, 11)], 'id': 'W01-0511.26'}, 'e2': {'word': 'approach', 'word_index': [(6, 6)], 'id': 'W01-0511.24'}}	We find that a very ENTITYUNRELATED ENTITYOTHER using a ENTITYUNRELATED learning ENTITY and a ENTITYUNRELATED ENTITYUNRELATED hierarchy successfully generalizes from ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED better on previously unseen ENTITYUNRELATED than a baseline consisting of ENTITYUNRELATED on the ENTITYUNRELATED themselves .
In this paper , we describe a source-side reordering method based on syntactic chunks for phrase-based statistical machine translation .	chunks	method	usage	{'e1': {'word': 'chunks', 'word_index': [(13, 13)], 'id': 'W07-0401.11'}, 'e2': {'word': 'method', 'word_index': [(9, 9)], 'id': 'W07-0401.8'}}	In this ENTITYUNRELATED , we describe a ENTITYUNRELATED reordering ENTITYOTHER ENTITYUNRELATED on ENTITYUNRELATED ENTITY for ENTITYUNRELATED ENTITYUNRELATED .
Then, reordering rules are automatically learned from source-side chunks and word alignments .	rules	chunks	part_whole	{'e1': {'word': 'rules', 'word_index': [(3, 3)], 'id': 'W07-0401.17'}, 'e2': {'word': 'chunks', 'word_index': [(9, 9)], 'id': 'W07-0401.19'}}	Then , reordering ENTITY are automatically learned from ENTITYUNRELATED ENTITYOTHER and ENTITYUNRELATED .
During translation , the rules are used to generate a reordering lattice for each sentence .	lattice	sentence	model-feature	{'e1': {'word': 'lattice', 'word_index': [(11, 11)], 'id': 'W07-0401.24'}, 'e2': {'word': 'sentence', 'word_index': [(14, 14)], 'id': 'W07-0401.25'}}	During ENTITYUNRELATED , the ENTITYUNRELATED are used to ENTITYUNRELATED a reordering ENTITY for each ENTITYOTHER .
Lexical analogies occur frequently in text and are useful in various natural language processing tasks .	analogies	text	part_whole	{'e1': {'word': 'analogies', 'word_index': [(1, 1)], 'id': 'D07-1059.11'}, 'e2': {'word': 'text', 'word_index': [(5, 5)], 'id': 'D07-1059.12'}}	ENTITYUNRELATED ENTITY occur frequently in ENTITYOTHER and are useful in various ENTITYUNRELATED .
In this study , we present a system that generates lexical analogies automatically from text data .	study	system	topic	{'e1': {'word': 'study', 'word_index': [(2, 2)], 'id': 'D07-1059.14'}, 'e2': {'word': 'system', 'word_index': [(7, 7)], 'id': 'D07-1059.15'}}	In this ENTITY , we present a ENTITYOTHER that ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED automatically from ENTITYUNRELATED ENTITYUNRELATED .
In this study , we present a system that generates lexical analogies automatically from text data .	analogies	data	part_whole	{'e1': {'word': 'analogies', 'word_index': [(11, 11)], 'id': 'D07-1059.18'}, 'e2': {'word': 'data', 'word_index': [(15, 15)], 'id': 'D07-1059.20'}}	In this ENTITYUNRELATED , we present a ENTITYUNRELATED that ENTITYUNRELATED ENTITYUNRELATED ENTITY automatically from ENTITYUNRELATED ENTITYOTHER .
Empirical evaluation shows that our system generates valid lexical analogies with a precision of 70% , and produces quality output although not at the level of the best human-generated lexical analogies .	system	precision	result	{'e1': {'word': 'system', 'word_index': [(5, 5)], 'id': 'D07-1059.35'}, 'e2': {'word': 'precision', 'word_index': [(12, 12)], 'id': 'D07-1059.39'}}	Empirical ENTITYUNRELATED shows that our ENTITY ENTITYUNRELATED valid ENTITYUNRELATED ENTITYUNRELATED with a ENTITYOTHER of 70 % , and produces ENTITYUNRELATED ENTITYUNRELATED although not at the ENTITYUNRELATED of the best human - generated ENTITYUNRELATED ENTITYUNRELATED .
In natural languages , sentences are usually part of discourse units just as words are part of sentences .	sentences	discourse	part_whole	{'e1': {'word': 'sentences', 'word_index': [(3, 3)], 'id': 'I08-3014.11'}, 'e2': {'word': 'discourse', 'word_index': [(8, 8)], 'id': 'I08-3014.13'}}	In ENTITYUNRELATED , ENTITY are usually ENTITYUNRELATED of ENTITYOTHER ENTITYUNRELATED just as ENTITYUNRELATED are ENTITYUNRELATED of ENTITYUNRELATED .
In natural languages , sentences are usually part of discourse units just as words are part of sentences .	words	sentences	part_whole	{'e1': {'word': 'words', 'word_index': [(12, 12)], 'id': 'I08-3014.15'}, 'e2': {'word': 'sentences', 'word_index': [(16, 16)], 'id': 'I08-3014.17'}}	In ENTITYUNRELATED , ENTITYUNRELATED are usually ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED just as ENTITY are ENTITYUNRELATED of ENTITYOTHER .
"This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora <entity id=""I08-3014.39"">resolution</entity> in Urdu."	paper	factors	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'I08-3014.31'}, 'e2': {'word': 'factors', 'word_index': [(9, 9)], 'id': 'I08-3014.34'}}	"This ENTITY is ENTITYUNRELATED on the ENTITYUNRELATED of various ENTITYOTHER and their ENTITYUNRELATED ENTITYUNRELATED that play an important ENTITYUNRELATED in personal anaphora < entity id = "" I08-3014.39 "" > resolution < / entity > in Urdu ."
Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate .	Algorithms	rate	result	{'e1': {'word': 'Algorithms', 'word_index': [(0, 0)], 'id': 'I08-3014.40'}, 'e2': {'word': 'rate', 'word_index': [(14, 14)], 'id': 'I08-3014.44'}}	ENTITY are ENTITYUNRELATED that resolves pronominal anaphoric ENTITYUNRELATED with 77 - 80 % ENTITYUNRELATED ENTITYOTHER .
Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .	method	gain	result	{'e1': {'word': 'method', 'word_index': [(13, 13)], 'id': 'I08-4004.47'}, 'e2': {'word': 'gain', 'word_index': [(6, 6)], 'id': 'I08-4004.45'}}	ENTITYUNRELATED ENTITYUNRELATED show that additional ENTITYUNRELATED ENTITYOTHER can be obtained by using ENTITYUNRELATED ENTITY ; ENTITYUNRELATED ENTITYUNRELATED has a great ENTITYUNRELATED on ENTITYUNRELATED 's ENTITYUNRELATED and our ENTITYUNRELATED ENTITYUNRELATED is very effective ; and the key of ENTITYUNRELATED is to ENTITYUNRELATED the ENTITYUNRELATED of the normal ENTITYUNRELATED 's ENTITYUNRELATED , especially the pronoun 's ENTITYUNRELATED .
Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .	algorithm	performance	result	{'e1': {'word': 'algorithm', 'word_index': [(16, 16)], 'id': 'I08-4004.49'}, 'e2': {'word': 'performance', 'word_index': [(24, 24)], 'id': 'I08-4004.52'}}	ENTITYUNRELATED ENTITYUNRELATED show that additional ENTITYUNRELATED ENTITYUNRELATED can be obtained by using ENTITYUNRELATED ENTITYUNRELATED ; ENTITYUNRELATED ENTITY has a great ENTITYUNRELATED on ENTITYUNRELATED 's ENTITYOTHER and our ENTITYUNRELATED ENTITYUNRELATED is very effective ; and the key of ENTITYUNRELATED is to ENTITYUNRELATED the ENTITYUNRELATED of the normal ENTITYUNRELATED 's ENTITYUNRELATED , especially the pronoun 's ENTITYUNRELATED .
Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets.	studies	parsing	topic	{'e1': {'word': 'studies', 'word_index': [(1, 1)], 'id': 'P07-1122.4'}, 'e2': {'word': 'parsing', 'word_index': [(5, 5)], 'id': 'P07-1122.7'}}	Previous ENTITY in ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER have shown that ENTITYUNRELATED ENTITYUNRELATED can ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED for specific ENTITYUNRELATED and ENTITYUNRELATED sets .
Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets.	transformations	accuracy	result	{'e1': {'word': 'transformations', 'word_index': [(10, 10)], 'id': 'P07-1122.9'}, 'e2': {'word': 'accuracy', 'word_index': [(14, 14)], 'id': 'P07-1122.12'}}	Previous ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED have shown that ENTITYUNRELATED ENTITY can ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER for specific ENTITYUNRELATED and ENTITYUNRELATED sets .
The results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties .	language	parsing	result	{'e1': {'word': 'language', 'word_index': [(19, 19)], 'id': 'P07-1122.31'}, 'e2': {'word': 'parsing', 'word_index': [(10, 10)], 'id': 'P07-1122.28'}}	The ENTITYUNRELATED indicate that the beneficial ENTITYUNRELATED of pseudo- projective ENTITYOTHER is independent of ENTITYUNRELATED ENTITYUNRELATED but sensitive to ENTITY or treebank specific ENTITYUNRELATED .
By contrast , the construction specific transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages .	strategy	transformations	result	{'e1': {'word': 'strategy', 'word_index': [(14, 14)], 'id': 'P07-1122.37'}, 'e2': {'word': 'transformations', 'word_index': [(6, 6)], 'id': 'P07-1122.35'}}	By ENTITYUNRELATED , the ENTITYUNRELATED specific ENTITYOTHER appear to be more sensitive to ENTITYUNRELATED ENTITY but have a constant positive ENTITYUNRELATED over several ENTITYUNRELATED .
Construction of Domain Dictionary for Fundamental Vocabulary	Vocabulary	Dictionary	part_whole	{'e1': {'word': 'Vocabulary', 'word_index': [(6, 6)], 'id': 'P07-2035.4'}, 'e2': {'word': 'Dictionary', 'word_index': [(3, 3)], 'id': 'P07-2035.3'}}	ENTITYUNRELATED of ENTITYUNRELATED ENTITYOTHER for Fundamental ENTITY
Extracting a Representation from Text for Semantic Analysis	Representation	Text	model-feature	{'e1': {'word': 'Representation', 'word_index': [(2, 2)], 'id': 'P08-2061.2'}, 'e2': {'word': 'Text', 'word_index': [(4, 4)], 'id': 'P08-2061.3'}}	ENTITYUNRELATED a ENTITY from ENTITYOTHER for ENTITYUNRELATED
This representation is largely extractable by today's technologies and facilitates more detailed semantic analysis .	representation	technologies	result	{'e1': {'word': 'representation', 'word_index': [(1, 1)], 'id': 'P08-2061.10'}, 'e2': {'word': 'technologies', 'word_index': [(8, 8)], 'id': 'P08-2061.11'}}	This ENTITY is largely extractable by today 's ENTITYOTHER and facilitates more detailed ENTITYUNRELATED .
"This paper presents a new fully automatic method for building highly dense and accurate knowledge <entity id=""C08-1021.7"">bases</entity> from existing <entity id=""C08-1021.8"">semantic</entity> <entity id=""C08-1021.9"">resources</entity> ."	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C08-1021.2'}, 'e2': {'word': 'method', 'word_index': [(7, 7)], 'id': 'C08-1021.4'}}	"This ENTITY presents a new fully ENTITYUNRELATED ENTITYOTHER for ENTITYUNRELATED highly dense and accurate knowledge < entity id = "" C08-1021.7 "" > bases < / entity > from existing < entity id = "" C08-1021.8 "" > semantic < / entity > < entity id = "" C08-1021.9 "" > resources < / entity > ."
Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web.	algorithm	method	usage	{'e1': {'word': 'algorithm', 'word_index': [(11, 11)], 'id': 'C08-1021.14'}, 'e2': {'word': 'method', 'word_index': [(3, 3)], 'id': 'C08-1021.10'}}	Basically , the ENTITYOTHER uses a ENTITYUNRELATED and accurate ENTITYUNRELATED ENTITYUNRELATED ENTITY to assign the most appropriate senses to large sets of topically related ENTITYUNRELATED acquired from the web .
KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora .	concepts	knowledge-base	part_whole	{'e1': {'word': 'concepts', 'word_index': [(11, 11)], 'id': 'C08-1021.19'}, 'e2': {'word': 'knowledge-base', 'word_index': [(4, 4)], 'id': 'C08-1021.17'}}	KnowNet , the ENTITYUNRELATED ENTITYOTHER which connects large sets of ENTITYUNRELATED ENTITY is a major ENTITYUNRELATED towards the autonomous ENTITYUNRELATED of ENTITYUNRELATED from raw ENTITYUNRELATED .
KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora .	knowledge	corpora	part_whole	{'e1': {'word': 'knowledge', 'word_index': [(21, 21)], 'id': 'C08-1021.22'}, 'e2': {'word': 'corpora', 'word_index': [(24, 24)], 'id': 'C08-1021.23'}}	KnowNet , the ENTITYUNRELATED ENTITYUNRELATED which connects large sets of ENTITYUNRELATED ENTITYUNRELATED is a major ENTITYUNRELATED towards the autonomous ENTITYUNRELATED of ENTITY from raw ENTITYOTHER .
In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework .	knowledge	resource	compare	{'e1': {'word': 'knowledge', 'word_index': [(20, 20)], 'id': 'C08-1021.28'}, 'e2': {'word': 'resource', 'word_index': [(26, 26)], 'id': 'C08-1021.29'}}	In fact , KnowNet is several ENTITYUNRELATED larger than any available ENTITYUNRELATED ENTITYUNRELATED encoding ENTITYUNRELATED between synsets , and the ENTITY KnowNet contains outperform any other ENTITYOTHER when is empirically ENTITYUNRELATED in a ENTITYUNRELATED ENTITYUNRELATED .
The parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem .	method	parser	usage	{'e1': {'word': 'method', 'word_index': [(9, 9)], 'id': 'D07-1122.10'}, 'e2': {'word': 'parser', 'word_index': [(1, 1)], 'id': 'D07-1122.7'}}	The ENTITYOTHER first identifies ENTITYUNRELATED using a deterministic ENTITYUNRELATED ENTITY and then labels those ENTITYUNRELATED as a ENTITYUNRELATED labeling ENTITYUNRELATED .
We present a stochastic parsing system consisting of a Lexical- Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model .	parser	system	part_whole	{'e1': {'word': 'parser', 'word_index': [(18, 18)], 'id': 'P02-1035.11'}, 'e2': {'word': 'system', 'word_index': [(5, 5)], 'id': 'P02-1035.8'}}	We present a stochastic ENTITYUNRELATED ENTITYOTHER consisting of a ENTITYUNRELATED Functional Grammar ( LFG ) , a ENTITYUNRELATED ENTITY and a stochastic ENTITYUNRELATED ENTITYUNRELATED .
The model combines full and partial parsing techniques to reach full grammar coverage on unseen data .	techniques	model	part_whole	{'e1': {'word': 'techniques', 'word_index': [(7, 7)], 'id': 'P02-1035.24'}, 'e2': {'word': 'model', 'word_index': [(1, 1)], 'id': 'P02-1035.21'}}	The ENTITYOTHER combines full and ENTITYUNRELATED ENTITYUNRELATED ENTITY to ENTITYUNRELATED full grammar ENTITYUNRELATED on unseen ENTITYUNRELATED .
The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models .	models	estimation	usage	{'e1': {'word': 'models', 'word_index': [(16, 16)], 'id': 'P02-1035.32'}, 'e2': {'word': 'estimation', 'word_index': [(13, 13)], 'id': 'P02-1035.31'}}	The treebank annotations are used to ENTITYUNRELATED partially labeled ENTITYUNRELATED for discriminative ENTITYUNRELATED ENTITYOTHER using exponential ENTITY .
Semantic Orientation Applied To Unsupervised Classification Of Reviews	Orientation	Classification	usage	{'e1': {'word': 'Orientation', 'word_index': [(1, 1)], 'id': 'P02-1053.2'}, 'e2': {'word': 'Classification', 'word_index': [(5, 5)], 'id': 'P02-1053.4'}}	ENTITYUNRELATED ENTITY ENTITYUNRELATED To Unsupervised ENTITYOTHER Of Reviews
Self-Organizing Markov Models And Their Application To Part- Of- Speech Tagging</title>	Models	Tagging	usage	{'e1': {'word': 'Models', 'word_index': [(2, 2)], 'id': 'P03-1038.1'}, 'e2': {'word': 'Tagging', 'word_index': [(11, 11)], 'id': 'P03-1038.5'}}	Self-Organizing Markov ENTITY And Their ENTITYUNRELATED To ENTITYUNRELATED Of - ENTITYUNRELATED ENTITYOTHER < / title >
"This paper presents a method to develop a class of variable memory Markov <entity id=""P03-1038.13"">models</entity> that have higher <entity id=""P03-1038.14"">memory</entity> <entity id=""P03-1038.15"">capacity</entity> than traditional (uniform <entity id=""P03-1038.16"">memory</entity> ) <entity id=""P03-1038.17"">Markov <entity id=""P03-1038.18"">models</entity></entity> ."	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'P03-1038.6'}, 'e2': {'word': 'method', 'word_index': [(4, 4)], 'id': 'P03-1038.7'}}	"This ENTITY presents a ENTITYOTHER to ENTITYUNRELATED a ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED Markov < entity id = "" P03-1038.13 "" > models < / entity > that have higher < entity id = "" P03-1038.14 "" > memory < / entity > < entity id = "" P03-1038.15 "" > capacity < / entity > than traditional ( uniform < entity id = "" P03-1038.16 "" > memory < / entity > ) < entity id = "" P03-1038.17 "" > Markov < entity id = "" P03-1038.18 "" > models < / entity > </ entity > ."
The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm .	structure	corpus	part_whole	{'e1': {'word': 'structure', 'word_index': [(1, 1)], 'id': 'P03-1038.19'}, 'e2': {'word': 'corpus', 'word_index': [(13, 13)], 'id': 'P03-1038.23'}}	The ENTITY of the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED is induced from a manually annotated ENTITYOTHER through a ENTITYUNRELATED learning ENTITYUNRELATED .
The second model , which we call the Concept model , is a hierarchical model that uses a concept latent variable to relate different language specific sense labels.	variable	model	usage	{'e1': {'word': 'variable', 'word_index': [(20, 20)], 'id': 'P04-1037.26'}, 'e2': {'word': 'model', 'word_index': [(14, 14)], 'id': 'P04-1037.24'}}	The second ENTITYUNRELATED , which we ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED , is a hierarchical ENTITYOTHER that uses a ENTITYUNRELATED latent ENTITY to relate different ENTITYUNRELATED specific ENTITYUNRELATED labels .
We show that both models improve performance on the word sense disambiguation task over previous unsu-pervised approaches , with the Concept model showing the largest improvement .	models	performance	result	{'e1': {'word': 'models', 'word_index': [(4, 4)], 'id': 'P04-1037.29'}, 'e2': {'word': 'performance', 'word_index': [(6, 6)], 'id': 'P04-1037.31'}}	We show that both ENTITY ENTITYUNRELATED ENTITYOTHER on the ENTITYUNRELATED ENTITYUNRELATED over previous unsu-pervised ENTITYUNRELATED , with the ENTITYUNRELATED ENTITYUNRELATED showing the largest ENTITYUNRELATED .
Furthermore, in learning the Concept model , as a by-product , we learn a sense inventory for the parallel language .	inventory	language	model-feature	{'e1': {'word': 'inventory', 'word_index': [(16, 16)], 'id': 'P04-1037.42'}, 'e2': {'word': 'language', 'word_index': [(20, 20)], 'id': 'P04-1037.43'}}	Furthermore , in learning the ENTITYUNRELATED ENTITYUNRELATED , as a ENTITYUNRELATED , we learn a ENTITYUNRELATED ENTITY for the parallel ENTITYOTHER .
This paper presents a Chinese word segmentation system which can adapt to different domains and standards .	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'P04-1059.2'}, 'e2': {'word': 'system', 'word_index': [(5, 5)], 'id': 'P04-1059.4'}}	This ENTITY presents a ENTITYUNRELATED ENTITYOTHER which can ENTITYUNRELATED to different ENTITYUNRELATED and ENTITYUNRELATED .
Evaluation of the proposed system on five test sets with different standards shows that the system achieves stateof-the-art performance on all of them.	Evaluation	system	topic	{'e1': {'word': 'Evaluation', 'word_index': [(0, 0)], 'id': 'P04-1059.27'}, 'e2': {'word': 'system', 'word_index': [(4, 4)], 'id': 'P04-1059.29'}}	ENTITY of the ENTITYUNRELATED ENTITYOTHER on five ENTITYUNRELATED with different ENTITYUNRELATED shows that the ENTITYUNRELATED achieves stateof - the - art ENTITYUNRELATED on all of them .
Evaluation of the proposed system on five test sets with different standards shows that the system achieves stateof-the-art performance on all of them.	system	performance	result	{'e1': {'word': 'system', 'word_index': [(14, 14)], 'id': 'P04-1059.32'}, 'e2': {'word': 'performance', 'word_index': [(21, 21)], 'id': 'P04-1059.33'}}	ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED on five ENTITYUNRELATED with different ENTITYUNRELATED shows that the ENTITY achieves stateof - the - art ENTITYOTHER on all of them .
This paper reports the corpus-oriented development of a wide-coverage Japanese HPSG parser .	paper	parser	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'P05-2024.4'}, 'e2': {'word': 'parser', 'word_index': [(11, 11)], 'id': 'P05-2024.10'}}	This ENTITY ENTITYUNRELATED the ENTITYUNRELATED ENTITYUNRELATED of a ENTITYUNRELATED ENTITYUNRELATED HPSG ENTITYOTHER .
Man-Assisted Machine Construction Of A Semantic Dictionary For Natural Language Processing</title>	Dictionary	Processing	usage	{'e1': {'word': 'Dictionary', 'word_index': [(6, 6)], 'id': 'C82-1067.4'}, 'e2': {'word': 'Processing', 'word_index': [(9, 9)], 'id': 'C82-1067.6'}}	Man-Assisted ENTITYUNRELATED ENTITYUNRELATED Of A ENTITYUNRELATED ENTITY For ENTITYUNRELATED ENTITYOTHER < / title >
This is a report on the semantic dictionary for natural language processing we are constructing now.	report	dictionary	topic	{'e1': {'word': 'report', 'word_index': [(3, 3)], 'id': 'C82-1067.7'}, 'e2': {'word': 'dictionary', 'word_index': [(7, 7)], 'id': 'C82-1067.9'}}	This is a ENTITY on the ENTITYUNRELATED ENTITYOTHER for ENTITYUNRELATED we are ENTITYUNRELATED now .
For development purposes we are using an existing corpus of 10,000 words of continuous prose from the PERQ's graphics documentation ; in the long term , the system will be extended for use by technical writers in fields other than software .	corpus	documentation	part_whole	{'e1': {'word': 'corpus', 'word_index': [(8, 8)], 'id': 'C86-1077.27'}, 'e2': {'word': 'documentation', 'word_index': [(20, 20)], 'id': 'C86-1077.29'}}	For ENTITYUNRELATED ENTITYUNRELATED we are using an existing ENTITY of 10,000 ENTITYUNRELATED of continuous prose from the PERQ 's graphics ENTITYOTHER ; in the long ENTITYUNRELATED , the ENTITYUNRELATED will be extended for use by technical writers in ENTITYUNRELATED other than ENTITYUNRELATED .
The design of the system takes advantage of the results of recent linguistic research into the structure of the lexicon , allowing START to attain a broader range of coverage than many existing systems while maintaining modular organization .	research	design	usage	{'e1': {'word': 'research', 'word_index': [(13, 13)], 'id': 'C88-1065.21'}, 'e2': {'word': 'design', 'word_index': [(1, 1)], 'id': 'C88-1065.17'}}	The ENTITYOTHER of the ENTITYUNRELATED takes ENTITYUNRELATED of the ENTITYUNRELATED of recent linguistic ENTITY into the ENTITYUNRELATED of the ENTITYUNRELATED , allowing START to attain a broader range of ENTITYUNRELATED than many existing ENTITYUNRELATED while maintaining modular ENTITYUNRELATED .
has borrowed a lot of ideas from We could not have developed even a simple parser without the research results in It is obviously nonsense to claim that we, computational linguists , do not care research results in However, the researchers in it seems to me, are very fond of especially, those who are called They always fight with each other by asserting that their are superior to the others'.	results	parser	usage	{'e1': {'word': 'results', 'word_index': [(19, 19)], 'id': 'C88-2095.7'}, 'e2': {'word': 'parser', 'word_index': [(15, 15)], 'id': 'C88-2095.5'}}	has borrowed a lot of ideas from We could not have ENTITYUNRELATED even a ENTITYUNRELATED ENTITYOTHER without the ENTITYUNRELATED ENTITY in It is obviously nonsense to ENTITYUNRELATED that we , ENTITYUNRELATED , do not care ENTITYUNRELATED ENTITYUNRELATED in However , the ENTITYUNRELATED in it seems to me , are very fond of especially , those who are ENTITYUNRELATED They always fight with each other by asserting that their are superior to the others '.
They promptly demonstrate that LFG is wrong, by showing a lot of peculiar sentences which rarely appear in real texts .Formalisms are prepared for accomplishing specific purposes .	sentences	texts	part_whole	{'e1': {'word': 'sentences', 'word_index': [(14, 14)], 'id': 'C88-2095.18'}, 'e2': {'word': 'texts', 'word_index': [(20, 20)], 'id': 'C88-2095.19'}}	They promptly demonstrate that LFG is wrong , by showing a lot of peculiar ENTITY which rarely appear in real ENTITYOTHER . Formalisms are prepared for accomplishing specific ENTITYUNRELATED .
Directing The Generation Of Living Space Descriptions	Generation	Space	usage	{'e1': {'word': 'Generation', 'word_index': [(2, 2)], 'id': 'C88-2130.1'}, 'e2': {'word': 'Space', 'word_index': [(5, 5)], 'id': 'C88-2130.2'}}	Directing The ENTITY Of Living ENTITYOTHER Descriptions
We have developed a Computational model of the process of describing the layout of an apartment or house, a much-studied discourse task first characterized linguistically by Linde (1974).	developed	Computational model	part_whole	{'e1': {'word': 'developed', 'word_index': [(2, 2)], 'id': 'C88-2130.3'}, 'e2': {'word': 'Computational model', 'word_index': [(4, 4)], 'id': 'C88-2130.4'}}	We have ENTITY a ENTITYOTHER of the ENTITYUNRELATED of describing the ENTITYUNRELATED of an apartment or house , a much - studied ENTITYUNRELATED ENTITYUNRELATED first characterized linguistically by Linde ( 1974 ) .
The model is embodied in a program , APT, that can reproduce segments of actual tape-recorded descriptions , using organizational and discourse strategies derived through analysis of our corpus .	analysis	corpus	topic	{'e1': {'word': 'analysis', 'word_index': [(28, 28)], 'id': 'C88-2130.15'}, 'e2': {'word': 'corpus', 'word_index': [(31, 31)], 'id': 'C88-2130.16'}}	The ENTITYUNRELATED is embodied in a ENTITYUNRELATED , APT , that can reproduce ENTITYUNRELATED of actual tape - recorded ENTITYUNRELATED , using organizational and ENTITYUNRELATED ENTITYUNRELATED derived through ENTITY of our ENTITYOTHER .
that TFA has an effect upon the semantic content of the sentence .	content	sentence	part_whole	{'e1': {'word': 'content', 'word_index': [(8, 8)], 'id': 'C88-2148.17'}, 'e2': {'word': 'sentence', 'word_index': [(11, 11)], 'id': 'C88-2148.18'}}	that TFA has an ENTITYUNRELATED upon the ENTITYUNRELATED ENTITY of the ENTITYOTHER .
An inforaal short description of an algorithm handling the TFA in the translation of tectogrammatical representations into the constructions of TIL is added.	description	algorithm	topic	{'e1': {'word': 'description', 'word_index': [(3, 3)], 'id': 'C88-2148.19'}, 'e2': {'word': 'algorithm', 'word_index': [(6, 6)], 'id': 'C88-2148.20'}}	An inforaal short ENTITY of an ENTITYOTHER handling the TFA in the ENTITYUNRELATED of tectogrammatical ENTITYUNRELATED into the ENTITYUNRELATED of TIL is added .
In  1986  the first experiments in text generation applied to weather forecasts resulted in a prototype system (RAREAS[6,3]) for producing English marine bulletins from forecast data .	experiments	system	result	{'e1': {'word': 'experiments', 'word_index': [(4, 4)], 'id': 'C90-1021.3'}, 'e2': {'word': 'system', 'word_index': [(15, 15)], 'id': 'C90-1021.8'}}	In 1986 the first ENTITY in ENTITYUNRELATED ENTITYUNRELATED to weather forecasts ENTITYUNRELATED in a ENTITYUNRELATED ENTITYOTHER ( RAREAS [ 6,3 ] ) for producing ENTITYUNRELATED marine bulletins from forecast ENTITYUNRELATED .
This paper presents a language for the description of morphological alternations which is based on syllable structure .	paper	language	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C90-3009.3'}, 'e2': {'word': 'language', 'word_index': [(4, 4)], 'id': 'C90-3009.4'}}	This ENTITY presents a ENTITYOTHER for the ENTITYUNRELATED of morphological ENTITYUNRELATED which is ENTITYUNRELATED on syllable ENTITYUNRELATED .
Knowledge Acquisition And Chinese Parsing Based On Corpus	Corpus	Parsing	usage	{'e1': {'word': 'Corpus', 'word_index': [(7, 7)], 'id': 'C92-4211.6'}, 'e2': {'word': 'Parsing', 'word_index': [(4, 4)], 'id': 'C92-4211.4'}}	ENTITYUNRELATED ENTITYUNRELATED And ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED On ENTITY
In this paper , we will introduce a corpus based Chinese parsing system .	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'C92-4211.13'}, 'e2': {'word': 'system', 'word_index': [(12, 12)], 'id': 'C92-4211.18'}}	In this ENTITY , we will introduce a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
The knowledge of this system is principally extracted from analyzed corpus , others are a few grammatical principles , i.e.	knowledge	corpus	part_whole	{'e1': {'word': 'knowledge', 'word_index': [(1, 1)], 'id': 'C92-4211.26'}, 'e2': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'C92-4211.29'}}	The ENTITY of this ENTITYUNRELATED is principally ENTITYUNRELATED from analyzed ENTITYOTHER , others are a few grammatical ENTITYUNRELATED , i.e.
In addition , we also propose the fifth axiom of DG to support the parsing of Chinese sentences .	parsing	sentences	usage	{'e1': {'word': 'parsing', 'word_index': [(14, 14)], 'id': 'C92-4211.37'}, 'e2': {'word': 'sentences', 'word_index': [(17, 17)], 'id': 'C92-4211.39'}}	In ENTITYUNRELATED , we also ENTITYUNRELATED the fifth ENTITYUNRELATED of DG to ENTITYUNRELATED the ENTITY of ENTITYUNRELATED ENTITYOTHER .
A Reestimation Algorithm For Probabilistic Ttecursive Transition Network</title>	Algorithm	Network	usage	{'e1': {'word': 'Algorithm', 'word_index': [(2, 2)], 'id': 'C94-2138.1'}, 'e2': {'word': 'Network', 'word_index': [(7, 7)], 'id': 'C94-2138.3'}}	A Reestimation ENTITY For Probabilistic Ttecursive ENTITYUNRELATED ENTITYOTHER < / title >
Recursive Transition Network (I'KTN) is an elevated version of RTN to model and process languages in stochastic, parameters .	Network	languages	usage	{'e1': {'word': 'Network', 'word_index': [(2, 2)], 'id': 'C94-2138.5'}, 'e2': {'word': 'languages', 'word_index': [(16, 16)], 'id': 'C94-2138.9'}}	Recursive ENTITYUNRELATED ENTITY ( I'KTN ) is an elevated ENTITYUNRELATED of RTN to ENTITYUNRELATED and ENTITYUNRELATED ENTITYOTHER in stochastic , ENTITYUNRELATED .
"Section 3 will discuss applications of DCKR to semantic processing of natural languages . """	Section	applications	topic	{'e1': {'word': 'Section', 'word_index': [(0, 0)], 'id': 'C86-1052.72'}, 'e2': {'word': 'applications', 'word_index': [(4, 4)], 'id': 'C86-1052.73'}}	"ENTITY 3 will discuss ENTITYOTHER of DCKR to ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED . """
"""Viewing the syntactic analysis of natural language as a search problem , the right choice of parsing strategy plays an important role in the performance of natural language parsers ."	strategy	performance	result	{'e1': {'word': 'strategy', 'word_index': [(16, 16)], 'id': 'C88-1048.10'}, 'e2': {'word': 'performance', 'word_index': [(23, 23)], 'id': 'C88-1048.12'}}	""" Viewing the ENTITYUNRELATED of ENTITYUNRELATED as a ENTITYUNRELATED ENTITYUNRELATED , the right ENTITYUNRELATED of ENTITYUNRELATED ENTITY plays an important ENTITYUNRELATED in the ENTITYOTHER of ENTITYUNRELATED ENTITYUNRELATED ."
On this basis systematic tests on different parsing strategies have been performed , the results of which are dicussed.	tests	strategies	topic	{'e1': {'word': 'tests', 'word_index': [(4, 4)], 'id': 'C88-1048.22'}, 'e2': {'word': 'strategies', 'word_index': [(8, 8)], 'id': 'C88-1048.24'}}	On this ENTITYUNRELATED systematic ENTITY on different ENTITYUNRELATED ENTITYOTHER have been ENTITYUNRELATED , the ENTITYUNRELATED of which are dicussed .
The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input .	generation	output	usage	{'e1': {'word': 'generation', 'word_index': [(1, 1)], 'id': 'C88-2122.23'}, 'e2': {'word': 'output', 'word_index': [(4, 4)], 'id': 'C88-2122.24'}}	The ENTITY of multimodal ENTITYOTHER poses specific ENTITYUNRELATED , which have no counterpart in the ENTITYUNRELATED of multimodal ENTITYUNRELATED .
The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input .	analysis	input	topic	{'e1': {'word': 'analysis', 'word_index': [(15, 15)], 'id': 'C88-2122.26'}, 'e2': {'word': 'input', 'word_index': [(18, 18)], 'id': 'C88-2122.27'}}	The ENTITYUNRELATED of multimodal ENTITYUNRELATED poses specific ENTITYUNRELATED , which have no counterpart in the ENTITY of multimodal ENTITYOTHER .
The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ).	part	strategy	topic	{'e1': {'word': 'part', 'word_index': [(2, 2)], 'id': 'C88-2122.28'}, 'e2': {'word': 'strategy', 'word_index': [(5, 5)], 'id': 'C88-2122.29'}}	The second ENTITY presents the ENTITYOTHER for ENTITYUNRELATED multimodal ENTITYUNRELATED which has been ENTITYUNRELATED within the ENTITYUNRELATED of the XTRA ENTITYUNRELATED ( a NL ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED ) .
The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ).	generating	output	usage	{'e1': {'word': 'generating', 'word_index': [(7, 7)], 'id': 'C88-2122.30'}, 'e2': {'word': 'output', 'word_index': [(9, 9)], 'id': 'C88-2122.31'}}	The second ENTITYUNRELATED presents the ENTITYUNRELATED for ENTITY multimodal ENTITYOTHER which has been ENTITYUNRELATED within the ENTITYUNRELATED of the XTRA ENTITYUNRELATED ( a NL ENTITYUNRELATED ENTITYUNRELATED to ENTITYUNRELATED ) .
The component POPEL generates referential expressions which may be accompanied by a pointing gesture .	component	expressions	usage	{'e1': {'word': 'component', 'word_index': [(1, 1)], 'id': 'C88-2122.47'}, 'e2': {'word': 'expressions', 'word_index': [(5, 5)], 'id': 'C88-2122.49'}}	The ENTITY POPEL ENTITYUNRELATED referential ENTITYOTHER which may be accompanied by a pointing ENTITYUNRELATED .
"The appearance of these gestures depends on several factors , e.g. the type of referent (whether it is a region or an entry of the form ) and its complexity . """	factors	gestures	result	{'e1': {'word': 'factors', 'word_index': [(8, 8)], 'id': 'C88-2122.52'}, 'e2': {'word': 'gestures', 'word_index': [(4, 4)], 'id': 'C88-2122.51'}}	"The appearance of these ENTITYOTHER depends on several ENTITY , e.g. the ENTITYUNRELATED of ENTITYUNRELATED ( whether it is a ENTITYUNRELATED or an ENTITYUNRELATED of the ENTITYUNRELATED ) and its ENTITYUNRELATED . """
Local word-reordering is effectively encoded in Hiero-like rules ; whereas non-local word-reordering , which allows for long-range movements of syntactic chunks , is represented in tree-based reordering rules , which contain variables correspond to source-side syntactic constituents .	rules	word-reordering	model-feature	{'e1': {'word': 'rules', 'word_index': [(8, 8)], 'id': 'D08-1060.19'}, 'e2': {'word': 'word-reordering', 'word_index': [(1, 1)], 'id': 'D08-1060.18'}}	Local ENTITYOTHER is effectively encoded in Hiero -like ENTITY ; whereas non- local ENTITYUNRELATED , which allows for long - range movements of ENTITYUNRELATED ENTITYUNRELATED , is represented in ENTITYUNRELATED reordering ENTITYUNRELATED , which contain ENTITYUNRELATED correspond to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
Automatic Acquisition of Basic Katakana Lexicon from a Given Corpus	Lexicon	Corpus	part_whole	{'e1': {'word': 'Lexicon', 'word_index': [(5, 5)], 'id': 'I05-1060.4'}, 'e2': {'word': 'Corpus', 'word_index': [(9, 9)], 'id': 'I05-1060.5'}}	ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED Katakana ENTITY from a Given ENTITYOTHER
The Influence of Data Homogeneity on NLP System Performance	Homogeneity	Performance	result	{'e1': {'word': 'Homogeneity', 'word_index': [(4, 4)], 'id': 'I05-2039.3'}, 'e2': {'word': 'Performance', 'word_index': [(7, 7)], 'id': 'I05-2039.5'}}	The ENTITYUNRELATED of ENTITYUNRELATED ENTITY on ENTITYUNRELATED ENTITYOTHER
In this work we study the influence of corpus homogeneity on corpus-based NLP system performance .	homogeneity	performance	result	{'e1': {'word': 'homogeneity', 'word_index': [(9, 9)], 'id': 'I05-2039.9'}, 'e2': {'word': 'performance', 'word_index': [(13, 13)], 'id': 'I05-2039.12'}}	In this work we ENTITYUNRELATED the ENTITYUNRELATED of ENTITYUNRELATED ENTITY on ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .	sizes	data	model-feature	{'e1': {'word': 'sizes', 'word_index': [(5, 5)], 'id': 'I05-2039.34'}, 'e2': {'word': 'data', 'word_index': [(8, 8)], 'id': 'I05-2039.36'}}	We show that beyond minimal ENTITY of ENTITYUNRELATED ENTITYOTHER the excessive ENTITYUNRELATED of heterogeneous ENTITYUNRELATED proves prejudicial in ENTITYUNRELATED of both ENTITYUNRELATED and ENTITYUNRELATED : excessively restricting the ENTITYUNRELATED ENTITYUNRELATED to a particular ENTITYUNRELATED may be prejudicial in ENTITYUNRELATED of In - ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , and that heterogeneous , Out - of - ENTITYUNRELATED ENTITYUNRELATED may in fact contribute to better sytem ENTITYUNRELATED .
We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .	Domain	performance	result	{'e1': {'word': 'Domain', 'word_index': [(54, 54)], 'id': 'I05-2039.49'}, 'e2': {'word': 'performance', 'word_index': [(63, 63)], 'id': 'I05-2039.51'}}	We show that beyond minimal ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED the excessive ENTITYUNRELATED of heterogeneous ENTITYUNRELATED proves prejudicial in ENTITYUNRELATED of both ENTITYUNRELATED and ENTITYUNRELATED : excessively restricting the ENTITYUNRELATED ENTITYUNRELATED to a particular ENTITYUNRELATED may be prejudicial in ENTITYUNRELATED of In - ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , and that heterogeneous , Out - of - ENTITY ENTITYUNRELATED may in fact contribute to better sytem ENTITYOTHER .
Since there is no consolidated nomenclature for most biomedical NEs, most NER systems relying on limited dictionaries or rules do not perform satisfactorily.	dictionaries	systems	usage	{'e1': {'word': 'dictionaries', 'word_index': [(17, 17)], 'id': 'I05-2046.11'}, 'e2': {'word': 'systems', 'word_index': [(13, 13)], 'id': 'I05-2046.10'}}	Since there is no consolidated nomenclature for most biomedical NEs , most NER ENTITYOTHER relying on limited ENTITY or ENTITYUNRELATED do not ENTITYUNRELATED satisfactorily .
While first proposed for English , the algorithm counts for its success on two characteristics that Chinese and English have in common .	characteristics	algorithm	usage	{'e1': {'word': 'characteristics', 'word_index': [(14, 14)], 'id': 'I05-3016.22'}, 'e2': {'word': 'algorithm', 'word_index': [(7, 7)], 'id': 'I05-3016.20'}}	While first ENTITYUNRELATED for ENTITYUNRELATED , the ENTITYOTHER counts for its ENTITYUNRELATED on two ENTITY that ENTITYUNRELATED and ENTITYUNRELATED have in ENTITYUNRELATED .
Both languages are SVO, and both are fixed word order languages .	order	languages	model-feature	{'e1': {'word': 'order', 'word_index': [(10, 10)], 'id': 'I05-3016.28'}, 'e2': {'word': 'languages', 'word_index': [(1, 1)], 'id': 'I05-3016.26'}}	Both ENTITYOTHER are SVO , and both are fixed ENTITYUNRELATED ENTITY ENTITYUNRELATED .
The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%.	algorithm	accuracy	result	{'e1': {'word': 'algorithm', 'word_index': [(4, 4)], 'id': 'I05-3016.34'}, 'e2': {'word': 'accuracy', 'word_index': [(1, 1)], 'id': 'I05-3016.33'}}	The ENTITYOTHER of the ENTITY on overt , third - person pronouns at the ENTITYUNRELATED ENTITYUNRELATED was 77.6 % , and the ENTITYUNRELATED for resolving ENTITYUNRELATED zero pronouns was 73.3 %.
"In contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki 's two-mechanism proposal for resolving inter- vs. """	algorithm	accuracy	result	{'e1': {'word': 'algorithm', 'word_index': [(7, 7)], 'id': 'I05-3016.41'}, 'e2': {'word': 'accuracy', 'word_index': [(4, 4)], 'id': 'I05-3016.40'}}	"In ENTITYUNRELATED , the ENTITYOTHER of the ENTITY on pronouns that appeared in subordinate ENTITYUNRELATED was only 43.3 % , ENTITYUNRELATED ENTITYUNRELATED for Miltsakaki 's ENTITYUNRELATED ENTITYUNRELATED for resolving inter - vs. """
Variationist data is available and challenging , in particular for dialectology, the study of geographical variation , which will be the focus of this paper , although we present approaches we expect to transfer smoothly to the study of variation correlating with other extralinguistic variables .	paper	variation	topic	{'e1': {'word': 'paper', 'word_index': [(25, 25)], 'id': 'E03-1088.12'}, 'e2': {'word': 'variation', 'word_index': [(16, 16)], 'id': 'E03-1088.10'}}	Variationist ENTITYUNRELATED is available and ENTITYUNRELATED , in particular for dialectology , the ENTITYUNRELATED of geographical ENTITYOTHER , which will be the ENTITYUNRELATED of this ENTITY , although we present ENTITYUNRELATED we expect to ENTITYUNRELATED smoothly to the ENTITYUNRELATED of ENTITYUNRELATED correlating with other extralinguistic ENTITYUNRELATED .
This paper describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text .	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C96-2164.4'}, 'e2': {'word': 'system', 'word_index': [(4, 4)], 'id': 'C96-2164.5'}}	This ENTITY describes a ENTITYOTHER which automatically creates an ENTITYUNRELATED of a ENTITYUNRELATED article by selecting important ENTITYUNRELATED of a given ENTITYUNRELATED .
This paper describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text .	abstract	newspaper	model-feature	{'e1': {'word': 'abstract', 'word_index': [(9, 9)], 'id': 'C96-2164.6'}, 'e2': {'word': 'newspaper', 'word_index': [(12, 12)], 'id': 'C96-2164.7'}}	This ENTITYUNRELATED describes a ENTITYUNRELATED which automatically creates an ENTITY of a ENTITYOTHER article by selecting important ENTITYUNRELATED of a given ENTITYUNRELATED .
This paper describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text .	sentences	text	part_whole	{'e1': {'word': 'sentences', 'word_index': [(17, 17)], 'id': 'C96-2164.8'}, 'e2': {'word': 'text', 'word_index': [(21, 21)], 'id': 'C96-2164.9'}}	This ENTITYUNRELATED describes a ENTITYUNRELATED which automatically creates an ENTITYUNRELATED of a ENTITYUNRELATED article by selecting important ENTITY of a given ENTITYOTHER .
To determine the importance of a sentence , several superficial features are considered, and weights for features are determined by multiple-regression analysis of a hand processed corpus .	analysis	corpus	topic	{'e1': {'word': 'analysis', 'word_index': [(22, 22)], 'id': 'C96-2164.16'}, 'e2': {'word': 'corpus', 'word_index': [(27, 27)], 'id': 'C96-2164.19'}}	To determine the ENTITYUNRELATED of a ENTITYUNRELATED , several superficial ENTITYUNRELATED are considered , and ENTITYUNRELATED for ENTITYUNRELATED are determined by ENTITYUNRELATED ENTITY of a ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER .
We advocate a radical approach where the surface disappears from the XML document altogether to be handled exclusively by rendering mechanisms .	mechanisms	surface	usage	{'e1': {'word': 'mechanisms', 'word_index': [(20, 20)], 'id': 'C00-1036.13'}, 'e2': {'word': 'surface', 'word_index': [(7, 7)], 'id': 'C00-1036.11'}}	We advocate a radical ENTITYUNRELATED where the ENTITYOTHER disappears from the XML ENTITYUNRELATED altogether to be handled exclusively by rendering ENTITY .
This move is based on the view that the author's choices when authoring XML documents are best seen as language-neutral semantic decisions , that the structure can then be viewed as interlingual content , and that the textual output should be derived from this content by language-specific realization mechanisms , thus assimilating XML authoring to Multilingual Document Authoring.	output	content	part_whole	{'e1': {'word': 'output', 'word_index': [(39, 39)], 'id': 'C00-1036.22'}, 'e2': {'word': 'content', 'word_index': [(45, 45)], 'id': 'C00-1036.23'}}	This move is ENTITYUNRELATED on the view that the author 's ENTITYUNRELATED when authoring XML ENTITYUNRELATED are best seen as ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , that the ENTITYUNRELATED can then be viewed as interlingual ENTITYUNRELATED , and that the textual ENTITY should be derived from this ENTITYOTHER by ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , thus assimilating XML authoring to Multilingual ENTITYUNRELATED Authoring .
However, standard XML tools have important limitations when used for such a purpose : (1) they are weak at propagating semantic dependencies between different parts of the structure , and, (2) current XML rendering tools are ill-suited for handling the grammatical combination of textual units .	limitations	tools	model-feature	{'e1': {'word': 'limitations', 'word_index': [(7, 7)], 'id': 'C00-1036.30'}, 'e2': {'word': 'tools', 'word_index': [(4, 4)], 'id': 'C00-1036.29'}}	However , ENTITYUNRELATED XML ENTITYOTHER have important ENTITY when used for such a ENTITYUNRELATED : ( 1 ) they are weak at propagating ENTITYUNRELATED ENTITYUNRELATED between different ENTITYUNRELATED of the ENTITYUNRELATED , and , ( 2 ) ENTITYUNRELATED XML rendering ENTITYUNRELATED are ill-suited for handling the grammatical ENTITYUNRELATED of textual ENTITYUNRELATED .
In this paper , we present three semantic grouping methods : similarity-based , verb-based and category-based grouping, and their implementation in the SLUI toolkit.	paper	methods	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'C02-1041.27'}, 'e2': {'word': 'methods', 'word_index': [(9, 9)], 'id': 'C02-1041.29'}}	In this ENTITY , we present three ENTITYUNRELATED grouping ENTITYOTHER : ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED grouping , and their ENTITYUNRELATED in the SLUI toolkit .
We present a notation for the declarative statement of morphological relationships and lexical rules , based on the traditional notion of Word and Paradigm Elsewhere Condition, string equations</abstract>	notation	relationships	model-feature	{'e1': {'word': 'notation', 'word_index': [(3, 3)], 'id': 'E89-1008.2'}, 'e2': {'word': 'relationships', 'word_index': [(10, 10)], 'id': 'E89-1008.3'}}	We present a ENTITY for the declarative statement of morphological ENTITYOTHER and ENTITYUNRELATED ENTITYUNRELATED , ENTITYUNRELATED on the traditional ENTITYUNRELATED of ENTITYUNRELATED and ENTITYUNRELATED Elsewhere Condition , ENTITYUNRELATED ENTITYUNRELATED < / abstract >
We propose a parser for constraint-logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top-down control .	control	parser	usage	{'e1': {'word': 'control', 'word_index': [(23, 23)], 'id': 'E99-1022.8'}, 'e2': {'word': 'parser', 'word_index': [(3, 3)], 'id': 'E99-1022.3'}}	We ENTITYUNRELATED a ENTITYOTHER for ENTITYUNRELATED grammars ENTITYUNRELATED HPSG that combines the ENTITYUNRELATED of dynamic bottom - up and ENTITYUNRELATED top - down ENTITY .
The parser allows the user to apply magic compilation to specific constraints in a grammar which as a result can be processed dynamically in a bottom-up and goal-directed fashion .	compilation	constraints	usage	{'e1': {'word': 'compilation', 'word_index': [(8, 8)], 'id': 'E99-1022.12'}, 'e2': {'word': 'constraints', 'word_index': [(11, 11)], 'id': 'E99-1022.13'}}	The ENTITYUNRELATED allows the ENTITYUNRELATED to ENTITYUNRELATED magic ENTITY to specific ENTITYOTHER in a grammar which as a ENTITYUNRELATED can be ENTITYUNRELATED dynamically in a bottom - up and ENTITYUNRELATED ENTITYUNRELATED .
State of the art top-down processing techniques are used to deal with the remaining constraints .	processing	constraints	usage	{'e1': {'word': 'processing', 'word_index': [(5, 5)], 'id': 'E99-1022.18'}, 'e2': {'word': 'constraints', 'word_index': [(14, 14)], 'id': 'E99-1022.21'}}	State of the art top-down ENTITY ENTITYUNRELATED are used to ENTITYUNRELATED with the remaining ENTITYOTHER .
We discuss various aspects concerning the implementation of the parser as part of a grammar development system .	parser	system	part_whole	{'e1': {'word': 'parser', 'word_index': [(9, 9)], 'id': 'E99-1022.25'}, 'e2': {'word': 'system', 'word_index': [(16, 16)], 'id': 'E99-1022.28'}}	We discuss various ENTITYUNRELATED ENTITYUNRELATED the ENTITYUNRELATED of the ENTITY as ENTITYUNRELATED of a grammar ENTITYUNRELATED ENTITYOTHER .
"""Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval ."	words	sentences	part_whole	{'e1': {'word': 'words', 'word_index': [(6, 6)], 'id': 'E99-1023.4'}, 'e2': {'word': 'sentences', 'word_index': [(2, 2)], 'id': 'E99-1023.2'}}	""" Dividing ENTITYOTHER in ENTITYUNRELATED of ENTITY is a useful preprocessing ENTITYUNRELATED for ENTITYUNRELATED , ENTITYUNRELATED and ENTITYUNRELATED ."
"""Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval ."	parsing	information extraction	usage	{'e1': {'word': 'parsing', 'word_index': [(13, 13)], 'id': 'E99-1023.6'}, 'e2': {'word': 'information extraction', 'word_index': [(15, 15)], 'id': 'E99-1023.7'}}	""" Dividing ENTITYUNRELATED in ENTITYUNRELATED of ENTITYUNRELATED is a useful preprocessing ENTITYUNRELATED for ENTITY , ENTITYOTHER and ENTITYUNRELATED ."
"( Ramshaw and  Marcus, 1995 ) have introduced a """"convenient""""data representation for chunking by converting it to a tagging task ."	data	representation	model-feature	{'e1': {'word': 'data', 'word_index': [(15, 15)], 'id': 'E99-1023.9'}, 'e2': {'word': 'representation', 'word_index': [(16, 16)], 'id': 'E99-1023.10'}}	"( Ramshaw and Marcus , 1995 ) have introduced a "" "" convenient "" "" ENTITY ENTITYOTHER for ENTITYUNRELATED by converting it to a ENTITYUNRELATED ENTITYUNRELATED ."
"( Ramshaw and  Marcus, 1995 ) have introduced a """"convenient""""data representation for chunking by converting it to a tagging task ."	chunking	tagging	result	{'e1': {'word': 'chunking', 'word_index': [(18, 18)], 'id': 'E99-1023.11'}, 'e2': {'word': 'tagging', 'word_index': [(24, 24)], 'id': 'E99-1023.12'}}	"( Ramshaw and Marcus , 1995 ) have introduced a "" "" convenient "" "" ENTITYUNRELATED ENTITYUNRELATED for ENTITY by converting it to a ENTITYOTHER ENTITYUNRELATED ."
In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks .	paper	data	result	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'E99-1023.14'}, 'e2': {'word': 'data', 'word_index': [(8, 8)], 'id': 'E99-1023.15'}}	In this ENTITY we will examine seven different ENTITYOTHER ENTITYUNRELATED for the ENTITYUNRELATED of recognizing ENTITYUNRELATED ENTITYUNRELATED .
We will show that the the data representation choice has a minor influence on chunking performance .	choice	performance	result	{'e1': {'word': 'choice', 'word_index': [(8, 8)], 'id': 'E99-1023.22'}, 'e2': {'word': 'performance', 'word_index': [(15, 15)], 'id': 'E99-1023.25'}}	We will show that the the ENTITYUNRELATED ENTITYUNRELATED ENTITY has a minor ENTITYUNRELATED on ENTITYUNRELATED ENTITYOTHER .
Detection Of Japanese Homophone Errors By A Decision List Including A Written Word As A Default Evidence	Word	List	part_whole	{'e1': {'word': 'Word', 'word_index': [(12, 12)], 'id': 'E99-1024.5'}, 'e2': {'word': 'List', 'word_index': [(8, 8)], 'id': 'E99-1024.4'}}	ENTITYUNRELATED Of ENTITYUNRELATED Homophone Errors By A ENTITYUNRELATED ENTITYOTHER Including A Written ENTITY As A ENTITYUNRELATED ENTITYUNRELATED
In this paper , we propose a practical method to detect Japanese homophone errors in Japanese texts .	method	errors	usage	{'e1': {'word': 'method', 'word_index': [(8, 8)], 'id': 'E99-1024.10'}, 'e2': {'word': 'errors', 'word_index': [(13, 13)], 'id': 'E99-1024.12'}}	In this ENTITYUNRELATED , we ENTITYUNRELATED a practical ENTITY to detect ENTITYUNRELATED homophone ENTITYOTHER in ENTITYUNRELATED ENTITYUNRELATED .
It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently.	errors	revision	part_whole	{'e1': {'word': 'errors', 'word_index': [(7, 7)], 'id': 'E99-1024.15'}, 'e2': {'word': 'revision', 'word_index': [(10, 10)], 'id': 'E99-1024.17'}}	It is very important to detect homophone ENTITY in ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED because ENTITYUNRELATED ENTITYUNRELATED suffer from homophone ENTITYUNRELATED frequently .
It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently.	errors	texts	model-feature	{'e1': {'word': 'errors', 'word_index': [(18, 18)], 'id': 'E99-1024.21'}, 'e2': {'word': 'texts', 'word_index': [(14, 14)], 'id': 'E99-1024.20'}}	It is very important to detect homophone ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED because ENTITYUNRELATED ENTITYOTHER suffer from homophone ENTITY frequently .
In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word .	word	list	part_whole	{'e1': {'word': 'word', 'word_index': [(8, 8)], 'id': 'E99-1024.36'}, 'e2': {'word': 'list', 'word_index': [(13, 13)], 'id': 'E99-1024.38'}}	In this ENTITYUNRELATED , we incorporate the written ENTITY into the original ENTITYUNRELATED ENTITYOTHER by obtaining the identifying ENTITYUNRELATED of the written ENTITYUNRELATED .
In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word .	strength	word	model-feature	{'e1': {'word': 'strength', 'word_index': [(18, 18)], 'id': 'E99-1024.39'}, 'e2': {'word': 'word', 'word_index': [(22, 22)], 'id': 'E99-1024.40'}}	In this ENTITYUNRELATED , we incorporate the written ENTITYUNRELATED into the original ENTITYUNRELATED ENTITYUNRELATED by obtaining the identifying ENTITY of the written ENTITYOTHER .
New Models For Improving Supertag Disambiguation</title>	Models	Disambiguation	usage	{'e1': {'word': 'Models', 'word_index': [(1, 1)], 'id': 'E99-1025.1'}, 'e2': {'word': 'Disambiguation', 'word_index': [(5, 5)], 'id': 'E99-1025.2'}}	New ENTITY For Improving Supertag ENTITYOTHER < / title >
In this paper we present two approaches : contextual models , which exploit a variety of features in order to improve supertag performance , and class-based models , which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity .	models	paper	topic	{'e1': {'word': 'models', 'word_index': [(9, 9)], 'id': 'E99-1025.10'}, 'e2': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'E99-1025.8'}}	In this ENTITYOTHER we present two ENTITYUNRELATED : contextual ENTITY , which exploit a ENTITYUNRELATED of ENTITYUNRELATED in ENTITYUNRELATED to ENTITYUNRELATED supertag ENTITYUNRELATED , and ENTITYUNRELATED , which assign sets of supertags to ENTITYUNRELATED in ENTITYUNRELATED to substantially ENTITYUNRELATED ENTITYUNRELATED with only a slight ENTITYUNRELATED in ENTITYUNRELATED .
In this paper we present two approaches : contextual models , which exploit a variety of features in order to improve supertag performance , and class-based models , which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity .	features	performance	result	{'e1': {'word': 'features', 'word_index': [(16, 16)], 'id': 'E99-1025.12'}, 'e2': {'word': 'performance', 'word_index': [(22, 22)], 'id': 'E99-1025.15'}}	In this ENTITYUNRELATED we present two ENTITYUNRELATED : contextual ENTITYUNRELATED , which exploit a ENTITYUNRELATED of ENTITY in ENTITYUNRELATED to ENTITYUNRELATED supertag ENTITYOTHER , and ENTITYUNRELATED , which assign sets of supertags to ENTITYUNRELATED in ENTITYUNRELATED to substantially ENTITYUNRELATED ENTITYUNRELATED with only a slight ENTITYUNRELATED in ENTITYUNRELATED .
In this paper we present two approaches : contextual models , which exploit a variety of features in order to improve supertag performance , and class-based models , which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity .	class-based models	words	usage	{'e1': {'word': 'class-based models', 'word_index': [(25, 25)], 'id': 'E99-1025.16'}, 'e2': {'word': 'words', 'word_index': [(33, 33)], 'id': 'E99-1025.17'}}	In this ENTITYUNRELATED we present two ENTITYUNRELATED : contextual ENTITYUNRELATED , which exploit a ENTITYUNRELATED of ENTITYUNRELATED in ENTITYUNRELATED to ENTITYUNRELATED supertag ENTITYUNRELATED , and ENTITY , which assign sets of supertags to ENTITYOTHER in ENTITYUNRELATED to substantially ENTITYUNRELATED ENTITYUNRELATED with only a slight ENTITYUNRELATED in ENTITYUNRELATED .
Japanese Dependency Structure Analysis Based On Maximum Entropy Models	Models	Analysis	usage	{'e1': {'word': 'Models', 'word_index': [(6, 6)], 'id': 'E99-1026.6'}, 'e2': {'word': 'Analysis', 'word_index': [(2, 2)], 'id': 'E99-1026.3'}}	ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED On ENTITYUNRELATED ENTITY
This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models .	paper	analysis	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'E99-1026.7'}, 'e2': {'word': 'analysis', 'word_index': [(5, 5)], 'id': 'E99-1026.9'}}	This ENTITY describes a ENTITYUNRELATED ENTITYOTHER of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED on the ENTITYUNRELATED .
Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units .	weights	model	usage	{'e1': {'word': 'weights', 'word_index': [(7, 7)], 'id': 'E99-1026.15'}, 'e2': {'word': 'model', 'word_index': [(1, 1)], 'id': 'E99-1026.14'}}	Our ENTITYOTHER is created by learning the ENTITY of some ENTITYUNRELATED from a ENTITYUNRELATED to predict the ENTITYUNRELATED between bunsetsus or phrasal ENTITYUNRELATED .
The dependency accuracy of our system is 87.2% using the Kyoto University corpus .	accuracy	system	model-feature	{'e1': {'word': 'accuracy', 'word_index': [(2, 2)], 'id': 'E99-1026.21'}, 'e2': {'word': 'system', 'word_index': [(5, 5)], 'id': 'E99-1026.22'}}	The ENTITYUNRELATED ENTITY of our ENTITYOTHER is 87.2 % using the Kyoto ENTITYUNRELATED ENTITYUNRELATED .
"""Statistical "" parsers trained and tested on the Penn Wall Street Journal (wsj) treebank have shown vast improvements over the last 10 years."	Wall Street Journal	parsers	usage	{'e1': {'word': 'Wall Street Journal', 'word_index': [(10, 10)], 'id': 'P06-1043.8'}, 'e2': {'word': 'parsers', 'word_index': [(3, 3)], 'id': 'P06-1043.5'}}	""" ENTITYUNRELATED "" ENTITYOTHER ENTITYUNRELATED and ENTITYUNRELATED on the Penn ENTITY ( wsj ) treebank have shown vast ENTITYUNRELATED over the last 10 years ."
Much of this improvement , however, is based upon an ever-increasing number of features to be trained on (typically) the wsj treebank data .	features	improvement	usage	{'e1': {'word': 'features', 'word_index': [(14, 14)], 'id': 'P06-1043.13'}, 'e2': {'word': 'improvement', 'word_index': [(3, 3)], 'id': 'P06-1043.10'}}	Much of this ENTITYOTHER , however , is ENTITYUNRELATED upon an ever-increasing ENTITYUNRELATED of ENTITY to be ENTITYUNRELATED on ( typically ) the wsj treebank ENTITYUNRELATED .
This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .	corpus	parsers	usage	{'e1': {'word': 'corpus', 'word_index': [(15, 15)], 'id': 'P06-1043.18'}, 'e2': {'word': 'parsers', 'word_index': [(7, 7)], 'id': 'P06-1043.17'}}	This has led to ENTITYUNRELATED that such ENTITYOTHER may be too finely tuned to this ENTITY at the expense of ENTITYUNRELATED to other ENTITYUNRELATED .
Automatic Classification Of Verbs In Biomedical Texts	Classification	Texts	usage	{'e1': {'word': 'Classification', 'word_index': [(1, 1)], 'id': 'P06-1044.2'}, 'e2': {'word': 'Texts', 'word_index': [(6, 6)], 'id': 'P06-1044.3'}}	ENTITYUNRELATED ENTITY Of Verbs In Biomedical ENTITYOTHER
"While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb <entity id=""P06-1044.19"">classes</entity> from <entity id=""P06-1044.20"">cross-domain</entity> <entity id=""P06-1044.21"">corpora</entity> with promising <entity id=""P06-1044.22"">accuracy</entity> ."	construction	classes	usage	{'e1': {'word': 'construction', 'word_index': [(2, 2)], 'id': 'P06-1044.15'}, 'e2': {'word': 'classes', 'word_index': [(5, 5)], 'id': 'P06-1044.16'}}	"While ENTITYUNRELATED ENTITY of such ENTITYOTHER is difficult , recent ENTITYUNRELATED shows that it is possible to automatically induce verb < entity id = "" P06-1044.19 "" > classes < / entity > from < entity id = "" P06-1044.20 "" > cross-domain < / entity > < entity id = "" P06-1044.21 "" > corpora < / entity > with promising < entity id = "" P06-1044.22 "" > accuracy < / entity > ."
We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine.	technology	domain	usage	{'e1': {'word': 'technology', 'word_index': [(7, 7)], 'id': 'P06-1044.25'}, 'e2': {'word': 'domain', 'word_index': [(15, 15)], 'id': 'P06-1044.28'}}	We ENTITYUNRELATED a novel ENTITYUNRELATED where similar ENTITY is ENTITYUNRELATED to the important , ENTITYUNRELATED ENTITYOTHER of biomedicine .
We show that the resulting classification , acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific .	corpus	classification	usage	{'e1': {'word': 'corpus', 'word_index': [(10, 10)], 'id': 'P06-1044.31'}, 'e2': {'word': 'classification', 'word_index': [(5, 5)], 'id': 'P06-1044.30'}}	We show that the ENTITYUNRELATED ENTITYOTHER , acquired from a ENTITY of biomedical ENTITYUNRELATED articles , is highly accurate and strongly ENTITYUNRELATED .
It can be used to aid bio-nlp directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .	verbs	texts	part_whole	{'e1': {'word': 'verbs', 'word_index': [(19, 19)], 'id': 'P06-1044.36'}, 'e2': {'word': 'texts', 'word_index': [(22, 22)], 'id': 'P06-1044.37'}}	It can be used to aid bio-nlp directly or as useful material for investigating the ENTITYUNRELATED and ENTITYUNRELATED of ENTITY in biomedical ENTITYOTHER .
The present paper discusses the new version of the algorithm and its performance in detail .	paper	algorithm	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'P06-1082.41'}, 'e2': {'word': 'algorithm', 'word_index': [(9, 9)], 'id': 'P06-1082.43'}}	The present ENTITY discusses the new ENTITYUNRELATED of the ENTITYOTHER and its ENTITYUNRELATED in ENTITYUNRELATED .
You Can't Beat Frequency (Unless You Use Linguistic Knowledge ) - A Qualitative Evaluation Of Association Measures For Collocation And Term Extraction</title>	Evaluation	Extraction	topic	{'e1': {'word': 'Evaluation', 'word_index': [(14, 14)], 'id': 'P06-1099.3'}, 'e2': {'word': 'Extraction', 'word_index': [(22, 22)], 'id': 'P06-1099.7'}}	You Ca n't Beat ENTITYUNRELATED ( Unless You Use ENTITYUNRELATED ) - A Qualitative ENTITY Of ENTITYUNRELATED Measures For ENTITYUNRELATED And ENTITYUNRELATED ENTITYOTHER < / title >
In this paper , we describe a rote extractor that learns patterns for finding semantic relationships in unrestricted text , with new procedures for pattern generalization and scoring	paper	extractor	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'P06-2002.7'}, 'e2': {'word': 'extractor', 'word_index': [(8, 8)], 'id': 'P06-2002.8'}}	In this ENTITY , we describe a rote ENTITYOTHER that learns ENTITYUNRELATED for finding ENTITYUNRELATED ENTITYUNRELATED in unrestricted ENTITYUNRELATED , with new ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED and scoring
In this paper , we describe a rote extractor that learns patterns for finding semantic relationships in unrestricted text , with new procedures for pattern generalization and scoring	procedures	generalization	usage	{'e1': {'word': 'procedures', 'word_index': [(22, 22)], 'id': 'P06-2002.13'}, 'e2': {'word': 'generalization', 'word_index': [(25, 25)], 'id': 'P06-2002.15'}}	In this ENTITYUNRELATED , we describe a rote ENTITYUNRELATED that learns ENTITYUNRELATED for finding ENTITYUNRELATED ENTITYUNRELATED in unrestricted ENTITYUNRELATED , with new ENTITY for ENTITYUNRELATED ENTITYOTHER and scoring
An Empirical Study Of Chinese Chunking	Study	Chinese	topic	{'e1': {'word': 'Study', 'word_index': [(2, 2)], 'id': 'P06-2013.1'}, 'e2': {'word': 'Chinese', 'word_index': [(4, 4)], 'id': 'P06-2013.2'}}	An Empirical ENTITY Of ENTITYOTHER Chunking
In this paper , we describe an empirical study of Chinese chunking on a corpus , which is extracted from UPENN Chinese Treebank-4 (CTB4).	chunking	corpus	usage	{'e1': {'word': 'chunking', 'word_index': [(11, 11)], 'id': 'P06-2013.6'}, 'e2': {'word': 'corpus', 'word_index': [(14, 14)], 'id': 'P06-2013.7'}}	In this ENTITYUNRELATED , we describe an empirical ENTITYUNRELATED of ENTITYUNRELATED ENTITY on a ENTITYOTHER , which is ENTITYUNRELATED from UPENN ENTITYUNRELATED Treebank - 4 ( CTB4 ) .
2) We propose two novel voting methods based on the characteristics of chunking task .	chunking	methods	usage	{'e1': {'word': 'chunking', 'word_index': [(13, 13)], 'id': 'P06-2013.35'}, 'e2': {'word': 'methods', 'word_index': [(7, 7)], 'id': 'P06-2013.32'}}	2 ) We ENTITYUNRELATED two novel voting ENTITYOTHER ENTITYUNRELATED on the ENTITYUNRELATED of ENTITY ENTITYUNRELATED .
Compared with traditional voting methods , the proposed voting methods consider long distance information .	methods	methods	compare	{'e1': {'word': 'methods', 'word_index': [(4, 4)], 'id': 'P06-2013.37'}, 'e2': {'word': 'methods', 'word_index': [(9, 9)], 'id': 'P06-2013.39'}}	Compared with traditional voting ENTITY , the ENTITYUNRELATED voting ENTITYOTHER consider long ENTITYUNRELATED ENTITYUNRELATED .
The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.	model	models	compare	{'e1': {'word': 'model', 'word_index': [(7, 7)], 'id': 'P06-2013.44'}, 'e2': {'word': 'models', 'word_index': [(11, 11)], 'id': 'P06-2013.45'}}	The ENTITYUNRELATED ENTITYUNRELATED show that the SVMs ENTITY outperforms the other ENTITYOTHER and that our ENTITYUNRELATED ENTITYUNRELATED can ENTITYUNRELATED ENTITYUNRELATED significantly .
The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.	approaches	performance	result	{'e1': {'word': 'approaches', 'word_index': [(16, 16)], 'id': 'P06-2013.47'}, 'e2': {'word': 'performance', 'word_index': [(19, 19)], 'id': 'P06-2013.49'}}	The ENTITYUNRELATED ENTITYUNRELATED show that the SVMs ENTITYUNRELATED outperforms the other ENTITYUNRELATED and that our ENTITYUNRELATED ENTITY can ENTITYUNRELATED ENTITYOTHER significantly .
"However, this hard constraint can also rule out correct alignments , and its utility decreases as alignment <entity id=""P06-2014.18"">models</entity> become more <entity id=""P06-2014.19"">complex</entity> ."	constraint	alignments	result	{'e1': {'word': 'constraint', 'word_index': [(4, 4)], 'id': 'P06-2014.13'}, 'e2': {'word': 'alignments', 'word_index': [(10, 10)], 'id': 'P06-2014.15'}}	"However , this hard ENTITY can also ENTITYUNRELATED out correct ENTITYOTHER , and its ENTITYUNRELATED decreases as alignment < entity id = "" P06-2014.18 "" > models < / entity > become more < entity id = "" P06-2014.19 "" > complex < / entity > ."
A Bio-Inspired Approach For Multi- Word Expression Extraction</title>	Approach	Extraction	usage	{'e1': {'word': 'Approach', 'word_index': [(2, 2)], 'id': 'P06-2023.1'}, 'e2': {'word': 'Extraction', 'word_index': [(8, 8)], 'id': 'P06-2023.4'}}	A Bio-Inspired ENTITY For Mult i- ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER < / title >
This paper proposes a new approach for Multi-word Expression (MWE) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis 	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'P06-2023.5'}, 'e2': {'word': 'approach', 'word_index': [(5, 5)], 'id': 'P06-2023.7'}}	This ENTITY ENTITYUNRELATED a new ENTITYOTHER for ENTITYUNRELATED ENTITYUNRELATED ( MWE ) ENTITYUNRELATED on the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED because textual ENTITYUNRELATED is similar to ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED
This paper proposes a new approach for Multi-word Expression (MWE) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis 	extraction	Expression	usage	{'e1': {'word': 'extraction', 'word_index': [(12, 12)], 'id': 'P06-2023.10'}, 'e2': {'word': 'Expression', 'word_index': [(8, 8)], 'id': 'P06-2023.9'}}	This ENTITYUNRELATED ENTITYUNRELATED a new ENTITYUNRELATED for ENTITYUNRELATED ENTITYOTHER ( MWE ) ENTITY on the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED because textual ENTITYUNRELATED is similar to ENTITYUNRELATED ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED
We perform this developed LCS technique combined with linguistic criteria in MWE extraction .	technique	extraction	usage	{'e1': {'word': 'technique', 'word_index': [(5, 5)], 'id': 'P06-2023.27'}, 'e2': {'word': 'extraction', 'word_index': [(12, 12)], 'id': 'P06-2023.29'}}	We ENTITYUNRELATED this developed LCS ENTITY combined with linguistic ENTITYUNRELATED in MWE ENTITYOTHER .
"This paper classifies the scored dependency graphs and discusses the specific features of the """"Dependency Forest """" (DF) which is the packed shared data structure adopted in the """"Preference Dependency Grammar """" (PDG), and proposes the """"Graph Branch Algorithm """" for computing the optimum dependency tree from a DF."	Algorithm	computing	usage	{'e1': {'word': 'Algorithm', 'word_index': [(49, 49)], 'id': 'P06-2047.26'}, 'e2': {'word': 'computing', 'word_index': [(53, 53)], 'id': 'P06-2047.27'}}	"This ENTITYUNRELATED classifies the scored ENTITYUNRELATED and discusses the specific ENTITYUNRELATED of the "" "" ENTITYUNRELATED ENTITYUNRELATED "" "" ( DF ) which is the packed shared ENTITYUNRELATED ENTITYUNRELATED adopted in the "" "" ENTITYUNRELATED ENTITYUNRELATED "" "" ( PDG ) , and ENTITYUNRELATED the "" "" Graph Branch ENTITY "" "" for ENTITYOTHER the optimum ENTITYUNRELATED from a DF."
"This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm . """	behavior	algorithm	part_whole	{'e1': {'word': 'behavior', 'word_index': [(11, 11)], 'id': 'P06-2047.34'}, 'e2': {'word': 'algorithm', 'word_index': [(16, 16)], 'id': 'P06-2047.35'}}	"This ENTITYUNRELATED also ENTITYUNRELATED the ENTITYUNRELATED showing the ENTITYUNRELATED ENTITYUNRELATED and ENTITY of the graph branch ENTITYOTHER . """
"The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic <entity id=""P06-2091.12"">representations</entity> of Typed Dynamic <entity id=""P06-2091.13"">Logic</entity> (TDL), a dynamic plural <entity id=""P06-2091.14"">semantics</entity> defined in <entity id=""P06-2091.15"">typed</entity> lambda <entity id=""P06-2091.16"">calculus</entity> ."	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'P06-2091.4'}, 'e2': {'word': 'method', 'word_index': [(5, 5)], 'id': 'P06-2091.6'}}	"The present ENTITY ENTITYUNRELATED a ENTITYOTHER by which to ENTITYUNRELATED ENTITYUNRELATED of a ENTITYUNRELATED HPSG ENTITYUNRELATED into semantic < entity id = "" P06-2091.12 "" > representations < / entity > of Typed Dynamic < entity id = "" P06-2091.13 "" > Logic < / entity > ( TDL ) , a dynamic plural < entity id = "" P06-2091.14 "" > semantics < / entity > defined in < entity id = "" P06-2091.15 "" > typed < / entity > lambda < entity id = "" P06-2091.16 "" > calculus < /entity > ."
With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner .	representations	contexts	model-feature	{'e1': {'word': 'representations', 'word_index': [(3, 3)], 'id': 'P06-2091.18'}, 'e2': {'word': 'contexts', 'word_index': [(5, 5)], 'id': 'P06-2091.19'}}	With its ENTITYUNRELATED ENTITY of ENTITYOTHER , TDL analyzes and describes the inherently inter-sentential ENTITYUNRELATED of ENTITYUNRELATED and anaphora in a strictly lexicalized and compositional ENTITYUNRELATED .
The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics .	robustness	method	model-feature	{'e1': {'word': 'robustness', 'word_index': [(11, 11)], 'id': 'P06-2091.27'}, 'e2': {'word': 'method', 'word_index': [(8, 8)], 'id': 'P06-2091.26'}}	The present ENTITYUNRELATED shows that the ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER successfully combines ENTITY and descriptive ENTITYUNRELATED of contemporary ENTITYUNRELATED .
The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .	implementation	coverage	result	{'e1': {'word': 'implementation', 'word_index': [(2, 2)], 'id': 'P06-2091.30'}, 'e2': {'word': 'coverage', 'word_index': [(5, 5)], 'id': 'P06-2091.31'}}	The present ENTITY achieves high ENTITYOTHER , approximately 90 % , for the real ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED .
The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .	text	corpus	part_whole	{'e1': {'word': 'text', 'word_index': [(14, 14)], 'id': 'P06-2091.32'}, 'e2': {'word': 'corpus', 'word_index': [(18, 18)], 'id': 'P06-2091.34'}}	The present ENTITYUNRELATED achieves high ENTITYUNRELATED , approximately 90 % , for the real ENTITY of the ENTITYUNRELATED ENTITYOTHER .
Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases	Texts	Generation	usage	{'e1': {'word': 'Texts', 'word_index': [(7, 7)], 'id': 'P06-2096.3'}, 'e2': {'word': 'Generation', 'word_index': [(10, 10)], 'id': 'P06-2096.4'}}	Adding ENTITYUNRELATED To ENTITYUNRELATED For Aligning Comparable ENTITY For The ENTITYOTHER Of Paraphrases
Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ).	translations	text	usage	{'e1': {'word': 'translations', 'word_index': [(22, 22)], 'id': 'P06-2096.21'}, 'e2': {'word': 'text', 'word_index': [(26, 26)], 'id': 'P06-2096.22'}}	Prior work falls into two ENTITYUNRELATED , depending on the ENTITYUNRELATED of ENTITYUNRELATED used : ( a) ENTITYUNRELATED ( e.g. , multiple ENTITY of the same ENTITYOTHER ) or ( b ) comparable ENTITYUNRELATED ( non - parallel but on the same ENTITYUNRELATED ) .
In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts .	paper	algorithm	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'P06-2096.32'}, 'e2': {'word': 'algorithm', 'word_index': [(7, 7)], 'id': 'P06-2096.33'}}	In this ENTITY , we describe an ENTITYOTHER for incorporating ENTITYUNRELATED in the ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED with the ENTITYUNRELATED of ENTITYUNRELATED novel paraphrases of existing ENTITYUNRELATED .
Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.	alignment	methods	compare	{'e1': {'word': 'alignment', 'word_index': [(5, 5)], 'id': 'P06-2096.52'}, 'e2': {'word': 'methods', 'word_index': [(8, 8)], 'id': 'P06-2096.54'}}	Our ENTITYUNRELATED show that ENTITYUNRELATED ENTITY outrivals ENTITYUNRELATED ENTITYOTHER by 20 % in both grammatically and fidelity when computed over the novel ENTITYUNRELATED ENTITYUNRELATED by ENTITYUNRELATED finite state automata .
Clavius: Bi-Directional Parsing For Generic Multimodal Interaction</title>	Parsing	Interaction	usage	{'e1': {'word': 'Parsing', 'word_index': [(3, 3)], 'id': 'P06-3015.1'}, 'e2': {'word': 'Interaction', 'word_index': [(7, 7)], 'id': 'P06-3015.2'}}	Clavius : Bi-Directional ENTITY For Generic Multimodal ENTITYOTHER < / title >
We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments .	parsing	interaction	usage	{'e1': {'word': 'parsing', 'word_index': [(5, 5)], 'id': 'P06-3015.3'}, 'e2': {'word': 'interaction', 'word_index': [(14, 14)], 'id': 'P06-3015.7'}}	We introduce a new multi-threaded ENTITY ENTITYUNRELATED on ENTITYUNRELATED grammars ENTITYUNRELATED specifically for multimodal ENTITYOTHER and noisy ENTITYUNRELATED .
By lifting some traditional constraints , namely those related to the ordering of constituents , we overcome several difficulties of other systems in this domain .	constraints	constituents	model-feature	{'e1': {'word': 'constraints', 'word_index': [(4, 4)], 'id': 'P06-3015.9'}, 'e2': {'word': 'constituents', 'word_index': [(13, 13)], 'id': 'P06-3015.11'}}	By lifting some traditional ENTITY , namely those related to the ENTITYUNRELATED of ENTITYOTHER , we overcome several ENTITYUNRELATED of other ENTITYUNRELATED in this ENTITYUNRELATED .
By lifting some traditional constraints , namely those related to the ordering of constituents , we overcome several difficulties of other systems in this domain .	difficulties	systems	model-feature	{'e1': {'word': 'difficulties', 'word_index': [(18, 18)], 'id': 'P06-3015.12'}, 'e2': {'word': 'systems', 'word_index': [(21, 21)], 'id': 'P06-3015.13'}}	By lifting some traditional ENTITYUNRELATED , namely those related to the ENTITYUNRELATED of ENTITYUNRELATED , we overcome several ENTITY of other ENTITYOTHER in this ENTITYUNRELATED .
We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions .	criteria	process	result	{'e1': {'word': 'criteria', 'word_index': [(4, 4)], 'id': 'P06-3015.15'}, 'e2': {'word': 'process', 'word_index': [(13, 13)], 'id': 'P06-3015.18'}}	We also present several ENTITY used in this ENTITYUNRELATED to constrain the ENTITYUNRELATED ENTITYOTHER using dynamically loadable scoring ENTITYUNRELATED .
- Towards Web- Based Evaluation Of Automatic Natural Language Phrase Generation</title>	Evaluation	Generation	topic	{'e1': {'word': 'Evaluation', 'word_index': [(6, 6)], 'id': 'P06-4002.2'}, 'e2': {'word': 'Generation', 'word_index': [(11, 11)], 'id': 'P06-4002.6'}}	- Towards We b - ENTITYUNRELATED ENTITY Of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER < / title >
This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases</abstract>	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'P06-4002.7'}, 'e2': {'word': 'approach', 'word_index': [(5, 5)], 'id': 'P06-4002.8'}}	This ENTITY describes a novel ENTITYOTHER for the ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED of a trivial ENTITYUNRELATED ENTITYUNRELATED < / abstract >
This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases</abstract>	generation	phrases	usage	{'e1': {'word': 'generation', 'word_index': [(9, 9)], 'id': 'P06-4002.10'}, 'e2': {'word': 'phrases', 'word_index': [(16, 16)], 'id': 'P06-4002.13'}}	This ENTITYUNRELATED describes a novel ENTITYUNRELATED for the ENTITYUNRELATED ENTITY and ENTITYUNRELATED of a trivial ENTITYUNRELATED ENTITYOTHER < / abstract >
A Model Of Natural Language Processing Of Time - Related Expressions	Model	Time	model-feature	{'e1': {'word': 'Model', 'word_index': [(1, 1)], 'id': 'C80-1015.1'}, 'e2': {'word': 'Time', 'word_index': [(5, 5)], 'id': 'C80-1015.3'}}	A ENTITY Of ENTITYUNRELATED Of ENTITYOTHER - ENTITYUNRELATED Expressions
The system aims at translation of computer manuals , and basically follows to the transfer approach .	translation	manuals	usage	{'e1': {'word': 'translation', 'word_index': [(4, 4)], 'id': 'C80-1063.12'}, 'e2': {'word': 'manuals', 'word_index': [(7, 7)], 'id': 'C80-1063.14'}}	The ENTITYUNRELATED aims at ENTITY of ENTITYUNRELATED ENTITYOTHER , and basically follows to the ENTITYUNRELATED ENTITYUNRELATED .
The design principles of the system are discussed in detail , together with the overall constructions of the system .	principles	system	part_whole	{'e1': {'word': 'principles', 'word_index': [(2, 2)], 'id': 'C80-1063.18'}, 'e2': {'word': 'system', 'word_index': [(5, 5)], 'id': 'C80-1063.19'}}	The ENTITYUNRELATED ENTITY of the ENTITYOTHER are discussed in ENTITYUNRELATED , together with the overall ENTITYUNRELATED of the ENTITYUNRELATED .
Especially, the effectiveness of lexicon-based procedures , i.e.	effectiveness	procedures	model-feature	{'e1': {'word': 'effectiveness', 'word_index': [(3, 3)], 'id': 'C80-1063.23'}, 'e2': {'word': 'procedures', 'word_index': [(6, 6)], 'id': 'C80-1063.25'}}	Especially , the ENTITY of ENTITYUNRELATED ENTITYOTHER , i.e.
Some translation results are also given to illustrate the current abilities of the system .	abilities	system	part_whole	{'e1': {'word': 'abilities', 'word_index': [(9, 9)], 'id': 'C80-1063.49'}, 'e2': {'word': 'system', 'word_index': [(12, 12)], 'id': 'C80-1063.50'}}	Some ENTITYUNRELATED are also given to illustrate the ENTITYUNRELATED ENTITY of the ENTITYOTHER .
In this paper I describe a model for representing and processing referential relations : referential nets with attributes.	paper	model	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'C82-1016.7'}, 'e2': {'word': 'model', 'word_index': [(6, 6)], 'id': 'C82-1016.8'}}	In this ENTITY I describe a ENTITYOTHER for representing and ENTITYUNRELATED referential ENTITYUNRELATED : referential nets with attributes .
In this paper I describe a model for representing and processing referential relations : referential nets with attributes.	processing	relations	usage	{'e1': {'word': 'processing', 'word_index': [(10, 10)], 'id': 'C82-1016.9'}, 'e2': {'word': 'relations', 'word_index': [(12, 12)], 'id': 'C82-1016.10'}}	In this ENTITYUNRELATED I describe a ENTITYUNRELATED for representing and ENTITY referential ENTITYOTHER : referential nets with attributes .
"This paper proposes a multilayered approach to word formation which treats derivatives and compounds on several different levels of processing within a natural <entity id=""C82-1021.18"">language</entity> <entity id=""C82-1021.19"">dialogue <entity id=""C82-1021.20"">system</entity></entity> ."	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C82-1021.10'}, 'e2': {'word': 'approach', 'word_index': [(5, 5)], 'id': 'C82-1021.12'}}	"This ENTITY ENTITYUNRELATED a multilayered ENTITYOTHER to ENTITYUNRELATED ENTITYUNRELATED which treats derivatives and compounds on several different ENTITYUNRELATED of ENTITYUNRELATED within a natural < entity id = "" C82-1021.18 "" > language < / entity > < entity id = "" C82-1021.19 "" > dialogue < entity id = "" C82-1021.20 "" > system < / entity > </ entity > ."
Generation of word formations is viewed as a process comparable to the generation of elliptical utterances .	word	utterances	compare	{'e1': {'word': 'word', 'word_index': [(2, 2)], 'id': 'C82-1021.44'}, 'e2': {'word': 'utterances', 'word_index': [(15, 15)], 'id': 'C82-1021.48'}}	ENTITYUNRELATED of ENTITY ENTITYUNRELATED is viewed as a ENTITYUNRELATED comparable to the ENTITYUNRELATED of elliptical ENTITYOTHER .
This paper addresses the problems of	paper	problems	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C82-1031.11'}, 'e2': {'word': 'problems', 'word_index': [(4, 4)], 'id': 'C82-1031.12'}}	This ENTITY addresses the ENTITYOTHER of
The first part of this paper is dedicated to an overview of the parser of the system VIE-LANG (Viennese Language Understanding System ).	paper	parser	topic	{'e1': {'word': 'paper', 'word_index': [(5, 5)], 'id': 'C82-1059.3'}, 'e2': {'word': 'parser', 'word_index': [(13, 13)], 'id': 'C82-1059.5'}}	The first ENTITYUNRELATED of this ENTITY is dedicated to an ENTITYUNRELATED of the ENTITYOTHER of the ENTITYUNRELATED VIE-LANG ( Viennese ENTITYUNRELATED ) .
It parses directly into the internal representation of the system , without producing an.	representation	system	model-feature	{'e1': {'word': 'representation', 'word_index': [(6, 6)], 'id': 'C82-1059.15'}, 'e2': {'word': 'system', 'word_index': [(9, 9)], 'id': 'C82-1059.16'}}	It ENTITYUNRELATED directly into the internal ENTITY of the ENTITYOTHER , without producing an .
The last part discusses the relationship between some special features of the German language , and properties of the parser that originate in the language .	features	language	model-feature	{'e1': {'word': 'features', 'word_index': [(9, 9)], 'id': 'C82-1059.20'}, 'e2': {'word': 'language', 'word_index': [(13, 13)], 'id': 'C82-1059.21'}}	The last ENTITYUNRELATED discusses the ENTITYUNRELATED between some special ENTITY of the German ENTITYOTHER , and ENTITYUNRELATED of the ENTITYUNRELATED that originate in the ENTITYUNRELATED .
The last part discusses the relationship between some special features of the German language , and properties of the parser that originate in the language .	language	parser	usage	{'e1': {'word': 'language', 'word_index': [(24, 24)], 'id': 'C82-1059.24'}, 'e2': {'word': 'parser', 'word_index': [(19, 19)], 'id': 'C82-1059.23'}}	The last ENTITYUNRELATED discusses the ENTITYUNRELATED between some special ENTITYUNRELATED of the German ENTITYUNRELATED , and ENTITYUNRELATED of the ENTITYOTHER that originate in the ENTITY .
Definite Noun Phrases And The Semantics Of Discourse</title>	Semantics	Discourse	topic	{'e1': {'word': 'Semantics', 'word_index': [(5, 5)], 'id': 'C86-1088.2'}, 'e2': {'word': 'Discourse', 'word_index': [(7, 7)], 'id': 'C86-1088.3'}}	Definite ENTITYUNRELATED Phrases And The ENTITY Of ENTITYOTHER < / title >
This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes .	context	interpretation	usage	{'e1': {'word': 'context', 'word_index': [(38, 38)], 'id': 'C86-1088.24'}, 'e2': {'word': 'interpretation', 'word_index': [(29, 29)], 'id': 'C86-1088.21'}}	This recent ENTITYUNRELATED in theoretical ENTITYUNRELATED indicates a ENTITYUNRELATED of interest towards ENTITYUNRELATED that have been familiar in ENTITYUNRELATED ENTITYUNRELATED for the last decade : among others , the ENTITYOTHER of new ENTITYUNRELATED with ENTITYUNRELATED to a given ENTITY , and ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED into that ENTITYUNRELATED ; the ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED for larger pieces of ENTITYUNRELATED ; the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ; and the ENTITYUNRELATED of referential ENTITYUNRELATED .
This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes .	information	utterance	part_whole	{'e1': {'word': 'information', 'word_index': [(45, 45)], 'id': 'C86-1088.27'}, 'e2': {'word': 'utterance', 'word_index': [(44, 44)], 'id': 'C86-1088.26'}}	This recent ENTITYUNRELATED in theoretical ENTITYUNRELATED indicates a ENTITYUNRELATED of interest towards ENTITYUNRELATED that have been familiar in ENTITYUNRELATED ENTITYUNRELATED for the last decade : among others , the ENTITYUNRELATED of new ENTITYUNRELATED with ENTITYUNRELATED to a given ENTITYUNRELATED , and ENTITYUNRELATED of the ENTITYOTHER ENTITY into that ENTITYUNRELATED ; the ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED for larger pieces of ENTITYUNRELATED ; the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ; and the ENTITYUNRELATED of referential ENTITYUNRELATED .
This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes .	investigation	phenomena	topic	{'e1': {'word': 'investigation', 'word_index': [(62, 62)], 'id': 'C86-1088.33'}, 'e2': {'word': 'phenomena', 'word_index': [(66, 66)], 'id': 'C86-1088.36'}}	This recent ENTITYUNRELATED in theoretical ENTITYUNRELATED indicates a ENTITYUNRELATED of interest towards ENTITYUNRELATED that have been familiar in ENTITYUNRELATED ENTITYUNRELATED for the last decade : among others , the ENTITYUNRELATED of new ENTITYUNRELATED with ENTITYUNRELATED to a given ENTITYUNRELATED , and ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED into that ENTITYUNRELATED ; the ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED for larger pieces of ENTITYUNRELATED ; the ENTITY of ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER ; and the ENTITYUNRELATED of referential ENTITYUNRELATED .
This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes .	description	processes	model-feature	{'e1': {'word': 'description', 'word_index': [(70, 70)], 'id': 'C86-1088.37'}, 'e2': {'word': 'processes', 'word_index': [(73, 73)], 'id': 'C86-1088.38'}}	This recent ENTITYUNRELATED in theoretical ENTITYUNRELATED indicates a ENTITYUNRELATED of interest towards ENTITYUNRELATED that have been familiar in ENTITYUNRELATED ENTITYUNRELATED for the last decade : among others , the ENTITYUNRELATED of new ENTITYUNRELATED with ENTITYUNRELATED to a given ENTITYUNRELATED , and ENTITYUNRELATED of the ENTITYUNRELATED ENTITYUNRELATED into that ENTITYUNRELATED ; the ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED for larger pieces of ENTITYUNRELATED ; the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ; and the ENTITY of referential ENTITYOTHER .
On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding .	context	interpretation	usage	{'e1': {'word': 'context', 'word_index': [(42, 42)], 'id': 'C86-1088.73'}, 'e2': {'word': 'interpretation', 'word_index': [(34, 34)], 'id': 'C86-1088.71'}}	On the one ENTITYUNRELATED , this ENTITYUNRELATED is quite ENTITYUNRELATED : As a semantically motivated ENTITYUNRELATED , DRT should not be expected to incorporate every ENTITYUNRELATED of ENTITYUNRELATED necessary to come up with an ENTITYOTHER for a specific ENTITYUNRELATED in a given ENTITY ; it can better be thought of as an ENTITYUNRELATED relating theoretical , truth - conditional ENTITYUNRELATED and the genuinely pragmatic work of ENTITYUNRELATED ENTITYUNRELATED .
Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora.	extensions	system	part_whole	{'e1': {'word': 'extensions', 'word_index': [(1, 1)], 'id': 'C86-1088.87'}, 'e2': {'word': 'system', 'word_index': [(5, 5)], 'id': 'C86-1088.89'}}	Several ENTITY of the ENTITYUNRELATED ENTITYOTHER are at work , e.g. for the ENTITYUNRELATED of plural and temporal anaphora .
Towards The Automatic Acquisition Of Lexical Data</title>	Acquisition	Data	usage	{'e1': {'word': 'Acquisition', 'word_index': [(3, 3)], 'id': 'C86-1091.2'}, 'e2': {'word': 'Data', 'word_index': [(6, 6)], 'id': 'C86-1091.4'}}	Towards The ENTITYUNRELATED ENTITY Of ENTITYUNRELATED ENTITYOTHER < / title >
Our: system is to be used by persons with no specific linguistic knowledge , thus linguistic expertise has been put into the system to ascertain correct classification of words.	classification	words	usage	{'e1': {'word': 'classification', 'word_index': [(26, 26)], 'id': 'C86-1091.26'}, 'e2': {'word': 'words', 'word_index': [(28, 28)], 'id': 'C86-1091.27'}}	Our : ENTITYUNRELATED is to be used by persons with no specific ENTITYUNRELATED , thus linguistic expertise has been put into the ENTITYUNRELATED to ascertain correct ENTITY of ENTITYOTHER .
Classification is done by means of a small rule based system with Lexical  knowledge and language-specific heuristics.	Lexical  knowledge	Classification	usage	{'e1': {'word': 'Lexical  knowledge', 'word_index': [(12, 12)], 'id': 'C86-1091.32'}, 'e2': {'word': 'Classification', 'word_index': [(0, 0)], 'id': 'C86-1091.28'}}	ENTITYOTHER is done by means of a small ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED with ENTITY and ENTITYUNRELATED heuristics .
 Introduction in this paper we introduce a system for the semi-automatic enlargement of a morphological 1 ex icon .	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(3, 3)], 'id': 'C86-1091.41'}, 'e2': {'word': 'system', 'word_index': [(7, 7)], 'id': 'C86-1091.42'}}	ENTITYUNRELATED in this ENTITY we introduce a ENTITYOTHER for the ENTITYUNRELATED enlargement of a morphological 1 ex ENTITYUNRELATED .
The PSI/PHI Architecture For Prosodic Parsing	Architecture	Parsing	usage	{'e1': {'word': 'Architecture', 'word_index': [(4, 4)], 'id': 'C88-1041.1'}, 'e2': {'word': 'Parsing', 'word_index': [(7, 7)], 'id': 'C88-1041.2'}}	The PSI / PHI ENTITY For Prosodic ENTITYOTHER
The paper contains the description of the four algorithms of the proposed type : 1.	description	algorithms	topic	{'e1': {'word': 'description', 'word_index': [(4, 4)], 'id': 'C88-2134.19'}, 'e2': {'word': 'algorithms', 'word_index': [(8, 8)], 'id': 'C88-2134.20'}}	The ENTITYUNRELATED contains the ENTITY of the four ENTITYOTHER of the ENTITYUNRELATED ENTITYUNRELATED : 1.
The algorithm which identifies the morphemes in the text without the boundaries between words .	boundaries	words	part_whole	{'e1': {'word': 'boundaries', 'word_index': [(11, 11)], 'id': 'C88-2134.28'}, 'e2': {'word': 'words', 'word_index': [(13, 13)], 'id': 'C88-2134.29'}}	The ENTITYUNRELATED which identifies the morphemes in the ENTITYUNRELATED without the ENTITY between ENTITYOTHER .
In the first part of this paper I draw a typology of defmiteness ; later I reflect on the defmiteness of NPs in an IL-representation .	paper	typology	topic	{'e1': {'word': 'paper', 'word_index': [(6, 6)], 'id': 'C90-2046.32'}, 'e2': {'word': 'typology', 'word_index': [(10, 10)], 'id': 'C90-2046.33'}}	In the first ENTITYUNRELATED of this ENTITY I draw a ENTITYOTHER of defmiteness ; later I reflect on the defmiteness of NPs in an ENTITYUNRELATED .
We propose a simple , intuitively satisfying treatment of the semantics of bare plural NPs .	treatment	semantics	topic	{'e1': {'word': 'treatment', 'word_index': [(7, 7)], 'id': 'C92-1037.3'}, 'e2': {'word': 'semantics', 'word_index': [(10, 10)], 'id': 'C92-1037.4'}}	We ENTITYUNRELATED a ENTITYUNRELATED , intuitively satisfying ENTITY of the ENTITYOTHER of bare plural NPs .
This treatment avoids the use of nonstandard logics , and avoids the need for systematic ambiguity of verb semantics .	ambiguity	semantics	model-feature	{'e1': {'word': 'ambiguity', 'word_index': [(15, 15)], 'id': 'C92-1037.7'}, 'e2': {'word': 'semantics', 'word_index': [(18, 18)], 'id': 'C92-1037.9'}}	This ENTITYUNRELATED avoids the use of nonstandard ENTITYUNRELATED , and avoids the need for systematic ENTITY of ENTITYUNRELATED ENTITYOTHER .
Using Linguistic, World, And Contextual Knowledge In A Plan Recognition Model Of Dialogue</title>	Model	Dialogue	model-feature	{'e1': {'word': 'Model', 'word_index': [(12, 12)], 'id': 'C92-1049.3'}, 'e2': {'word': 'Dialogue', 'word_index': [(14, 14)], 'id': 'C92-1049.4'}}	Using Linguistic , World , And Contextual ENTITYUNRELATED In A Plan ENTITYUNRELATED ENTITY Of ENTITYOTHER < / title >
Temporal Structure Of Discourse</title>	Structure	Discourse	model-feature	{'e1': {'word': 'Structure', 'word_index': [(1, 1)], 'id': 'C92-1052.1'}, 'e2': {'word': 'Discourse', 'word_index': [(3, 3)], 'id': 'C92-1052.2'}}	Temporal ENTITY Of ENTITYOTHER < / title >
In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed .	discourse	segments	model-feature	{'e1': {'word': 'discourse', 'word_index': [(3, 3)], 'id': 'C92-1052.4'}, 'e2': {'word': 'segments', 'word_index': [(4, 4)], 'id': 'C92-1052.5'}}	In this ENTITYUNRELATED ENTITY ENTITYOTHER are defined and a ENTITYUNRELATED for ENTITYUNRELATED segmentation primarily ENTITYUNRELATED on abduction of ENTITYUNRELATED between ENTITYUNRELATED is ENTITYUNRELATED .
In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed .	method	discourse	usage	{'e1': {'word': 'method', 'word_index': [(9, 9)], 'id': 'C92-1052.6'}, 'e2': {'word': 'discourse', 'word_index': [(11, 11)], 'id': 'C92-1052.7'}}	In this ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED are defined and a ENTITY for ENTITYOTHER segmentation primarily ENTITYUNRELATED on abduction of ENTITYUNRELATED between ENTITYUNRELATED is ENTITYUNRELATED .
Because accepted discourse structures are used to connect a new goal to the existing template , goals are organised into sub-groups that follow conventional, coherent patterns of discourse .	patterns	discourse	model-feature	{'e1': {'word': 'patterns', 'word_index': [(25, 25)], 'id': 'C92-1053.25'}, 'e2': {'word': 'discourse', 'word_index': [(27, 27)], 'id': 'C92-1053.26'}}	Because accepted ENTITYUNRELATED are used to connect a new ENTITYUNRELATED to the existing ENTITYUNRELATED , ENTITYUNRELATED are organised into sub-groups that follow conventional , coherent ENTITY of ENTITYOTHER .
It is generally agreed that processes of Gricean implicature help determine the interpretation of text in context .	context	interpretation	usage	{'e1': {'word': 'context', 'word_index': [(16, 16)], 'id': 'C92-2108.16'}, 'e2': {'word': 'interpretation', 'word_index': [(12, 12)], 'id': 'C92-2108.14'}}	It is generally agreed that ENTITYUNRELATED of Gricean implicature ENTITYUNRELATED determine the ENTITYOTHER of ENTITYUNRELATED in ENTITY .
Design Tool Combining Keyword Analyzer And Case- Based Parser For Developing Natural Language Database Interface	Tool	Database	usage	{'e1': {'word': 'Tool', 'word_index': [(1, 1)], 'id': 'C92-2110.2'}, 'e2': {'word': 'Database', 'word_index': [(12, 12)], 'id': 'C92-2110.11'}}	ENTITYUNRELATED ENTITY ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED And ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED For ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER Interface
Since it uses only keywords in any query , this analyzer is robust with regard to extra-grammatical expressions .	robust	analyzer	model-feature	{'e1': {'word': 'robust', 'word_index': [(12, 12)], 'id': 'C92-2110.43'}, 'e2': {'word': 'analyzer', 'word_index': [(10, 10)], 'id': 'C92-2110.42'}}	Since it uses only ENTITYUNRELATED in any ENTITYUNRELATED , this ENTITYOTHER is ENTITY with regard to extra-grammatical ENTITYUNRELATED .
This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues .	paper	possibilities	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C92-2119.18'}, 'e2': {'word': 'possibilities', 'word_index': [(4, 4)], 'id': 'C92-2119.19'}}	This ENTITY presents some ENTITYOTHER of a ENTITYUNRELATED ENTITYUNRELATED for handling ENTITYUNRELATED failures in ENTITYUNRELATED oral ENTITYUNRELATED .
This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues .	task-oriented	dialogues	model-feature	{'e1': {'word': 'task-oriented', 'word_index': [(14, 14)], 'id': 'C92-2119.23'}, 'e2': {'word': 'dialogues', 'word_index': [(16, 16)], 'id': 'C92-2119.24'}}	This ENTITYUNRELATED presents some ENTITYUNRELATED of a ENTITYUNRELATED ENTITYUNRELATED for handling ENTITYUNRELATED failures in ENTITY oral ENTITYOTHER .
We first recall some aspects of the model and then describe the strategies for preventing and repairing communication failure in oral conversations with a system .	system	repairing	usage	{'e1': {'word': 'system', 'word_index': [(24, 24)], 'id': 'C92-2119.46'}, 'e2': {'word': 'repairing', 'word_index': [(16, 16)], 'id': 'C92-2119.43'}}	We first ENTITYUNRELATED some ENTITYUNRELATED of the ENTITYUNRELATED and then describe the ENTITYUNRELATED for preventing and ENTITYOTHER ENTITYUNRELATED failure in oral ENTITYUNRELATED with a ENTITY .
This research deals with the representation of causal relations found in texts written in natural language , in order for KALIPSOS [1], an NL-understanding and question-answering system , to encode causal information in conceptual graphs so as to handle causal information and reasoning .	representation	relations	model-feature	{'e1': {'word': 'representation', 'word_index': [(5, 5)], 'id': 'C92-3131.6'}, 'e2': {'word': 'relations', 'word_index': [(8, 8)], 'id': 'C92-3131.7'}}	This ENTITYUNRELATED ENTITYUNRELATED with the ENTITY of causal ENTITYOTHER found in ENTITYUNRELATED written in ENTITYUNRELATED , in ENTITYUNRELATED for KALIPSOS [ 1 ] , an ENTITYUNRELATED and ENTITYUNRELATED , to encode causal ENTITYUNRELATED in conceptual graphs so as to handle causal ENTITYUNRELATED and ENTITYUNRELATED .
Tliis paper presents a system that is capable of representing situations , states, and nondeterniinistic nonmonotonic-outcome actions occurring in multiple possible worlds.	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C92-3147.5'}, 'e2': {'word': 'system', 'word_index': [(4, 4)], 'id': 'C92-3147.6'}}	Tliis ENTITY presents a ENTITYOTHER that is capable of representing ENTITYUNRELATED , states , and nondeterniinistic ENTITYUNRELATED ENTITYUNRELATED occurring in multiple possible worlds .
Situations mid actions can have expected values, allowing the system to support decision-making and decision-based plan inferencing .	system	decision-making	usage	{'e1': {'word': 'system', 'word_index': [(10, 10)], 'id': 'C92-3147.26'}, 'e2': {'word': 'decision-making', 'word_index': [(13, 13)], 'id': 'C92-3147.28'}}	Situations mid ENTITYUNRELATED can have expected values , allowing the ENTITY to ENTITYUNRELATED ENTITYOTHER and ENTITYUNRELATED plan ENTITYUNRELATED .
"To get better results , we apply other """"logical"""" compression mechanisms based on the structure of the language itself."	structure	mechanisms	usage	{'e1': {'word': 'structure', 'word_index': [(18, 18)], 'id': 'C94-1047.22'}, 'e2': {'word': 'mechanisms', 'word_index': [(14, 14)], 'id': 'C94-1047.20'}}	"To get better ENTITYUNRELATED , we ENTITYUNRELATED other "" "" logical "" "" ENTITYUNRELATED ENTITYOTHER ENTITYUNRELATED on the ENTITY of the ENTITYUNRELATED itself ."
Experiments with multilingual dictionaries show a significant reduction rate attributable to our logic compression alone and even better results when using our method in conjunction with existing methods .	Experiments	results	result	{'e1': {'word': 'Experiments', 'word_index': [(0, 0)], 'id': 'C94-1047.24'}, 'e2': {'word': 'results', 'word_index': [(18, 18)], 'id': 'C94-1047.30'}}	ENTITY with multilingual ENTITYUNRELATED show a significant ENTITYUNRELATED ENTITYUNRELATED attributable to our ENTITYUNRELATED ENTITYUNRELATED alone and even better ENTITYOTHER when using our ENTITYUNRELATED in ENTITYUNRELATED with existing ENTITYUNRELATED .
CLAWS4: The Tagging Of The British National Corpus</title>	Tagging	Corpus	usage	{'e1': {'word': 'Tagging', 'word_index': [(3, 3)], 'id': 'C94-1103.1'}, 'e2': {'word': 'Corpus', 'word_index': [(8, 8)], 'id': 'C94-1103.2'}}	CLAWS4 : The ENTITY Of The British National ENTITYOTHER < / title >
The main purpose of this paper is to describe the CLAWS4 general-purpose grammatical tagger, used for the tagging of the 100- million-word British National Corpus , of which c.70 million words have been tagged at the time of writing (April  1994 ).tagsets input formats .	tagging	million-word	usage	{'e1': {'word': 'tagging', 'word_index': [(18, 18)], 'id': 'C94-1103.7'}, 'e2': {'word': 'million-word', 'word_index': [(23, 23)], 'id': 'C94-1103.8'}}	The ENTITYUNRELATED ENTITYUNRELATED of this ENTITYUNRELATED is to describe the CLAWS4 ENTITYUNRELATED grammatical tagger , used for the ENTITY of the 100 - ENTITYOTHER British National ENTITYUNRELATED , of which c.70 million ENTITYUNRELATED have been ENTITYUNRELATED at the ENTITYUNRELATED of writing ( April 1994 ) .tagsets ENTITYUNRELATED ENTITYUNRELATED .
In example-based NLP, the problem of computational cost of example retrieval is severe , since the retrieval time increases in proportion to the number of examples in the database .	cost	retrieval	model-feature	{'e1': {'word': 'cost', 'word_index': [(8, 8)], 'id': 'C94-2169.10'}, 'e2': {'word': 'retrieval', 'word_index': [(11, 11)], 'id': 'C94-2169.12'}}	In ENTITYUNRELATED NLP , the ENTITYUNRELATED of ENTITYUNRELATED ENTITY of ENTITYUNRELATED ENTITYOTHER is severe , since the ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED in proportion to the ENTITYUNRELATED of ENTITYUNRELATED in the ENTITYUNRELATED .
This paper proposes a novel example retrieval method for avoiding full retrieval of examples .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C94-2169.19'}, 'e2': {'word': 'method', 'word_index': [(7, 7)], 'id': 'C94-2169.23'}}	This ENTITY ENTITYUNRELATED a novel ENTITYUNRELATED ENTITYUNRELATED ENTITYOTHER for avoiding full ENTITYUNRELATED of ENTITYUNRELATED .
It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and  Sidner, 1986 ).	segments	discourses	part_whole	{'e1': {'word': 'segments', 'word_index': [(9, 9)], 'id': 'C96-1001.4'}, 'e2': {'word': 'discourses', 'word_index': [(5, 5)], 'id': 'C96-1001.3'}}	It is widely accepted that ENTITYOTHER are composed of ENTITY and that the ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED is essential to a ENTITYUNRELATED of ENTITYUNRELATED meaning ( Grosz and Sidner , 1986 ) .
It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and  Sidner, 1986 ).	recognition	segment	usage	{'e1': {'word': 'recognition', 'word_index': [(13, 13)], 'id': 'C96-1001.5'}, 'e2': {'word': 'segment', 'word_index': [(15, 15)], 'id': 'C96-1001.6'}}	It is widely accepted that ENTITYUNRELATED are composed of ENTITYUNRELATED and that the ENTITY of ENTITYOTHER ENTITYUNRELATED is essential to a ENTITYUNRELATED of ENTITYUNRELATED meaning ( Grosz and Sidner , 1986 ) .
Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure .	boundaries	language	part_whole	{'e1': {'word': 'boundaries', 'word_index': [(11, 11)], 'id': 'C96-1001.14'}, 'e2': {'word': 'language', 'word_index': [(1, 1)], 'id': 'C96-1001.10'}}	Written ENTITYOTHER has orthographic ENTITYUNRELATED such as ENTITYUNRELATED headings , ENTITYUNRELATED ENTITY , and ENTITYUNRELATED which can assist in identifying ENTITYUNRELATED .
An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech .	generation	speech	usage	{'e1': {'word': 'generation', 'word_index': [(25, 25)], 'id': 'C96-1001.63'}, 'e2': {'word': 'speech', 'word_index': [(27, 27)], 'id': 'C96-1001.64'}}	An ENTITYUNRELATED of intonational ENTITYUNRELATED and the ways in which it carries ENTITYUNRELATED about ENTITYUNRELATED ENTITYUNRELATED of spoken ENTITYUNRELATED is important for ENTITYUNRELATED ENTITYUNRELATED and ENTITY of ENTITYOTHER .
Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure .	semantics	discourse structure	model-feature	{'e1': {'word': 'semantics', 'word_index': [(5, 5)], 'id': 'C96-1001.107'}, 'e2': {'word': 'discourse structure', 'word_index': [(1, 1)], 'id': 'C96-1001.106'}}	Because ENTITYOTHER is rooted in ENTITY rather than ENTITYUNRELATED , this has proved more difficult than ENTITYUNRELATED ENTITYUNRELATED for ENTITYUNRELATED .
Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure .	tagging	corpora	usage	{'e1': {'word': 'tagging', 'word_index': [(16, 16)], 'id': 'C96-1001.109'}, 'e2': {'word': 'corpora', 'word_index': [(17, 17)], 'id': 'C96-1001.110'}}	Because ENTITYUNRELATED is rooted in ENTITYUNRELATED rather than ENTITYUNRELATED , this has proved more difficult than ENTITY ENTITYOTHER for ENTITYUNRELATED .
Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions .	segment	discourses	usage	{'e1': {'word': 'segment', 'word_index': [(14, 14)], 'id': 'C96-1001.141'}, 'e2': {'word': 'discourses', 'word_index': [(16, 16)], 'id': 'C96-1001.142'}}	Next I will describe the ENTITYUNRELATED of annotation ENTITYUNRELATED used to ENTITYUNRELATED labelers to ENTITY spoken ENTITYOTHER ( Nakatani et ah , 1995 b ) and will discuss ENTITYUNRELATED among segmentations on the Boston Directions ENTITYUNRELATED obtained using these ENTITYUNRELATED .
"""From a """"well represented sample of world languages Steele (1978) shows that about """	languages	sample	part_whole	{'e1': {'word': 'languages', 'word_index': [(10, 10)], 'id': 'C86-1014.7'}, 'e2': {'word': 'sample', 'word_index': [(7, 7)], 'id': 'C86-1014.6'}}	""" From a "" "" well represented ENTITYOTHER of world ENTITY Steele ( 1978 ) shows that about """
This paper discusses the translation of temporal expressions , in the framework of the machine translation system Rosetta.	paper	translation	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C86-1074.4'}, 'e2': {'word': 'translation', 'word_index': [(4, 4)], 'id': 'C86-1074.5'}}	This ENTITY discusses the ENTITYOTHER of temporal ENTITYUNRELATED , in the ENTITYUNRELATED of the ENTITYUNRELATED Rosetta .
It is shown that a compositional approach leads to a transparent account of the complex aspects of time in natural language and can be used for the translation of temporal expressions .	translation	expressions	usage	{'e1': {'word': 'translation', 'word_index': [(26, 26)], 'id': 'C86-1074.19'}, 'e2': {'word': 'expressions', 'word_index': [(29, 29)], 'id': 'C86-1074.20'}}	It is shown that a compositional ENTITYUNRELATED leads to a transparent account of the ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED in ENTITYUNRELATED and can be used for the ENTITY of temporal ENTITYOTHER .
This paper describes an implemented tutoring system (2), designed to help students to generate clitic-constructions in French.	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C86-1133.6'}, 'e2': {'word': 'system', 'word_index': [(6, 6)], 'id': 'C86-1133.8'}}	This ENTITY describes an ENTITYUNRELATED tutoring ENTITYOTHER ( 2 ) , ENTITYUNRELATED to ENTITYUNRELATED students to ENTITYUNRELATED clitic-constructions in French .
A parsing scheme for spoken utterances is proposed that deviates from traditional 'one go' left to right sentence parsing in that it d	parsing	utterances	usage	{'e1': {'word': 'parsing', 'word_index': [(1, 1)], 'id': 'C86-1139.4'}, 'e2': {'word': 'utterances', 'word_index': [(5, 5)], 'id': 'C86-1139.6'}}	A ENTITY ENTITYUNRELATED for spoken ENTITYOTHER is ENTITYUNRELATED that deviates from traditional ' one go ' left to right ENTITYUNRELATED ENTITYUNRELATED in that it d
Research into text-to-speech systems has become a rather important topic in the areas of linguistics and phonetics	Research	text-to-speech	topic	{'e1': {'word': 'Research', 'word_index': [(0, 0)], 'id': 'C86-1144.3'}, 'e2': {'word': 'text-to-speech', 'word_index': [(2, 2)], 'id': 'C86-1144.4'}}	ENTITY into ENTITYOTHER ENTITYUNRELATED has become a rather important ENTITYUNRELATED in the ENTITYUNRELATED of ENTITYUNRELATED and phonetics
In this paper we will be concerned with the grapheme-to-phoneme conversion component as part of the Dutch text-to-speech system which is being developed in Utrecht, Leyden and Eindhoven.	grapheme-to-phoneme	text-to-speech	part_whole	{'e1': {'word': 'grapheme-to-phoneme', 'word_index': [(9, 9)], 'id': 'C86-1144.21'}, 'e2': {'word': 'text-to-speech', 'word_index': [(17, 17)], 'id': 'C86-1144.25'}}	In this ENTITYUNRELATED we will be ENTITYUNRELATED with the ENTITY ENTITYUNRELATED ENTITYUNRELATED as ENTITYUNRELATED of the Dutch ENTITYOTHER ENTITYUNRELATED which is being ENTITYUNRELATED in Utrecht , Leyden and Eindhoven .
Thus, the system has to satisfy the following demands: - its output must form a proper and flexible input for diphone as well as allophone synthesis ; - it must be possible to easily generate phonematized lists on the basis of orthographic input ; - it must be possible to automatically obtain information regarding the relation between graphemes and phonemes in texts ; - the system has to be user-friendly , so that it can be addressed by linguists without computer training (for example to test their phonological rules ).	user-friendly	system	model-feature	{'e1': {'word': 'user-friendly', 'word_index': [(71, 71)], 'id': 'C86-1144.51'}, 'e2': {'word': 'system', 'word_index': [(67, 67)], 'id': 'C86-1144.50'}}	Thus , the ENTITYUNRELATED has to satisfy the ENTITYUNRELATED demands : - its ENTITYUNRELATED must ENTITYUNRELATED a proper and flexible ENTITYUNRELATED for diphone as well as allophone ENTITYUNRELATED ; - it must be possible to easily ENTITYUNRELATED phonematized ENTITYUNRELATED on the ENTITYUNRELATED of orthographic ENTITYUNRELATED ; - it must be possible to automatically obtain ENTITYUNRELATED regarding the ENTITYUNRELATED between graphemes and ENTITYUNRELATED in ENTITYUNRELATED ; - the ENTITYOTHER has to be ENTITY , so that it can be addressed by ENTITYUNRELATED without ENTITYUNRELATED ENTITYUNRELATED ( for ENTITYUNRELATED to ENTITYUNRELATED their phonological ENTITYUNRELATED ) .
In this paper we propose a new method to express quantification and especially quantifier scope in French generation .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'C88-1037.4'}, 'e2': {'word': 'method', 'word_index': [(7, 7)], 'id': 'C88-1037.6'}}	In this ENTITY we ENTITYUNRELATED a new ENTITYOTHER to express ENTITYUNRELATED and especially ENTITYUNRELATED ENTITYUNRELATED in French ENTITYUNRELATED .
"""Abstract : This paper outlines a theory of constituent coordination for Lexical- Functional Grammar."	paper	coordination	topic	{'e1': {'word': 'paper', 'word_index': [(4, 4)], 'id': 'C88-1061.5'}, 'e2': {'word': 'coordination', 'word_index': [(10, 10)], 'id': 'C88-1061.9'}}	""" ENTITYUNRELATED : This ENTITY ENTITYUNRELATED a ENTITYUNRELATED of ENTITYUNRELATED ENTITYOTHER for ENTITYUNRELATED Functional Grammar ."
On this theory LFG's flat, unstructured sets arc used as the functional representation of coordinate constructions .	arc	constructions	model-feature	{'e1': {'word': 'arc', 'word_index': [(9, 9)], 'id': 'C88-1061.12'}, 'e2': {'word': 'constructions', 'word_index': [(17, 17)], 'id': 'C88-1061.14'}}	On this ENTITYUNRELATED LFG 's flat , unstructured sets ENTITY used as the functional ENTITYUNRELATED of coordinate ENTITYOTHER .
Typically these have been solved by special devices that are added to the parsing algorithms to analyze coordinate constructions that cannot easily be characterized in explicit rules of grammar.	devices	algorithms	usage	{'e1': {'word': 'devices', 'word_index': [(7, 7)], 'id': 'C88-1061.46'}, 'e2': {'word': 'algorithms', 'word_index': [(14, 14)], 'id': 'C88-1061.48'}}	Typically these have been ENTITYUNRELATED by special ENTITY that are added to the ENTITYUNRELATED ENTITYOTHER to analyze coordinate ENTITYUNRELATED that cannot easily be characterized in explicit ENTITYUNRELATED of grammar .
An Integrated Model For The Treatment Of Time In MT-Systems</title>	Model	Time	model-feature	{'e1': {'word': 'Model', 'word_index': [(2, 2)], 'id': 'C88-2089.1'}, 'e2': {'word': 'Time', 'word_index': [(7, 7)], 'id': 'C88-2089.3'}}	An Integrated ENTITY For The ENTITYUNRELATED Of ENTITYOTHER In ENTITYUNRELATED < / title >
Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper .	translation	verbs	usage	{'e1': {'word': 'translation', 'word_index': [(18, 18)], 'id': 'C88-2089.38'}, 'e2': {'word': 'verbs', 'word_index': [(20, 20)], 'id': 'C88-2089.39'}}	Our ENTITYUNRELATED relies on the fact that tense / ENTITYUNRELATED ENTITYUNRELATED is relevant not only for a good ENTITY of ENTITYOTHER , but also for a good ENTITYUNRELATED of ENTITYUNRELATED , PPs , temporal Ni-n and ENTITYUNRELATED , as we have intended to demonstrate in this ENTITYUNRELATED .
Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper .	translation	adverbs	usage	{'e1': {'word': 'translation', 'word_index': [(27, 27)], 'id': 'C88-2089.40'}, 'e2': {'word': 'adverbs', 'word_index': [(29, 29)], 'id': 'C88-2089.41'}}	Our ENTITYUNRELATED relies on the fact that tense / ENTITYUNRELATED ENTITYUNRELATED is relevant not only for a good ENTITYUNRELATED of ENTITYUNRELATED , but also for a good ENTITY of ENTITYOTHER , PPs , temporal Ni-n and ENTITYUNRELATED , as we have intended to demonstrate in this ENTITYUNRELATED .
Introduction Thin article deals with a methodology to achieve the right translation of temporal expressions by giving account of the temporal reference and temporal relations in/ between sentences .	methodology	translation	topic	{'e1': {'word': 'methodology', 'word_index': [(6, 6)], 'id': 'C88-2089.46'}, 'e2': {'word': 'translation', 'word_index': [(11, 11)], 'id': 'C88-2089.47'}}	ENTITYUNRELATED Thin article ENTITYUNRELATED with a ENTITY to achieve the right ENTITYOTHER of temporal ENTITYUNRELATED by giving account of the temporal ENTITYUNRELATED and ENTITYUNRELATED in / between ENTITYUNRELATED .
The present proposal presumes an analysis and a generation component that deliver a set of S- trees whose leaves correspond to words .	trees	words	model-feature	{'e1': {'word': 'trees', 'word_index': [(16, 16)], 'id': 'C88-2089.72'}, 'e2': {'word': 'words', 'word_index': [(21, 21)], 'id': 'C88-2089.73'}}	The present ENTITYUNRELATED presumes an ENTITYUNRELATED and a ENTITYUNRELATED ENTITYUNRELATED that deliver a set of S- ENTITY whose leaves correspond to ENTITYOTHER .
As usual, features am percolated and nodes get features assigned .	features	nodes	model-feature	{'e1': {'word': 'features', 'word_index': [(9, 9)], 'id': 'C88-2089.78'}, 'e2': {'word': 'nodes', 'word_index': [(7, 7)], 'id': 'C88-2089.77'}}	As usual , ENTITYUNRELATED am percolated and ENTITYOTHER get ENTITY assigned .
Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model</title>	Alignment	Sentence	usage	{'e1': {'word': 'Alignment', 'word_index': [(2, 2)], 'id': 'D08-1053.2'}, 'e2': {'word': 'Sentence', 'word_index': [(1, 1)], 'id': 'D08-1053.1'}}	Improved ENTITYOTHER ENTITY on Parallel Web Pages Using a Stochastic ENTITYUNRELATED ENTITYUNRELATED < / title >
"In this paper , we present a new approach to sentence alignment on parallel web <entity id=""D08-1053.16"">pages</entity> ."	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'D08-1053.11'}, 'e2': {'word': 'approach', 'word_index': [(8, 8)], 'id': 'D08-1053.12'}}	"In this ENTITY , we present a new ENTITYOTHER to ENTITYUNRELATED ENTITYUNRELATED on parallel web < entity id = "" D08-1053.16 "" > pages < / entity > ."
"Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as """"Hansard""""."	method	accuracy	result	{'e1': {'word': 'method', 'word_index': [(4, 4)], 'id': 'D08-1053.34'}, 'e2': {'word': 'accuracy', 'word_index': [(8, 8)], 'id': 'D08-1053.36'}}	"ENTITYUNRELATED show that this ENTITY significantly enhances ENTITYUNRELATED ENTITYOTHER and ENTITYUNRELATED for parallel ENTITYUNRELATED which are much more diverse and noisy than ENTITYUNRELATED ENTITYUNRELATED such as "" "" Hansard "" "" ."
"With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. """	systems	quality	result	{'e1': {'word': 'systems', 'word_index': [(8, 8)], 'id': 'D08-1053.45'}, 'e2': {'word': 'quality', 'word_index': [(17, 17)], 'id': 'D08-1053.47'}}	"With ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED , web mining ENTITY are able to acquire parallel ENTITYUNRELATED of higher ENTITYOTHER from the web . """
"This paper describes a new automatic method for Japanese predicate <entity id=""D08-1055.10"">argument structure</entity> <entity id=""D08-1055.11"">analysis</entity> ."	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'D08-1055.5'}, 'e2': {'word': 'method', 'word_index': [(6, 6)], 'id': 'D08-1055.7'}}	"This ENTITY describes a new ENTITYUNRELATED ENTITYOTHER for ENTITYUNRELATED predicate < entity id = "" D08-1055.10 "" > argument structure < / entity > < entity id = "" D08-1055.11 "" > analysis < / entity > ."
In this paper , we present the first un-supervised approach that is competitive with supervised ones.	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'D08-1068.10'}, 'e2': {'word': 'approach', 'word_index': [(11, 11)], 'id': 'D08-1068.11'}}	In this ENTITY , we present the first un - supervised ENTITYOTHER that is competitive with supervised ones .
We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts .	information	papers	part_whole	{'e1': {'word': 'information', 'word_index': [(17, 17)], 'id': 'E99-1043.14'}, 'e2': {'word': 'papers', 'word_index': [(20, 20)], 'id': 'E99-1043.16'}}	We present an ENTITYUNRELATED of the genome ENTITYUNRELATED ENTITYUNRELATED ( GENIA ) ENTITYUNRELATED for automatically ENTITYUNRELATED biochemical ENTITY from ENTITYUNRELATED ENTITYOTHER and ENTITYUNRELATED .
The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet.	papers	databases	part_whole	{'e1': {'word': 'papers', 'word_index': [(4, 4)], 'id': 'E99-1043.25'}, 'e2': {'word': 'databases', 'word_index': [(8, 8)], 'id': 'E99-1043.26'}}	The vast ENTITYUNRELATED of ENTITY available online in ENTITYOTHER such as MEDLINE is a ENTITYUNRELATED ENTITYUNRELATED in which to ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and ENTITYUNRELATED and is an opportunity to show how ENTITYUNRELATED ENTITYUNRELATED can play a key ENTITYUNRELATED on the Internet .
We investigate the lexical and syntactic flexibility of a class of idiomatic expressions .	expressions	class	part_whole	{'e1': {'word': 'expressions', 'word_index': [(12, 12)], 'id': 'E06-1043.8'}, 'e2': {'word': 'class', 'word_index': [(9, 9)], 'id': 'E06-1043.7'}}	We investigate the ENTITYUNRELATED and ENTITYUNRELATED ENTITYUNRELATED of a ENTITYOTHER of idiomatic ENTITY .
We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation .	representation	idiom	model-feature	{'e1': {'word': 'representation', 'word_index': [(26, 26)], 'id': 'E06-1043.20'}, 'e2': {'word': 'idiom', 'word_index': [(13, 13)], 'id': 'E06-1043.17'}}	We also ENTITYUNRELATED a means for automatically determining which ENTITYUNRELATED ENTITYUNRELATED a particular ENTITYOTHER can appear in , and hence should be ENTITYUNRELATED in its ENTITYUNRELATED ENTITY .
An Education And Research Tool For Computational Semantics</title>	Tool	Semantics	usage	{'e1': {'word': 'Tool', 'word_index': [(4, 4)], 'id': 'C96-2197.2'}, 'e2': {'word': 'Semantics', 'word_index': [(7, 7)], 'id': 'C96-2197.4'}}	An Education And ENTITYUNRELATED ENTITY For ENTITYUNRELATED ENTITYOTHER < / title >
The system provides a teaching tool , a stand alone extendible grapher, and a library of algorithms together with test suites .	library	algorithms	part_whole	{'e1': {'word': 'library', 'word_index': [(15, 15)], 'id': 'C96-2197.12'}, 'e2': {'word': 'algorithms', 'word_index': [(17, 17)], 'id': 'C96-2197.13'}}	The ENTITYUNRELATED ENTITYUNRELATED a teaching ENTITYUNRELATED , a stand alone extendible grapher , and a ENTITY of ENTITYOTHER together with ENTITYUNRELATED .
Word order-sensitive approaches are demonstrated to generally outperform bag-of-words methods , with source language segment-level edit distance proving the most effective similarity metric .	order-sensitive	bag-of-words	compare	{'e1': {'word': 'order-sensitive', 'word_index': [(1, 1)], 'id': 'C00-1006.49'}, 'e2': {'word': 'bag-of-words', 'word_index': [(8, 8)], 'id': 'C00-1006.51'}}	ENTITYUNRELATED ENTITY ENTITYUNRELATED are demonstrated to generally outperform ENTITYOTHER ENTITYUNRELATED , with ENTITYUNRELATED ENTITYUNRELATED edit ENTITYUNRELATED proving the most effective ENTITYUNRELATED ENTITYUNRELATED .
It is however possible to reinterpret the dop-model as a pattern-matching model , which tries to maximize the size of the substructures that construct the parse , rather than the probability of the parse .	substructures	parse	part_whole	{'e1': {'word': 'substructures', 'word_index': [(21, 21)], 'id': 'C00-1035.28'}, 'e2': {'word': 'parse', 'word_index': [(25, 25)], 'id': 'C00-1035.30'}}	It is however possible to reinterpret the ENTITYUNRELATED as a ENTITYUNRELATED ENTITYUNRELATED , which tries to maximize the ENTITYUNRELATED of the ENTITY that ENTITYUNRELATED the ENTITYOTHER , rather than the ENTITYUNRELATED of the ENTITYUNRELATED .
This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C00-1060.5'}, 'e2': {'word': 'method', 'word_index': [(6, 6)], 'id': 'C00-1060.7'}}	This ENTITY describes a hybrid ENTITYUNRELATED ENTITYOTHER for ENTITYUNRELATED which uses both a ENTITYUNRELATED grammar and a ENTITYUNRELATED ENTITYUNRELATED .
This paper describes an approach to using semantic representations information extraction (IE) inductive logic programming (ILP)	paper	approach	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C00-2101.6'}, 'e2': {'word': 'approach', 'word_index': [(4, 4)], 'id': 'C00-2101.7'}}	This ENTITY describes an ENTITYOTHER to using ENTITYUNRELATED ENTITYUNRELATED ( IE ) inductive ENTITYUNRELATED ( ILP )
A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text .	rules	parsing	usage	{'e1': {'word': 'rules', 'word_index': [(9, 9)], 'id': 'C00-2105.9'}, 'e2': {'word': 'parsing', 'word_index': [(14, 14)], 'id': 'C00-2105.11'}}	A manually ENTITYUNRELATED grammar was semi-automatically extended with ENTITYUNRELATED ENTITY in ENTITYUNRELATED to allow ENTITYOTHER of unrestricted ENTITYUNRELATED .
This paper proposes a new method for automatic acquisition of Chinese bracketing knowledge from English- Chinese sentence-aligned bilingual corpora .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C02-1003.6'}, 'e2': {'word': 'method', 'word_index': [(5, 5)], 'id': 'C02-1003.8'}}	This ENTITY ENTITYUNRELATED a new ENTITYOTHER for ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED from ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED bilingual ENTITYUNRELATED .
This paper proposes a new method for automatic acquisition of Chinese bracketing knowledge from English- Chinese sentence-aligned bilingual corpora .	knowledge	corpora	part_whole	{'e1': {'word': 'knowledge', 'word_index': [(12, 12)], 'id': 'C02-1003.13'}, 'e2': {'word': 'corpora', 'word_index': [(18, 18)], 'id': 'C02-1003.17'}}	This ENTITYUNRELATED ENTITYUNRELATED a new ENTITYUNRELATED for ENTITYUNRELATED ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED ENTITY from ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED bilingual ENTITYOTHER .
It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects .	performance	performance	compare	{'e1': {'word': 'performance', 'word_index': [(8, 8)], 'id': 'C02-1007.45'}, 'e2': {'word': 'performance', 'word_index': [(17, 17)], 'id': 'C02-1007.48'}}	It turns out that for both ENTITYUNRELATED the ENTITY of the ENTITYUNRELATED ENTITYUNRELATED is comparable to the ENTITYOTHER of human subjects .
Here we propose a method allowing a purely semantic-based analysis of sequences of semantic units .	analysis	sequences	topic	{'e1': {'word': 'analysis', 'word_index': [(9, 9)], 'id': 'C02-1023.27'}, 'e2': {'word': 'sequences', 'word_index': [(11, 11)], 'id': 'C02-1023.28'}}	Here we ENTITYUNRELATED a ENTITYUNRELATED allowing a purely ENTITYUNRELATED ENTITY of ENTITYOTHER of ENTITYUNRELATED ENTITYUNRELATED .
This paper presents a method for augmenting taxonomies with domain information using a simple combination of three existing lexical similarity metrics .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C02-1038.5'}, 'e2': {'word': 'method', 'word_index': [(4, 4)], 'id': 'C02-1038.6'}}	This ENTITY presents a ENTITYOTHER for augmenting ENTITYUNRELATED with ENTITYUNRELATED ENTITYUNRELATED using a ENTITYUNRELATED ENTITYUNRELATED of three existing ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
A Method Of Cluster- Based Indexing Of Textual Data</title>	Indexing	Data	usage	{'e1': {'word': 'Indexing', 'word_index': [(5, 5)], 'id': 'C02-1045.4'}, 'e2': {'word': 'Data', 'word_index': [(8, 8)], 'id': 'C02-1045.5'}}	A ENTITYUNRELATED Of ENTITYUNRELATED ENTITYUNRELATED ENTITY Of Textual ENTITYOTHER < / title >
This paper presents a framework for clustering in text-based information retrieval systems .	paper	framework	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C02-1045.6'}, 'e2': {'word': 'framework', 'word_index': [(4, 4)], 'id': 'C02-1045.7'}}	This ENTITY presents a ENTITYOTHER for ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED .
In this paper , we propose a method to overcome these problems in Japanese speech dialog .	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'C02-1059.23'}, 'e2': {'word': 'method', 'word_index': [(7, 7)], 'id': 'C02-1059.25'}}	In this ENTITY , we ENTITYUNRELATED a ENTITYOTHER to overcome these ENTITYUNRELATED in ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED .
This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions : double nominative sentences , non-gapping relation of relative clauses , and case change.	paper	method	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C02-1122.8'}, 'e2': {'word': 'method', 'word_index': [(4, 4)], 'id': 'C02-1122.10'}}	This ENTITY ENTITYUNRELATED a ENTITYOTHER of fertilizing a ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED to handle complicated ENTITYUNRELATED : double nominative ENTITYUNRELATED , non- gapping ENTITYUNRELATED of ENTITYUNRELATED ENTITYUNRELATED , and ENTITYUNRELATED change .
Voice-Activated Question Answering (VAQA) systems represent the next generation capability for universal access by integrating state-of-the-art in question answering Q&amp;A and automatic speech recognition (ASR) in such a way that the performance of the combined system is better than the individual components .	system	components	compare	{'e1': {'word': 'system', 'word_index': [(44, 44)], 'id': 'C02-1169.11'}, 'e2': {'word': 'components', 'word_index': [(50, 50)], 'id': 'C02-1169.13'}}	Voice-Activated ENTITYUNRELATED ( VAQA ) ENTITYUNRELATED represent the next ENTITYUNRELATED ENTITYUNRELATED for universal ENTITYUNRELATED by integrating state - of - the - art in ENTITYUNRELATED answering Q&amp ; A and ENTITYUNRELATED ( ASR ) in such a way that the ENTITYUNRELATED of the combined ENTITY is better than the ENTITYUNRELATED ENTITYOTHER .
This paper presents an implemented VAQA system and describes the techniques that enable the terative refinement of both Q&amp;A and ASR.	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C02-1169.14'}, 'e2': {'word': 'system', 'word_index': [(6, 6)], 'id': 'C02-1169.16'}}	This ENTITY presents an ENTITYUNRELATED VAQA ENTITYOTHER and describes the ENTITYUNRELATED that enable the terative ENTITYUNRELATED of both Q&amp ; A and ASR .
This paper presents a system for automatically generating discourse structures from written text .	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C04-1048.5'}, 'e2': {'word': 'system', 'word_index': [(4, 4)], 'id': 'C04-1048.6'}}	This ENTITY presents a ENTITYOTHER for automatically ENTITYUNRELATED ENTITYUNRELATED from written ENTITYUNRELATED .
This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system .	paper	system	topic	{'e1': {'word': 'paper', 'word_index': [(1, 1)], 'id': 'C04-1050.4'}, 'e2': {'word': 'system', 'word_index': [(19, 19)], 'id': 'C04-1050.12'}}	This ENTITY ENTITYUNRELATED unsupervised ENTITYUNRELATED ENTITYUNRELATED on automatically ENTITYUNRELATED ENTITYUNRELATED ENTITYUNRELATED and its incorporation into our zero pronoun ENTITYUNRELATED ENTITYOTHER .
Both of the experimental results indicated the effectiveness of our approach .	approach	results	result	{'e1': {'word': 'approach', 'word_index': [(10, 10)], 'id': 'C04-1050.46'}, 'e2': {'word': 'results', 'word_index': [(4, 4)], 'id': 'C04-1050.44'}}	Both of the ENTITYUNRELATED ENTITYOTHER indicated the ENTITYUNRELATED of our ENTITY .
In this paper we present a lexicalist prediction technique for CUG and show that this may lead to considerable gains in efficiency for both bottom-up and top-down parsing .	paper	technique	topic	{'e1': {'word': 'paper', 'word_index': [(2, 2)], 'id': 'E91-1031.22'}, 'e2': {'word': 'technique', 'word_index': [(8, 8)], 'id': 'E91-1031.24'}}	In this ENTITY we present a lexicalist ENTITYUNRELATED ENTITYOTHER for CUG and show that this may lead to considerable ENTITYUNRELATED in ENTITYUNRELATED for both bottom - up and top-down ENTITYUNRELATED .
